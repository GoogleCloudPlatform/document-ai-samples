{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "707aac26-5f83-4dfe-a9c3-73f9eb34dea4",
   "metadata": {},
   "source": [
    "# Document AI Parser Result Merger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff479c9-a573-48c7-bd26-d9c13cf5be7f",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5efc16-e389-48af-90d6-5d99019a1059",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the DocAI Incubator Team. No guarantees of performance are implied.\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47eed16a-e66d-4c76-860f-16f08ae45867",
   "metadata": {},
   "source": [
    "## Objective\n",
    "Document AI Parser Result Merger is a tool built using Python programming language. Its purpose is to address the issue of merging the two or more resultant json files of Document AI processors. This document highlights the working of the tool(script) and its requirements. The documents usually contain multiple pages. There are 2 use cases by which this solution can be operated. \n",
    "### Case 1: Different documents, parser results  json merger (Default).\n",
    " * Case 1 deals when we are using two or multiple parser output Jsons are from different documents\n",
    " * To Enable this case the flag should be ‘1’\n",
    "### Case 2: Same document, different parsers json merger(Added functionality).\n",
    " * Case 2 deals when we are using two or multiple parser outputs from the same document.\n",
    " * To Enable this case the flag should be ‘2’"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ef6ea3-79e0-42a0-a2bb-84ece51bff74",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64bc02d-ec9b-42b6-b3b0-ed70fa6c3808",
   "metadata": {},
   "source": [
    "This tool requires the following services:\n",
    "\n",
    " * Google Jupyter Notebook or Colab.\n",
    " * Google Cloud Storage \n",
    " * DocumentAI processor and JSON files\n",
    " \n",
    "Google Jupyter Notebook or Colab is used for running the python notebook file. Cloud Storage Buckets have the input files to this script. The multiple input files are the json files which are the result of a Document AI processor (for eg., Bank Statement Parser). These json files include multiple pages in its document. After the script executes, the output file is a single merged json file stored in the output bucket path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b4964c-d228-4aad-a6fb-346465791fe7",
   "metadata": {},
   "source": [
    "## Workflow overview\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a6f212-d41a-4ae7-b374-1c97ffb03931",
   "metadata": {},
   "source": [
    "![](https://screenshot.googleplex.com/9F5qLEtZJ4Kdj8m.png)\n",
    "\n",
    "The above diagram shows the flow diagram of the tool. As highlighted there are input and output GCP buckets and there is a python script which processes the request. The input bucket holds the multiple json files which need to be merged into a single file and this is achieved by the python script. This script accepts the input json files and prompts users to switch between the default case-1 or the case-2 mode as highlighted in the previous sections.  Finally there is an output GCP bucket to store the single merged file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3155aca4-1aeb-4a22-a0b7-a3e9b43e69c0",
   "metadata": {},
   "source": [
    "## Script walkthrough\n",
    "Insights and details about the script are explained in detail as follows.\n",
    "1. Config file Creation\n",
    "    Run the below code and create a config.ini file for providing input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bdf742-c286-4735-bae5-91eb5c1a1ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "# Add the structure to the file we will create\n",
    "config.add_section(\"Parameters\")\n",
    "config.set(\"Parameters\", \"project_id\", \"xxxx-xxxx-xxxx\")\n",
    "config.set(\"Parameters\", \"Input_Multiple_jsons_URI\", \"gs://\")\n",
    "config.set(\"Parameters\", \"Output_Multiple_jsons_URI\", \"gs://\")\n",
    "config.set(\"Parameters\", \"Name_output_Json\", \"merged_json\")\n",
    "config.set(\"Parameters\", \"merger_type_flag(1-for different docs,2-same doc)\",\n",
    "           \"1\")\n",
    "\n",
    "# Write the new structure to the new file\n",
    "with open(r\"configfile.ini\", \"w\") as configfile:\n",
    "    config.write(configfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758afb38-cede-4042-b9e1-9c847eef818f",
   "metadata": {},
   "source": [
    "2. Input Details : Entering Project details in Config files:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444df062-df9e-48c0-9f96-fe2789f3e1f0",
   "metadata": {},
   "source": [
    "![](https://screenshot.googleplex.com/7DTVnTRHPQUgLBG.png)\n",
    "\n",
    "\n",
    "Once the config.ini file is created with the above step 1 , open the config.ini file and enter the input details specific to your project and GCP bucket paths. As shown in the diagram above, the following parameters are to be entered.\n",
    " * **project_id:** provide your GCP project ID\n",
    " * **input_multiple_jsons_uri:** provide the uri link of folder containing the input files\n",
    " * **output_multiple_jsons_uri:** provide the folder name of the output file which gets generated post execution of the script.\n",
    " * **Name_output_json:** enter a name for the generated file which is saved in the output bucket.\n",
    " * **merger_type_flag(1-for different docs,2-same doc) :** based on user need, values 1 or 2 can be provided as mentioned in the earlier part of this document.\n",
    "\n",
    " - Case 1 deals when we are using two or multiple parser output Jsons are from different documents\n",
    "\n",
    " - Case 2 deals when we are using two or multiple parser outputs from the same document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dba5180-f972-4d5e-9802-974885efe2d4",
   "metadata": {},
   "source": [
    "## 3. Run the below code.\n",
    "\n",
    "Use the below code and Run all the cells (Update the Path parameter if it is not available in the current working directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffdf37c-332f-4ba5-a55f-349d2cccb432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import gc\n",
    "import json\n",
    "# importing Libraries\n",
    "import os\n",
    "import re\n",
    "import urllib.request\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "import gcsfs\n",
    "import pandas as pd\n",
    "from google.api_core.client_options import ClientOptions\n",
    "from google.api_core.exceptions import InternalServerError, RetryError\n",
    "from google.cloud import storage\n",
    "\n",
    "# Getting Input from config file\n",
    "Path = \"configfile.ini\"  # Enter the path of config file\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(Path)\n",
    "project_id = config.get(\"Parameters\", \"project_id\")\n",
    "input_multiple_jsons_uri = config.get(\"Parameters\", \"input_multiple_jsons_uri\")\n",
    "JSON_DIRECTORY_PATH_OUTPUT = config.get(\"Parameters\",\n",
    "                                        \"Output_Multiple_jsons_URI\")\n",
    "output_file_name = config.get(\"Parameters\", \"Name_output_Json\")\n",
    "merger_type_flag = config.get(\n",
    "    \"Parameters\", \"merger_type_flag(1-for different docs,2-same doc)\")\n",
    "\n",
    "\n",
    "# Functions\n",
    "### CASE - 1\n",
    "def merger(doc_first, doc_second):\n",
    "    doc_merged = {}\n",
    "\n",
    "    ### Entities ###\n",
    "    for entity in doc_second[\"entities\"]:\n",
    "        try:\n",
    "            # print(\"\\n\\n+++++++++ PAGE ANCHOR +++++++++\")\n",
    "            for x in range(0, len(entity[\"pageAnchor\"][\"pageRefs\"])):\n",
    "                try:\n",
    "                    entity[\"pageAnchor\"][\"pageRefs\"][x][\"page\"] = str(\n",
    "                        int(entity[\"pageAnchor\"][\"pageRefs\"][x][\"page\"]) +\n",
    "                        len(doc_first[\"pages\"]))\n",
    "                except:\n",
    "                    entity[\"pageAnchor\"][\"pageRefs\"][x][\"page\"] = str(\n",
    "                        len(doc_first[\"pages\"]))\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # print(\"--- Properties ---\")\n",
    "\n",
    "        try:\n",
    "            for x in range(0, len(entity[\"properties\"])):\n",
    "                for xx in range(\n",
    "                        0,\n",
    "                        len(entity[\"properties\"][x][\"pageAnchor\"]\n",
    "                            [\"pageRefs\"])):\n",
    "                    try:\n",
    "                        entity[\"properties\"][x][\"pageAnchor\"][\"pageRefs\"][xx][\n",
    "                            \"page\"] = int(entity[\"properties\"][x][\"pageAnchor\"]\n",
    "                                          [\"pageRefs\"][xx][\"page\"]) + len(\n",
    "                                              doc_first[\"pages\"])\n",
    "                    except:\n",
    "                        entity[\"properties\"][x][\"pageAnchor\"][\"pageRefs\"][xx][\n",
    "                            \"page\"] = len(doc_first[\"pages\"])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # print(\"+++++++++ TEXT ANCHOR +++++++++\")\n",
    "\n",
    "        try:\n",
    "            textAnchor = entity[\"textAnchor\"]\n",
    "            for y in range(0, len(entity[\"textAnchor\"][\"textSegments\"])):\n",
    "                entity[\"textAnchor\"][\"textSegments\"][y][\"endIndex\"] = int(\n",
    "                    entity[\"textAnchor\"][\"textSegments\"][y][\"endIndex\"]) + len(\n",
    "                        doc_first[\"text\"])\n",
    "                try:\n",
    "                    entity[\"textAnchor\"][\"textSegments\"][y][\n",
    "                        \"startIndex\"] = int(\n",
    "                            entity[\"textAnchor\"][\"textSegments\"][y]\n",
    "                            [\"startIndex\"]) + len(doc_first[\"text\"])\n",
    "                except:  # if startIndex is absent\n",
    "                    entity[\"textAnchor\"][\"textSegments\"][y][\n",
    "                        \"startIndex\"] = len(doc_first[\"text\"])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # print(\"--- Properties ---\")\n",
    "\n",
    "        try:\n",
    "            for y in range(0, len(entity[\"properties\"])):\n",
    "                for yy in range(\n",
    "                        0,\n",
    "                        len(entity[\"properties\"][y][\"textAnchor\"]\n",
    "                            [\"textSegments\"])):\n",
    "                    entity[\"properties\"][y][\"textAnchor\"][\"textSegments\"][yy][\n",
    "                        \"endIndex\"] = int(\n",
    "                            entity[\"properties\"][y][\"textAnchor\"]\n",
    "                            [\"textSegments\"][yy][\"endIndex\"]) + len(\n",
    "                                doc_first[\"text\"])\n",
    "                    try:\n",
    "                        entity[\"properties\"][y][\"textAnchor\"][\"textSegments\"][\n",
    "                            yy][\"startIndex\"] = int(\n",
    "                                entity[\"properties\"][y][\"textAnchor\"]\n",
    "                                [\"textSegments\"][yy][\"startIndex\"]) + len(\n",
    "                                    doc_first[\"text\"])\n",
    "                    except:  # if startIndex is absent\n",
    "                        entity[\"properties\"][y][\"textAnchor\"][\"textSegments\"][\n",
    "                            yy][\"startIndex\"] = len(doc_first[\"text\"])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    doc_merged[\"entities\"] = doc_first[\"entities\"] + doc_second[\"entities\"]\n",
    "\n",
    "    ### Page\n",
    "    ### Page No increment in second doc\n",
    "    for pg in doc_second[\"pages\"]:\n",
    "        print(pg[\"pageNumber\"])\n",
    "        pg[\"pageNumber\"] = int(pg[\"pageNumber\"]) + len(doc_first[\"pages\"])\n",
    "        print(\"\\t\", pg[\"pageNumber\"])\n",
    "\n",
    "    ### Page\n",
    "    ### page . blocks . layout . textanchor . textsegment\n",
    "    for pg in range(0, len(doc_second[\"pages\"])):\n",
    "        for pg_ in range(0, len(doc_second[\"pages\"][pg][\"blocks\"])):\n",
    "            for pg_textSegment in range(\n",
    "                    0,\n",
    "                    len(doc_second[\"pages\"][pg][\"blocks\"][pg_][\"layout\"]\n",
    "                        [\"textAnchor\"][\"textSegments\"]),\n",
    "            ):\n",
    "                doc_second[\"pages\"][pg][\"blocks\"][pg_][\"layout\"][\"textAnchor\"][\n",
    "                    \"textSegments\"][pg_textSegment][\"endIndex\"] = int(\n",
    "                        doc_second[\"pages\"][pg][\"blocks\"][pg_][\"layout\"]\n",
    "                        [\"textAnchor\"][\"textSegments\"][pg_textSegment]\n",
    "                        [\"endIndex\"]) + len(doc_first[\"text\"])\n",
    "                try:\n",
    "                    doc_second[\"pages\"][pg][\"blocks\"][pg_][\"layout\"][\n",
    "                        \"textAnchor\"][\"textSegments\"][pg_textSegment][\n",
    "                            \"startIndex\"] = int(\n",
    "                                doc_second[\"pages\"][pg][\"blocks\"][pg_]\n",
    "                                [\"layout\"][\"textAnchor\"][\"textSegments\"]\n",
    "                                [pg_textSegment][\"startIndex\"]) + len(\n",
    "                                    doc_first[\"text\"])\n",
    "                except:\n",
    "                    doc_second[\"pages\"][pg][\"blocks\"][pg_][\"layout\"][\n",
    "                        \"textAnchor\"][\"textSegments\"][pg_textSegment][\n",
    "                            \"startIndex\"] = len(doc_first[\"text\"])\n",
    "\n",
    "    ### page . layout . textanchor . textsegment\n",
    "    for pg in range(0, len(doc_second[\"pages\"])):\n",
    "        # print(\"----\")\n",
    "        # print(doc_second['pages'][pg]['layout']['textAnchor']['textSegments'])\n",
    "        for pg_textSegment in range(\n",
    "                0,\n",
    "                len(doc_second[\"pages\"][pg][\"layout\"][\"textAnchor\"]\n",
    "                    [\"textSegments\"])):\n",
    "            doc_second[\"pages\"][pg][\"layout\"][\"textAnchor\"][\"textSegments\"][\n",
    "                pg_textSegment][\"endIndex\"] = int(\n",
    "                    doc_second[\"pages\"][pg][\"layout\"][\"textAnchor\"]\n",
    "                    [\"textSegments\"][pg_textSegment][\"endIndex\"]) + len(\n",
    "                        doc_first[\"text\"])\n",
    "            try:\n",
    "                doc_second[\"pages\"][pg][\"layout\"][\"textAnchor\"][\n",
    "                    \"textSegments\"][pg_textSegment][\"startIndex\"] = int(\n",
    "                        doc_second[\"pages\"][pg][\"layout\"][\"textAnchor\"]\n",
    "                        [\"textSegments\"][pg_textSegment][\"startIndex\"]) + len(\n",
    "                            doc_first[\"text\"])\n",
    "            except:\n",
    "                doc_second[\"pages\"][pg][\"layout\"][\"textAnchor\"][\n",
    "                    \"textSegments\"][pg_textSegment][\"startIndex\"] = len(\n",
    "                        doc_first[\"text\"])\n",
    "\n",
    "    ### page . lines . layout . textanchor . textsegment\n",
    "    for pg in range(0, len(doc_second[\"pages\"])):\n",
    "        for pg_line in range(0, len(doc_second[\"pages\"][pg][\"lines\"])):\n",
    "            for pg_textSegment in range(\n",
    "                    0,\n",
    "                    len(doc_second[\"pages\"][pg][\"lines\"][pg_line][\"layout\"]\n",
    "                        [\"textAnchor\"][\"textSegments\"]),\n",
    "            ):\n",
    "                doc_second[\"pages\"][pg][\"lines\"][pg_line][\"layout\"][\n",
    "                    \"textAnchor\"][\"textSegments\"][pg_textSegment][\n",
    "                        \"endIndex\"] = int(\n",
    "                            doc_second[\"pages\"][pg][\"lines\"][pg_line][\"layout\"]\n",
    "                            [\"textAnchor\"][\"textSegments\"][pg_textSegment]\n",
    "                            [\"endIndex\"]) + len(doc_first[\"text\"])\n",
    "                try:\n",
    "                    doc_second[\"pages\"][pg][\"lines\"][pg_line][\"layout\"][\n",
    "                        \"textAnchor\"][\"textSegments\"][pg_textSegment][\n",
    "                            \"startIndex\"] = int(\n",
    "                                doc_second[\"pages\"][pg][\"lines\"][pg_line]\n",
    "                                [\"layout\"][\"textAnchor\"][\"textSegments\"]\n",
    "                                [pg_textSegment][\"startIndex\"]) + len(\n",
    "                                    doc_first[\"text\"])\n",
    "                except:\n",
    "                    doc_second[\"pages\"][pg][\"lines\"][pg_line][\"layout\"][\n",
    "                        \"textAnchor\"][\"textSegments\"][pg_textSegment][\n",
    "                            \"startIndex\"] = len(doc_first[\"text\"])\n",
    "\n",
    "    ### page . paragraph . layout . textanchor . textsegment\n",
    "    for pg in range(0, len(doc_second[\"pages\"])):\n",
    "        for pg_paragraph in range(0,\n",
    "                                  len(doc_second[\"pages\"][pg][\"paragraphs\"])):\n",
    "            for pg_textSegment in range(\n",
    "                    0,\n",
    "                    len(doc_second[\"pages\"][pg][\"paragraphs\"][pg_paragraph]\n",
    "                        [\"layout\"][\"textAnchor\"][\"textSegments\"]),\n",
    "            ):\n",
    "                doc_second[\"pages\"][pg][\"paragraphs\"][pg_paragraph][\"layout\"][\n",
    "                    \"textAnchor\"][\"textSegments\"][pg_textSegment][\n",
    "                        \"endIndex\"] = int(\n",
    "                            doc_second[\"pages\"][pg][\"paragraphs\"][pg_paragraph]\n",
    "                            [\"layout\"][\"textAnchor\"][\"textSegments\"]\n",
    "                            [pg_textSegment][\"endIndex\"]) + len(\n",
    "                                doc_first[\"text\"])\n",
    "                try:\n",
    "                    doc_second[\"pages\"][pg][\"paragraphs\"][pg_paragraph][\n",
    "                        \"layout\"][\"textAnchor\"][\"textSegments\"][\n",
    "                            pg_textSegment][\"startIndex\"] = int(\n",
    "                                doc_second[\"pages\"][pg][\"paragraphs\"]\n",
    "                                [pg_paragraph][\"layout\"][\"textAnchor\"]\n",
    "                                [\"textSegments\"][pg_textSegment]\n",
    "                                [\"startIndex\"]) + len(doc_first[\"text\"])\n",
    "                except:\n",
    "                    doc_second[\"pages\"][pg][\"paragraphs\"][pg_paragraph][\n",
    "                        \"layout\"][\"textAnchor\"][\"textSegments\"][\n",
    "                            pg_textSegment][\"startIndex\"] = len(\n",
    "                                doc_first[\"text\"])\n",
    "\n",
    "    ### page . tokens . layout . textanchor . textsegment\n",
    "    for pg in range(0, len(doc_second[\"pages\"])):\n",
    "        for pg_token in range(0, len(doc_second[\"pages\"][pg][\"tokens\"])):\n",
    "            for pg_textSegment in range(\n",
    "                    0,\n",
    "                    len(doc_second[\"pages\"][pg][\"tokens\"][pg_token][\"layout\"]\n",
    "                        [\"textAnchor\"][\"textSegments\"]),\n",
    "            ):\n",
    "                doc_second[\"pages\"][pg][\"tokens\"][pg_token][\"layout\"][\n",
    "                    \"textAnchor\"][\"textSegments\"][pg_textSegment][\n",
    "                        \"endIndex\"] = int(\n",
    "                            doc_second[\"pages\"][pg][\"tokens\"][pg_token]\n",
    "                            [\"layout\"][\"textAnchor\"][\"textSegments\"]\n",
    "                            [pg_textSegment][\"endIndex\"]) + len(\n",
    "                                doc_first[\"text\"])\n",
    "                try:\n",
    "                    doc_second[\"pages\"][pg][\"tokens\"][pg_token][\"layout\"][\n",
    "                        \"textAnchor\"][\"textSegments\"][pg_textSegment][\n",
    "                            \"startIndex\"] = int(\n",
    "                                doc_second[\"pages\"][pg][\"tokens\"][pg_token]\n",
    "                                [\"layout\"][\"textAnchor\"][\"textSegments\"]\n",
    "                                [pg_textSegment][\"startIndex\"]) + len(\n",
    "                                    doc_first[\"text\"])\n",
    "                except:\n",
    "                    doc_second[\"pages\"][pg][\"tokens\"][pg_token][\"layout\"][\n",
    "                        \"textAnchor\"][\"textSegments\"][pg_textSegment][\n",
    "                            \"startIndex\"] = len(doc_first[\"text\"])\n",
    "\n",
    "    doc_merged[\"pages\"] = doc_first[\"pages\"] + doc_second[\"pages\"]\n",
    "\n",
    "    ### Text\n",
    "    doc_merged[\"text\"] = doc_first[\"text\"] + doc_second[\"text\"]\n",
    "\n",
    "    ### shardInfo & uri\n",
    "    if \"shardInfo\" in doc_first:\n",
    "        doc_merged[\"shardInfo\"] = doc_first[\"shardInfo\"]\n",
    "    if \"uri\" in doc_first:\n",
    "        doc_merged[\"uri\"] = doc_first[\"uri\"]\n",
    "\n",
    "    return doc_merged\n",
    "\n",
    "\n",
    "### CASE -2\n",
    "def SameDocDiffParser_merger(doc_first, doc_second):\n",
    "    doc_first[\"entities\"] = doc_first[\"entities\"] + doc_second[\"entities\"]\n",
    "    doc_merged = doc_first\n",
    "    return doc_merged\n",
    "\n",
    "\n",
    "def file_names(file_path):\n",
    "    \"\"\"This Function will load the bucket and get the list of files\n",
    "    in the gs path given\n",
    "    args: gs path\n",
    "    output: file names as list and dictionary with file names as keys and file path as values\n",
    "    \"\"\"\n",
    "    bucket = file_path.split(\"/\")[2]\n",
    "    file_names_list = []\n",
    "    file_dict = {}\n",
    "    storage_client = storage.Client()\n",
    "    source_bucket = storage_client.get_bucket(bucket)\n",
    "    filenames = [\n",
    "        filename.name for filename in list(\n",
    "            source_bucket.list_blobs(\n",
    "                prefix=((\"/\").join(file_path.split(\"/\")[3:]))))\n",
    "    ]\n",
    "    for i in range(len(filenames)):\n",
    "        x = filenames[i].split(\"/\")[-1]\n",
    "        if x != \"\":\n",
    "            file_names_list.append(x)\n",
    "            file_dict[x] = filenames[i]\n",
    "    return file_names_list, file_dict\n",
    "\n",
    "\n",
    "file_names_list, file_dict = file_names(input_multiple_jsons_uri)\n",
    "\n",
    "input_bucket_files = []\n",
    "for fldrFile in file_names_list:\n",
    "    if fldrFile.endswith(\".json\"):\n",
    "        print(fldrFile)\n",
    "        input_bucket_files.append(fldrFile)\n",
    "print(input_bucket_files)\n",
    "\n",
    "storage_client = storage.Client()\n",
    "source_bucket = storage_client.get_bucket(\n",
    "    input_multiple_jsons_uri.split(\"/\")[2])\n",
    "\n",
    "fs = gcsfs.GCSFileSystem(project=project_id)\n",
    "\n",
    "### CASE - 1\n",
    "if merger_type_flag == \"1\":\n",
    "    print(\">>> \\t Using Default Merger... \")\n",
    "\n",
    "    if len(input_bucket_files) == 2:  # For 2 docs\n",
    "        print(\"2 files...\")\n",
    "        print(input_bucket_files[0])\n",
    "        print(input_bucket_files[1])\n",
    "        doc_first = json.loads(\n",
    "            source_bucket.blob(file_dict[\n",
    "                input_bucket_files[0]]).download_as_string().decode(\"utf-8\"))\n",
    "        doc_second = json.loads(\n",
    "            source_bucket.blob(file_dict[\n",
    "                input_bucket_files[1]]).download_as_string().decode(\"utf-8\"))\n",
    "        x = merger(doc_first, doc_second)\n",
    "\n",
    "    else:  # For 2+ docs\n",
    "        print(\"more than 2 files....\")\n",
    "        print(input_bucket_files[0])\n",
    "        print(input_bucket_files[1])\n",
    "        doc_first = json.loads(\n",
    "            source_bucket.blob(file_dict[\n",
    "                input_bucket_files[0]]).download_as_string().decode(\"utf-8\"))\n",
    "        doc_second = json.loads(\n",
    "            source_bucket.blob(file_dict[\n",
    "                input_bucket_files[1]]).download_as_string().decode(\"utf-8\"))\n",
    "        x = merger(doc_first, doc_second)\n",
    "\n",
    "        print(\"---------- 2+ Files ... -----------\")\n",
    "        for file in input_bucket_files[2:]:  # skip first 2 files\n",
    "            print(file)\n",
    "            # doc_first = json.loads(bucket.blob(doc_merged).download_as_string().decode('utf-8'))\n",
    "            doc_second = json.loads(\n",
    "                source_bucket.blob(\n",
    "                    file_dict[file]).download_as_string().decode(\"utf-8\"))\n",
    "            x = merger(x, doc_second)\n",
    "\n",
    "### CASE - 2\n",
    "elif merger_type_flag == \"2\":\n",
    "    print(\">>> \\t Using Different Processor Result jsons merger... \")\n",
    "\n",
    "    if len(input_bucket_files) == 2:  # For 2 docs\n",
    "        print(\"2 files...\")\n",
    "        print(input_bucket_files[0])\n",
    "        print(input_bucket_files[1])\n",
    "        doc_first = json.loads(\n",
    "            source_bucket.blob(file_dict[\n",
    "                input_bucket_files[0]]).download_as_string().decode(\"utf-8\"))\n",
    "        doc_second = json.loads(\n",
    "            source_bucket.blob(file_dict[\n",
    "                input_bucket_files[1]]).download_as_string().decode(\"utf-8\"))\n",
    "        x = SameDocDiffParser_merger(doc_first, doc_second)\n",
    "\n",
    "    else:  # For 2+ docs\n",
    "        print(\"more than 2 files....\")\n",
    "        print(input_bucket_files[0])\n",
    "        print(input_bucket_files[1])\n",
    "        doc_first = json.loads(\n",
    "            source_bucket.blob(file_dict[\n",
    "                input_bucket_files[0]]).download_as_string().decode(\"utf-8\"))\n",
    "        doc_second = json.loads(\n",
    "            source_bucket.blob(file_dict[\n",
    "                input_bucket_files[1]]).download_as_string().decode(\"utf-8\"))\n",
    "        x = SameDocDiffParser_merger(doc_first, doc_second)\n",
    "\n",
    "        print(\"---------- 2+ Files ... -----------\")\n",
    "        for file in input_bucket_files[2:]:  # skip first 2 files\n",
    "            print(file)\n",
    "            # doc_first = json.loads(bucket.blob(doc_merged).download_as_string().decode('utf-8'))\n",
    "            doc_second = json.loads(\n",
    "                source_bucket.blob(\n",
    "                    file_dict[file]).download_as_string().decode(\"utf-8\"))\n",
    "            x = SameDocDiffParser_merger(x, doc_second)\n",
    "\n",
    "else:\n",
    "    print(\"invalid input\")\n",
    "\n",
    "print(\"deleting ID under Entities\")\n",
    "for z in x[\"entities\"]:\n",
    "    try:\n",
    "        print(z[\"id\"])\n",
    "        del z[\"id\"]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"deleting ID under Entities - properties\")\n",
    "\n",
    "for z in x[\"entities\"]:\n",
    "    # print(z)\n",
    "    try:\n",
    "        for a in z[\"properties\"]:\n",
    "            print(a[\"id\"])\n",
    "            del a[\"id\"]\n",
    "    except:\n",
    "        pass\n",
    "merged_json_path = JSON_DIRECTORY_PATH_OUTPUT + \"/\" + output_file_name\n",
    "fs.pipe(merged_json_path,\n",
    "        bytes(json.dumps(x), \"utf-8\"),\n",
    "        content_type=\"application/json\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051b1c28-c998-4788-92e6-e9b50c671043",
   "metadata": {},
   "source": [
    "## 4. Output \n",
    "\n",
    "The output of the tool is a **single json file**. Let's examine the outputs for each of the case types. We’ll consider 3 json docs for our experiment and examine the output formats.\n",
    "\n",
    "Consider following 3 input json files residing the input GCS Bucket: \n",
    "\n",
    "json_doc_merge / 0 / doc-0.json\n",
    "json_doc_merge / 1 / doc-1.json\n",
    "json_doc_merge / 2 / doc-2.json\n",
    "\n",
    "Upon running the script for both the cases, the below output details are observed as follows.\n",
    "\n",
    "### CASE - 1 Output \n",
    "Let's suppose the three json files are from different documents (The parser used may be same or different )\n",
    "In Case - 1, we observe in the output that the Pages and Entities count increases with the number of pages and entities present in the input files upon merging. The same applies for the and Text, the value is changed and texts are concatenated and stored as a single value for the Text key of the output file.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4288801-7881-4c5f-9ead-c7a55f298120",
   "metadata": {},
   "source": [
    "| Input json files | Screenshot highlighting the number of entities and number of pages in each of the input json files | The output single merged json file                         |\n",
    "|:----------------:|----------------------------------------------------------------------------------------------------|------------------------------------------------------------|\n",
    "|    **doc-0.json**    | ![](https://screenshot.googleplex.com/7Cn7bf5HKA62omx.png)                                         | ![](https://screenshot.googleplex.com/7zWP7zPZkLeZSra.png) |\n",
    "|    **doc-1.json**    | ![](https://screenshot.googleplex.com/BMGMEcW3EFxWrRc.png)                                         |                                                            |\n",
    "|    **doc-2.json**    | ![](https://screenshot.googleplex.com/3wCEqP9i3Bm9dqB.png)                                         |                                                            |\n",
    "\n",
    "**For example :** each json has  2 pages and 21 entities , the final output merged json has 6 pages and 63 entities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80cd3e7-ee35-4003-ab4a-094a7a935f16",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CASE - 2 Output \n",
    "\n",
    "Let's suppose the three json files are from the single document and from different parser results.\n",
    "\n",
    "In Case - 2, we observe the pages count remains the same and there is only an increase in the count of Entities upon merging the multiple input json files. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b055a42-20dd-4b34-b624-89218224e7ea",
   "metadata": {},
   "source": [
    "| Input json files | Screenshot highlighting the number of entities and number of pages in each of the input json files | The output single merged json file                         |\n",
    "|:----------------:|----------------------------------------------------------------------------------------------------|------------------------------------------------------------|\n",
    "|    **doc-0.json**    | ![](https://screenshot.googleplex.com/ZofmvdULKVFvZ9w.png)                                         | ![](https://screenshot.googleplex.com/Bx2WNCxdcv3pN8p.png) |\n",
    "|    **doc-1.json**    | ![](https://screenshot.googleplex.com/6fgDDEEtRaxNJ2N.png)                                         |                                                            |\n",
    "|    **doc-2.json**    | ![](https://screenshot.googleplex.com/BwYcWwMuT6byLTm.png)                                         |                                                            |\n",
    "\n",
    "**For example :** each json has  2 pages and 21 entities , the final output merged json has 2 pages and 63 entities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c85adc-ff16-494d-8e42-9b8da336778b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m104",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m104"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
