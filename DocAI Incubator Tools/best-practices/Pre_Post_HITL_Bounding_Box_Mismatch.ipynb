{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea6835d4-cda8-44c5-873f-5687dc564ff9",
   "metadata": {},
   "source": [
    "# PRE - POST HITL Bounding Box Mismatch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcf8e7b-1374-4a4a-9b63-2f04e76d63cd",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b31e10-66da-4e9c-bea3-22be7c232ad6",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the DocAI Incubator Team. No guarantees of performance are implied.\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790ad613-2fe3-4e0f-a5f5-2217497f5329",
   "metadata": {},
   "source": [
    "## Purpose of the script\n",
    "\n",
    "Pre and POST HITL comparison tool which detect two issues - Parser issue and OCR issue.\n",
    "And the result output contains a summary json file which shows basic stats, count of the OCR and Parser issues for entities present in each document and corresponding analysis csv files.\n",
    "\n",
    " * **Parser issue :** This issue is identified with the parser when the bounding box is not covering the text region completely and hence the required text was not captured completely. The user accesses HITL worker UI and adjusts the bounding box to include the text region and save. The script highlight such cases\n",
    "\n",
    " * **OCR issue :** This issue is identified with the parser when the bounding box covers the whole text region and as result the expected text was not captured completely. The script highlight such cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1352d4-1a7a-4232-9f1c-c3e4fa47ab7a",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    " * Vertex AI Notebook\n",
    " * Google Cloud Storage bucket\n",
    " * Pre HITL and Post HITL Json files (filename should be same) in GCS Folders\n",
    " * DocumentAI and HITL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f1381d-f69a-4dca-a65b-eae04761c40d",
   "metadata": {},
   "source": [
    "## Step by Step procedure \n",
    "### 1. Config file Creation\n",
    "   **Config file Creation** \\\n",
    "    Run the below code and create a config.ini file for providing input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f484e8-82df-478c-a06c-5e087b24c5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "# Add the structure to the file we will create\n",
    "config.add_section(\"Parameters\")\n",
    "config.set(\"Parameters\", \"project_id\", \"xxxx-xxxxxx-xxxxxx\")\n",
    "config.set(\"Parameters\", \"Pre_HITL_Output_URI\", \"gs://\")\n",
    "config.set(\"Parameters\", \"Post_HITL_Output_URI\", \"gs://\")\n",
    "# Write the new structure to the new file\n",
    "with open(r\"configfile.ini\", \"w\") as configfile:\n",
    "    config.write(configfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf9b882-aafc-494b-8772-322fe90c59e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Input Details\n",
    "\n",
    "Once config.ini file is created with the above step , enter the input in the config file with necessary details as below \n",
    " * **project_id**: provide the project id \n",
    " * **Pre_HITL_Output_URI:** provide the gcs path of pre HITL jsons (processed jsons) \n",
    " * **Post_HITL_Output_URI:** provide the gcs path of post HITL jsons (Jsons processed through HITL) \n",
    " \n",
    "![](https://screenshot.googleplex.com/4hLVtBgjU4vJeCo.png)\n",
    "\n",
    "**NOTE:** The Name of Post-HITL Json will not be the same as the original file name by default. This has to be updated manually before using this tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb704ae-f7dc-4bce-b8b4-8d56cf401f7e",
   "metadata": {},
   "source": [
    "## 3. Run the Code\n",
    "Copy the code provided in this document, Enter the path of the Config file and Run without any edits. The complete notebook script is found in the last section of this document. The output is the summary of entities updated through HITL which has the comparison of pre and post HITL jsons and count of Parser or OCR issue per document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc3910d-5802-4bfa-b4f9-5300bbe5b9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "# input\n",
    "Path = \"configfile.ini\"  # Enter the path of config file\n",
    "config = configparser.ConfigParser()\n",
    "config.read(Path)\n",
    "\n",
    "project_id = config.get(\"Parameters\", \"project_id\")\n",
    "pre_HITL_output_URI = config.get(\"Parameters\", \"pre_hitl_output_uri\")\n",
    "post_HITL_output_URI = config.get(\"Parameters\", \"post_hitl_output_uri\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e98fc7d-4e80-4e49-a2ba-60851864ff0b",
   "metadata": {},
   "source": [
    "##  4. Output\n",
    "Result summary table is obtained which highlight the count of parser and ocr issues for each file. The result table contain details related to pre and post HITL entity changes, whether there were bounding box coordinates mismatched upon post HITL processing. The below screenshots showcases the parser or ocr issue.\n",
    "\n",
    "![](https://screenshot.googleplex.com/6S47qFm5SjP8eMC.png)\n",
    "![](https://screenshot.googleplex.com/6HyQwucSQPZR4ii.png)\n",
    "\n",
    "Summary json file is generated which highlight count of bounding box mismatches, OCR and Parser errors and analysis path to result table for each of the processed files.\n",
    "\n",
    "![](https://screenshot.googleplex.com/55R5NKSuVYmyP9H.png)\n",
    "\n",
    "Entity wise analysis for each file can be observed in the following csv files under analysis/ folder.\n",
    "\n",
    "![](https://screenshot.googleplex.com/BKd5QCidEJac9Jy.png)\n",
    "\n",
    "**Table columns:**\n",
    "\n",
    "The result output table has following columns and its details are as follows:\n",
    " * File Name : name of the file\n",
    " * Entity Type : type of the entity \n",
    " * Pre_HITL_Output : entity text before HITL \n",
    " * Pre_HITL_bbox : entity bounding box coordinates before HITL\n",
    " * Post_HITL_Output : entity text before HITL \n",
    " * Hitl_update : if there was HITL update for that particular entity\n",
    " * Post_HITL_bbox : entity bounding box coordinates after HITL\n",
    " * Fuzzy Ratio : text match %\n",
    " * Bbox_mismatch : if the bounding box coordinates are mismatched\n",
    " * OCR issue : represents if its classified as OCR Issue\n",
    " * Parser issue : represents if its classified as Parser Issue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe86372-45ba-443b-818c-0b4a75fee96c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Notebook Script\n",
    "\n",
    "**Install the below libraries before executing the script** \\\n",
    "If you encounter an error while importing libraries, please verify that you have installed them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1daa5fc-1259-4c5c-a0d3-e81d81b78637",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-cloud-documentai\n",
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a18e66-dd30-4f7e-9236-cc04a371fd6b",
   "metadata": {},
   "source": [
    "**Script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8751be86-e5d3-49e2-a38c-bf362cb5dab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import configparser\n",
    "import difflib\n",
    "import io\n",
    "import json\n",
    "import operator\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from collections.abc import Container, Iterable, Iterator, Mapping, Sequence\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import gcsfs\n",
    "import numpy as np\n",
    "# Import the libraries\n",
    "import pandas as pd\n",
    "from google.cloud import documentai_v1beta3, storage\n",
    "from PIL import Image\n",
    "from PyPDF2 import PdfFileReader\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "# input\n",
    "Path = \"configfile.ini\"  # Enter the path of config file\n",
    "config = configparser.ConfigParser()\n",
    "config.read(Path)\n",
    "\n",
    "project_id = config.get(\"Parameters\", \"project_id\")\n",
    "pre_HITL_output_URI = config.get(\"Parameters\", \"pre_hitl_output_uri\")\n",
    "post_HITL_output_URI = config.get(\"Parameters\", \"post_hitl_output_uri\")\n",
    "\n",
    "\n",
    "# checking whether bucket exists else create temperary bucket\n",
    "def check_create_bucket(bucket_name):\n",
    "    \"\"\"This Function is to create a temperary bucket\n",
    "    for storing the processed files\n",
    "    args: name of bucket\"\"\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    try:\n",
    "        bucket = storage_client.get_bucket(bucket_name)\n",
    "        print(f\"Bucket {bucket_name} already exists.\")\n",
    "    except:\n",
    "        bucket = storage_client.create_bucket(bucket_name)\n",
    "        print(f\"Bucket {bucket_name} created.\")\n",
    "    return bucket\n",
    "\n",
    "\n",
    "def bucket_delete(bucket_name):\n",
    "    print(\"Deleting bucket : \", bucket_name)\n",
    "    \"\"\"This function deltes the bucket and used for deleting the temporary\n",
    "    bucket\n",
    "    args: bucket name\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    try:\n",
    "        bucket = storage_client.get_bucket(bucket_name)\n",
    "        bucket.delete(force=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def file_names(file_path):\n",
    "    \"\"\"This Function will load the bucket and get the list of files\n",
    "    in the gs path given\n",
    "    args: gs path\n",
    "    output: file names as list and dictionary with file names as keys and file path as values\n",
    "    \"\"\"\n",
    "    bucket = file_path.split(\"/\")[2]\n",
    "    file_names_list = []\n",
    "    file_dict = {}\n",
    "    storage_client = storage.Client()\n",
    "    source_bucket = storage_client.get_bucket(bucket)\n",
    "    filenames = [\n",
    "        filename.name for filename in list(\n",
    "            source_bucket.list_blobs(\n",
    "                prefix=((\"/\").join(file_path.split(\"/\")[3:]))))\n",
    "    ]\n",
    "    for i in range(len(filenames)):\n",
    "        x = filenames[i].split(\"/\")[-1]\n",
    "        if x != \"\":\n",
    "            file_names_list.append(x)\n",
    "            file_dict[x] = filenames[i]\n",
    "    return file_names_list, file_dict\n",
    "\n",
    "\n",
    "# list\n",
    "def list_blobs(bucket_name):\n",
    "    \"\"\"This function will give the list of files in a bucket\n",
    "    args: gcs bucket name\n",
    "    output: list of files\"\"\"\n",
    "    blob_list = []\n",
    "    storage_client = storage.Client()\n",
    "    blobs = storage_client.list_blobs(bucket_name)\n",
    "    for blob in blobs:\n",
    "        blob_list.append(blob.name)\n",
    "    return blob_list\n",
    "\n",
    "\n",
    "# Bucket operations\n",
    "def relation_dict_generator(pre_hitl_output_bucket, post_hitl_output_bucket):\n",
    "    \"\"\"This Function will check the files from pre_hitl_output_bucket and post_hitl_output_bucket\n",
    "    and finds the json with same names(relation)\"\"\"\n",
    "    pre_hitl_bucket_blobs = list_blobs(pre_hitl_output_bucket)\n",
    "    post_hitl_bucket_blobs = list_blobs(post_hitl_output_bucket)\n",
    "\n",
    "    relation_dict = {}\n",
    "    non_relation_dict = {}\n",
    "    for i in pre_hitl_bucket_blobs:\n",
    "        for j in post_hitl_bucket_blobs:\n",
    "            matched_score = difflib.SequenceMatcher(None, i, j).ratio()\n",
    "            print(\"matched_score : \", matched_score)\n",
    "            if (\n",
    "                    matched_score == 1\n",
    "            ):  # 0.9 This is for file name. pre and post hitl json files are to be same\n",
    "                relation_dict[i] = j\n",
    "            else:\n",
    "                non_relation_dict[i] = \"NO POST HITL OUTPUT AVAILABLE\"\n",
    "                # print(i)\n",
    "    for i in relation_dict:\n",
    "        if i in non_relation_dict.keys():\n",
    "            del non_relation_dict[i]\n",
    "    print(\"relation_dict = \", relation_dict)\n",
    "    print(\"non_relation_dict = \", non_relation_dict)\n",
    "    return relation_dict, non_relation_dict\n",
    "\n",
    "\n",
    "def blob_downloader(bucket_name, blob_name):\n",
    "    \"\"\"This Function is used to download the files from gcs bucket\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    contents = blob.download_as_string()\n",
    "    return json.loads(contents.decode())\n",
    "\n",
    "\n",
    "def copy_blob(bucket_name, blob_name, destination_bucket_name,\n",
    "              destination_blob_name):\n",
    "    \"\"\"This Method will copy files from one bucket(or folder) to another\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    source_bucket = storage_client.bucket(bucket_name)\n",
    "    source_blob = source_bucket.blob(blob_name)\n",
    "    destination_bucket = storage_client.bucket(destination_bucket_name)\n",
    "    blob_copy = source_bucket.copy_blob(source_blob, destination_bucket,\n",
    "                                        destination_blob_name)\n",
    "\n",
    "\n",
    "def bbox_maker(boundingPoly):\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    for i in boundingPoly:\n",
    "        x_list.append(i[\"x\"])\n",
    "        y_list.append(i[\"y\"])\n",
    "    bbox = [min(x_list), min(y_list), max(x_list), max(y_list)]\n",
    "    return bbox\n",
    "\n",
    "\n",
    "def JsonToDataframe(data):\n",
    "    \"\"\"Returns entities in dataframe format\"\"\"\n",
    "    df = pd.DataFrame(columns=[\"type\", \"mentionText\", \"bbox\"])\n",
    "\n",
    "    if \"entities\" not in data.keys():\n",
    "        return df\n",
    "\n",
    "    for entity in data[\"entities\"]:\n",
    "        if \"properties\" in entity and len(entity[\"properties\"]) > 0:\n",
    "            for sub_entity in entity[\"properties\"]:\n",
    "                if \"type\" in sub_entity:\n",
    "                    try:\n",
    "                        boundingPoly = sub_entity[\"pageAnchor\"][\"pageRefs\"][0][\n",
    "                            \"boundingPoly\"][\"normalizedVertices\"]\n",
    "                        bbox = bbox_maker(boundingPoly)\n",
    "                        # bbox = [boundingPoly[0]['x'], boundingPoly[0]['y'], boundingPoly[2]['x'], boundingPoly[2]['y']]\n",
    "                        df.loc[len(df.index)] = [\n",
    "                            sub_entity[\"type\"],\n",
    "                            sub_entity[\"mentionText\"],\n",
    "                            bbox,\n",
    "                        ]\n",
    "                    except KeyError:\n",
    "                        if \"mentionText\" in sub_entity:\n",
    "                            df.loc[len(df.index)] = [\n",
    "                                sub_entity[\"type\"],\n",
    "                                sub_entity[\"mentionText\"],\n",
    "                                [],\n",
    "                            ]\n",
    "                        else:\n",
    "                            df.loc[len(df.index)] = [\n",
    "                                sub_entity[\"type\"],\n",
    "                                \"Entity not found.\",\n",
    "                                [],\n",
    "                            ]\n",
    "        elif \"type\" in entity:\n",
    "            try:\n",
    "                boundingPoly = entity[\"pageAnchor\"][\"pageRefs\"][0][\n",
    "                    \"boundingPoly\"][\"normalizedVertices\"]\n",
    "                bbox = bbox_maker(boundingPoly)\n",
    "                # bbox = [boundingPoly[0]['x'], boundingPoly[0]['y'], boundingPoly[2]['x'], boundingPoly[2]['y']]\n",
    "                df.loc[len(\n",
    "                    df.index)] = [entity[\"type\"], entity[\"mentionText\"], bbox]\n",
    "            except KeyError:\n",
    "                if \"mentionText\" in entity:\n",
    "                    df.loc[len(df.index)] = [\n",
    "                        entity[\"type\"], entity[\"mentionText\"], []\n",
    "                    ]\n",
    "                else:\n",
    "                    df.loc[len(\n",
    "                        df.index)] = [entity[\"type\"], \"Entity not found.\", []]\n",
    "    return df\n",
    "\n",
    "\n",
    "def RemoveRow(df, entity):\n",
    "    \"\"\"Drops the entity passed from the dataframe\"\"\"\n",
    "    return df[df[\"type\"] != entity]\n",
    "\n",
    "\n",
    "def FindMatch(entity_file1, df_file2):\n",
    "    \"\"\"Finds the matching entity from the dataframe using\n",
    "    the area of IOU between bboxes reference\n",
    "    \"\"\"\n",
    "    bbox_file1 = entity_file1[2]\n",
    "    # Entity not present in json file\n",
    "    if not bbox_file1:\n",
    "        return None\n",
    "\n",
    "    # filtering entities with the same name\n",
    "    df_file2 = df_file2[df_file2[\"type\"] == entity_file1[0]]\n",
    "\n",
    "    # calculating IOU values for the entities\n",
    "    index_iou_pairs = []\n",
    "    for index, entity_file2 in enumerate(df_file2.values):\n",
    "        if entity_file2[2]:\n",
    "            iou = BBIntersectionOverUnion(bbox_file1, entity_file2[2])\n",
    "            index_iou_pairs.append((index, iou))\n",
    "\n",
    "    # choose entity with highest IOU, IOU should be atleast > 0.5\n",
    "    matched_index = None\n",
    "    for index_iou in sorted(index_iou_pairs,\n",
    "                            key=operator.itemgetter(1),\n",
    "                            reverse=True):\n",
    "        if index_iou[1] > 0.2:  # 0.5\n",
    "            matched_index = df_file2.index[index_iou[0]]\n",
    "            break\n",
    "    return matched_index\n",
    "\n",
    "\n",
    "def BBIntersectionOverUnion(box1, box2):\n",
    "    \"\"\"Calculates the area of IOU between two bounding boxes\"\"\"\n",
    "    print(\"++ BBIntersectionOverUnion ++\")\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "\n",
    "    inter_area = abs(max((x2 - x1, 0)) * max((y2 - y1), 0))\n",
    "    if inter_area == 0:\n",
    "        return 0\n",
    "    box1_area = abs((box1[2] - box1[0]) * (box1[3] - box1[1]))\n",
    "    box2_area = abs((box2[2] - box2[0]) * (box2[3] - box2[1]))\n",
    "    iou = inter_area / float(box1_area + box2_area - inter_area)\n",
    "\n",
    "    return iou\n",
    "\n",
    "\n",
    "def GetMatchRatio(values):\n",
    "    file1_value = values[1]\n",
    "    file2_value = values[3]\n",
    "    if file1_value == \"Entity not found.\" or file2_value == \"Entity not found.\":\n",
    "        return 0\n",
    "    else:\n",
    "        return difflib.SequenceMatcher(a=file1_value, b=file2_value).ratio()\n",
    "\n",
    "\n",
    "def compare_pre_hitl_and_post_hitl_output(file1, file2):\n",
    "    \"\"\"Compares the entities between two files and returns\n",
    "    the results in a dataframe\n",
    "    \"\"\"\n",
    "    print(\"== compare_pre_hitl_and_post_hitl_output ==\")\n",
    "    df_file1 = JsonToDataframe(file1)\n",
    "    df_file2 = JsonToDataframe(file2)\n",
    "    file1_entities = [entity[0] for entity in df_file1.values]\n",
    "    print(file1_entities, \"\\n\")\n",
    "    file2_entities = [entity[0] for entity in df_file2.values]\n",
    "    print(file2_entities)\n",
    "\n",
    "    # find entities which are present only once in both files\n",
    "    # these entities will be matched directly\n",
    "    common_entities = set(file1_entities).intersection(set(file2_entities))\n",
    "    exclude_entities = []\n",
    "    for entity in common_entities:\n",
    "        print(\"entity  -- : \", entity)\n",
    "        if file1_entities.count(entity) > 1 or file2_entities.count(\n",
    "                entity) > 1:\n",
    "            exclude_entities.append(entity)\n",
    "\n",
    "    print(\"exclude_entities : \", exclude_entities)\n",
    "    for entity in exclude_entities:\n",
    "        common_entities.remove(entity)\n",
    "    df_compare = pd.DataFrame(columns=[\n",
    "        \"Entity Type\",\n",
    "        \"Pre_HITL_Output\",\n",
    "        \"Pre_HITL_bbox\",\n",
    "        \"Post_HITL_Output\",\n",
    "        \"Post_HITL_bbox\",\n",
    "    ])\n",
    "    print(\"df_compare:--- \\n\", df_compare)\n",
    "    for entity in common_entities:\n",
    "        value1 = df_file1[df_file1[\"type\"] == entity].iloc[0][\"mentionText\"]\n",
    "        value2 = df_file2[df_file2[\"type\"] == entity].iloc[0][\"mentionText\"]\n",
    "        bbox1 = df_file1[df_file1[\"type\"] == entity].iloc[0][\"bbox\"]\n",
    "        bbox2 = df_file2[df_file2[\"type\"] == entity].iloc[0][\"bbox\"]\n",
    "        df_compare.loc[len(\n",
    "            df_compare.index)] = [entity, value1, bbox1, value2, bbox2]\n",
    "\n",
    "        # common entities are removed from df_file1 and df_file2\n",
    "        df_file1 = RemoveRow(df_file1, entity)\n",
    "        df_file2 = RemoveRow(df_file2, entity)\n",
    "\n",
    "    # remaining entities are matched comparing the area of IOU across them\n",
    "    mentionText2 = pd.Series(dtype=str)\n",
    "    bbox2 = pd.Series(dtype=str)\n",
    "    for index, row in enumerate(df_file1.values):\n",
    "        matched_index = FindMatch(row, df_file2)\n",
    "        if matched_index != None:\n",
    "            mentionText2.loc[index] = df_file2.loc[matched_index][1]\n",
    "            bbox2.loc[index] = df_file2.loc[matched_index][2]\n",
    "            df_file2 = df_file2.drop(matched_index)\n",
    "        else:\n",
    "            mentionText2.loc[index] = \"Entity not found.\"\n",
    "            bbox2.loc[index] = \"bbox not found\"\n",
    "\n",
    "    df_file1[\"mentionText2\"] = mentionText2.values\n",
    "    df_file1[\"bbox2\"] = bbox2.values\n",
    "    # df_file1 = df_file1.drop(['bbox'], axis=1)\n",
    "    df_file1.rename(\n",
    "        columns={\n",
    "            \"type\": \"Entity Type\",\n",
    "            \"mentionText\": \"Pre_HITL_Output\",\n",
    "            \"bbox\": \"Pre_HITL_bbox\",\n",
    "            \"mentionText2\": \"Post_HITL_Output\",\n",
    "            \"bbox2\": \"Post_HITL_bbox\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "    df_compare = df_compare._append(df_file1, ignore_index=True)\n",
    "\n",
    "    # adding entities which are present in file2 but not in file1\n",
    "    for row in df_file2.values:\n",
    "        df_compare.loc[len(df_compare.index)] = [\n",
    "            row[0],\n",
    "            \"Entity not found.\",\n",
    "            \"bbox not present\",\n",
    "            row[1],\n",
    "            row[2],\n",
    "        ]\n",
    "\n",
    "    # df_compare['Match'] = df_compare['Ground Truth Text'] == df_compare['Output Text']\n",
    "    match_array = []\n",
    "    for i in range(0, len(df_compare)):\n",
    "        match_string = \"\"\n",
    "        if (df_compare.iloc[i][\"Pre_HITL_Output\"] == \"Entity not found.\" and\n",
    "                df_compare.iloc[i][\"Post_HITL_Output\"] == \"Entity not found.\"):\n",
    "            match_string = \"TN\"\n",
    "        elif (df_compare.iloc[i][\"Pre_HITL_Output\"] != \"Entity not found.\" and\n",
    "              df_compare.iloc[i][\"Post_HITL_Output\"] == \"Entity not found.\"):\n",
    "            match_string = \"FN\"\n",
    "        elif (df_compare.iloc[i][\"Pre_HITL_Output\"] == \"Entity not found.\" and\n",
    "              df_compare.iloc[i][\"Post_HITL_Output\"] != \"Entity not found.\"):\n",
    "            match_string = \"FP\"\n",
    "        elif (df_compare.iloc[i][\"Pre_HITL_Output\"] != \"Entity not found.\" and\n",
    "              df_compare.iloc[i][\"Post_HITL_Output\"] != \"Entity not found.\"):\n",
    "            if (df_compare.iloc[i][\"Pre_HITL_Output\"] == df_compare.iloc[i]\n",
    "                [\"Post_HITL_Output\"]):\n",
    "                match_string = \"TP\"\n",
    "            else:\n",
    "                match_string = \"FP\"\n",
    "        else:\n",
    "            match_string = \"Something went Wrong.\"\n",
    "\n",
    "        match_array.append(match_string)\n",
    "\n",
    "    df_compare[\"Match\"] = match_array\n",
    "\n",
    "    df_compare[\"Fuzzy Ratio\"] = df_compare.apply(GetMatchRatio, axis=1)\n",
    "    if list(df_compare.index):\n",
    "        score = df_compare[\"Fuzzy Ratio\"].sum() / len(df_compare.index)\n",
    "    else:\n",
    "        score = 0\n",
    "\n",
    "    print(\"match_array\")\n",
    "    print(match_array)\n",
    "    return df_compare, score\n",
    "\n",
    "\n",
    "# Execute the below code\n",
    "\n",
    "pre_HITL_output_URI = config.get(\"Parameters\", \"pre_hitl_output_uri\")\n",
    "post_HITL_output_URI = config.get(\"Parameters\", \"post_hitl_output_uri\")\n",
    "# print(pre_HITL_output_URI)\n",
    "# print(post_HITL_output_URI)\n",
    "\n",
    "# creating temperary buckets\n",
    "import datetime\n",
    "\n",
    "now = str(datetime.datetime.now())\n",
    "now = re.sub(r\"\\W+\", \"\", now)\n",
    "\n",
    "print(\"Creating temporary buckets\")\n",
    "pre_HITL_bucket_name_temp = \"pre_hitl_output\" + \"_\" + now\n",
    "post_HITL_bucket_name_temp = \"post_hitl_output_temp\" + \"_\" + now\n",
    "# bucket name and prefix\n",
    "pre_HITL_bucket = pre_HITL_output_URI.split(\"/\")[2]\n",
    "post_HITL_bucket = post_HITL_output_URI.split(\"/\")[2]\n",
    "# getting all files and copying to temporary folder\n",
    "\n",
    "try:\n",
    "    check_create_bucket(pre_HITL_bucket_name_temp)\n",
    "    check_create_bucket(post_HITL_bucket_name_temp)\n",
    "except Exception as e:\n",
    "    print(\"unable to create bucket because of exception : \", e)\n",
    "\n",
    "try:\n",
    "    pre_HITL_output_files, pre_HITL_output_dict = file_names(\n",
    "        pre_HITL_output_URI)\n",
    "    # print(pre_HITL_output_files,pre_HITL_output_dict)\n",
    "    post_HITL_output_files, post_HITL_output_dict = file_names(\n",
    "        post_HITL_output_URI)\n",
    "    # print(post_HITL_output_files,post_HITL_output_dict)\n",
    "    print(\"copying files to temporary bucket\")\n",
    "    for i in pre_HITL_output_files:\n",
    "        copy_blob(pre_HITL_bucket, pre_HITL_output_dict[i],\n",
    "                  pre_HITL_bucket_name_temp, i)\n",
    "    for i in post_HITL_output_files:\n",
    "        copy_blob(post_HITL_bucket, post_HITL_output_dict[i],\n",
    "                  post_HITL_bucket_name_temp, i)\n",
    "    pre_HITL_files_list = list_blobs(pre_HITL_bucket_name_temp)\n",
    "    post_HITL_files_list = list_blobs(post_HITL_bucket_name_temp)\n",
    "except Exception as e:\n",
    "    print(\"unable to get list of files in buckets because : \", e)\n",
    "# processing the files and saving the files in temporary GCP bucket\n",
    "fs = gcsfs.GCSFileSystem(project_id)\n",
    "relation_dict, non_relation_dict = relation_dict_generator(\n",
    "    pre_HITL_bucket_name_temp, post_HITL_bucket_name_temp)\n",
    "\n",
    "time_stamp = datetime.datetime.now().strftime(\"%d_%m_%y-%H%M%S\")\n",
    "filename_error_count_dict = {}\n",
    "\n",
    "compare_merged = pd.DataFrame()\n",
    "accuracy_docs = []\n",
    "print(\"comparing the PRE-HITL Jsons and POST-HITL jsons ....Wait for Summary \")\n",
    "for i in relation_dict:\n",
    "    # print(\"***** i : \", i)\n",
    "    pre_HITL_json = blob_downloader(pre_HITL_bucket_name_temp, i)\n",
    "    post_HITL_json = blob_downloader(post_HITL_bucket_name_temp,\n",
    "                                     relation_dict[i])\n",
    "    # print('pre_HITL_json : ', pre_HITL_json)\n",
    "    # print('post_HITL_json : ', post_HITL_json)\n",
    "    compare_output = compare_pre_hitl_and_post_hitl_output(\n",
    "        pre_HITL_json, post_HITL_json)[0]\n",
    "    # print('compare_output :',compare_output)\n",
    "    column = [relation_dict[i]] * compare_output.shape[0]\n",
    "    # print(\"++++column++++\")\n",
    "    # print(column)\n",
    "    compare_output.insert(loc=0, column=\"File Name\", value=column)\n",
    "\n",
    "    compare_output.insert(loc=5, column=\"hitl_update\", value=\" \")\n",
    "    for j in range(len(compare_output)):\n",
    "        if compare_output[\"Fuzzy Ratio\"][j] != 1.0:  # strict\n",
    "            if (compare_output[\"Pre_HITL_Output\"][j] == \"Entity not found.\"\n",
    "                    and compare_output[\"Post_HITL_Output\"][j]\n",
    "                    == \"Entity not found.\"):\n",
    "                compare_output[\"hitl_update\"][j] = \"NO\"\n",
    "            else:\n",
    "                compare_output[\"hitl_update\"][j] = \"YES\"\n",
    "        else:\n",
    "            compare_output[\"hitl_update\"][j] = \"NO\"\n",
    "    for k in range(len(compare_output)):\n",
    "        if compare_output[\"Fuzzy Ratio\"][k] != 1.0:  # strict\n",
    "            hitl_update = \"HITL UPDATED\"\n",
    "            break\n",
    "        else:\n",
    "            compare_output[\"hitl_update\"][k] = \"NO\"\n",
    "\n",
    "    ##\n",
    "    compare_output[\"bbox_mismatch\"] = (compare_output[\"Pre_HITL_bbox\"]\n",
    "                                       != compare_output[\"Post_HITL_bbox\"])\n",
    "\n",
    "    # OCR Issue\n",
    "    compare_output[\"OCR Issue\"] = \"No\"\n",
    "    # compare_output.loc[(compare_output['Pre_HITL_Output'] != compare_output['Post_HITL_Output']), 'OCR Issue']  = 'Yes' # & cordinates are same\n",
    "    compare_output.loc[\n",
    "        (compare_output[\"Pre_HITL_Output\"] !=\n",
    "         compare_output[\"Post_HITL_Output\"])\n",
    "        &\n",
    "        (compare_output[\"Pre_HITL_bbox\"] == compare_output[\"Post_HITL_bbox\"]),\n",
    "        \"OCR Issue\",\n",
    "    ] = \"Yes\"\n",
    "\n",
    "    # Parser Issue\n",
    "    compare_output[\"Parser Issue\"] = \"No\"\n",
    "    compare_output.loc[\n",
    "        (compare_output[\"hitl_update\"] == \"YES\")\n",
    "        & (compare_output[\"bbox_mismatch\"] == True),\n",
    "        \"Parser Issue\",\n",
    "    ] = \"Yes\"  # & cordinates are different\n",
    "    # compare_output.loc[\n",
    "    #                ((compare_output['hitl_update'] == 'YES')  & (compare_output['bbox_mismatch'] == True))\n",
    "    #                  &\n",
    "    #                (compare_output['Pre_HITL_bbox'] != compare_output['Post_HITL_bbox']), 'Parser Issue'\n",
    "    #              ] = 'Yes'\n",
    "\n",
    "    # Parser Issue - entity not found cases | skip if both are 'Entity not found'\n",
    "    try:\n",
    "        compare_merged.loc[\n",
    "            (compare_merged[\"Post_HITL_Output\"] == \"Entity not found.\")\n",
    "            | (compare_merged[\"Pre_HITL_Output\"] == \"Entity not found.\"),\n",
    "            \"Parser Issue\",\n",
    "        ] = \"Yes\"\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    ## global dict : no of parser error / file\n",
    "    temp = {}\n",
    "    temp[\"bbox_mismatch\"] = len(\n",
    "        compare_output[compare_output[\"bbox_mismatch\"] == True])\n",
    "\n",
    "    temp[\"OCR_issue\"] = len(\n",
    "        compare_output.loc[(compare_output[\"Pre_HITL_Output\"] !=\n",
    "                            compare_output[\"Post_HITL_Output\"])\n",
    "                           & (compare_output[\"Pre_HITL_bbox\"] ==\n",
    "                              compare_output[\"Post_HITL_bbox\"])])\n",
    "    temp[\"Parser_issue\"] = len(\n",
    "        compare_output.loc[(compare_output[\"hitl_update\"] == \"YES\")\n",
    "                           & (compare_output[\"bbox_mismatch\"] == True)])\n",
    "    temp[\"output_file\"] = \"analysis_\" + time_stamp + \"/\" + i.replace(\n",
    "        \"json\", \"csv\")\n",
    "\n",
    "    filename_error_count_dict[i] = temp\n",
    "\n",
    "    new_row = pd.Series(\n",
    "        [\n",
    "            i,\n",
    "            \"Entities\",\n",
    "            \"are updated\",\n",
    "            \"by HITL\",\n",
    "            \":\",\n",
    "            np.nan,\n",
    "            hitl_update,\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "        ],\n",
    "        index=compare_output.columns,\n",
    "    )\n",
    "    compare_output = compare_output._append(new_row, ignore_index=True)\n",
    "    frames = [compare_merged, compare_output]\n",
    "    compare_merged = pd.concat(frames)\n",
    "\n",
    "with open(\"summary_\" + time_stamp + \".json\", \"w\") as ofile:\n",
    "    ofile.write(json.dumps(filename_error_count_dict))\n",
    "\n",
    "for x in relation_dict:\n",
    "    # print(x)\n",
    "    file_out = compare_merged[compare_merged[\"File Name\"] == x]\n",
    "    try:\n",
    "        os.mkdir(\"analysis_\" + time_stamp)\n",
    "    except:\n",
    "        pass\n",
    "    file_out.to_csv(\"analysis_\" + time_stamp + \"/\" + x.replace(\"json\", \"csv\"))\n",
    "\n",
    "bucket_delete(pre_HITL_bucket_name_temp)\n",
    "bucket_delete(post_HITL_bucket_name_temp)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m104",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m104"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
