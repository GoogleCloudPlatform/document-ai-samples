{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72bc511c-2282-4d2b-a2d4-28d25c07ffa2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DocAI - Script for Removing Empty Bounding Boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778291f7-ed93-4f0f-abc9-c9f82245cf23",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c92559f-6a39-4318-b8ae-064325723cc7",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the DocAI Incubator Team. No guarantees of performance are implied.\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8e2b95-c07f-40ac-bb50-93b20b143ded",
   "metadata": {},
   "source": [
    "## Purpose of the Script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a2c845-2df8-41f4-9e3d-59c559a3c4b4",
   "metadata": {},
   "source": [
    "\n",
    "The purpose of this document is to provide instructions and a Python script for removing empty bounding boxes from a labeled JSON file. The script identifies and removes any bounding boxes (entities) in the JSON file that do not contain any mentionText or textAnchors, streamlining the labeling process and improving the accuracy of the labeling data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fa7cdd-9654-42c2-8612-ea779a20a48a",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728a861b-21e4-4300-aef8-650cb390a271",
   "metadata": {},
   "source": [
    "1. Python : Jupyter notebook (Vertex AI) \n",
    "2. Service account permissions in projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fa8919-7096-4b92-8604-a4a6daa224f9",
   "metadata": {},
   "source": [
    "## Installation Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60803eb8-6452-49f2-992c-08465e55b770",
   "metadata": {},
   "source": [
    "The script consists of Python code. It can be loaded and run via: \n",
    "1.  Upload the IPYNB file or copy the code to the Vertex Notebook and follow the operation procedure. \\\n",
    "**NOTE:** Don’t Execute the Script with Processor Dataset Path. Export the dataset to json and then use that bucket as an input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6993aead-9bb6-45a4-8fba-c3be04fd3322",
   "metadata": {},
   "source": [
    "##  Operation Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d18824a-111d-44b4-8df5-c8aadff24202",
   "metadata": {},
   "source": [
    "### 1. Import the modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ec98b1-f8fa-4bab-b2cb-5bd49b4c6c38",
   "metadata": {},
   "source": [
    "**Note :** external modules are used so they need to be installed. To install run these commands : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d0f9226-89cd-4fca-8090-7303b48df66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gcsfs\n",
    "# !pip install google-cloud\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import gcsfs\n",
    "import google.auth\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "from tqdm import tqdm\n",
    "from google.cloud import documentai_v1beta3 as documentai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d8482e-0450-498c-940c-e775037ef907",
   "metadata": {},
   "source": [
    "### 2. Setup the required inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6e1955-4667-49b8-b6fa-5eb06546d7e1",
   "metadata": {},
   "source": [
    "* **PROJECT_ID** - Your Google project id or name\n",
    "* **BUCKET_NAME** - Name of the bucket\n",
    "* **INPUT_FOLDER_PATH** - The path of the folder containing the JSON files to be processed, without the bucket name.\n",
    "* **OUTPUT_FOLDER_PATH** - The path of the folder where the JSON files need to be stored after process, without the bucket * name.\n",
    "\n",
    "**Note :**  Both Input and output paths should be in the same bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbbc4b67-fbae-4e7b-8518-ddde8b2f30c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"rand-automl-project\"\n",
    "BUCKET_NAME = \"accenture_line_items_samples\"\n",
    "INPUT_FOLDER_PATH = \"output/output/2839778604252110189/0\"  # Path without bucket name\n",
    "OUTPUT_FOLDER_PATH = \"output_atul/output/\"  # Path without bucket name\n",
    "credentials, _ = google.auth.default()\n",
    "fs = gcsfs.GCSFileSystem(project=PROJECT_ID, token=credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5b0743-d63b-4960-a46d-f2ffd9bcd252",
   "metadata": {},
   "source": [
    "### 3. Execute the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d9e2908-2566-49b4-b5e0-a8b005b8821c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of files :  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.62it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_file(file_path: str) -> documentai.Document:\n",
    "    \"\"\"\n",
    "    To read files from cloud storage.\n",
    "    \"\"\"\n",
    "    file_object = fs.cat(file_path)\n",
    "    doc = documentai.Document.from_json(file_object)  # JSON to DocumentProto Format\n",
    "    return doc\n",
    "\n",
    "\n",
    "def store_blob(document, file: str):\n",
    "    \"\"\"\n",
    "    Store files in cloud storage.\n",
    "    \"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    result_bucket = storage_client.get_bucket(BUCKET_NAME)\n",
    "    document_blob = storage.Blob(name=str(file), bucket=result_bucket)\n",
    "    document_blob.upload_from_string(\n",
    "        documentai.Document.to_json(document), content_type=\"application/json\"\n",
    "    )\n",
    "\n",
    "\n",
    "def main():\n",
    "    logs = pd.DataFrame(columns=[\"FileName\"])\n",
    "\n",
    "    files = [\n",
    "        i for i in fs.find(f\"{BUCKET_NAME}/{INPUT_FOLDER_PATH}\") if i.endswith(\".json\")\n",
    "    ]\n",
    "    document_files_list = [get_file(i) for i in files]\n",
    "    print(\"No. of files : \", len(files))\n",
    "\n",
    "    for index in tqdm(range(len(files))):\n",
    "        file_name = files[index].split(\"/\", 1)[-1]\n",
    "        output_file_name = file_name.replace(INPUT_FOLDER_PATH, OUTPUT_FOLDER_PATH)\n",
    "        is_updated = False\n",
    "        doc = document_files_list[index]\n",
    "        # print(doc)\n",
    "        sub_log = pd.DataFrame(columns=[file_name])\n",
    "        # for i in reversed(range(len(doc[\"entities\"]))):\n",
    "        #     entity = doc[\"entities\"][i]\n",
    "        if doc.entities:\n",
    "            for entity in doc.entities:\n",
    "                if not entity.mention_text:\n",
    "                    sub_log = sub_log.append(\n",
    "                        {file_name: entity.type}, ignore_index=True\n",
    "                    )\n",
    "                    doc.entities.remove(entity)\n",
    "                    is_updated = True\n",
    "                    continue\n",
    "                else:\n",
    "                    if entity.properties and entity.mention_text.strip():\n",
    "                        for sub_entity in entity.properties:\n",
    "                            if sub_entity.mention_text:\n",
    "                                if sub_entity.mention_text.strip() == \"\":\n",
    "                                    sub_log = sub_log.append(\n",
    "                                        {file_name: sub_entity.type}, ignore_index=True\n",
    "                                    )\n",
    "                                    entity.properties.remove(sub_entity)\n",
    "                                    is_updated = True\n",
    "                                    continue\n",
    "                            elif not sub_entity.mention_text:\n",
    "                                sub_log = sub_log.append(\n",
    "                                    {file_name: sub_entity.type}, ignore_index=True\n",
    "                                )\n",
    "                                entity.properties.remove(sub_entity)\n",
    "                                is_updated = True\n",
    "                                continue\n",
    "                if not sub_entity.text_anchor:\n",
    "                    sub_log = sub_log.append(\n",
    "                        {file_name: sub_entity.type}, ignore_index=True\n",
    "                    )\n",
    "                    entity.properties.remove(sub_entity)\n",
    "                    is_updated = True\n",
    "                    continue\n",
    "                elif sub_entity.text_anchor:\n",
    "                    if not sub_entity.text_anchor.text_segments:\n",
    "                        sub_log = sub_log.append(\n",
    "                            {file_name: sub_entity.type}, ignore_index=True\n",
    "                        )\n",
    "                        entity.properties.remove(sub_entity)\n",
    "                        is_updated = True\n",
    "                        continue\n",
    "                    elif len(sub_entity.text_anchor.text_segments) < 1:\n",
    "                        sub_log = sub_log.append(\n",
    "                            {file_name: sub_entity.type}, ignore_index=True\n",
    "                        )\n",
    "                        entity.properties.remove(sub_entity)\n",
    "                        is_updated = True\n",
    "                        continue\n",
    "        else:\n",
    "            print(\"Entities missing : \", files[index])\n",
    "            # if is_updated:\n",
    "        store_blob(doc, output_file_name)\n",
    "        if not sub_log.empty:\n",
    "            logs = pd.concat([logs, sub_log], axis=1)\n",
    "    # logs.drop(\"FileName\", axis=1, inplace=True)\n",
    "    logs.to_csv(\"output.csv\", index=False)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5345184-bb87-4b08-bda0-0bb7e7eca45e",
   "metadata": {},
   "source": [
    "## Output File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef59951f-930c-4c7c-8df5-e2738b0d8ff8",
   "metadata": {},
   "source": [
    "The script deletes all bounding boxes (entities) in the JSON file that do not contain any mentionText or textAnchors, and overwrites the file. The script will also create a CSV file containing a list of deleted entities."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m104",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m104"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
