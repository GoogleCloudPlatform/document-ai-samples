{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b5abe54-f34f-471d-a4b5-013bd24f640a",
   "metadata": {},
   "source": [
    "# HITL REJECTED DOCUMENTS TRACKING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6920dc-510d-44c0-b118-a9703d704ebc",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaceb7f-8003-4613-b0ba-479a8bd0dbdc",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the DocAI Incubator Team. No guarantees of performance are implied.\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8642daa6-42e1-4dd6-80a0-e194ce97b37f",
   "metadata": {},
   "source": [
    "## Purpose and Description\n",
    "This tool is used to track the HITL rejected documents. List of Long Running Operation(LRO) ids are used as an input to show the list of rejected files in HITL for those LROs and also copying the processed json to the GCS folder provided with entity HITL_Status added in the json.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47784964-c0eb-49da-b899-f64a95d51cc3",
   "metadata": {},
   "source": [
    "## Prerequisite\n",
    " * Vertex AI Notebook\n",
    " * List of LROs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee3c1a0-be2e-490d-b155-a3ab7ebb4317",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step by Step procedure \n",
    "Install the latest version of document ai using below command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66c8067-7657-4873-99c0-f215d2b1fcea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install google-cloud-documentai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8121daac-658e-44f5-a6a8-2a1e24c6ea52",
   "metadata": {},
   "source": [
    "### 1. Setup the required Input Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f940181-5a26-4965-996b-64cfe3215fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LRO_list = [\"1945491532426716613\", \"1945491532426716613\"]  # should be a list\n",
    "gcs_hitl_rejected_path = \"gs://XXX/XXX/XX/\"  # path should end with '/'\n",
    "project_id = \"XXXX-XXXX-XXXX\"\n",
    "location = \"us\"  # Processor location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ab8c09-8b20-4610-b1d4-bd2522a6e1fd",
   "metadata": {},
   "source": [
    " * **LRO_list:** provide the list of LROs (operation id after batch processing)\n",
    " * **gcs_hitl_rejected_path:** provide the gcs path to save the json file\n",
    " * **project_id:** provide the name of the project \n",
    " * **Location:** provide the location of processor like ‘us’ or ‘eu’"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d34d13-2429-4651-bc85-b35136b95e95",
   "metadata": {},
   "source": [
    "### 2. Run the Code\n",
    "\n",
    "Run the rest of the code without any edit for CSV file and the modified json to save in the gcs path.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81be593a-fc24-4019-9cc2-aa98fb8ded02",
   "metadata": {},
   "source": [
    "### 3. Output\n",
    "\n",
    "The output after execution is in 2 formats, the first one is a CSV file (HITL_Status_Update.csv) with File names, HITL status and Reason for the rejection in HITL if rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615d2dc1-91ec-4aa4-86b8-13c6124dfebc",
   "metadata": {},
   "source": [
    "![](https://screenshot.googleplex.com/8c2dvEnGEX3eq68.png)\n",
    "\n",
    "The second output after execution is the JSON file with the entity HITL_Status added in the DocumentProto json file and saved in the GCS path provided.\n",
    "\n",
    "**Example:** {'type': 'HITL_Status', 'mentionText': 'REJECTED_Too blurry'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9dbaed-aab7-40cd-a740-1dd427f4a633",
   "metadata": {},
   "source": [
    "### **Sample Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7891e9f7-514b-48a8-90b2-a748d8c32f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT DETAILS TO BE PROVIDED\n",
    "LRO_list = [\"1945491532426716613\", \"1945211532426712334\"]  # should be a list\n",
    "gcs_hitl_rejected_path = \"gs://xxx/xxxx/xxx/\"  # path should end with '/'\n",
    "project_id = \"xxxx-xxxxx-xxxxx\"\n",
    "location = \"us\"  # Processor location\n",
    "\n",
    "import json\n",
    "\n",
    "import gcsfs\n",
    "import pandas as pd\n",
    "# Run the code\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from google.cloud import storage\n",
    "\n",
    "fs = gcsfs.GCSFileSystem(project=project_id)\n",
    "\n",
    "\n",
    "def load_json_files_from_folder_uri(folder_uri, hitl_dict, gcs_output_path):\n",
    "    \"\"\"Loads all the JSON files in a GCP folder URI as JSON objects.\"\"\"\n",
    "    client = storage.Client()\n",
    "    uri_parts = folder_uri.split(\"/\")\n",
    "    bucket_name = uri_parts[2]\n",
    "    folder_name = \"/\".join(uri_parts[3:])\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blobs = bucket.list_blobs(prefix=folder_name)\n",
    "    for blob in blobs:\n",
    "        if blob.content_type == \"application/json\":\n",
    "            blob_content = blob.download_as_bytes()\n",
    "            json_data = json.loads(blob_content.decode(\"utf-8\"))\n",
    "        if \"entities\" in json_data.keys():\n",
    "            json_data[\"entities\"].append(hitl_dict)\n",
    "            # print(json_data['entities'])\n",
    "            file_name = blob.name.split(\"/\")[-1]\n",
    "            # comment the below line if you dont want the JSON file to be saved in GCS path\n",
    "            fs.pipe(\n",
    "                gcs_output_path + file_name,\n",
    "                bytes(json.dumps(json_data, ensure_ascii=False), \"utf-8\"),\n",
    "                content_type=\"application/json\",\n",
    "            )\n",
    "\n",
    "\n",
    "def dataframe_HITL_status(metadata_op):\n",
    "    df = pd.DataFrame(columns=[\"File_Name\", \"HITL_Status\", \"Reason\"])\n",
    "    for i in range(len(metadata_op.individual_process_statuses)):\n",
    "        if (\"human_review_operation\" in metadata_op.\n",
    "                individual_process_statuses[i].human_review_status):\n",
    "            File_Name = metadata_op.individual_process_statuses[\n",
    "                i].input_gcs_source.split(\"/\")[-1]\n",
    "            hitl_op_id_1 = (\n",
    "                metadata_op.individual_process_statuses[i].human_review_status.\n",
    "                human_review_operation).split(\"/\")[-1]\n",
    "            x_hitl_1 = client.get_operation({\n",
    "                \"name\":\n",
    "                f\"projects/{project_id}/locations/{location}/operations/{hitl_op_id_1}\"\n",
    "            })\n",
    "            hitl_status_1 = documentai.ReviewDocumentResponse.deserialize(\n",
    "                x_hitl_1.response.value)\n",
    "            hitl_status_1_state = \"\"\n",
    "            Reason_1 = \"\"\n",
    "            if hitl_status_1.state.name == \"REJECTED\":\n",
    "                hitl_status_1_state = hitl_status_1.state.name\n",
    "                Reason_1 = hitl_status_1.rejection_reason\n",
    "            elif hitl_status_1.state.name == \"SUCCEEDED\":\n",
    "                hitl_status_1_state = hitl_status_1.state.name\n",
    "                Reason_1 = \" \"\n",
    "            df = pd.concat(\n",
    "                [\n",
    "                    df,\n",
    "                    pd.DataFrame(\n",
    "                        {\n",
    "                            \"File_Name\": File_Name,\n",
    "                            \"HITL_Status\": hitl_status_1_state,\n",
    "                            \"Reason\": Reason_1,\n",
    "                        },\n",
    "                        index=[0],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "    return df\n",
    "\n",
    "\n",
    "df_merge = pd.DataFrame()\n",
    "for i in LRO_list:\n",
    "    opts = {}\n",
    "    if location == \"eu\":\n",
    "        opts = {\"api_endpoint\": \"eu-documentai.googleapis.com\"}\n",
    "    elif location == \"us\":\n",
    "        opts = {\"api_endpoint\": \"us-documentai.googleapis.com\"}\n",
    "        # opts = {\"api_endpoint\": \"us-autopush-documentai.sandbox.googleapis.com\"}\n",
    "    client = documentai.DocumentProcessorServiceClient(client_options=opts)\n",
    "    x = client.get_operation(\n",
    "        {\"name\": f\"projects/{project_id}/locations/{location}/operations/{i}\"})\n",
    "    metadata_op = documentai.BatchProcessMetadata.deserialize(x.metadata.value)\n",
    "    hitl_file_dict = {}\n",
    "    for i in range(len(metadata_op.individual_process_statuses)):\n",
    "        if (\"human_review_operation\" in metadata_op.\n",
    "                individual_process_statuses[i].human_review_status):\n",
    "            pre_hitl_file = metadata_op.individual_process_statuses[\n",
    "                i].output_gcs_destination\n",
    "            post_hitl_file = (\n",
    "                metadata_op.individual_process_statuses[i].human_review_status.\n",
    "                human_review_operation).split(\"/\")[-1]\n",
    "            hitl_file_dict[pre_hitl_file] = post_hitl_file\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    for processed_path, hitl_path in hitl_file_dict.items():\n",
    "        hitl_op_id = hitl_path.split(\"/\")[-1]\n",
    "        # print(hitl_op_id)\n",
    "        x_hitl = client.get_operation({\n",
    "            \"name\":\n",
    "            f\"projects/{project_id}/locations/{location}/operations/{hitl_op_id}\"\n",
    "        })\n",
    "        hitl_status = documentai.ReviewDocumentResponse.deserialize(\n",
    "            x_hitl.response.value)\n",
    "        HITL_dict = {}\n",
    "        if hitl_status.state.name == \"REJECTED\":\n",
    "            HITL_dict[\"type\"] = \"HITL_Status\"\n",
    "            HITL_dict[\"mentionText\"] = (hitl_status.state.name + \"_\" +\n",
    "                                        hitl_status.rejection_reason)\n",
    "            # print(processed_path)\n",
    "            # print(HITL_dict)\n",
    "            load_json_files_from_folder_uri(processed_path, HITL_dict,\n",
    "                                            gcs_hitl_rejected_path)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    df = dataframe_HITL_status(metadata_op)\n",
    "    df_merge = pd.concat([df_merge, df], ignore_index=True)\n",
    "\n",
    "df_merge.to_csv(\"HITL_Status_Update.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0ea347-be14-481d-8695-4abb87b1761a",
   "metadata": {},
   "source": [
    "## HITL Rejected Document Tracking for Single Doc / Sync:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f556f877-22c6-4eba-95e3-2defd8198fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hitl_lro_status(project_id, location, operation_id):\n",
    "    \"\"\"\n",
    "    Checks the status of the operation_id, if the status is done and the HITL state is REJECTED,\n",
    "    then it returns the status and an entity with type=\"HITL_Status\", mention_text=\"REJECTED_rejected_reason\"\n",
    "    \"\"\"\n",
    "    if location == \"eu\":\n",
    "        opts = {\"api_endpoint\": \"eu-documentai.googleapis.com\"}\n",
    "    elif location == \"us\":\n",
    "        opts = {\"api_endpoint\": \"us-documentai.googleapis.com\"}\n",
    "    # DocumentAI Client Instance\n",
    "    client = documentai.DocumentProcessorServiceClient(client_options=opts)\n",
    "    # An empty Entity\n",
    "    HITL_entity = documentai.Document.Entity()\n",
    "\n",
    "    # Get the status of the operation_id\n",
    "    hitl_status_response = client.get_operation({\n",
    "        \"name\":\n",
    "        f\"projects/{project_id}/locations/{location}/operations/{operation_id}\"\n",
    "    })\n",
    "    if hitl_status_response.done:\n",
    "        # get the response from HITL console\n",
    "        hitl_response = documentai.ReviewDocumentResponse.deserialize(\n",
    "            hitl_status_response.response.value)\n",
    "        hitl_status = hitl_response.state.name\n",
    "        if hitl_status == \"REJECTED\":\n",
    "            HITL_entity.type = \"HITL_Status\"\n",
    "            HITL_entity.mention_text = (hitl_response.state.name + \"_\" +\n",
    "                                        hitl_response.rejection_reason)\n",
    "            return (\n",
    "                hitl_status,\n",
    "                HITL_entity,\n",
    "            )  # returns the hitl_status and a dictionary with 'type':'HITL_status', 'mentionText':'REJECTED_rejected_reason'\n",
    "        else:\n",
    "            return hitl_status, HITL_entity  # returns the hitl_status and an empty dict\n",
    "    else:\n",
    "        # Returns and an empty dictionary if HITL is in progress\n",
    "        return (\n",
    "            documentai.ReviewDocumentOperationMetadata.deserialize(\n",
    "                hitl_status_response.metadata.value).common_metadata.state.\n",
    "            name,\n",
    "            HITL_entity,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c1f532-c84b-4375-a9b4-044f5f2c948b",
   "metadata": {},
   "source": [
    "## Sample Use Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fefced-2541-4623-87f6-45685ee04f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.api_core.client_options import ClientOptions\n",
    "\n",
    "project_id = \"xxx-xxxx-xxxx\"  # project_id\n",
    "location = \"us\"  # Format is 'us' or 'eu'\n",
    "list_of_operation_id = [\"xxxxxxxxxxxxx\", \"xxxxxxxxxxxxxxxx\"]\n",
    "# For each operation_id get the hitl_status, if hitl_status=='REJECTED' , then add an entity reflecting the status and save the file\n",
    "\n",
    "for operation_id in list_of_operation_id:\n",
    "    hitl_status, hitl_entity = get_hitl_lro_status(project_id, location,\n",
    "                                                   operation_id)\n",
    "    inline_document = process_document(\n",
    "        project_id, location, processor_id, file_path, mime_type\n",
    "    )  # Get the processor Output, either through calling the api or by reading from cloud storage\n",
    "    if hitl_status == \"REJECTED\":\n",
    "        print(\n",
    "            operation_id,\n",
    "            \"|\",\n",
    "            hitl_status,\n",
    "            \"|\",\n",
    "            hitl_entity.type,\n",
    "            \"|\",\n",
    "            hitl_entity.mention_text,\n",
    "        )\n",
    "        inline_document.entities.append(\n",
    "            hitl_entity)  # add new entity to the output\n",
    "        # Write logic to save the output\n",
    "    else:\n",
    "        print(operation_id, \"|\", hitl_status)\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m104",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m104"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
