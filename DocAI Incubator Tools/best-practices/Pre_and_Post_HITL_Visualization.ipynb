{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12ccafbd-d094-4604-9f85-975f709f7038",
   "metadata": {},
   "source": [
    "# Pre and Post HITL Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370b84a9-28f8-4e8b-9dd4-8b8363e59282",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270ddad0-5d76-4dd2-a863-74c3eb4effba",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the DocAI Incubator Team. No guarantees of performance are implied.\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351f231a-bd30-4ee3-a617-c12bf2261d57",
   "metadata": {},
   "source": [
    "## Purpose of the script\n",
    "This tool uses Pre-HITL JSON files (Parsed from a processor) and Post HITL JSON files(Updated through HITL) from GCS bucket as input, compares the Json files and differences are shown in an Excel with bounding boxes added images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30079506-4bc3-462b-aecb-dd95a1f1958a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prerequisite\n",
    " * Vertex AI Notebook\n",
    " * Pre HITL and Post HITL Json files (filename should be same) in GCS Folders\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418be9ca-916f-41a1-9196-800401bddd9d",
   "metadata": {},
   "source": [
    "## Step by Step procedure \n",
    "\n",
    "**1. Config file Creation**  \\\n",
    "    Run the below code and create a config.ini file for providing input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f46bad-fc9d-4439-9284-67265b519b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "# Add the structure to the file we will create\n",
    "config.add_section(\"Parameters\")\n",
    "config.set(\"Parameters\", \"project_id\", \"xxxx-xxxx-xxxx\")\n",
    "config.set(\"Parameters\", \"Pre_HITL_Output_URI\", \"gs://\")\n",
    "config.set(\"Parameters\", \"Post_HITL_Output_URI\", \"gs://\")\n",
    "# Write the new structure to the new file\n",
    "with open(r\"configfile.ini\", \"w\") as configfile:\n",
    "    config.write(configfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364c5f84-6284-4c39-a422-51c1f1109028",
   "metadata": {},
   "source": [
    "**2. Input Details**  \n",
    "\n",
    "Once **config.ini** file is created with the above step , enter the input in the config file with necessary details as below\n",
    " * project_id: provide the project id\n",
    " * Pre_HITL_Output_URI: provide the gcs path of pre HITL jsons (processed jsons)\n",
    " * Post_HITL_Output_URI: provide the gcs path of post HITL jsons (Jsons processed thru HITL)\n",
    " \n",
    "![](https://screenshot.googleplex.com/7DMhDW8d5GZnUBG.png)\n",
    "\n",
    "**NOTE:** The Name of Post-HITL Json will not be the same as the original file name by default. This has to be updated manually before using this tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6793a405-73d5-466c-bb7a-10f219bbd6dc",
   "metadata": {},
   "source": [
    "**3. Run the Code**\n",
    "\n",
    "Copy the code provided in this document, Enter the path of Config file and Run without any edits\n",
    "![](https://screenshot.googleplex.com/BP8v3wHicSEs6xr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceae5bee-4d39-460b-a7ef-5071e4d140ca",
   "metadata": {},
   "source": [
    "**4. Output** \n",
    "\n",
    "The output of the tool will be in an Excel format showing the entities which are updated in HITL and unchanged as well with images of labeled docs (both pre and post HITL).\n",
    "\n",
    "The Excel sheet which is created will have a summary of all the file files in “Consolidated_Data” and comparison in a separate sheet for each file.\n",
    "\n",
    "Each Excel sheet created will have  a batch of 20 files in it.\n",
    "\n",
    "![](https://screenshot.googleplex.com/6nL7E3hrRSEi6ST.png)\n",
    "\n",
    "The Excel file will have all the details of Pre-HITL text, Post-HITL text and whether the entity is updated in HITL in the form YES and NO as shown below .\n",
    "\n",
    "![](https://screenshot.googleplex.com/8wqPTMyUY5ASKZA.png)\n",
    "\n",
    "There will be a list of documents for which either the required confidence threshold is met or no HITL output is created yet is updated as “NO POST HITL OUTPUT AVAILABLE” at the end of excel in consolidated sheets.\n",
    "\n",
    "![](https://screenshot.googleplex.com/8tpFZsVfFdTBoKA.png)\n",
    "\n",
    "\n",
    "Blue Bounding Box⇒ Entities in Pre-HITL Json\n",
    "Red Bounding Box⇒ Entities updated in HITL\n",
    "Green Bounding Box⇒ Entities deleted in HITL( Entities which are detected by parser are deleted in HITL)\n",
    "\n",
    "**Bounding box color coding in images**\n",
    "\n",
    "![](https://screenshot.googleplex.com/9aph7w2N2vywPFP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8a8f42-9d73-40f1-a356-39de0853cf33",
   "metadata": {},
   "source": [
    "## **Sample Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5352a2-cc8c-480e-8336-0e886b4cac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install below libraries for one time\n",
    "\n",
    "#!pip install configparser\n",
    "#!pip install google.cloud\n",
    "#!pip install ast\n",
    "#!pip install openpyxl\n",
    "\n",
    "import ast\n",
    "import configparser\n",
    "import difflib\n",
    "import io\n",
    "import json\n",
    "import operator\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from collections.abc import Container, Iterable, Iterator, Mapping, Sequence\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import cv2\n",
    "import gcsfs\n",
    "import numpy\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "# installing libraries\n",
    "import pandas as pd\n",
    "from google.cloud import documentai_v1beta3, storage\n",
    "from PIL import Image, ImageDraw\n",
    "from PyPDF2 import PdfFileReader\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# input\n",
    "Path = \"configfile.ini\"  # Enter the path of config file\n",
    "config = configparser.ConfigParser()\n",
    "config.read(Path)\n",
    "\n",
    "project_id = config.get(\"Parameters\", \"project_id\")\n",
    "pre_HITL_output_URI = config.get(\"Parameters\", \"pre_hitl_output_uri\")\n",
    "post_HITL_output_URI = config.get(\"Parameters\", \"post_hitl_output_uri\")\n",
    "\n",
    "# FUNCTIONS\n",
    "\n",
    "\n",
    "# checking whether bucket exists else create temperary bucket\n",
    "def check_create_bucket(bucket_name):\n",
    "    \"\"\"This Function is to create a temperary bucket\n",
    "    for storing the processed files\n",
    "    args: name of bucket\"\"\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    try:\n",
    "        bucket = storage_client.get_bucket(bucket_name)\n",
    "        print(f\"Bucket {bucket_name} already exists.\")\n",
    "    except:\n",
    "        bucket = storage_client.create_bucket(bucket_name)\n",
    "        print(f\"Bucket {bucket_name} created.\")\n",
    "    return bucket\n",
    "\n",
    "\n",
    "def bucket_delete(bucket_name):\n",
    "    \"\"\"This function deltes the bucket and used for deleting the temporary\n",
    "    bucket\n",
    "    args: bucket name\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    try:\n",
    "        bucket = storage_client.get_bucket(bucket_name)\n",
    "        bucket.delete(force=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def file_names(file_path):\n",
    "    \"\"\"This Function will load the bucket and get the list of files\n",
    "    in the gs path given\n",
    "    args: gs path\n",
    "    output: file names as list and dictionary with file names as keys and file path as values\n",
    "    \"\"\"\n",
    "    bucket = file_path.split(\"/\")[2]\n",
    "    file_names_list = []\n",
    "    file_dict = {}\n",
    "    storage_client = storage.Client()\n",
    "    source_bucket = storage_client.get_bucket(bucket)\n",
    "    filenames = [\n",
    "        filename.name for filename in list(\n",
    "            source_bucket.list_blobs(\n",
    "                prefix=((\"/\").join(file_path.split(\"/\")[3:]))))\n",
    "    ]\n",
    "    for i in range(len(filenames)):\n",
    "        x = filenames[i].split(\"/\")[-1]\n",
    "        if x != \"\":\n",
    "            file_names_list.append(x)\n",
    "            file_dict[x] = filenames[i]\n",
    "    return file_names_list, file_dict\n",
    "\n",
    "\n",
    "# list\n",
    "def list_blobs(bucket_name):\n",
    "    \"\"\"This function will give the list of files in a bucket\n",
    "    args: gcs bucket name\n",
    "    output: list of files\"\"\"\n",
    "    blob_list = []\n",
    "    storage_client = storage.Client()\n",
    "    blobs = storage_client.list_blobs(bucket_name)\n",
    "    for blob in blobs:\n",
    "        blob_list.append(blob.name)\n",
    "    return blob_list\n",
    "\n",
    "\n",
    "# Bucket operations\n",
    "def relation_dict_generator(pre_hitl_output_bucket, post_hitl_output_bucket):\n",
    "    \"\"\"This Function will check the files from pre_hitl_output_bucket and post_hitl_output_bucket\n",
    "    and finds the json with same names(relation)\"\"\"\n",
    "    pre_hitl_bucket_blobs = list_blobs(pre_hitl_output_bucket)\n",
    "    post_hitl_bucket_blobs = list_blobs(post_hitl_output_bucket)\n",
    "\n",
    "    relation_dict = {}\n",
    "    non_relation_dict = {}\n",
    "    for i in pre_hitl_bucket_blobs:\n",
    "        for j in post_hitl_bucket_blobs:\n",
    "            matched_score = difflib.SequenceMatcher(None, i, j).ratio()\n",
    "            if matched_score > 0.9:\n",
    "                relation_dict[i] = j\n",
    "            else:\n",
    "                non_relation_dict[i] = \"NO POST HITL OUTPUT AVAILABLE\"\n",
    "                # print(i)\n",
    "    for i in relation_dict:\n",
    "        if i in non_relation_dict.keys():\n",
    "            del non_relation_dict[i]\n",
    "\n",
    "    return relation_dict, non_relation_dict\n",
    "\n",
    "\n",
    "def blob_downloader(bucket_name, blob_name):\n",
    "    \"\"\"This Function is used to download the files from gcs bucket\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    contents = blob.download_as_string()\n",
    "    return json.loads(contents.decode())\n",
    "\n",
    "\n",
    "def copy_blob(bucket_name, blob_name, destination_bucket_name,\n",
    "              destination_blob_name):\n",
    "    \"\"\"This Method will copy files from one bucket(or folder) to another\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    source_bucket = storage_client.bucket(bucket_name)\n",
    "    source_blob = source_bucket.blob(blob_name)\n",
    "    destination_bucket = storage_client.bucket(destination_bucket_name)\n",
    "    blob_copy = source_bucket.copy_blob(source_blob, destination_bucket,\n",
    "                                        destination_blob_name)\n",
    "\n",
    "\n",
    "def bbox_maker(boundingPoly):\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    for i in boundingPoly:\n",
    "        x_list.append(i[\"x\"])\n",
    "        y_list.append(i[\"y\"])\n",
    "    bbox = [min(x_list), min(y_list), max(x_list), max(y_list)]\n",
    "    return bbox\n",
    "\n",
    "\n",
    "def JsonToDataframe(data):\n",
    "    \"\"\"Returns entities in dataframe format\"\"\"\n",
    "    df = pd.DataFrame(columns=[\"type\", \"mentionText\", \"bbox\", \"page\"])\n",
    "\n",
    "    if \"entities\" not in data.keys():\n",
    "        return df\n",
    "\n",
    "    for entity in data[\"entities\"]:\n",
    "        if \"properties\" in entity and len(entity[\"properties\"]) > 0:\n",
    "            for sub_entity in entity[\"properties\"]:\n",
    "                if \"type\" in sub_entity:\n",
    "                    try:\n",
    "                        boundingPoly = sub_entity[\"pageAnchor\"][\"pageRefs\"][0][\n",
    "                            \"boundingPoly\"][\"normalizedVertices\"]\n",
    "                        bbox = bbox_maker(boundingPoly)\n",
    "                        # page=sub_entity['pageAnchor']['pageRefs'][0]['page']\n",
    "                        # bbox = [boundingPoly[0]['x'], boundingPoly[0]['y'], boundingPoly[2]['x'], boundingPoly[2]['y']]\n",
    "                        # df.loc[len(df.index)] = [sub_entity['type'], sub_entity['mentionText'], bbox]\n",
    "                        try:\n",
    "                            page = sub_entity[\"pageAnchor\"][\"pageRefs\"][0][\n",
    "                                \"page\"]\n",
    "                            df.loc[len(df.index)] = [\n",
    "                                sub_entity[\"type\"],\n",
    "                                sub_entity[\"mentionText\"],\n",
    "                                bbox,\n",
    "                                page,\n",
    "                            ]\n",
    "                        except KeyError:\n",
    "                            df.loc[len(df.index)] = [\n",
    "                                sub_entity[\"type\"],\n",
    "                                sub_entity[\"mentionText\"],\n",
    "                                bbox,\n",
    "                                \"0\",\n",
    "                            ]\n",
    "                    except KeyError:\n",
    "                        if \"mentionText\" in sub_entity:\n",
    "                            df.loc[len(df.index)] = [\n",
    "                                sub_entity[\"type\"],\n",
    "                                sub_entity[\"mentionText\"],\n",
    "                                [],\n",
    "                                \"no\",\n",
    "                            ]\n",
    "                        else:\n",
    "                            df.loc[len(df.index)] = [\n",
    "                                sub_entity[\"type\"],\n",
    "                                \"Entity not found.\",\n",
    "                                [],\n",
    "                                \"no\",\n",
    "                            ]\n",
    "        elif \"type\" in entity:\n",
    "            try:\n",
    "                boundingPoly = entity[\"pageAnchor\"][\"pageRefs\"][0][\n",
    "                    \"boundingPoly\"][\"normalizedVertices\"]\n",
    "                bbox = bbox_maker(boundingPoly)\n",
    "                # bbox = [boundingPoly[0]['x'], boundingPoly[0]['y'], boundingPoly[2]['x'], boundingPoly[2]['y']]\n",
    "                # df.loc[len(df.index)] = [entity['type'], entity['mentionText'], bbox]\n",
    "                try:\n",
    "                    page = entity[\"pageAnchor\"][\"pageRefs\"][0][\"page\"]\n",
    "                    df.loc[len(df.index)] = [\n",
    "                        entity[\"type\"],\n",
    "                        entity[\"mentionText\"],\n",
    "                        bbox,\n",
    "                        page,\n",
    "                    ]\n",
    "                except KeyError:\n",
    "                    df.loc[len(df.index)] = [\n",
    "                        entity[\"type\"],\n",
    "                        entity[\"mentionText\"],\n",
    "                        bbox,\n",
    "                        \"0\",\n",
    "                    ]\n",
    "\n",
    "            except KeyError:\n",
    "                if \"mentionText\" in entity:\n",
    "                    df.loc[len(df.index)] = [\n",
    "                        entity[\"type\"],\n",
    "                        entity[\"mentionText\"],\n",
    "                        [],\n",
    "                        \"no\",\n",
    "                    ]\n",
    "                else:\n",
    "                    df.loc[len(df.index)] = [\n",
    "                        entity[\"type\"],\n",
    "                        \"Entity not found.\",\n",
    "                        [],\n",
    "                        \"no\",\n",
    "                    ]\n",
    "    return df\n",
    "\n",
    "\n",
    "def RemoveRow(df, entity):\n",
    "    \"\"\"Drops the entity passed from the dataframe\"\"\"\n",
    "    return df[df[\"type\"] != entity]\n",
    "\n",
    "\n",
    "def FindMatch(entity_file1, df_file2):\n",
    "    \"\"\"Finds the matching entity from the dataframe using\n",
    "    the area of IOU between bboxes reference\n",
    "    \"\"\"\n",
    "    bbox_file1 = entity_file1[2]\n",
    "    # Entity not present in json file\n",
    "    if not bbox_file1:\n",
    "        return None\n",
    "\n",
    "    # filtering entities with the same name\n",
    "    df_file2 = df_file2[df_file2[\"type\"] == entity_file1[0]]\n",
    "\n",
    "    # calculating IOU values for the entities\n",
    "    index_iou_pairs = []\n",
    "    for index, entity_file2 in enumerate(df_file2.values):\n",
    "        if entity_file2[2]:\n",
    "            iou = BBIntersectionOverUnion(bbox_file1, entity_file2[2])\n",
    "            index_iou_pairs.append((index, iou))\n",
    "\n",
    "    # choose entity with highest IOU, IOU should be atleast > 0.5\n",
    "    matched_index = None\n",
    "    for index_iou in sorted(index_iou_pairs,\n",
    "                            key=operator.itemgetter(1),\n",
    "                            reverse=True):\n",
    "        if index_iou[1] > 0.5:\n",
    "            matched_index = df_file2.index[index_iou[0]]\n",
    "            break\n",
    "    return matched_index\n",
    "\n",
    "\n",
    "def BBIntersectionOverUnion(box1, box2):\n",
    "    \"\"\"Calculates the area of IOU between two bounding boxes\"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "\n",
    "    inter_area = abs(max((x2 - x1, 0)) * max((y2 - y1), 0))\n",
    "    if inter_area == 0:\n",
    "        return 0\n",
    "    box1_area = abs((box1[2] - box1[0]) * (box1[3] - box1[1]))\n",
    "    box2_area = abs((box2[2] - box2[0]) * (box2[3] - box2[1]))\n",
    "    iou = inter_area / float(box1_area + box2_area - inter_area)\n",
    "\n",
    "    return iou\n",
    "\n",
    "\n",
    "def GetMatchRatio(values):\n",
    "    file1_value = values[1]\n",
    "    file2_value = values[2]\n",
    "    if file1_value == \"Entity not found.\" or file2_value == \"Entity not found.\":\n",
    "        return 0\n",
    "    else:\n",
    "        return difflib.SequenceMatcher(a=file1_value, b=file2_value).ratio()\n",
    "\n",
    "\n",
    "def compare_pre_hitl_and_post_hitl_output(file1, file2):\n",
    "    \"\"\"Compares the entities between two files and returns\n",
    "    the results in a dataframe\n",
    "    \"\"\"\n",
    "    df_file1 = JsonToDataframe(file1)\n",
    "    df_file2 = JsonToDataframe(file2)\n",
    "    # df_file1.to_csv(\"1.csv\")\n",
    "    # df_file2.to_csv(\"2.csv\")\n",
    "    file1_entities = [entity[0] for entity in df_file1.values]\n",
    "    file2_entities = [entity[0] for entity in df_file2.values]\n",
    "\n",
    "    # find entities which are present only once in both files\n",
    "    # these entities will be matched directly\n",
    "    common_entities = set(file1_entities).intersection(set(file2_entities))\n",
    "    exclude_entities = []\n",
    "    for entity in common_entities:\n",
    "        if file1_entities.count(entity) > 1 or file2_entities.count(\n",
    "                entity) > 1:\n",
    "            exclude_entities.append(entity)\n",
    "    for entity in exclude_entities:\n",
    "        common_entities.remove(entity)\n",
    "    df_compare = pd.DataFrame(columns=[\n",
    "        \"Entity Type\",\n",
    "        \"Pre_HITL_Output\",\n",
    "        \"Post_HITL_Output\",\n",
    "        \"pre_bbox\",\n",
    "        \"post_bbox\",\n",
    "        \"page1\",\n",
    "        \"page2\",\n",
    "    ])\n",
    "    for entity in common_entities:\n",
    "        value1 = df_file1[df_file1[\"type\"] == entity].iloc[0][\"mentionText\"]\n",
    "        value2 = df_file2[df_file2[\"type\"] == entity].iloc[0][\"mentionText\"]\n",
    "        pre_bbox = df_file1[df_file1[\"type\"] == entity].iloc[0][\"bbox\"]\n",
    "        post_bbox = df_file2[df_file2[\"type\"] == entity].iloc[0][\"bbox\"]\n",
    "        page1 = df_file1[df_file1[\"type\"] == entity].iloc[0][\"page\"]\n",
    "        page2 = df_file2[df_file2[\"type\"] == entity].iloc[0][\"page\"]\n",
    "        df_compare.loc[len(df_compare.index)] = [\n",
    "            entity,\n",
    "            value1,\n",
    "            value2,\n",
    "            pre_bbox,\n",
    "            post_bbox,\n",
    "            page1,\n",
    "            page2,\n",
    "        ]\n",
    "        # common entities are removed from df_file1 and df_file2\n",
    "        df_file1 = RemoveRow(df_file1, entity)\n",
    "        df_file2 = RemoveRow(df_file2, entity)\n",
    "\n",
    "    # remaining entities are matched comparing the area of IOU across them\n",
    "    mentionText2 = pd.Series(dtype=str)\n",
    "    bbox2 = pd.Series(dtype=object)\n",
    "    bbox1 = pd.Series(dtype=object)\n",
    "    page_1 = pd.Series(dtype=object)\n",
    "    page_2 = pd.Series(dtype=object)\n",
    "\n",
    "    for index, row in enumerate(df_file1.values):\n",
    "        matched_index = FindMatch(row, df_file2)\n",
    "        if matched_index != None:\n",
    "            mentionText2.loc[index] = df_file2.loc[matched_index][1]\n",
    "            bbox2.loc[index] = df_file2.loc[matched_index][2]\n",
    "            bbox1.loc[index] = row[2]\n",
    "            page_2.loc[index] = df_file2.loc[matched_index][3]\n",
    "            page_1.loc[index] = row[3]\n",
    "            df_file2 = df_file2.drop(matched_index)\n",
    "        else:\n",
    "            mentionText2.loc[index] = \"Entity not found.\"\n",
    "            bbox2.loc[index] = \"Entity not found.\"\n",
    "            bbox1.loc[index] = row[2]\n",
    "            page_1.loc[index] = row[3]\n",
    "            page_2.loc[index] = \"no\"\n",
    "\n",
    "    df_file1[\"mentionText2\"] = mentionText2.values\n",
    "    df_file1[\"bbox2\"] = bbox2.values\n",
    "    df_file1[\"bbox1\"] = bbox1.values\n",
    "    df_file1[\"page_1\"] = page_1.values\n",
    "    df_file1[\"page_2\"] = page_2.values\n",
    "\n",
    "    df_file1 = df_file1.drop([\"bbox\"], axis=1)\n",
    "    df_file1 = df_file1.drop([\"page\"], axis=1)\n",
    "    df_file1.rename(\n",
    "        columns={\n",
    "            \"type\": \"Entity Type\",\n",
    "            \"mentionText\": \"Pre_HITL_Output\",\n",
    "            \"mentionText2\": \"Post_HITL_Output\",\n",
    "            \"bbox1\": \"pre_bbox\",\n",
    "            \"bbox2\": \"post_bbox\",\n",
    "            \"page_1\": \"page1\",\n",
    "            \"page_2\": \"page2\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "    df_compare = df_compare.append(df_file1, ignore_index=True)\n",
    "    # adding entities which are present in file2 but not in file1\n",
    "    for row in df_file2.values:\n",
    "        df_compare.loc[len(df_compare.index)] = [\n",
    "            row[0],\n",
    "            \"Entity not found.\",\n",
    "            row[1],\n",
    "            \"[]\",\n",
    "            row[2],\n",
    "            \"[]\",\n",
    "            row[3],\n",
    "        ]\n",
    "\n",
    "    # df_compare['Match'] = df_compare['Ground Truth Text'] == df_compare['Output Text']\n",
    "    match_array = []\n",
    "    for i in range(0, len(df_compare)):\n",
    "        match_string = \"\"\n",
    "        if (df_compare.iloc[i][\"Pre_HITL_Output\"] == \"Entity not found.\" and\n",
    "                df_compare.iloc[i][\"Post_HITL_Output\"] == \"Entity not found.\"):\n",
    "            match_string = \"TN\"\n",
    "        elif (df_compare.iloc[i][\"Pre_HITL_Output\"] != \"Entity not found.\" and\n",
    "              df_compare.iloc[i][\"Post_HITL_Output\"] == \"Entity not found.\"):\n",
    "            match_string = \"FN\"\n",
    "        elif (df_compare.iloc[i][\"Pre_HITL_Output\"] == \"Entity not found.\" and\n",
    "              df_compare.iloc[i][\"Post_HITL_Output\"] != \"Entity not found.\"):\n",
    "            match_string = \"FP\"\n",
    "        elif (df_compare.iloc[i][\"Pre_HITL_Output\"] != \"Entity not found.\" and\n",
    "              df_compare.iloc[i][\"Post_HITL_Output\"] != \"Entity not found.\"):\n",
    "            if (df_compare.iloc[i][\"Pre_HITL_Output\"] == df_compare.iloc[i]\n",
    "                [\"Post_HITL_Output\"]):\n",
    "                match_string = \"TP\"\n",
    "            else:\n",
    "                match_string = \"FP\"\n",
    "        else:\n",
    "            match_string = \"Something went Wrong.\"\n",
    "\n",
    "        match_array.append(match_string)\n",
    "\n",
    "    df_compare[\"Match\"] = match_array\n",
    "\n",
    "    df_compare[\"Fuzzy Ratio\"] = df_compare.apply(GetMatchRatio, axis=1)\n",
    "    if list(df_compare.index):\n",
    "        score = df_compare[\"Fuzzy Ratio\"].sum() / len(df_compare.index)\n",
    "    else:\n",
    "        score = 0\n",
    "    return df_compare, score\n",
    "\n",
    "\n",
    "def create_pdf_bytes(path):\n",
    "    \"\"\"THis Function will create pdf bytes from the image\n",
    "    content of the ground truth JSONS which will be used for processing of files\n",
    "    args: gs path of json file\n",
    "    output : pdf bytes\"\"\"\n",
    "\n",
    "    def decode_image(image_bytes: bytes) -> Image.Image:\n",
    "        with io.BytesIO(image_bytes) as image_file:\n",
    "            image = Image.open(image_file)\n",
    "            image.load()\n",
    "        return image\n",
    "\n",
    "    def create_pdf_from_images(images: Sequence[Image.Image]) -> bytes:\n",
    "        \"\"\"Creates a PDF from a sequence of images.\n",
    "\n",
    "        The PDF will contain 1 page per image, in the same order.\n",
    "\n",
    "        Args:\n",
    "          images: A sequence of images.\n",
    "\n",
    "        Returns:\n",
    "          The PDF bytes.\n",
    "        \"\"\"\n",
    "        if not images:\n",
    "            raise ValueError(\"At least one image is required to create a PDF\")\n",
    "\n",
    "        # PIL PDF saver does not support RGBA images\n",
    "        images = [\n",
    "            image.convert(\"RGB\") if image.mode == \"RGBA\" else image\n",
    "            for image in images\n",
    "        ]\n",
    "\n",
    "        with io.BytesIO() as pdf_file:\n",
    "            images[0].save(pdf_file,\n",
    "                           save_all=True,\n",
    "                           append_images=images[1:],\n",
    "                           format=\"PDF\")\n",
    "            return pdf_file.getvalue()\n",
    "\n",
    "    d = documentai_v1beta3.Document\n",
    "    document = d.from_json(fs.cat(path))\n",
    "    synthesized_images = []\n",
    "    for i in range(len(document.pages)):\n",
    "        synthesized_images.append(decode_image(\n",
    "            document.pages[i].image.content))\n",
    "    pdf_bytes = create_pdf_from_images(synthesized_images)\n",
    "\n",
    "    return pdf_bytes, synthesized_images\n",
    "\n",
    "\n",
    "def find_excel_name():\n",
    "    i = 1\n",
    "    excel_file_name = \"HITL_VISUAL\" + str(i) + \".xlsx\"\n",
    "    comapare_analysis = compare_merged.drop(\n",
    "        [\"pre_bbox\", \"post_bbox\", \"page1\", \"page2\"], axis=1)\n",
    "    try:\n",
    "        workbook = openpyxl.load_workbook(excel_file_name)\n",
    "        num_sheets = len(workbook.sheetnames)\n",
    "        # print(num_sheets)\n",
    "        if num_sheets > 20:\n",
    "            excel_file = \"HITL_VISUAL\" + str(i + 1) + \".xlsx\"\n",
    "            comapare_analysis.to_excel(excel_file,\n",
    "                                       sheet_name=\"Consolidated_Data\")\n",
    "        else:\n",
    "            excel_file = \"HITL_VISUAL\" + str(i) + \".xlsx\"\n",
    "    except FileNotFoundError:\n",
    "        excel_file = \"HITL_VISUAL\" + str(i) + \".xlsx\"\n",
    "        comapare_analysis.to_excel(excel_file, sheet_name=\"Consolidated_Data\")\n",
    "    return excel_file\n",
    "\n",
    "\n",
    "def get_visualization_excel(pre_HITL_output_URI, compare_merged,\n",
    "                            relation_dict):\n",
    "    # compare_merged.to_excel(\"HITL_VISUAL1.xlsx\",sheet_name='Consolidated_Data')\n",
    "    pre_HITL_bucket = pre_HITL_output_URI.split(\"/\")[2]\n",
    "    pre_HITL_output_files, pre_HITL_output_dict = file_names(\n",
    "        pre_HITL_output_URI)\n",
    "\n",
    "    for file in pre_HITL_output_dict:\n",
    "        excel_file = find_excel_name()\n",
    "        df = compare_merged.drop([\"pre_bbox\", \"post_bbox\", \"page1\", \"page2\"],\n",
    "                                 axis=1)\n",
    "        if file in relation_dict.keys():\n",
    "            df_file = df[df[\"File Name\"] == file]\n",
    "            with pd.ExcelWriter(excel_file, engine=\"openpyxl\",\n",
    "                                mode=\"a\") as writer:\n",
    "                df_file.to_excel(writer, sheet_name=str(file))\n",
    "\n",
    "            path = \"gs://\" + pre_HITL_bucket + \"/\" + pre_HITL_output_dict[file]\n",
    "            pdf_bytes, synthesized_images = create_pdf_bytes(path)\n",
    "            list_bbox_no = {}\n",
    "            list_bbox_yes_changed = {}\n",
    "            list_bbox_yes_old = {}\n",
    "            for row in compare_merged.values:\n",
    "                if row[0] == file:\n",
    "                    if row[8] == \"NO\":\n",
    "                        if type(row[4]) == list and row[4] != []:\n",
    "                            try:\n",
    "                                if row[6] in list_bbox_no.keys():\n",
    "                                    list_bbox_no[row[6]].append(row[4])\n",
    "                                else:\n",
    "                                    list_bbox_no[row[6]] = [row[4]]\n",
    "                                # print({row[6]:row[4]})\n",
    "                            except:\n",
    "                                pass\n",
    "                    elif row[8] == \"YES\":\n",
    "                        if type(row[5]) == list and row[5] != []:\n",
    "                            try:\n",
    "                                if row[7] in list_bbox_yes_changed.keys():\n",
    "                                    list_bbox_yes_changed[row[7]].append(\n",
    "                                        row[5])\n",
    "                                else:\n",
    "                                    list_bbox_yes_changed[row[7]] = [row[5]]\n",
    "\n",
    "                            except:\n",
    "                                pass\n",
    "                        elif type(row[4]) == list and row[4] != []:\n",
    "                            if row[6] in list_bbox_yes_old.keys():\n",
    "                                list_bbox_yes_old[row[6]].append(row[4])\n",
    "                            else:\n",
    "                                list_bbox_yes_old[row[6]] = [row[4]]\n",
    "\n",
    "            open_cv_image = {}\n",
    "            for i in range(len(synthesized_images)):\n",
    "                open_cv_image[i] = numpy.array(\n",
    "                    synthesized_images[i].convert(\"RGB\"))\n",
    "            # print(list_bbox_yes_changed)\n",
    "            img_list = []\n",
    "            for i in range(len(open_cv_image)):\n",
    "                size = open_cv_image[i].shape\n",
    "                try:\n",
    "                    for bbox in list_bbox_no[str(i)]:\n",
    "                        x1 = int(bbox[0] * size[1])\n",
    "                        x2 = int(bbox[2] * size[1])\n",
    "                        y1 = int(bbox[1] * size[0])\n",
    "                        y2 = int(bbox[3] * size[0])\n",
    "                        # print(bbox[0]*size[0])\n",
    "                        cv2.rectangle(open_cv_image[i], (x1, y1), (x2, y2),\n",
    "                                      (0, 0, 255), 2)\n",
    "                        # cv2.putText(open_cv_image[i],'He',(x1,y1),font,2,(255,255,255),1)\n",
    "\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    for bbox in list_bbox_yes_changed[str(i)]:\n",
    "                        x1 = int(bbox[0] * size[1])\n",
    "                        x2 = int(bbox[2] * size[1])\n",
    "                        y1 = int(bbox[1] * size[0])\n",
    "                        y2 = int(bbox[3] * size[0])\n",
    "                        cv2.rectangle(open_cv_image[i], (x1, y1), (x2, y2),\n",
    "                                      (255, 0, 0), 2)\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    for bbox in list_bbox_yes_old[str(i)]:\n",
    "                        x1 = int(bbox[0] * size[1])\n",
    "                        x2 = int(bbox[2] * size[1])\n",
    "                        y1 = int(bbox[1] * size[0])\n",
    "                        y2 = int(bbox[3] * size[0])\n",
    "                        cv2.rectangle(open_cv_image[i], (x1, y1), (x2, y2),\n",
    "                                      (0, 255, 0), 2)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                img1 = Image.fromarray(open_cv_image[i])\n",
    "                # img_list.append(img)\n",
    "                # img.save(file+str(i)+'.png')\n",
    "                # img.show()\n",
    "                import openpyxl\n",
    "\n",
    "                workbook = openpyxl.load_workbook(excel_file)\n",
    "                worksheet = workbook[str(file)]\n",
    "\n",
    "                img1.save(f\"open_cv_image[i].png\", \"PNG\")\n",
    "                img = openpyxl.drawing.image.Image(f\"open_cv_image[i].png\")\n",
    "                # if len(open_cv_image)>0:\n",
    "                #     for i in open_cv_image:\n",
    "                #         img.anchor = 'K'+str(int(i)*200)\n",
    "                img.anchor = \"K\" + str(1 + int(i) * 50)\n",
    "                worksheet.add_image(img)\n",
    "                img.width = 500\n",
    "                img.height = 700\n",
    "                workbook.save(excel_file)\n",
    "\n",
    "\n",
    "# Execute the below code\n",
    "\n",
    "pre_HITL_output_URI = config.get(\"Parameters\", \"pre_hitl_output_uri\")\n",
    "post_HITL_output_URI = config.get(\"Parameters\", \"post_hitl_output_uri\")\n",
    "\n",
    "try:\n",
    "    # creating temperary buckets\n",
    "    import datetime\n",
    "\n",
    "    now = str(datetime.datetime.now())\n",
    "    now = re.sub(r\"\\W+\", \"\", now)\n",
    "\n",
    "    print(\"Creating temporary buckets\")\n",
    "    pre_HITL_bucket_name_temp = \"pre_hitl_output\" + \"_\" + now\n",
    "    post_HITL_bucket_name_temp = \"post_hitl_output_temp\" + \"_\" + now\n",
    "    # bucket name and prefix\n",
    "    pre_HITL_bucket = pre_HITL_output_URI.split(\"/\")[2]\n",
    "    post_HITL_bucket = post_HITL_output_URI.split(\"/\")[2]\n",
    "    # getting all files and copying to temporary folder\n",
    "\n",
    "    try:\n",
    "        check_create_bucket(pre_HITL_bucket_name_temp)\n",
    "        check_create_bucket(post_HITL_bucket_name_temp)\n",
    "    except Exception as e:\n",
    "        print(\"unable to create bucket because of exception : \", e)\n",
    "\n",
    "    try:\n",
    "        pre_HITL_output_files, pre_HITL_output_dict = file_names(\n",
    "            pre_HITL_output_URI)\n",
    "        post_HITL_output_files, post_HITL_output_dict = file_names(\n",
    "            post_HITL_output_URI)\n",
    "        print(\"copying files to temporary bucket\")\n",
    "        for i in pre_HITL_output_files:\n",
    "            copy_blob(pre_HITL_bucket, pre_HITL_output_dict[i],\n",
    "                      pre_HITL_bucket_name_temp, i)\n",
    "        for i in post_HITL_output_files:\n",
    "            copy_blob(\n",
    "                post_HITL_bucket,\n",
    "                post_HITL_output_dict[i],\n",
    "                post_HITL_bucket_name_temp,\n",
    "                i,\n",
    "            )\n",
    "        pre_HITL_files_list = list_blobs(pre_HITL_bucket_name_temp)\n",
    "        post_HITL_files_list = list_blobs(post_HITL_bucket_name_temp)\n",
    "    except Exception as e:\n",
    "        print(\"unable to get list of files in buckets because : \", e)\n",
    "    # processing the files and saving the files in temporary gCP bucket\n",
    "    fs = gcsfs.GCSFileSystem(project_id)\n",
    "    relation_dict, non_relation_dict = relation_dict_generator(\n",
    "        pre_HITL_bucket_name_temp, post_HITL_bucket_name_temp)\n",
    "    compare_merged = pd.DataFrame()\n",
    "    accuracy_docs = []\n",
    "    print(\n",
    "        \"comparing the PRE-HITL Jsons and POST-HITL jsons ....Wait for Summary \"\n",
    "    )\n",
    "    for i in relation_dict:\n",
    "        pre_HITL_json = blob_downloader(pre_HITL_bucket_name_temp, i)\n",
    "        post_HITL_json = blob_downloader(post_HITL_bucket_name_temp,\n",
    "                                         relation_dict[i])\n",
    "        compare_output = compare_pre_hitl_and_post_hitl_output(\n",
    "            pre_HITL_json, post_HITL_json)[0]\n",
    "        column = [relation_dict[i]] * compare_output.shape[0]\n",
    "        # print(column)\n",
    "        compare_output.insert(loc=0, column=\"File Name\", value=column)\n",
    "\n",
    "        compare_output.insert(loc=8, column=\"hitl_update\", value=\" \")\n",
    "        for j in range(len(compare_output)):\n",
    "            if compare_output[\"Fuzzy Ratio\"][j] != 1.0:\n",
    "                if (compare_output[\"Pre_HITL_Output\"][j] == \"Entity not found.\"\n",
    "                        and compare_output[\"Post_HITL_Output\"][j]\n",
    "                        == \"Entity not found.\"):\n",
    "                    compare_output[\"hitl_update\"][j] = \"NO\"\n",
    "                else:\n",
    "                    compare_output[\"hitl_update\"][j] = \"YES\"\n",
    "            else:\n",
    "                compare_output[\"hitl_update\"][j] = \"NO\"\n",
    "        for k in range(len(compare_output)):\n",
    "            if compare_output[\"Fuzzy Ratio\"][k] != 1.0:\n",
    "                hitl_update = \"HITL UPDATED\"\n",
    "                break\n",
    "            else:\n",
    "                compare_output[\"hitl_update\"][k] = \"NO\"\n",
    "\n",
    "        # new_row=pd.Series([i,\"Entities\",\"are updated\",\"by HITL\",\":\",np.nan,hitl_update], index=compare_output.columns)\n",
    "        # compare_output=compare_output.append(new_row,ignore_index= True)\n",
    "        frames = [compare_merged, compare_output]\n",
    "        compare_merged = pd.concat(frames)\n",
    "    try:\n",
    "        bucket_delete(pre_HITL_bucket_name_temp)\n",
    "        print(\"Deleting temperary buckets created\")\n",
    "        bucket_delete(post_HITL_bucket_name_temp)\n",
    "    except:\n",
    "        pass\n",
    "    compare_merged.drop([\"Match\", \"Fuzzy Ratio\"], axis=1, inplace=True)\n",
    "\n",
    "    def highlight(s):\n",
    "        if s.hitl_update == \"YES\":\n",
    "            return [\"background-color: yellow\"] * len(s)\n",
    "        else:\n",
    "            return [\"background-color: white\"] * len(s)\n",
    "\n",
    "    for k in non_relation_dict:\n",
    "        new_row = pd.Series(\n",
    "            [k, \"-\", \"-\", \"-\", \"\", \"\", \"\", \"\", non_relation_dict[k]],\n",
    "            index=compare_merged.columns,\n",
    "        )\n",
    "        compare_merged = compare_merged.append(new_row, ignore_index=True)\n",
    "        comapare_analysis1 = compare_merged.drop(\n",
    "            [\"pre_bbox\", \"post_bbox\", \"page1\", \"page2\"], axis=1)\n",
    "    # comapare_analysis1.to_csv('compare_analysis.csv')\n",
    "    entity_change = compare_merged.loc[compare_merged[\"hitl_update\"] == \"YES\"]\n",
    "    compare_merged_style = compare_merged.style.apply(highlight, axis=1)\n",
    "    try:\n",
    "        print(\"HITL Comparision excel is getting prepared\")\n",
    "        get_visualization_excel(pre_HITL_output_URI, compare_merged,\n",
    "                                relation_dict)\n",
    "        print(\"Completed creating the HITL Comparision Excel\")\n",
    "    except Exception as e:\n",
    "        print(\"Unable to create HITL comparision excel because of :\", e)\n",
    "except Exception as e:\n",
    "    try:\n",
    "        bucket_delete(pre_HITL_bucket_name_temp)\n",
    "        bucket_delete(post_HITL_bucket_name_temp)\n",
    "        print(\"unable to process the file   : \", e)\n",
    "    except:\n",
    "        print(\"unable to process the file   : \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95823f2c-a91b-4bb9-85aa-5d90f17fff05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m104",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m104"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
