{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72bc511c-2282-4d2b-a2d4-28d25c07ffa2",
   "metadata": {},
   "source": [
    "# DocAI - Script for Removing Empty Bounding Boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778291f7-ed93-4f0f-abc9-c9f82245cf23",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c92559f-6a39-4318-b8ae-064325723cc7",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the DocAI Incubator Team. No guarantees of performance are implied.\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8e2b95-c07f-40ac-bb50-93b20b143ded",
   "metadata": {},
   "source": [
    "## Purpose of the Script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a2c845-2df8-41f4-9e3d-59c559a3c4b4",
   "metadata": {},
   "source": [
    "\n",
    "The purpose of this document is to provide instructions and a Python script for removing empty bounding boxes from a labeled JSON file. The script identifies and removes any bounding boxes (entities) in the JSON file that do not contain any mentionText or textAnchors, streamlining the labeling process and improving the accuracy of the labeling data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fa7cdd-9654-42c2-8612-ea779a20a48a",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728a861b-21e4-4300-aef8-650cb390a271",
   "metadata": {},
   "source": [
    "1. Python : Jupyter notebook (Vertex AI) \n",
    "2. Service account permissions in projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fa8919-7096-4b92-8604-a4a6daa224f9",
   "metadata": {},
   "source": [
    "## Installation Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60803eb8-6452-49f2-992c-08465e55b770",
   "metadata": {},
   "source": [
    "The script consists of Python code. It can be loaded and run via: \n",
    "1.  Upload the IPYNB file or copy the code to the Vertex Notebook and follow the operation procedure. \\\n",
    "**NOTE:** Donâ€™t Execute the Script with Processor Dataset Path. Export the dataset to json and then use that bucket as an input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6993aead-9bb6-45a4-8fba-c3be04fd3322",
   "metadata": {},
   "source": [
    "##  Operation Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d18824a-111d-44b4-8df5-c8aadff24202",
   "metadata": {},
   "source": [
    "### 1. Import the modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ec98b1-f8fa-4bab-b2cb-5bd49b4c6c38",
   "metadata": {},
   "source": [
    "**Note :** external modules are used so they need to be installed. To install run these commands : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f321c289-fb5f-4fbe-901b-5e5725f71b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gcsfs\n",
    "!pip install google-cloud\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import gcsfs\n",
    "import google.auth\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d8482e-0450-498c-940c-e775037ef907",
   "metadata": {},
   "source": [
    "### 2. Setup the required inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6e1955-4667-49b8-b6fa-5eb06546d7e1",
   "metadata": {},
   "source": [
    "* **PROJECT_ID** - Your Google project id or name\n",
    "* **BUCKET_NAME** - Name of the bucket\n",
    "* **INPUT_FOLDER_PATH** - The path of the folder containing the JSON files to be processed, without the bucket name.\n",
    "* **OUTPUT_FOLDER_PATH** - The path of the folder where the JSON files need to be stored after process, without the bucket * name.\n",
    "\n",
    "**Note :**  Both Input and output paths should be in the same bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbc4b67-fbae-4e7b-8518-ddde8b2f30c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"xxxxxx-xxxxxx-xxxxx\"\n",
    "BUCKET_NAME = \"xxxxxxx\"\n",
    "INPUT_FOLDER_PATH = \"xxxxxxxx/xxxxxxxxx/xxxxxx\"  # Path without bucket name\n",
    "OUTPUT_FOLDER_PATH = \"xxxxxxxx/xxxxxxx/xxxxxxx/xxx\"  # Path without bucket name\n",
    "credentials, _ = google.auth.default()\n",
    "fs = gcsfs.GCSFileSystem(project=PROJECT_ID, token=credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5b0743-d63b-4960-a46d-f2ffd9bcd252",
   "metadata": {},
   "source": [
    "### 3. Execute the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e06c32a-74d9-497a-907b-2a37265a0984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file(file_path: str):\n",
    "    \"\"\"\n",
    "    To read files from cloud storage.\n",
    "    \"\"\"\n",
    "    file_object = json.loads(fs.cat(file_path))\n",
    "    return file_object\n",
    "\n",
    "\n",
    "def store_blob(document, file: str):\n",
    "    \"\"\"\n",
    "    Store files in cloud storage.\n",
    "    \"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    result_bucket = storage_client.get_bucket(BUCKET_NAME)\n",
    "    document_blob = storage.Blob(name=str(file), bucket=result_bucket)\n",
    "    document_blob.upload_from_string(json.dumps(document),\n",
    "                                     content_type=\"application/json\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    logs = pd.DataFrame(columns=[\"FileName\"])\n",
    "\n",
    "    files = [\n",
    "        i for i in fs.find(f\"{BUCKET_NAME}/{INPUT_FOLDER_PATH}\")\n",
    "        if i.endswith(\".json\")\n",
    "    ]\n",
    "    json_files_list = [get_file(i) for i in files]\n",
    "    print(\"No. of files : \", len(files))\n",
    "\n",
    "    for index in tqdm(range(len(files))):\n",
    "        file_name = files[index].split(\"/\", 1)[-1]\n",
    "        output_file_name = file_name.replace(INPUT_FOLDER_PATH,\n",
    "                                             OUTPUT_FOLDER_PATH)\n",
    "        is_updated = False\n",
    "        json_content = json_files_list[index]\n",
    "        sub_log = pd.DataFrame(columns=[file_name])\n",
    "        if \"mime_type\" in json_content.keys():\n",
    "            mention_text_key = \"mention_text\"\n",
    "            text_anchor_key = \"text_anchor\"\n",
    "            text_segment_key = \"text_segments\"\n",
    "        else:\n",
    "            mention_text_key = \"mentionText\"\n",
    "            text_anchor_key = \"textAnchor\"\n",
    "            text_segment_key = \"textSegments\"\n",
    "        if \"entities\" in json_content.keys():\n",
    "            for i in reversed(range(len(json_content[\"entities\"]))):\n",
    "                entity = json_content[\"entities\"][i]\n",
    "                if mention_text_key not in entity.keys():\n",
    "                    sub_log = sub_log.append({file_name: entity[\"type\"]},\n",
    "                                             ignore_index=True)\n",
    "                    del json_content[\"entities\"][i]\n",
    "                    is_updated = True\n",
    "                    continue\n",
    "                else:\n",
    "                    if (\"properties\" in json_content[\"entities\"][i].keys()\n",
    "                            and entity[mention_text_key].strip()):\n",
    "                        for j in range(\n",
    "                                len(json_content[\"entities\"][i][\"properties\"])\n",
    "                                - 1, -1, -1):\n",
    "                            if (mention_text_key in json_content[\"entities\"][i]\n",
    "                                [\"properties\"][j].keys()):\n",
    "                                if (json_content[\"entities\"][i][\"properties\"]\n",
    "                                    [j][mention_text_key].strip() == \"\"):\n",
    "                                    sub_log = sub_log.append(\n",
    "                                        {\n",
    "                                            file_name:\n",
    "                                            json_content[\"entities\"][i]\n",
    "                                            [\"properties\"][j][\"type\"]\n",
    "                                        },\n",
    "                                        ignore_index=True,\n",
    "                                    )\n",
    "                                    del json_content[\"entities\"][i][\n",
    "                                        \"properties\"][j]\n",
    "                                    is_updated = True\n",
    "                                    continue\n",
    "                            elif (mention_text_key\n",
    "                                  not in json_content[\"entities\"][i]\n",
    "                                  [\"properties\"][j].keys()):\n",
    "                                sub_log = sub_log.append(\n",
    "                                    {\n",
    "                                        file_name:\n",
    "                                        json_content[\"entities\"][i]\n",
    "                                        [\"properties\"][j][\"type\"]\n",
    "                                    },\n",
    "                                    ignore_index=True,\n",
    "                                )\n",
    "                                del json_content[\"entities\"][i][\"properties\"][\n",
    "                                    j]\n",
    "                                is_updated = True\n",
    "                                continue\n",
    "                            if (text_anchor_key not in json_content[\"entities\"]\n",
    "                                [i][\"properties\"][j].keys()):\n",
    "                                sub_log = sub_log.append(\n",
    "                                    {\n",
    "                                        file_name:\n",
    "                                        json_content[\"entities\"][i]\n",
    "                                        [\"properties\"][j][\"type\"]\n",
    "                                    },\n",
    "                                    ignore_index=True,\n",
    "                                )\n",
    "                                del json_content[\"entities\"][i]\n",
    "                                is_updated = True\n",
    "                                continue\n",
    "                            elif (text_anchor_key in json_content[\"entities\"]\n",
    "                                  [i][\"properties\"][j].keys()):\n",
    "                                if (text_segment_key\n",
    "                                        not in json_content[\"entities\"][i]\n",
    "                                    [\"properties\"][j][text_anchor_key].keys()):\n",
    "                                    sub_log = sub_log.append(\n",
    "                                        {\n",
    "                                            file_name:\n",
    "                                            json_content[\"entities\"][i]\n",
    "                                            [\"properties\"][j][\"type\"]\n",
    "                                        },\n",
    "                                        ignore_index=True,\n",
    "                                    )\n",
    "                                    del json_content[\"entities\"][i][\n",
    "                                        \"properties\"][j]\n",
    "                                    is_updated = True\n",
    "                                    continue\n",
    "                                elif (len(json_content[\"entities\"][i]\n",
    "                                          [\"properties\"][j][text_anchor_key]\n",
    "                                          [text_segment_key]) < 1):\n",
    "                                    sub_log = sub_log.append(\n",
    "                                        {\n",
    "                                            file_name:\n",
    "                                            json_content[\"entities\"][i]\n",
    "                                            [\"properties\"][j][\"type\"]\n",
    "                                        },\n",
    "                                        ignore_index=True,\n",
    "                                    )\n",
    "                                    del json_content[\"entities\"][i][\n",
    "                                        \"properties\"][j]\n",
    "                                    is_updated = True\n",
    "                                    continue\n",
    "\n",
    "                    elif not entity[mention_text_key].strip():\n",
    "                        sub_log = sub_log.append({file_name: entity[\"type\"]},\n",
    "                                                 ignore_index=True)\n",
    "                        del json_content[\"entities\"][i]\n",
    "                        is_updated = True\n",
    "                        continue\n",
    "\n",
    "                if text_anchor_key not in entity.keys():\n",
    "                    sub_log = sub_log.append({file_name: entity[\"type\"]},\n",
    "                                             ignore_index=True)\n",
    "                    del json_content[\"entities\"][i]\n",
    "                    is_updated = True\n",
    "                    continue\n",
    "                elif text_anchor_key in entity.keys():\n",
    "                    if text_segment_key not in entity[text_anchor_key].keys():\n",
    "                        sub_log = sub_log.append({file_name: entity[\"type\"]},\n",
    "                                                 ignore_index=True)\n",
    "                        del json_content[\"entities\"][i]\n",
    "                        is_updated = True\n",
    "                        continue\n",
    "                    elif len(entity[text_anchor_key][text_segment_key]) < 1:\n",
    "                        sub_log = sub_log.append({file_name: entity[\"type\"]},\n",
    "                                                 ignore_index=True)\n",
    "                        del json_content[\"entities\"][i]\n",
    "                        is_updated = True\n",
    "                        continue\n",
    "        else:\n",
    "            print(\"Entities missing : \", files[index])\n",
    "        # if is_updated:\n",
    "        store_blob(json_content, output_file_name)\n",
    "        if not sub_log.empty:\n",
    "            logs = pd.concat([logs, sub_log], axis=1)\n",
    "    logs.drop(\"FileName\", axis=1, inplace=True)\n",
    "    logs.to_csv(\"output.csv\", index=False)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5345184-bb87-4b08-bda0-0bb7e7eca45e",
   "metadata": {},
   "source": [
    "## Output File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef59951f-930c-4c7c-8df5-e2738b0d8ff8",
   "metadata": {},
   "source": [
    "The script deletes all bounding boxes (entities) in the JSON file that do not contain any mentionText or textAnchors, and overwrites the file. The script will also create a CSV file containing a list of deleted entities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0caa1d1-783f-4c66-814f-945c402fe415",
   "metadata": {},
   "source": [
    "## Reference Links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465741c1-2f9a-4303-bdb4-bf688bb4c128",
   "metadata": {},
   "source": [
    "Drive Link to IPYNB File : [empty_bounding_box_removal_script.ipynb](https://drive.google.com/file/d/1rQJUFCYYwpex8agJPDId01eow0T91MN7/view?usp=sharing)\n",
    "\n",
    "Sample CSV output File : [empty_entity_output.csv](https://drive.google.com/file/d/1lgJsyu0Wkttox2pO2f7ex4c0w0ec3vPA/view?usp=share_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05007337-77b0-44b0-9010-4fb3b165f9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m104",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m104"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
