{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd54d653-9889-4360-b906-ec5d73384a31",
   "metadata": {},
   "source": [
    "# Child Entity Tag Using Header Keyword"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9dbafb-8af8-4b93-a949-44468baaa57a",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8923889f-fc12-4dfb-a899-20afd9fcbd24",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the DocAI Incubator Team. No guarantees of performance are implied.\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28425178-9c4e-4874-8d41-bdaafed22338",
   "metadata": {},
   "source": [
    "## Purpose and Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89800ca-a72e-4f4b-987d-2eb5bc719692",
   "metadata": {},
   "source": [
    "This tool uses labeled json files in GCS bucket and header words as input and creates a new child entity tagging the values under the header keyword matching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5e12b0-c231-4169-985e-ecd0bdf70708",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d3b6c1-0594-45d4-8231-2adc8bcd5aa9",
   "metadata": {},
   "source": [
    "1. Vertex AI Notebook\n",
    "2. Labeled json files in GCS Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c1e9af-e86d-497d-93a7-b5bfc0feb79e",
   "metadata": {},
   "source": [
    "##  Step by Step procedure "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d95dc5-7402-4d68-a0f9-e8806d9bffb6",
   "metadata": {},
   "source": [
    "### 1. Input Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a642209-19ed-4b50-be6e-61464b18884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input details\n",
    "Gcs_input_path = \"gs://xxxx/xxxx/xxxx/\"\n",
    "Gcs_output_path = \"gs://xxxx/xxxx/xxxx/\"\n",
    "list_total_amount = [\n",
    "    \"Total value\",\n",
    "    \"Amount\",\n",
    "    \"Nettowert\",\n",
    "    \"Nettowert in EUR\",\n",
    "    \"Wert\",\n",
    "    \"Importo\",\n",
    "    \"Nettobetrag\",\n",
    "    \"Extension\",\n",
    "    \"Net value\",\n",
    "    \"Ext. price\",\n",
    "    \"Extended Amt\",\n",
    "    \"Costo riga\",\n",
    "    \"Imp. Netto\",\n",
    "    \"Summe\",\n",
    "    \"Gesamtpreis\",\n",
    "    \"Gesamt\",\n",
    "    \"Gesamtgewicht\",\n",
    "    \"Betrag\",\n",
    "    \"Bedrag\",\n",
    "    \"Wartość\",\n",
    "    \"Wartość netto\",\n",
    "    \"Value\",\n",
    "    \"TOTAL\",\n",
    "    \"Line Total\",\n",
    "    \"Net\",\n",
    "    \"Net Amount\",\n",
    "    \"cost\",\n",
    "    \"Subtotal\",\n",
    "]\n",
    "project_id = \"xxxx-xxxx-xxx\"\n",
    "total_amount_type = \"line_item/total_amount\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52ae8d7-d250-4c2b-b64f-5818e9bbf5eb",
   "metadata": {},
   "source": [
    "In the above input , the **list_total_amount** is the list of header words have to be used and the values under those headers will be tagged with child type **total_amount_type**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fc6767-f38b-4b2a-81f2-93c338cd9956",
   "metadata": {},
   "source": [
    "![Alt Text](https://screenshot.googleplex.com/AofLjF6XqhZ4aEC.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd852f45-92ca-449a-bfa4-431f4fbe0621",
   "metadata": {},
   "source": [
    "**THIS TOOL ONLY CREATES A CHILD ENTITY , TO GROUP THE CHILD ITEM TO PARENT ITEM USE [go/docai-line-items-improver-post-processing](go/docai-line-items-improver-post-processing)\n",
    " AFTER TAGGING CHILD ITEM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049b9e6d-4e3a-4727-80f5-143a4181a1ae",
   "metadata": {},
   "source": [
    "### 2. Run the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0f48a8-d694-490d-b4be-d49d260a3158",
   "metadata": {},
   "source": [
    "Copy the code provided in the sample code section and run the code  to get the updated json files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cb7ced-5aca-413d-b3a7-236ee8569a16",
   "metadata": {},
   "source": [
    "### 3. Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cb4da9-b541-4da4-b457-2d9674565426",
   "metadata": {},
   "source": [
    "The items which are below the matched keyword will be tagged as entity name given"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a947c2ff-60e8-48da-bf0b-3f81ea6b3663",
   "metadata": {},
   "source": [
    "## **Sample Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9413a9df-d7be-4dd3-8053-67b286a51efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "def file_names(file_path):\n",
    "    \"\"\"This Function will load the bucket and get the list of files\n",
    "    in the gs path given\n",
    "    args: gs path\n",
    "    output: file names as list and dictionary with file names as keys and file path as values\n",
    "    \"\"\"\n",
    "\n",
    "    from google.cloud import storage\n",
    "\n",
    "    bucket = file_path.split(\"/\")[2]\n",
    "    file_names_list = []\n",
    "    file_dict = {}\n",
    "    storage_client = storage.Client()\n",
    "    source_bucket = storage_client.get_bucket(bucket)\n",
    "    filenames = [\n",
    "        filename.name for filename in list(\n",
    "            source_bucket.list_blobs(\n",
    "                prefix=((\"/\").join(file_path.split(\"/\")[3:]))))\n",
    "    ]\n",
    "    for i in range(len(filenames)):\n",
    "        x = filenames[i].split(\"/\")[-1]\n",
    "        if x != \"\":\n",
    "            file_names_list.append(x)\n",
    "            file_dict[x] = filenames[i]\n",
    "    return file_names_list, file_dict\n",
    "\n",
    "\n",
    "def load_json(path):\n",
    "    import json\n",
    "\n",
    "    import gcsfs\n",
    "\n",
    "    gcs_file_system = gcsfs.GCSFileSystem(project=project_id)\n",
    "    with gcs_file_system.open(path) as f:\n",
    "        json_l = json.load(f)\n",
    "    return json_l\n",
    "\n",
    "\n",
    "def get_page_wise_entities(json_dict):\n",
    "    \"\"\"Args: loaded json file\n",
    "    THIS FUNCTION GIVES THE ENTITIES SPEPERATED FROM EACH PAGE IN DICTIONARY FORMAT\n",
    "    RETURNS: {page: [entities]}\"\"\"\n",
    "\n",
    "    entities_page = {}\n",
    "    for entity in json_dict[\"entities\"]:\n",
    "        page = 0\n",
    "        try:\n",
    "            if \"page\" in entity[\"pageAnchor\"][\"pageRefs\"][0].keys():\n",
    "                page = entity[\"pageAnchor\"][\"pageRefs\"][0][\"page\"]\n",
    "\n",
    "            if page in entities_page.keys():\n",
    "                entities_page[page].append(entity)\n",
    "            else:\n",
    "                entities_page[page] = [entity]\n",
    "        except:\n",
    "            pass\n",
    "    return entities_page\n",
    "\n",
    "\n",
    "def get_token(json_dict, page, text_anchors_check):\n",
    "    \"\"\"THIS FUNCITON USED LOADED JSON, PAGE NUMBER AND TEXT ANCHORS AS INPUT AND GIVES THE X AND Y COORDINATES\"\"\"\n",
    "\n",
    "    temp_xy = {\"x\": [], \"y\": []}\n",
    "    min_x = \"\"\n",
    "    for token in json_dict[\"pages\"][page][\"tokens\"]:\n",
    "        text_anc = token[\"layout\"][\"textAnchor\"][\"textSegments\"]\n",
    "        for anc in text_anc:\n",
    "            try:\n",
    "                start_temp = int(anc[\"startIndex\"])\n",
    "            except:\n",
    "                start_temp = \"0\"\n",
    "            end_temp = int(anc[\"endIndex\"])\n",
    "\n",
    "        for anc3 in text_anchors_check:\n",
    "            start_check = int(anc3[\"startIndex\"]) - 2\n",
    "            end_check = int(anc3[\"endIndex\"]) + 2\n",
    "        if (int(start_temp) >= start_check and end_temp <= end_check\n",
    "                and end_temp - int(start_temp) > 3):\n",
    "            normalized_vertices_temp = token[\"layout\"][\"boundingPoly\"][\n",
    "                \"normalizedVertices\"]\n",
    "            for ver_xy in normalized_vertices_temp:\n",
    "                try:\n",
    "                    temp_xy[\"x\"].append(ver_xy[\"x\"])\n",
    "                    temp_xy[\"y\"].append(ver_xy[\"y\"])\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    try:\n",
    "        min_x = min(temp_xy[\"x\"])\n",
    "    except:\n",
    "        min_x = \"\"\n",
    "    try:\n",
    "        min_y = min(temp_xy[\"y\"])\n",
    "    except:\n",
    "        min_y = \"\"\n",
    "    try:\n",
    "        max_x = max(temp_xy[\"x\"])\n",
    "    except:\n",
    "        max_x = \"\"\n",
    "    try:\n",
    "        max_y = max(temp_xy[\"y\"])\n",
    "    except:\n",
    "        max_y = \"\"\n",
    "\n",
    "    return {\"min_x\": min_x, \"min_y\": min_y, \"max_x\": max_x, \"max_y\": max_y}\n",
    "\n",
    "\n",
    "def tag_ref_child_item(\n",
    "    json_dict,\n",
    "    page,\n",
    "    ent_min_dict,\n",
    "    consider_ent,\n",
    "    total_amount_type,\n",
    "    min_y_start,\n",
    "    max_stop_y,\n",
    "):\n",
    "    \"\"\"THIS FUNCTION USED THE LOADED JSON, PAGE NUMBER , DICTIONARY OF HEADER KEYWORD AND VALUES AS  X AND Y COORDINATES\n",
    "    AND THE STOP WORD Y COORDINATE\n",
    "\n",
    "    ARGS: LOADED JSON, PAGE NUMBER, FIRST ENTITY TO BE TAGGED, STOP WORD Y COORDINATE\n",
    "\n",
    "    RETURNS: LIST OF LINE ITEMS TAGGING FIRST ENTITY PROVIDED\n",
    "\n",
    "    \"\"\"\n",
    "    consider_type = total_amount_type\n",
    "    line_items_temp = []\n",
    "    for token in json_dict[\"pages\"][page][\"tokens\"]:\n",
    "        line_item_ent = {\n",
    "            \"confidence\": 1,\n",
    "            \"mentionText\": \"\",\n",
    "            \"pageAnchor\": {\n",
    "                \"pageRefs\": [{\n",
    "                    \"boundingPoly\": {\n",
    "                        \"normalizedVertices\": []\n",
    "                    },\n",
    "                    \"page\": str(page)\n",
    "                }]\n",
    "            },\n",
    "            \"properties\": [],\n",
    "            \"textAnchor\": {\n",
    "                \"textSegments\": []\n",
    "            },\n",
    "            \"type\": \"line_item\",\n",
    "        }\n",
    "        sub_ent = {\n",
    "            \"confidence\": 1,\n",
    "            \"mentionText\": \"\",\n",
    "            \"pageAnchor\": {\n",
    "                \"pageRefs\": [{\n",
    "                    \"boundingPoly\": {\n",
    "                        \"normalizedVertices\": []\n",
    "                    },\n",
    "                    \"page\": str(page)\n",
    "                }]\n",
    "            },\n",
    "            \"textAnchor\": {\n",
    "                \"textSegments\": []\n",
    "            },\n",
    "            \"type\": \"\",\n",
    "        }\n",
    "        normalized_vertices = token[\"layout\"][\"boundingPoly\"]\n",
    "        try:\n",
    "            min_x = min(\n",
    "                vertex[\"x\"]\n",
    "                for vertex in normalized_vertices[\"normalizedVertices\"])\n",
    "            min_y = min(\n",
    "                vertex[\"y\"]\n",
    "                for vertex in normalized_vertices[\"normalizedVertices\"])\n",
    "            max_x = max(\n",
    "                vertex[\"x\"]\n",
    "                for vertex in normalized_vertices[\"normalizedVertices\"])\n",
    "            max_y = max(\n",
    "                vertex[\"y\"]\n",
    "                for vertex in normalized_vertices[\"normalizedVertices\"])\n",
    "            if (min_y > min_y_start\n",
    "                    and min_x >= ent_min_dict[consider_ent][\"min_x\"] - 0.05\n",
    "                    and max_x <= ent_min_dict[consider_ent][\"max_x\"] + 0.1\n",
    "                    and max_y <= max_stop_y\n",
    "                    and max_x > ent_min_dict[consider_ent][\"min_x\"]):\n",
    "                end_index = token[\"layout\"][\"textAnchor\"][\"textSegments\"][0][\n",
    "                    \"endIndex\"]\n",
    "                start_index = token[\"layout\"][\"textAnchor\"][\"textSegments\"][0][\n",
    "                    \"startIndex\"]\n",
    "                # pattern = re.compile(r'[^a-zA-Z]')\n",
    "                pattern_1 = re.compile(r\"[0-9\\s\\\\\\/]+\")\n",
    "                if (not bool(\n",
    "                        pattern_1.search(\n",
    "                            json_dict[\"text\"][int(start_index):int(end_index)].\n",
    "                            replace(\" \", \"\").replace(\"\\n\", \"\"))) == False):\n",
    "                    # float(json_dict['text'][int(start_index):int(end_index)].replace(\" \", \"\").replace(\"\\n\", \"\"))\n",
    "                    # print(json_dict['text'][int(start_index):int(end_index)])\n",
    "                    line_item_ent[\"mentionText\"] = json_dict[\"text\"][\n",
    "                        int(start_index):int(end_index)]\n",
    "                    line_item_ent[\"pageAnchor\"][\"pageRefs\"][0][\"boundingPoly\"][\n",
    "                        \"normalizedVertices\"] = token[\"layout\"][\n",
    "                            \"boundingPoly\"][\"normalizedVertices\"]\n",
    "                    line_item_ent[\"textAnchor\"][\"textSegments\"] = token[\n",
    "                        \"layout\"][\"textAnchor\"][\"textSegments\"]\n",
    "                    sub_ent[\"mentionText\"] = json_dict[\"text\"][\n",
    "                        int(start_index):int(end_index)]\n",
    "                    sub_ent[\"pageAnchor\"][\"pageRefs\"][0][\"boundingPoly\"][\n",
    "                        \"normalizedVertices\"] = token[\"layout\"][\n",
    "                            \"boundingPoly\"][\"normalizedVertices\"]\n",
    "                    sub_ent[\"textAnchor\"][\"textSegments\"] = token[\"layout\"][\n",
    "                        \"textAnchor\"][\"textSegments\"]\n",
    "                    sub_ent[\"type\"] = consider_type\n",
    "                    line_item_ent[\"properties\"].append(sub_ent)\n",
    "                    line_items_temp.append(line_item_ent)\n",
    "        except:\n",
    "            pass\n",
    "    # print(line_items_temp)\n",
    "    same_y_ent = []\n",
    "    for dup in line_items_temp:\n",
    "        temp_same_y = {\n",
    "            \"mentionText\": \"\",\n",
    "            \"min_y\": \"\",\n",
    "            \"max_y\": \"\",\n",
    "            \"min_x\": \"\",\n",
    "            \"text_anc\": [],\n",
    "        }\n",
    "        temp_same_y[\"mentionText\"] = dup[\"mentionText\"]\n",
    "        temp_norm_same_y = dup[\"pageAnchor\"][\"pageRefs\"][0][\"boundingPoly\"]\n",
    "        temp_same_y[\"min_y\"] = min(\n",
    "            vertex[\"y\"] for vertex in temp_norm_same_y[\"normalizedVertices\"])\n",
    "        temp_same_y[\"max_y\"] = max(\n",
    "            vertex[\"y\"] for vertex in temp_norm_same_y[\"normalizedVertices\"])\n",
    "        temp_same_y[\"min_x\"] = min(\n",
    "            vertex[\"x\"] for vertex in temp_norm_same_y[\"normalizedVertices\"])\n",
    "        temp_same_y[\"text_anc\"] = dup[\"textAnchor\"][\"textSegments\"]\n",
    "        same_y_ent.append(temp_same_y)\n",
    "    same_y_ent\n",
    "    sorted_same_y_ent = sorted(same_y_ent, key=lambda x: x[\"min_y\"])\n",
    "    groups_same_y = []\n",
    "    if len(sorted_same_y_ent) != 0:\n",
    "        current_group = [sorted_same_y_ent[0]]\n",
    "        for i in range(1, len(sorted_same_y_ent)):\n",
    "            if sorted_same_y_ent[i][\"min_y\"] - current_group[-1][\n",
    "                    \"min_y\"] < 0.005:\n",
    "                current_group.append(sorted_same_y_ent[i])\n",
    "            else:\n",
    "                groups_same_y.append(current_group)\n",
    "                current_group = [sorted_same_y_ent[i]]\n",
    "\n",
    "        # Append the last group\n",
    "        groups_same_y.append(current_group)\n",
    "    min_x_diff_list = [[\n",
    "        abs(elem[\"min_x\"] - ent_min_dict[consider_ent][\"min_x\"])\n",
    "        for elem in lst\n",
    "    ] for lst in groups_same_y]\n",
    "\n",
    "    selected_elements = [\n",
    "        min(\n",
    "            lst,\n",
    "            key=lambda elem: abs(elem[\"min_x\"] - ent_min_dict[consider_ent][\n",
    "                \"min_x\"]),\n",
    "        ) for lst in groups_same_y\n",
    "    ]\n",
    "    if len(groups_same_y) != 0:\n",
    "        for group in groups_same_y:\n",
    "            for element in selected_elements:\n",
    "                for dup3 in group:\n",
    "                    if dup3[\"text_anc\"] == element[\"text_anc\"]:\n",
    "                        group.remove(dup3)\n",
    "        for group in groups_same_y:\n",
    "            for dup4 in group:\n",
    "                for e5 in line_items_temp:\n",
    "                    if e5[\"textAnchor\"][\"textSegments\"] == dup4[\"text_anc\"]:\n",
    "                        line_items_temp.remove(e5)\n",
    "\n",
    "    return line_items_temp\n",
    "\n",
    "\n",
    "def total_amount_entities(json_dict, total_amount_type):\n",
    "    for ent2 in json_dict[\"entities\"]:\n",
    "        if \"properties\" in ent2.keys() and ent2[\"type\"] == \"line_item\":\n",
    "            for sub_ent2 in ent2[\"properties\"]:\n",
    "                if \"line_item\" in sub_ent2[\"type\"]:\n",
    "                    consider_ent_type = \"line_item/total_amount\"\n",
    "                else:\n",
    "                    consider_ent_type = \"total_amount\"\n",
    "\n",
    "    if \"/\" in consider_ent_type:\n",
    "        if \"/\" in total_amount_type:\n",
    "            pass\n",
    "        else:\n",
    "            total_amount_type = \"line_item\" + \"/\" + total_amount_type\n",
    "    else:\n",
    "        if \"/\" in total_amount_type:\n",
    "            total_amount_type = total_amount_type.split(\"/\")[-1]\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    page_wise_ent = get_page_wise_entities(json_dict)\n",
    "    previous_page_headers = \"\"\n",
    "    total_amount_entities = []\n",
    "\n",
    "    for page, ent2 in page_wise_ent.items():\n",
    "        line_items_all = []\n",
    "        # print(page)\n",
    "        for entity in ent2:\n",
    "            if \"properties\" in entity.keys() and entity[\"type\"] == \"line_item\":\n",
    "                line_items_all.append(entity)\n",
    "        # print(len(line_items_all))\n",
    "        if line_items_all != []:\n",
    "            if len(line_items_all) > 1 or len(\n",
    "                    line_items_all[0][\"properties\"]) > 2:\n",
    "                min_y_line = 1\n",
    "                max_y_line = 0\n",
    "                min_y_child = 1\n",
    "                min_y_child_Mt = \"\"\n",
    "                entity_mentiontext = \"\"\n",
    "                for line_item in line_items_all:\n",
    "                    norm_ver = line_item[\"pageAnchor\"][\"pageRefs\"][0][\n",
    "                        \"boundingPoly\"][\"normalizedVertices\"]\n",
    "                    for ver in norm_ver:\n",
    "                        min_y_temp = min(vertex[\"y\"] for vertex in norm_ver)\n",
    "                        max_y_temp = max(vertex[\"y\"] for vertex in norm_ver)\n",
    "                        if min_y_line > min_y_temp:\n",
    "                            min_y_line = min_y_temp\n",
    "                            entity_mentiontext = line_item[\"mentionText\"]\n",
    "                            for child_ent in line_item[\"properties\"]:\n",
    "                                norm_ver_child = child_ent[\"pageAnchor\"][\n",
    "                                    \"pageRefs\"][0][\"boundingPoly\"][\n",
    "                                        \"normalizedVertices\"]\n",
    "                                for ver_child in norm_ver_child:\n",
    "                                    min_y_child_temp = min(\n",
    "                                        vertex[\"y\"]\n",
    "                                        for vertex in norm_ver_child)\n",
    "                                    if min_y_child > min_y_child_temp:\n",
    "                                        min_y_child = min_y_child_temp\n",
    "                                        try:\n",
    "                                            min_y_child_Mt = child_ent[\n",
    "                                                \"mentionText\"]\n",
    "                                        except:\n",
    "                                            pass\n",
    "                                            # print(child_ent)\n",
    "                        if max_y_line < max_y_temp:\n",
    "                            max_y_line = max_y_temp\n",
    "                        else:\n",
    "                            pass\n",
    "                # print(min_y_line,max_y_line)\n",
    "                check_text = \"\"\n",
    "                start_temp = 100000000\n",
    "                end_temp = 0\n",
    "                total_amount_textanc = {}\n",
    "                for token in json_dict[\"pages\"][int(page)][\"tokens\"]:\n",
    "                    normalized_vertices = token[\"layout\"][\"boundingPoly\"]\n",
    "                    try:\n",
    "                        max_y_temp_token = max(\n",
    "                            vertex[\"y\"] for vertex in\n",
    "                            normalized_vertices[\"normalizedVertices\"])\n",
    "                        min_y_temp_token = min(\n",
    "                            vertex[\"y\"] for vertex in\n",
    "                            normalized_vertices[\"normalizedVertices\"])\n",
    "                        if (min_y_line >= max_y_temp_token - 0.02 and\n",
    "                                abs(min_y_line - min_y_temp_token) <= 0.15):\n",
    "                            end_index = token[\"layout\"][\"textAnchor\"][\n",
    "                                \"textSegments\"][0][\"endIndex\"]\n",
    "                            start_index = token[\"layout\"][\"textAnchor\"][\n",
    "                                \"textSegments\"][0][\"startIndex\"]\n",
    "                            check_text = (check_text + json_dict[\"text\"]\n",
    "                                          [int(start_index):int(end_index)])\n",
    "                            if int(start_temp) > int(start_index):\n",
    "                                start_temp = int(start_index)\n",
    "                            if int(end_temp) < int(end_index):\n",
    "                                end_temp = int(end_index)\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "                        # print(e)\n",
    "\n",
    "                for i in list_total_amount:\n",
    "                    if i.lower() in check_text.lower():\n",
    "                        # print(i)\n",
    "                        matches = re.finditer(\n",
    "                            i.lower(),\n",
    "                            json_dict[\"text\"]\n",
    "                            [int(start_temp):int(end_temp)].lower(),\n",
    "                        )\n",
    "                        starting_indices = [match.start() for match in matches]\n",
    "                        start_index_temp1 = max(starting_indices)\n",
    "                        # start_index_temp1=json_dict['text'][int(start_temp):int(end_temp)].lower().find(i.lower())\n",
    "                        # print(start_index_temp1)\n",
    "                        start_index_1 = start_index_temp1 + int(start_temp)\n",
    "                        end_index_1 = start_index_1 + len(i)\n",
    "                        total_amount_textanc[i] = [{\n",
    "                            \"startIndex\":\n",
    "                            str(start_index_1),\n",
    "                            \"endIndex\":\n",
    "                            str(end_index_1),\n",
    "                        }]\n",
    "                # print(start_temp,end_temp)\n",
    "                # print(check_text)\n",
    "                final_key = \"\"\n",
    "                for k, v in total_amount_textanc.items():\n",
    "                    if len(final_key) < len(k):\n",
    "                        final_key = k\n",
    "                # print(total_amount_textanc)\n",
    "                # print(final_key)\n",
    "                if final_key != \"\":\n",
    "                    total_amount_dict = {\n",
    "                        \"total_amount\":\n",
    "                        get_token(json_dict, int(page),\n",
    "                                  total_amount_textanc[final_key])\n",
    "                    }\n",
    "                    previous_page_headers = total_amount_dict\n",
    "                else:\n",
    "                    total_amount_dict = previous_page_headers\n",
    "                if len(total_amount_dict) != 0:\n",
    "                    total_amount_line_items = tag_ref_child_item(\n",
    "                        json_dict,\n",
    "                        int(page),\n",
    "                        total_amount_dict,\n",
    "                        \"total_amount\",\n",
    "                        total_amount_type,\n",
    "                        min_y_line,\n",
    "                        max_y_line,\n",
    "                    )\n",
    "                    for item in total_amount_line_items:\n",
    "                        total_amount_entities.append(item)\n",
    "                from pprint import pprint\n",
    "                # pprint(total_amount_entities)\n",
    "\n",
    "    from pprint import pprint\n",
    "\n",
    "    for total_en in total_amount_entities:\n",
    "        json_dict[\"entities\"].append(total_en)\n",
    "        # pprint(total_en)\n",
    "\n",
    "    return json_dict\n",
    "\n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "import gcsfs\n",
    "from tqdm import tqdm\n",
    "\n",
    "fs = gcsfs.GCSFileSystem(project_id)\n",
    "file_names_list, file_dict = file_names(Gcs_input_path)\n",
    "count = 0\n",
    "issue_files = {}\n",
    "for filename, filepath in tqdm(file_dict.items(), desc=\"Progress\"):\n",
    "    input_bucket_name = Gcs_input_path.split(\"/\")[2]\n",
    "    if \".json\" in filepath:\n",
    "        filepath = \"gs://\" + input_bucket_name + \"/\" + filepath\n",
    "        json_dict = load_json(filepath)\n",
    "        print(filepath)\n",
    "        try:\n",
    "            if json_dict[\"pages\"][0][\"tokens\"] != []:\n",
    "                try:\n",
    "                    json_dict = total_amount_entities(json_dict,\n",
    "                                                      total_amount_type)\n",
    "                    fs.pipe(\n",
    "                        Gcs_output_path + \"/\" + filename,\n",
    "                        bytes(json.dumps(json_dict, ensure_ascii=False),\n",
    "                              \"utf-8\"),\n",
    "                        content_type=\"application/json\",\n",
    "                    )\n",
    "                except:\n",
    "                    issue_files[filepath] = \"Some issue with Json\"\n",
    "                    fs.pipe(\n",
    "                        Gcs_output_path + \"/\" + filename,\n",
    "                        bytes(json.dumps(json_dict, ensure_ascii=False),\n",
    "                              \"utf-8\"),\n",
    "                        content_type=\"application/json\",\n",
    "                    )\n",
    "                    pass\n",
    "            else:\n",
    "                issue_files[filepath] = \"No Tokens\"\n",
    "                count = count + 1\n",
    "                fs.pipe(\n",
    "                    Gcs_output_path + \"/\" + filename,\n",
    "                    bytes(json.dumps(json_dict, ensure_ascii=False), \"utf-8\"),\n",
    "                    content_type=\"application/json\",\n",
    "                )\n",
    "        except:\n",
    "            fs.pipe(\n",
    "                Gcs_output_path + \"/\" + filename,\n",
    "                bytes(json.dumps(json_dict, ensure_ascii=False), \"utf-8\"),\n",
    "                content_type=\"application/json\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb8bfda-6814-45e8-8441-f212d2c80dd5",
   "metadata": {},
   "source": [
    "After this tool , run [go/docai-line-items-improver-post-processing](go/docai-line-items-improver-post-processing) for grouping the line items with respect to the new child item created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539486c3-0047-489b-a347-d1c904d4d9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m104",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m104"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
