{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line Items Improver (Post-Processing) User Guide\n",
    "\n",
    "- Author: docai-incubator@google.com\n",
    "\n",
    "## Purpose and Description\n",
    "\n",
    "This script will guide to group the child entities into correect line items.\n",
    "\n",
    "This tool takes in parsed json (prediction result) files and a list of child entities which occurs only once (per line item) in the schema of line_items to better group the child entities into correct line_items(parents).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites \n",
    "1. Access to a Google Cloud project to create Document AI processors.\n",
    "   - Permission to Google project is needed to access Document AI processors.\n",
    "1. Python: Jupyter notebook (Vertex AI) or Google Colab.\n",
    "2. List of child entities which occur once(optional once or required once)\n",
    "3. Output folder to upload the updated json files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Operation Procedure\n",
    "\n",
    "### 1. Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: google-cloud-documentai in /opt/conda/lib/python3.7/site-packages (2.18.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-documentai) (3.20.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-documentai) (1.22.2)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-documentai) (1.34.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-documentai) (1.58.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-documentai) (2.28.2)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-documentai) (2.22.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-documentai) (1.51.3)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-documentai) (1.48.2)\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-documentai) (1.26.14)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-documentai) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-documentai) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-documentai) (4.2.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-documentai) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-documentai) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-documentai) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-documentai) (3.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-documentai) (0.4.8)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.7/site-packages (2.7.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (2.3.2)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (1.34.0)\n",
      "Requirement already satisfied: google-resumable-media>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (2.4.1)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (2.28.2)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (2.22.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (1.58.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (3.20.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (1.16.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (1.26.14)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.2.8)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media>=2.3.2->google-cloud-storage) (1.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.4.8)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: google-api-core in /opt/conda/lib/python3.7/site-packages (1.34.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core) (2.28.2)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core) (2.22.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core) (1.58.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /opt/conda/lib/python3.7/site-packages (from google-api-core) (3.20.3)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core) (4.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core) (4.2.4)\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core) (2022.12.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core) (0.4.8)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement json (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for json\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement collections (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for collections\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement copy (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for copy\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.64.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement pprint (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pprint\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install google-cloud-documentai\n",
    "%pip install google-cloud-storage\n",
    "%pip install google-api-core\n",
    "%pip install json\n",
    "%pip install collections\n",
    "%pip install copy\n",
    "%pip install tqdm\n",
    "%pip install pprint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import gcsfs\n",
    "import json\n",
    "from collections import Counter\n",
    "import copy\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Input Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input\n",
    "Gcs_input_path = 'gs://xxxx/xxxx/'  # Parsed json files path , end '/' is mandatory\n",
    "project_id = 'xxxx-xxxx-xxxx'  # project ID\n",
    "Gcs_output_path = 'gs://xxxxxxx/xxxxx/xxxxxxx/'  #output path where the updated jsons to be saved, end '/' is mandatory\n",
    "#list of child entities in line_item which is optional_once or required_once in schema like below example#\n",
    "unique_entities = [\n",
    "    'line_item/product_code', 'line_item/unit_price', 'line_item/quantity'\n",
    "]  #example given\n",
    "## FOR CDE outout remove line_item as child entity name doesnt contain any parent name in it##\n",
    "Desc_merge_update = 'Yes'  # update to Yes if you want to combine description within the line item, else NO#\n",
    "line_item_across_pages = 'Yes'  # update to Yes if you want to group line items across pages#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_names(file_path):\n",
    "    \"\"\"This Function will load the bucket and get the list of files\n",
    "    in the gs path given\n",
    "    args: gs path\n",
    "    output: file names as list and dictionary with file names as keys and file path as values\"\"\"\n",
    "\n",
    "    bucket = file_path.split(\"/\")[2]\n",
    "    file_names_list = []\n",
    "    file_dict = {}\n",
    "    storage_client = storage.Client()\n",
    "    source_bucket = storage_client.get_bucket(bucket)\n",
    "    filenames = [\n",
    "        filename.name for filename in list(\n",
    "            source_bucket.list_blobs(\n",
    "                prefix=(('/').join(file_path.split('/')[3:]))))\n",
    "    ]\n",
    "    for i in range(len(filenames)):\n",
    "        x = filenames[i].split('/')[-1]\n",
    "        if x != \"\":\n",
    "            file_names_list.append(x)\n",
    "            file_dict[x] = filenames[i]\n",
    "    return file_names_list, file_dict\n",
    "\n",
    "\n",
    "def load_json(path):\n",
    "    \"\"\" This Function will load the json from the gs path \n",
    "    \n",
    "    args: gs path\n",
    "    output: loaded json \"\"\"\n",
    "\n",
    "    gcs_file_system = gcsfs.GCSFileSystem(project=project_id)\n",
    "    with gcs_file_system.open(path) as f:\n",
    "        json_l = json.load(f)\n",
    "    return json_l\n",
    "\n",
    "\n",
    "def line_item_check(entities, unique_entities):\n",
    "    \"\"\" \n",
    "        This Function is to check the number of line items in a json\n",
    "    args:entities and unique entities list\n",
    "    output: line item count\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    entity_types = []\n",
    "    for entity in entities:\n",
    "        if 'properties' in entity.keys() and entity['properties'] != []:\n",
    "            for subentity in entity['properties']:\n",
    "                entity_types.append(subentity['type'])\n",
    "        else:\n",
    "            pass\n",
    "            #entity_types.append(entity['type'])\n",
    "\n",
    "    line_items_count = 0\n",
    "    entity_unique_count = {}\n",
    "\n",
    "    for unique in unique_entities:\n",
    "        entity_unique_count[unique] = entity_types.count(unique)\n",
    "    li_en = []\n",
    "    #print(entity_unique_count)\n",
    "    for i in list(entity_unique_count.values()):\n",
    "        if i > 1:\n",
    "            li_en.append(i)\n",
    "\n",
    "    #print(li_en)\n",
    "    for unique in unique_entities:\n",
    "        if entity_types.count(unique) < 1:\n",
    "            continue\n",
    "        elif entity_types.count(unique) == 1:\n",
    "            line_items_count = 1\n",
    "        elif len(unique_entities) >= 3:\n",
    "            if entity_types.count(unique) > 1 and len(li_en) >= 1:\n",
    "                line_items_count = 2\n",
    "                break\n",
    "        elif entity_types.count(unique) > 1 and len(li_en) >= 1:\n",
    "            line_items_count = 2\n",
    "            break\n",
    "\n",
    "    #print(line_items_count)\n",
    "    return line_items_count\n",
    "\n",
    "\n",
    "def single_line_item_merge(entities, page):\n",
    "    \"\"\" \n",
    "    This Function will group the line item if there is only single line item in a document \n",
    "    \n",
    "    args: entities and page number\n",
    "    output: grouped entities\n",
    "            \"\"\"\n",
    "\n",
    "    line_item_entity = []\n",
    "    line_item_sub_entities = []\n",
    "    for entity in entities:\n",
    "        if entity['type'] == 'line_item':\n",
    "            for subentity in entity['properties']:\n",
    "                line_item_sub_entities.append(subentity)\n",
    "            line_item_entity.append(entity)\n",
    "\n",
    "    line_item = {\n",
    "        'mentionText': '',\n",
    "        'pageAnchor': {\n",
    "            'pageRefs': [{\n",
    "                'boundingPoly': {\n",
    "                    'normalizedVertices': []\n",
    "                },\n",
    "                'page': page\n",
    "            }]\n",
    "        },\n",
    "        'properties': [],\n",
    "        'textAnchor': {\n",
    "            'textSegments': []\n",
    "        },\n",
    "        'type': 'line_item'\n",
    "    }\n",
    "\n",
    "    text_anchors_sub_entities = []\n",
    "    for item in line_item_sub_entities:\n",
    "        text_anchors_sub_entities.append(item['textAnchor']['textSegments'][0])\n",
    "    line_item['textAnchor']['textSegments'] = text_anchors_sub_entities\n",
    "    sorted_list_text1 = sorted(text_anchors_sub_entities,\n",
    "                               key=lambda x: int(x['endIndex']))\n",
    "\n",
    "    line_item_mention_text = ''\n",
    "    line_item_properties = []\n",
    "    line_item_text_segments = []\n",
    "    line_item_normalizedvertices = []\n",
    "    subentities_classified = []\n",
    "    for index in sorted_list_text1:\n",
    "        for item in line_item_sub_entities:\n",
    "            if index in item['textAnchor']['textSegments']:\n",
    "                if item not in subentities_classified:\n",
    "                    subentities_classified.append(item)\n",
    "                    line_item_mention_text = line_item_mention_text + ' ' + item[\n",
    "                        'mentionText']\n",
    "                    line_item_properties.append(item)\n",
    "                    line_item_text_segments.append(index)\n",
    "                    for i in item['pageAnchor']['pageRefs'][0]['boundingPoly'][\n",
    "                            'normalizedVertices']:\n",
    "                        line_item_normalizedvertices.append(i)\n",
    "    min_x_ln = min(line_item_normalizedvertices, key=lambda d: d['x'])['x']\n",
    "    max_x_ln = max(line_item_normalizedvertices, key=lambda d: d['x'])['x']\n",
    "    min_y_ln = min(line_item_normalizedvertices, key=lambda d: d['y'])['y']\n",
    "    max_y_ln = max(line_item_normalizedvertices, key=lambda d: d['y'])['y']\n",
    "    line_item_normalizedvertices_final = [{\n",
    "        'x': min_x_ln,\n",
    "        'y': min_y_ln\n",
    "    }, {\n",
    "        'x': min_x_ln,\n",
    "        'y': max_y_ln\n",
    "    }, {\n",
    "        'x': max_x_ln,\n",
    "        'y': min_y_ln\n",
    "    }, {\n",
    "        'x': max_x_ln,\n",
    "        'y': max_y_ln\n",
    "    }]\n",
    "\n",
    "    line_item['pageAnchor']['pageRefs'][0]['boundingPoly'][\n",
    "        'normalizedVertices'] = line_item_normalizedvertices_final\n",
    "    line_item['mentionText'] = line_item_mention_text\n",
    "    line_item['properties'] = line_item_properties\n",
    "    line_item['textAnchor']['textSegments'] = line_item_text_segments\n",
    "\n",
    "    return line_item\n",
    "\n",
    "\n",
    "#line_items_across pages grouping\n",
    "def get_lineitems_grouped_pages_across(json_dict, schema={}):\n",
    "    \"\"\" \n",
    "    This Function will group the line items which are across pages \n",
    "    line items continued into next page\n",
    "    \n",
    "    args: loaded json and optional line item schems\n",
    "    \n",
    "    output: json with grouped entities\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    #this function will be used to group the line item entities across pages\n",
    "    def get_line_items_temp_schema(json_dict):\n",
    "        \"\"\" \n",
    "        This Function will get the schema of the line item based on the line items available in the json\n",
    "        (if there is no schema provided to the  main function)\n",
    "\n",
    "        args: loaded json\n",
    "\n",
    "        output: line item schema\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        line_items = []\n",
    "        for entity in json_dict['entities']:\n",
    "            if 'properties' in entity.keys():\n",
    "                if entity['type'] == 'line_item':\n",
    "                    line_items.append(entity)\n",
    "        x = 1\n",
    "        line_types = {}\n",
    "        for i in line_items:\n",
    "            line_types['line' + '_' + str(x)] = {}\n",
    "            for child in i['properties']:\n",
    "                if child['type'] in line_types['line' + '_' + str(x)].keys():\n",
    "                    line_types['line' + '_' + str(x)][\n",
    "                        child['type']] = line_types['line' + '_' +\n",
    "                                                    str(x)][child['type']] + 1\n",
    "                else:\n",
    "                    line_types['line' + '_' + str(x)][child['type']] = 1\n",
    "            x += 1\n",
    "\n",
    "        all_child = []\n",
    "        for key, val in line_types.items():\n",
    "            #print(val)\n",
    "            all_child.append(val)\n",
    "        counts = {\n",
    "            k: Counter([d[k] for d in all_child if k in d])\n",
    "            for k in set().union(*all_child)\n",
    "        }\n",
    "        #print(line_items)\n",
    "        # Get the majority value for each key without any minimum count criteria,*****ISSUES WITH OUTLIERS OR PARSER ISSUES********\n",
    "        temp_schema = {k: max(counts[k], key=counts[k].get) for k in counts}\n",
    "\n",
    "        #considered if atleast half of line items have that entity inside line item\n",
    "        #temp_schema = {key: value for key, counter in counts.items() for value in counter.keys() if counter[value] >=(round((len(all_child)/2),0))}\n",
    "\n",
    "        #considered if atleast 3 of line items have that entity inside line item\n",
    "        #temp_schema = {key: value for key, counter in counts.items() for value in counter.keys() if counter[value] >2}\n",
    "        #print(temp_schema)\n",
    "\n",
    "        return temp_schema\n",
    "\n",
    "    if schema == {}:\n",
    "        schema = get_line_items_temp_schema(json_dict)\n",
    "\n",
    "    line_items_1 = []\n",
    "    for entity in json_dict['entities']:\n",
    "        if 'properties' in entity.keys() and len(entity['properties']) != 0:\n",
    "            line_items_1.append(entity)\n",
    "\n",
    "    line_item_sorted_first = {}\n",
    "    line_item_sorted_last = {}\n",
    "    line_item_across_pages_first = {}\n",
    "    line_item_across_pages_last = {}\n",
    "    for li_1 in line_items_1:\n",
    "        try:\n",
    "            page = li_1['pageAnchor']['pageRefs'][0]['page']\n",
    "        except:\n",
    "            page = str(0)\n",
    "        max_y = max(vertex['y'] for vertex in li_1['pageAnchor']['pageRefs'][0]\n",
    "                    ['boundingPoly']['normalizedVertices'])\n",
    "        min_y = min(vertex['y'] for vertex in li_1['pageAnchor']['pageRefs'][0]\n",
    "                    ['boundingPoly']['normalizedVertices'])\n",
    "        if page in line_item_sorted_last.keys():\n",
    "            if line_item_sorted_last[page] >= max_y:\n",
    "                pass\n",
    "            else:\n",
    "                line_item_sorted_last[page] = max_y\n",
    "                line_item_across_pages_last[page] = li_1\n",
    "        else:\n",
    "            line_item_sorted_last[page] = max_y\n",
    "            line_item_across_pages_last[page] = li_1\n",
    "        if page in line_item_sorted_first.keys():\n",
    "            if line_item_sorted_first[page] <= min_y:\n",
    "                pass\n",
    "            else:\n",
    "                line_item_sorted_first[page] = min_y\n",
    "                line_item_across_pages_first[page] = li_1\n",
    "        else:\n",
    "            line_item_sorted_first[page] = min_y\n",
    "            line_item_across_pages_first[page] = li_1\n",
    "\n",
    "    groups_across = {}\n",
    "    p = 0\n",
    "\n",
    "    for page_last, ent_last in line_item_across_pages_last.items():\n",
    "        try:\n",
    "            groups_across[p] = [\n",
    "                line_item_across_pages_last[page_last],\n",
    "                line_item_across_pages_first[str(int(page_last) + 1)]\n",
    "            ]\n",
    "            p += 1\n",
    "        except KeyError:\n",
    "            pass\n",
    "    #getting schema of each line item in groups\n",
    "    schema_across = {}\n",
    "    for group, match in groups_across.items():\n",
    "        for i in range(len(match)):\n",
    "            for subitem in match[i]['properties']:\n",
    "                if group in schema_across.keys():\n",
    "                    if i in schema_across[group].keys():\n",
    "                        if subitem['type'] in schema_across[group][i].keys():\n",
    "                            schema_across[group][i][subitem['type']] += 1\n",
    "                            #print('yes')\n",
    "                        else:\n",
    "                            schema_across[group][i][subitem['type']] = 1\n",
    "                            #print(subitem['type'])\n",
    "                    else:\n",
    "                        schema_across[group][i] = {subitem['type']: 1}\n",
    "                        #print('yes')\n",
    "                else:\n",
    "                    schema_across[group] = {i: {subitem['type']: 1}}\n",
    "                    #print('yes')\n",
    "    group_entites_spread = {}\n",
    "    for selected_group, schema_ent in schema_across.items():\n",
    "        missing_ent_0 = {}\n",
    "        for key in schema.keys():\n",
    "            if key not in schema_ent[0]:\n",
    "                missing_ent_0[key] = schema[key]\n",
    "            else:\n",
    "                missing_ent_0[key] = schema[key] - schema_ent[0][key]\n",
    "        for k1, v1 in missing_ent_0.items():\n",
    "            if v1 > 0:\n",
    "                if k1 in schema_ent[1]:\n",
    "                    if schema_ent[1][k1] > schema[k1] or len(\n",
    "                            schema_ent[1]) < (len(schema) / 2):\n",
    "                        if selected_group in group_entites_spread.keys():\n",
    "                            group_entites_spread[selected_group][\n",
    "                                k1] = schema_ent[1][k1] - schema[k1]\n",
    "                        else:\n",
    "                            if len(schema_ent[1]) < (len(schema) / 2):\n",
    "                                group_entites_spread[selected_group] = {\n",
    "                                    k1: schema_ent[1][k1]\n",
    "                                }\n",
    "                            else:\n",
    "                                group_entites_spread[selected_group] = {\n",
    "                                    k1: schema_ent[1][k1] - schema[k1]\n",
    "                                }\n",
    "\n",
    "    def get_ent_schffle(group, index_1):\n",
    "        \"\"\" \n",
    "        \n",
    "        Internal function to find the right grouping of line items \n",
    "        \n",
    "        \"\"\"\n",
    "        ent_min_y = {}\n",
    "        ent_sort_ent = {}\n",
    "        for sube1 in groups_across[group][index_1]['properties']:\n",
    "            if sube1['type'] in group_entites_spread[group].keys():\n",
    "                min_y_temp = min(vertex['y']\n",
    "                                 for vertex in sube1['pageAnchor']['pageRefs']\n",
    "                                 [0]['boundingPoly']['normalizedVertices'])\n",
    "                if sube1['type'] in ent_min_y.keys():\n",
    "                    ent_min_y[sube1['type']].append(min_y_temp)\n",
    "                    if sube1['type'] in ent_sort_ent.keys():\n",
    "                        ent_sort_ent[sube1['type']][min_y_temp] = sube1\n",
    "                    else:\n",
    "                        ent_sort_ent[sube1['type']] = {min_y_temp: sube1}\n",
    "                else:\n",
    "                    ent_min_y[sube1['type']] = [min_y_temp]\n",
    "                    if sube1['type'] in ent_sort_ent.keys():\n",
    "                        ent_sort_ent[sube1['type']][min_y_temp] = sube1\n",
    "                    else:\n",
    "                        ent_sort_ent[sube1['type']] = {min_y_temp: sube1}\n",
    "\n",
    "        sorted_ent_min_y = {\n",
    "            key: sorted(values)\n",
    "            for key, values in ent_min_y.items()\n",
    "        }\n",
    "        ent_shuffle = []\n",
    "        for en1, val1 in sorted_ent_min_y.items():\n",
    "            b = 0\n",
    "            for num in range(group_entites_spread[group][en1]):\n",
    "                for miny in range(len(sorted_ent_min_y[en1])):\n",
    "                    if num >= b:\n",
    "                        ent_shuffle.append(\n",
    "                            ent_sort_ent[en1][sorted_ent_min_y[en1][miny]])\n",
    "                        b += 1\n",
    "        return ent_shuffle\n",
    "\n",
    "    def ent_move(group, index_1, index_0):\n",
    "        \"\"\" \n",
    "        \n",
    "        Internal function to find the right grouping of line items \n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        temp_group = copy.deepcopy(groups_across[group])\n",
    "        for ent_sh in ent_shuffle:\n",
    "            for sub_en3 in temp_group[index_1]['properties']:\n",
    "                if ent_sh['type'] == sub_en3['type'] and ent_sh[\n",
    "                        'textAnchor'] == sub_en3['textAnchor']:\n",
    "                    temp_group[index_1]['properties'].remove(ent_sh)\n",
    "            temp_group[index_0]['properties'].append(ent_sh)\n",
    "            for t1 in ent_sh['pageAnchor']['pageRefs']:\n",
    "                temp_group[index_0]['pageAnchor']['pageRefs'].append(t1)\n",
    "            temp_group[index_0]['mentionText'] = temp_group[index_0][\n",
    "                'mentionText'] + ' ' + ent_sh['mentionText']\n",
    "            for t2 in ent_sh['textAnchor']['textSegments']:\n",
    "                temp_group[index_0]['textAnchor']['textSegments'].append(t2)\n",
    "        return temp_group\n",
    "\n",
    "    def correct_page_text(temp_group_1, index_1):\n",
    "        \"\"\" \n",
    "        \n",
    "        Internal function to fiix the text anchors and page anchors of parent items \n",
    "        \n",
    "        \"\"\"\n",
    "        temp_x = []\n",
    "        temp_y = []\n",
    "        temp_text_anc = []\n",
    "        temp_mention_text = ''\n",
    "        for suben in temp_group_1[index_1]['properties']:\n",
    "            for tex_an1 in suben['textAnchor']['textSegments']:\n",
    "                temp_text_anc.append(tex_an1)\n",
    "            for page_an1 in suben['pageAnchor']['pageRefs'][0]['boundingPoly'][\n",
    "                    'normalizedVertices']:\n",
    "                temp_x.append(page_an1['x'])\n",
    "                temp_y.append(page_an1['y'])\n",
    "\n",
    "        updated_ver = [{\n",
    "            'x': min(temp_x),\n",
    "            'y': min(temp_y)\n",
    "        }, {\n",
    "            'x': max(temp_x),\n",
    "            'y': max(temp_y)\n",
    "        }, {\n",
    "            'x': min(temp_x),\n",
    "            'y': max(temp_y)\n",
    "        }, {\n",
    "            'x': max(temp_x),\n",
    "            'y': min(temp_y)\n",
    "        }]\n",
    "        sorted_temp_text_anc = sorted(temp_text_anc,\n",
    "                                      key=lambda x: int(x['endIndex']))\n",
    "        temp_group_1[index_1]['textAnchor'][\n",
    "            'textSegments'] = sorted_temp_text_anc\n",
    "        temp_group_1[index_1]['pageAnchor']['pageRefs'][0]['boundingPoly'][\n",
    "            'normalizedVertices'] = updated_ver\n",
    "        for t5 in sorted_temp_text_anc:\n",
    "            s1 = t5['startIndex']\n",
    "            e1 = t5['endIndex']\n",
    "            temp_mention_text = temp_mention_text + ' ' + json_dict['text'][\n",
    "                int(s1):int(e1)]\n",
    "        temp_group_1[index_1]['mentionText'] = temp_mention_text\n",
    "\n",
    "        return temp_group_1\n",
    "\n",
    "    if len(group_entites_spread) > 0:\n",
    "        for group, entity_move in group_entites_spread.items():\n",
    "            # def modify_across_page(groups_across[0],group_entites_spread[0]):\n",
    "            try:\n",
    "                page_1 = groups_across[group][0]['pageAnchor']['pageRefs'][0][\n",
    "                    'page']\n",
    "            except:\n",
    "                page_1 = '0'\n",
    "            try:\n",
    "                page_2 = groups_across[group][1]['pageAnchor']['pageRefs'][0][\n",
    "                    'page']\n",
    "            except:\n",
    "                page_2 = '0'\n",
    "            if page_1 < page_2:\n",
    "                ent_shuffle = get_ent_schffle(group, 1)\n",
    "                temp_group_1 = ent_move(group, 1, 0)\n",
    "                if len(temp_group_1[1]['properties']) > 0:\n",
    "                    temp_group_updated = correct_page_text(temp_group_1, 1)\n",
    "                else:\n",
    "                    temp_group_updated = [temp_group_1[0]]\n",
    "\n",
    "            elif page_1 > page_2:\n",
    "                ent_shuffle = get_ent_schffle(group, 0)\n",
    "                temp_group_1 = ent_move(group, 0, 1)\n",
    "                if len(temp_group_1[0]['properties']) > 0:\n",
    "                    temp_group_updated = correct_page_text(temp_group_1, 0)\n",
    "                else:\n",
    "                    temp_group_updated = [temp_group_1[1]]\n",
    "\n",
    "            for ent_remove in groups_across[group]:\n",
    "                json_dict['entities'].remove(ent_remove)\n",
    "\n",
    "            for ent_add in temp_group_updated:\n",
    "                json_dict['entities'].append(ent_add)\n",
    "\n",
    "    return json_dict\n",
    "\n",
    "\n",
    "def get_page_wise_entities(json_dict):\n",
    "    \"\"\" \n",
    "    This Function will provide the entities page wise\n",
    "    \n",
    "    args: loaded json \n",
    "    \n",
    "    output: dictionary with page as key and entities as values\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    entities_page = {}\n",
    "\n",
    "    for entity in json_dict['entities']:\n",
    "        page = '0'\n",
    "        try:\n",
    "            if 'page' in entity['pageAnchor']['pageRefs'][0].keys():\n",
    "                page = entity['pageAnchor']['pageRefs'][0]['page']\n",
    "\n",
    "            if page in entities_page.keys():\n",
    "                entities_page[page].append(entity)\n",
    "            else:\n",
    "                entities_page[page] = [entity]\n",
    "        except:\n",
    "            pass\n",
    "    return entities_page\n",
    "\n",
    "\n",
    "def multi_page_entites(entities_pagewise, page):\n",
    "    \"\"\" \n",
    "    This Function will group the entities in case of multiple line items page wise\n",
    "    \n",
    "    args: entities page wise and page number\n",
    "    \n",
    "    output: list of entities grouped\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    entity_types = []\n",
    "    line_item_sub_entities = []\n",
    "    for entity in entities_pagewise:\n",
    "        if 'properties' in entity.keys() and entity['properties'] != []:\n",
    "            if entity['type'] == 'line_item':\n",
    "                for subentity in entity['properties']:\n",
    "                    entity_types.append(subentity['type'])\n",
    "                    line_item_sub_entities.append(subentity)\n",
    "        else:\n",
    "            entity_types.append(entity['type'])\n",
    "    #print(line_item_sub_entities)\n",
    "    line_items_multi_dict = {}\n",
    "    for unique in unique_entities:\n",
    "        if entity_types.count(unique) > 1:\n",
    "            line_items_multi_dict[unique] = entity_types.count(unique)\n",
    "    #print(line_items_multi_dict)\n",
    "\n",
    "    value_counts = Counter(line_items_multi_dict.values())\n",
    "    max_count = max(value_counts.values())\n",
    "    entity_types_keys = [\n",
    "        key for key, value in line_items_multi_dict.items()\n",
    "        if value_counts[value] == max_count\n",
    "    ]\n",
    "\n",
    "    #print(entity_types_keys)\n",
    "    dict_unique_ent = {}\n",
    "    for entity_type in entity_types_keys:\n",
    "        for entity in entities_pagewise:\n",
    "            if 'properties' in entity.keys():\n",
    "                for subentity in entity['properties']:\n",
    "                    if subentity['type'] == entity_type:\n",
    "                        if entity_type in dict_unique_ent.keys():\n",
    "                            dict_unique_ent[entity_type].append(subentity)\n",
    "                        else:\n",
    "                            dict_unique_ent[entity_type] = [subentity]\n",
    "\n",
    "    #print(dict_unique_ent)\n",
    "    region_line_items = {}\n",
    "    region_line_items_x = {}\n",
    "    region_line_items_y = {}\n",
    "    opt_region = {}\n",
    "    for ent in dict_unique_ent:\n",
    "        region = []\n",
    "        dict_x_y = []\n",
    "        count_product_code = 0\n",
    "        min_x_1 = []\n",
    "        min_y_1 = []\n",
    "        for item in dict_unique_ent[ent]:\n",
    "            x_y = {}\n",
    "            x = []\n",
    "            y = []\n",
    "            x_min = ''\n",
    "            y_min = ''\n",
    "            count_product_code += 1\n",
    "            for i in item['pageAnchor']['pageRefs'][0]['boundingPoly'][\n",
    "                    'normalizedVertices']:\n",
    "                y.append(i['y'])\n",
    "                x.append(i['x'])\n",
    "            x_min = min(x)\n",
    "            y_min = min(y)\n",
    "            diff = max(y) - min(y)\n",
    "            x_y[count_product_code] = [{\n",
    "                'x': min(x),\n",
    "                'y': min(y)\n",
    "            }, {\n",
    "                'x': max(x),\n",
    "                'y': max(y)\n",
    "            }]\n",
    "            dict_x_y.append(x_y)\n",
    "            min_x_1.append(x_min)\n",
    "            min_y_1.append(y_min)\n",
    "        region_line_items_x[ent] = min_x_1\n",
    "        region_line_items_y[ent] = min_y_1\n",
    "        sorted_lst_x_y = sorted(dict_x_y,\n",
    "                                key=lambda x: list(x.values())[0][0]['y'])\n",
    "        region_line_items[ent] = sorted_lst_x_y\n",
    "        opt_region[ent] = diff\n",
    "    sorted_region_line_y = {\n",
    "        key: sorted(values)\n",
    "        for key, values in region_line_items_y.items()\n",
    "    }\n",
    "    sorted_region_line_x = {\n",
    "        key: sorted(values)\n",
    "        for key, values in region_line_items_x.items()\n",
    "    }\n",
    "\n",
    "    #print(region_line_items)\n",
    "    regions_line_y_final = {}\n",
    "    opt_region_ent = {}\n",
    "    for region_line in sorted_region_line_y:\n",
    "        line_no = 0\n",
    "        line_range = {}\n",
    "        for i in range(len(sorted_region_line_y[region_line])):\n",
    "            line_no += 1\n",
    "            try:\n",
    "                pair = (sorted_region_line_y[region_line][i],\n",
    "                        sorted_region_line_y[region_line][i + 1])\n",
    "            except IndexError:\n",
    "                pair = (sorted_region_line_y[region_line][i])\n",
    "            line_range[line_no] = pair\n",
    "        if region_line in regions_line_y_final.keys():\n",
    "            regions_line_y_final[region_line].append(line_range)\n",
    "        else:\n",
    "            regions_line_y_final[region_line] = [line_range]\n",
    "        opt_region_ent[region_line] = max(\n",
    "            sorted_region_line_y[region_line]) - min(\n",
    "                sorted_region_line_y[region_line])\n",
    "\n",
    "    max_value = max(opt_region_ent.values())  # Find the maximum value\n",
    "\n",
    "    selected_values = [\n",
    "        key for key, value in opt_region_ent.items()\n",
    "        if abs(value - max_value) < 0.005\n",
    "    ]  # Select values satisfying the condition\n",
    "\n",
    "    if len(selected_values) > 1:\n",
    "        considered_boundry_ent = ''\n",
    "        final_y = 1\n",
    "        for selected_ent in selected_values:\n",
    "            if final_y > min(sorted_region_line_y[selected_ent]):\n",
    "                final_y = min(sorted_region_line_y[selected_ent])\n",
    "                considered_boundry_ent = selected_ent\n",
    "            else:\n",
    "                pass\n",
    "    else:\n",
    "        considered_boundry_ent = selected_values[0]\n",
    "\n",
    "    sub_entities_list = copy.deepcopy(line_item_sub_entities)\n",
    "    line_item_dict_final = {}\n",
    "    sub_entities_categorized = []\n",
    "    count = 0\n",
    "\n",
    "    for subentity in sub_entities_list:\n",
    "        y_ent = []\n",
    "        for ver in subentity['pageAnchor']['pageRefs'][0]['boundingPoly'][\n",
    "                'normalizedVertices']:\n",
    "            y_ent.append(ver['y'])\n",
    "        for line, region in regions_line_y_final[considered_boundry_ent][\n",
    "                0].items():\n",
    "\n",
    "            try:\n",
    "                if (min(y_ent) >= region[0] or max(y_ent)\n",
    "                        >= region[0]) and (max(y_ent) < region[1]):\n",
    "\n",
    "                    if line in line_item_dict_final.keys():\n",
    "                        count = count + 1\n",
    "                        if subentity not in sub_entities_categorized:\n",
    "                            line_item_dict_final[line].append(subentity)\n",
    "                            sub_entities_categorized.append(subentity)\n",
    "                    else:\n",
    "                        count = count + 1\n",
    "\n",
    "                        if subentity not in sub_entities_categorized:\n",
    "                            line_item_dict_final[line] = [subentity]\n",
    "                            sub_entities_categorized.append(subentity)\n",
    "\n",
    "            except TypeError:\n",
    "                if min(y_ent) >= region:  #-0.005:\n",
    "                    if line in line_item_dict_final.keys():\n",
    "                        count = count + 1\n",
    "                        if subentity not in sub_entities_categorized:\n",
    "                            line_item_dict_final[line].append(subentity)\n",
    "                            sub_entities_categorized.append(subentity)\n",
    "                    else:\n",
    "                        count = count + 1\n",
    "                        if subentity not in sub_entities_categorized:\n",
    "                            line_item_dict_final[line] = [subentity]\n",
    "                            sub_entities_categorized.append(subentity)\n",
    "\n",
    "    for item in sub_entities_list:\n",
    "        if item not in sub_entities_categorized:\n",
    "            y_ent = []\n",
    "            #print(regions_line_y_final[considered_boundry_ent])\n",
    "            for ver in item['pageAnchor']['pageRefs'][0]['boundingPoly'][\n",
    "                    'normalizedVertices']:\n",
    "                y_ent.append(ver['y'])\n",
    "            diff_line = {}\n",
    "            for line1, y1 in regions_line_y_final[considered_boundry_ent][\n",
    "                    0].items():\n",
    "                try:\n",
    "                    diff = abs(min(y_ent) - y1[0])\n",
    "                    diff_line[line1] = diff\n",
    "                except TypeError:\n",
    "                    diff = abs(min(y_ent) - y1)\n",
    "                    diff_line[line1] = diff\n",
    "\n",
    "            min_dist = min(diff_line.values())\n",
    "            line_item_2 = [\n",
    "                key for key, value in diff_line.items() if value == min_dist\n",
    "            ]\n",
    "            if line_item_2[0] in line_item_dict_final.keys():\n",
    "                line_item_dict_final[line_item_2[0]].append(item)\n",
    "                sub_entities_categorized.append(item)\n",
    "            else:\n",
    "                line_item_dict_final[line_item_2[0]] = [item]\n",
    "                sub_entities_categorized.append(item)\n",
    "\n",
    "    temp3 = []\n",
    "    for element in line_item_sub_entities:\n",
    "        if element not in sub_entities_categorized:\n",
    "            temp3.append(element)\n",
    "    if len(sub_entities_categorized) < len(line_item_sub_entities):\n",
    "        left_out = len(line_item_sub_entities) - len(sub_entities_categorized)\n",
    "        print(\n",
    "            f'out of {len(line_item_sub_entities)} subentities,{left_out}  are not yet classified '\n",
    "        )\n",
    "        pass\n",
    "    elif len(sub_entities_categorized) == len(line_item_sub_entities):\n",
    "        print('All lineitems are classified')\n",
    "        pass\n",
    "    else:\n",
    "        print('something is wrong in classified')\n",
    "        pass\n",
    "\n",
    "    def create_lineitem(line_item_sub_entities, page):\n",
    "        \"\"\" \n",
    "         \n",
    "            This Function will create a parent item and add the child items into it \n",
    "    \n",
    "            args: list of child entities and page number\n",
    "    \n",
    "            output: grouped parent items \n",
    "            \n",
    "        \"\"\"\n",
    "        line_item = {\n",
    "            'mentionText': '',\n",
    "            'pageAnchor': {\n",
    "                'pageRefs': [{\n",
    "                    'boundingPoly': {\n",
    "                        'normalizedVertices': []\n",
    "                    },\n",
    "                    'page': page\n",
    "                }]\n",
    "            },\n",
    "            'properties': [],\n",
    "            'textAnchor': {\n",
    "                'textSegments': []\n",
    "            },\n",
    "            'type': 'line_item'\n",
    "        }\n",
    "\n",
    "        text_anchors_sub_entities = []\n",
    "        for item in line_item_sub_entities:\n",
    "            text_anchors_sub_entities.append(\n",
    "                item['textAnchor']['textSegments'][0])\n",
    "        line_item['textAnchor']['textSegments'] = text_anchors_sub_entities\n",
    "        sorted_list_text1 = sorted(text_anchors_sub_entities,\n",
    "                                   key=lambda x: int(x['endIndex']))\n",
    "        #print(sorted_list_text1)\n",
    "        line_item_mention_text = ''\n",
    "        line_item_properties = []\n",
    "        line_item_text_segments = []\n",
    "        line_item_normalizedvertices = []\n",
    "        subentities_classified = []\n",
    "        for index in sorted_list_text1:\n",
    "            for item in line_item_sub_entities:\n",
    "                if index in item['textAnchor']['textSegments']:\n",
    "                    if item not in subentities_classified:\n",
    "                        subentities_classified.append(item)\n",
    "                        line_item_mention_text = line_item_mention_text + ' ' + item[\n",
    "                            'mentionText']\n",
    "                        line_item_properties.append(item)\n",
    "                        line_item_text_segments.append(index)\n",
    "                        for i in item['pageAnchor']['pageRefs'][0][\n",
    "                                'boundingPoly']['normalizedVertices']:\n",
    "                            line_item_normalizedvertices.append(i)\n",
    "\n",
    "        min_x_ln_1 = min(line_item_normalizedvertices,\n",
    "                         key=lambda d: d['x'])['x']\n",
    "        max_x_ln_1 = max(line_item_normalizedvertices,\n",
    "                         key=lambda d: d['x'])['x']\n",
    "        min_y_ln_1 = min(line_item_normalizedvertices,\n",
    "                         key=lambda d: d['y'])['y']\n",
    "        max_y_ln_1 = max(line_item_normalizedvertices,\n",
    "                         key=lambda d: d['y'])['y']\n",
    "        line_item_normalizedvertices_final = [{\n",
    "            'x': min_x_ln_1,\n",
    "            'y': min_y_ln_1\n",
    "        }, {\n",
    "            'x': min_x_ln_1,\n",
    "            'y': max_y_ln_1\n",
    "        }, {\n",
    "            'x': max_x_ln_1,\n",
    "            'y': min_y_ln_1\n",
    "        }, {\n",
    "            'x': max_x_ln_1,\n",
    "            'y': max_y_ln_1\n",
    "        }]\n",
    "\n",
    "        line_item['pageAnchor']['pageRefs'][0]['boundingPoly'][\n",
    "            'normalizedVertices'] = line_item_normalizedvertices_final\n",
    "        line_item['mentionText'] = line_item_mention_text\n",
    "        line_item['properties'] = line_item_properties\n",
    "        line_item['textAnchor']['textSegments'] = line_item_text_segments\n",
    "\n",
    "        return line_item\n",
    "\n",
    "    line_items_classified = []\n",
    "\n",
    "    for key, line_item_1 in line_item_dict_final.items():\n",
    "\n",
    "        line_item = create_lineitem(line_item_1, page)\n",
    "\n",
    "        line_items_classified.append(line_item)\n",
    "    return line_items_classified, considered_boundry_ent\n",
    "\n",
    "\n",
    "def merge_entities(json_dict):\n",
    "    \"\"\" \n",
    "    This Function will consolidate and group the child items into correct parent entities\n",
    "    consolidated function\n",
    "    \n",
    "    args: loaded json\n",
    "    \n",
    "    output: updated json with grouped entities\n",
    "    \n",
    "    \"\"\"\n",
    "    entitites_page_wise = get_page_wise_entities(json_dict)\n",
    "\n",
    "    line_entities_classified_pagewise = []\n",
    "    for page, entities in entitites_page_wise.items():\n",
    "        line_entities_temp = ''\n",
    "        line_item_count = line_item_check(entities, unique_entities)\n",
    "        if line_item_count == 1:\n",
    "            line_entities_temp = single_line_item_merge(entities, page)\n",
    "            line_entities_classified_pagewise.append(line_entities_temp)\n",
    "        elif line_item_count > 1:\n",
    "            line_entities_temp, considered_boundry_ent = multi_page_entites(\n",
    "                entities, page)\n",
    "            #print(considered_boundry_ent)\n",
    "            for ent1 in line_entities_temp:\n",
    "                line_entities_classified_pagewise.append(ent1)\n",
    "        elif line_item_count == 0:\n",
    "            print('no line items')\n",
    "\n",
    "    #print(line_entities_classified_pagewise)\n",
    "    final_entities = []\n",
    "    if len(line_entities_classified_pagewise) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        for entity in json_dict['entities']:\n",
    "            if entity['type'] != 'line_item':\n",
    "                final_entities.append(entity)\n",
    "\n",
    "        #final_entities.append(line_entities_classified_pagewise)\n",
    "        for ent in line_entities_classified_pagewise:\n",
    "            final_entities.append(ent)\n",
    "        #print(json_dict['entities'])\n",
    "        json_dict['entities'] = final_entities\n",
    "\n",
    "    return json_dict\n",
    "\n",
    "\n",
    "def desc_merge_update(json_dict):\n",
    "    \"\"\" \n",
    "    This Function will group multiple descriptions in a line item , in case description has multiple lines\n",
    "    into single description \n",
    "    \n",
    "    args: loaded json\n",
    "    \n",
    "    output: grouped description inside line items\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def desc_merge_1(ent_desc):\n",
    "        desc_merge = {\n",
    "            'mentionText': '',\n",
    "            'pageAnchor': {\n",
    "                'pageRefs': ''\n",
    "            },\n",
    "            'textAnchor': {\n",
    "                'textSegments': []\n",
    "            },\n",
    "            'type': 'line_item/description'\n",
    "        }\n",
    "\n",
    "        text_anchors_desc_merge = []\n",
    "        pagerefs = ''\n",
    "        for item in ent_desc:\n",
    "            text_anchors_desc_merge.append(\n",
    "                item['textAnchor']['textSegments'][0])\n",
    "            pagerefs = item['pageAnchor']['pageRefs']\n",
    "        desc_merge['pageAnchor']['pageRefs'] = pagerefs\n",
    "        desc_merge['textAnchor']['textSegments'] = text_anchors_desc_merge\n",
    "        sorted_list_text1 = sorted(text_anchors_desc_merge,\n",
    "                                   key=lambda x: int(x['endIndex']))\n",
    "        # print(sorted_list_text1)\n",
    "        desc_mention_text = ''\n",
    "        desc_text_segments = []\n",
    "        desc_normalizedvertices = []\n",
    "        subentities_classified = []\n",
    "        for index in sorted_list_text1:\n",
    "            for item in ent_desc:\n",
    "                if index in item['textAnchor']['textSegments']:\n",
    "                    if item not in subentities_classified:\n",
    "                        subentities_classified.append(item)\n",
    "                        desc_mention_text = (desc_mention_text + ' ' +\n",
    "                                             item['mentionText'])\n",
    "                        desc_text_segments.append(index)\n",
    "                        for i in item['pageAnchor']['pageRefs'][0][\n",
    "                                'boundingPoly']['normalizedVertices']:\n",
    "                            desc_normalizedvertices.append(i)\n",
    "\n",
    "        #print(desc_normalizedvertices)\n",
    "        min_x_dsc = min(desc_normalizedvertices, key=lambda d: d['x'])['x']\n",
    "        max_x_dsc = max(desc_normalizedvertices, key=lambda d: d['x'])['x']\n",
    "        min_y_dsc = min(desc_normalizedvertices, key=lambda d: d['y'])['y']\n",
    "        max_y_dsc = max(desc_normalizedvertices, key=lambda d: d['y'])['y']\n",
    "        #print(min_x_dsc,max_x_dsc,min_y_dsc,max_y_dsc)\n",
    "        desc_normalizedvertices_final = [{\n",
    "            'x': min_x_dsc,\n",
    "            'y': min_y_dsc\n",
    "        }, {\n",
    "            'x': min_x_dsc,\n",
    "            'y': max_y_dsc\n",
    "        }, {\n",
    "            'x': max_x_dsc,\n",
    "            'y': min_y_dsc\n",
    "        }, {\n",
    "            'x': max_x_dsc,\n",
    "            'y': max_y_dsc\n",
    "        }]\n",
    "        #print(desc_normalizedvertices_final)\n",
    "\n",
    "        desc_merge['pageAnchor']['pageRefs'][0]['boundingPoly'][\n",
    "            'normalizedVertices'] = desc_normalizedvertices_final\n",
    "        desc_merge['mentionText'] = desc_mention_text\n",
    "        desc_merge['textAnchor']['textSegments'] = desc_text_segments\n",
    "\n",
    "        return desc_merge\n",
    "\n",
    "    for entity in json_dict['entities']:\n",
    "        line_en = []\n",
    "        ent_desc = []\n",
    "        desc_ent_merge = {}\n",
    "        if entity['type'] == 'line_item':\n",
    "            line_en.append(entity['properties'])\n",
    "\n",
    "        for itm in line_en:\n",
    "            for ent1 in itm:\n",
    "                if ent1['type'] == 'line_item/description':\n",
    "                    ent_desc.append(ent1)\n",
    "        if len(ent_desc) > 1:\n",
    "            desc_merge = desc_merge_1(ent_desc)\n",
    "            if entity['type'] == 'line_item':\n",
    "                for en2 in ent_desc:\n",
    "                    if en2 in entity['properties']:\n",
    "                        del entity['properties'][entity['properties'].index(\n",
    "                            en2)]\n",
    "            entity['properties'].append(desc_merge)\n",
    "            #pprint(entity)\n",
    "\n",
    "    return json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Client X - Format 1 - Dummy Invoice - Sprint 4.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  20%|██        | 1/5 [00:00<00:01,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Client X - Format 3 - Dummy Invoice - Sprint 4 Demo.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  40%|████      | 2/5 [00:00<00:01,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21dcad73e9467e51-Client Y Format 1 - Multi-line - v1.2.json\n",
      "All lineitems are classified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  60%|██████    | 3/5 [00:01<00:00,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Client Y - Format 1 - Dummy Invoice - Sprint 4 Demo.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  80%|████████  | 4/5 [00:01<00:00,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Multi-Party Dummy Invoice - Sprint 4 Demo.json\n",
      "All lineitems are classified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 5/5 [00:02<00:00,  2.47it/s]\n"
     ]
    }
   ],
   "source": [
    "##Calling functions\n",
    "\n",
    "fs = gcsfs.GCSFileSystem(project_id)\n",
    "file_names_list, file_dict = file_names(Gcs_input_path)\n",
    "for filename, filepath in tqdm(file_dict.items(), desc='Progress'):\n",
    "    input_bucket_name = Gcs_input_path.split('/')[2]\n",
    "    if '.json' in filepath:\n",
    "        #output_bucket_name=Gcs_output_path.split('/')[2]\n",
    "        filepath = \"gs://\" + input_bucket_name + '/' + filepath\n",
    "        print(filename)\n",
    "        json_dict = load_json(filepath)\n",
    "        json_dict_updated = merge_entities(json_dict)\n",
    "\n",
    "        if line_item_across_pages == 'Yes':\n",
    "            json_dict_updated = get_lineitems_grouped_pages_across(\n",
    "                json_dict_updated)\n",
    "\n",
    "        if Desc_merge_update == 'Yes':\n",
    "            json_dict_updated = desc_merge_update(json_dict_updated)\n",
    "\n",
    "        fs.pipe(Gcs_output_path + '/' + filename,\n",
    "                bytes(json.dumps(json_dict_updated, ensure_ascii=False),\n",
    "                      'utf-8'),\n",
    "                content_type='application/json')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m104",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m104"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
