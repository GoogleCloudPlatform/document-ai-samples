{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72fd064f-24f5-4d61-b0ad-2b2f3fe9427d",
   "metadata": {},
   "source": [
    "# Post-Processing of Negative Values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5756f1a-631f-4c8a-bba0-98c6821d31a9",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1d12ef-55dd-4fbd-8389-db14ed038eb1",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the DocAI Incubator Team. No guarantees of performance are implied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94527514-1ae2-470b-96e2-0f48e4aa5e81",
   "metadata": {},
   "source": [
    "## Purpose and Description\n",
    "If you require a post processing script which corrects the negative values especially the amount. These values are enclosed within a round brackets, for example ‘(123.99)’, which indicates a negative value provided by the OCR. For every documents such entities are to identified, their round brackets are removed and prefixed with a minus ‘-’ symbol for every occurences using a python script. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8783f52-627b-4efa-b5d9-664ae2ca2564",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Vertex AI Notebook\n",
    "2. Parsed json files in GCS Folder.\n",
    "3. Output folder to upload the updated json files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cc5540-deb1-4449-8278-716488c54e5c",
   "metadata": {},
   "source": [
    "## Step by Step procedure "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49f5b2d-f7fd-4403-a175-b95cc804f6ba",
   "metadata": {},
   "source": [
    "### 1. Input details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62ef3a7a-e8ea-4045-8f54-ffe5dd4c4d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89120af5-c5f4-4897-a640-4ad9c5ce4739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT : storage bucket name\n",
    "input_path = \"gs://xxxxxxxx_xxxx_xxxx/JSON_OUTPUT/output_/\"\n",
    "# OUTPUT : storage bucket's path\n",
    "output_path = \"gs://xxxxxxx_xxxx_xxxx/JSON_OUTPUT/output_PostProcessed_xxxx/\"\n",
    "types = [\"entity_1\", \"entity_2\", \"entity_3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6a140e-bdd2-4014-9e10-2a11a6f5a0ef",
   "metadata": {},
   "source": [
    "**`input_path`**: GCS Storage name. It is DocAI processed output json files. This bucket is used for processing input files and saving output files in the folders.<br>\n",
    "**`output_path`**: GCS URI of the folder, where the dataset is exported from the processor.<br>\n",
    "**`types`**:Input the name of the entities type for which the correction should happen in the list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84356940-95a8-4489-bfc3-b85611f9558a",
   "metadata": {},
   "source": [
    "### 2. Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b50561-6cd5-433b-88db-b1bcfbbaaacb",
   "metadata": {},
   "source": [
    "The post processed json field can be found in the storage path provided by the user during the script execution that is output_bucket_path. <br><hr>\n",
    "<b>Comparison Between Input and Output File</b><br><br>\n",
    "<i><h4>Post processing results<h4><i><br>\n",
    "The following table shows the result of correction of entities having negative values from a sample json document. The key:value pairs which are to be processed are as shown.  There are two cases in the example. One is presence of the negative value in the entities key and other is the example of presence of negative value in the properties key belonging to the entities key. The script works by correcting both cases.\n",
    "\n",
    "<img src=\"./Images/negative_value_comparison_1.png\" width=800 height=400 alt=\"Negative value pre post comparison image\">\n",
    "<img src=\"./Images/negative_value_comparison_2.png\" width=800 height=400 alt=\"Negative value pre post comparison image\">\n",
    "    \n",
    "<i><h4>Processor: Before and After images</h4></i><br>\n",
    "The differences observed before and after post-processing json docs and importing into Document AI processor\n",
    "<table style=\"float:left\">\n",
    "<tr style=\"border: 1px solid black\"><td>\n",
    "<img src=\"./Images/negative_value_comparison_3.png\" width=800 height=400 alt=\"Negative value pre post comparison image\"></td></tr>\n",
    "<tr style=\"border: 1px solid black\"><td>\n",
    "<img src=\"./Images/negative_value_comparison_4.png\" width=800 height=400 alt=\"Negative value pre post comparison image\"></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fa4328-3d99-4de4-a5eb-0f2033d78b79",
   "metadata": {},
   "source": [
    "### 3. Run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00cc7a6-1635-4e78-bea9-60539670b231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import json\n",
    "from google.cloud import storage\n",
    "from tqdm.notebook import tqdm\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from utilities import (\n",
    "    documentai_json_proto_downloader,\n",
    "    store_document_as_json,\n",
    "    file_names,\n",
    ")\n",
    "\n",
    "# INPUT : storage bucket name\n",
    "input_path = \"gs://xxxx/xxxxxx/xxxxxxxx/\"\n",
    "# OUTPUT : storage bucket's path\n",
    "output_path = \"gs://xxxxx/xxxxxxx/xxxxxxxx/\"\n",
    "\n",
    "\n",
    "input_storage_bucket_name = input_path.split(\"/\")[2]\n",
    "input_bucket_path_prefix = \"/\".join(input_path.split(\"/\")[3:])\n",
    "output_storage_bucket_name = output_path.split(\"/\")[2]\n",
    "output_bucket_path_prefix = \"/\".join(output_path.split(\"/\")[3:])\n",
    "\n",
    "\n",
    "# Provide the list entity types for which are having negative values in '()' formats\n",
    "types = [\"case_number\", \"first_name\", \"last_name\", \"annual_income\", \"date_signed\"]\n",
    "\n",
    "json_files = file_names(input_path)[1].values()\n",
    "list_of_files = [i for i in list(json_files) if i.endswith(\".json\")]\n",
    "\n",
    "for k in tqdm(range(0, len(list_of_files))):\n",
    "    document = documentai_json_proto_downloader(\n",
    "        input_storage_bucket_name, list_of_files[k]\n",
    "    )\n",
    "\n",
    "    for entity in document.entities:\n",
    "        if entity.type in types:\n",
    "            try:\n",
    "                textAnchor_Content = entity.text_anchor.content\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            textAnchor_textSegments_startidx = entity.text_anchor.text_segments[\n",
    "                0\n",
    "            ].start_index\n",
    "            textAnchor_textSegments_endidx = entity.text_anchor.text_segments[\n",
    "                0\n",
    "            ].end_index\n",
    "\n",
    "            startidx_increment = int(textAnchor_textSegments_startidx)  # - 5\n",
    "            endidx_decrement = int(textAnchor_textSegments_endidx)  # + 5\n",
    "\n",
    "            text_select_left = document.text[startidx_increment - 5 : endidx_decrement]\n",
    "            text_select_right = document.text[startidx_increment : endidx_decrement + 5]\n",
    "\n",
    "            if (\n",
    "                (\"(\" in text_select_left)\n",
    "                and (\")\") in text_select_right\n",
    "                or (\"{\" in text_select_left)\n",
    "                and (\"}\") in text_select_right\n",
    "            ):\n",
    "                entity.mention_text = \"-\" + entity.mention_text.replace(\n",
    "                    \"(\", \"\"\n",
    "                ).replace(\")\", \"\")\n",
    "                try:\n",
    "                    entity.normalized_value.text = (\n",
    "                        \"-\"\n",
    "                        + entity.normalized_value.text.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "                    )\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    entity.text_anchor.content = (\n",
    "                        \"-\"\n",
    "                        + entity.text_anchor.content.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "                    )\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            if entity.properties:\n",
    "                for lt in range(0, len(entity.properties)):\n",
    "                    entity_prop = entity.properties[lt]\n",
    "\n",
    "                    textAnchor_Content = entity_prop.text_anchor.content\n",
    "                    textAnchor_textSegments_startidx = (\n",
    "                        entity_prop.text_anchor.text_segments[0].start_index\n",
    "                    )\n",
    "                    textAnchor_textSegments_endidx = (\n",
    "                        entity_prop.text_anchor.text_segments[0].end_index\n",
    "                    )\n",
    "\n",
    "                    startidx_increment = int(textAnchor_textSegments_startidx) - 5\n",
    "                    endidx_decrement = int(textAnchor_textSegments_endidx) + 5\n",
    "\n",
    "                    text_select = document.text[startidx_increment:endidx_decrement]\n",
    "\n",
    "                    if ((\"(\" in text_select) and (\")\") in text_select) or (\n",
    "                        (\"{\" in text_select) and (\"}\") in text_select\n",
    "                    ):\n",
    "                        entity_prop.mention_text = (\n",
    "                            \"-\"\n",
    "                            + entity_prop.mention_text.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "                        )\n",
    "                        try:\n",
    "                            entity_prop.normalized_value.text = (\n",
    "                                \"-\"\n",
    "                                + entity_prop.normalized_value.text.replace(\n",
    "                                    \"(\", \"\"\n",
    "                                ).replace(\")\", \"\")\n",
    "                            )\n",
    "                        except:\n",
    "                            pass\n",
    "                        try:\n",
    "                            entity_prop.text_anchor.content = (\n",
    "                                \"-\"\n",
    "                                + entity_prop.text_anchor.content.replace(\n",
    "                                    \"(\", \"\"\n",
    "                                ).replace(\")\", \"\")\n",
    "                            )\n",
    "                        except:\n",
    "                            pass\n",
    "    store_document_as_json(\n",
    "        documentai.Document.to_json(document),\n",
    "        output_storage_bucket_name,\n",
    "        output_bucket_path_prefix + list_of_files[k].split(\"/\")[-1],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4cd9f2-ed3f-4df2-bd01-a2479a93bccc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
