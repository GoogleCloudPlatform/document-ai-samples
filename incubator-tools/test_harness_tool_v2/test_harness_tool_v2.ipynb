{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cbcbc6b-0c0b-42d5-8619-ebd1f4bf8657",
   "metadata": {},
   "source": [
    "# Test Harness Tool V2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a054d9da-3034-4fb0-b828-cd536198f68c",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108c2600-51ad-4085-9555-7f3332ccc1b4",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the **DocAI Incubator Team**. No guarantees of performance are implied.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8d9223-2537-4ae0-98a5-32e372d1419d",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "This tool automates the evaluation of Google Cloud Document AI (DocAI) Custom Document Extractor (CDE) processors by fetching a sample of documents from Google Cloud Storage (GCS), processing them through multiple iterations of a Generative AI-based CDE model, and comparing extraction results for accuracy and consistency. It measures entity matching percentage, fuzzy ratios, document-level and entity-level consistency, and latency metrics. The final output is a Google Sheets report summarizing extraction accuracy, consistency, and performance insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07918d4f-aae3-428d-8a71-4fbe2d6c8c90",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* Vertex AI JupyterLab Environment\n",
    "* Google Cloud Storage Bucket\n",
    "* CDE Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92d9b0c-4c03-41b6-aa2e-fcf838791610",
   "metadata": {},
   "source": [
    "## Step by Step procedure "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d50f73-088d-4b58-9cfc-f276f2201f9c",
   "metadata": {},
   "source": [
    "### 1.Importing Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad01d013-4198-463f-9c6b-2792ec407efc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a512cb48-2d05-4f19-9669-64af75a1edcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install google-cloud-documentai google-api-core google-cloud-storage tqdm pillow pypdf2 openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad415a6d-6b54-4241-a781-678a527f0ed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.api_core.operation import Operation\n",
    "from google.longrunning import operations_pb2\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from google.api_core.client_options import ClientOptions\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from google.cloud import storage\n",
    "from typing import (\n",
    "    Container,\n",
    "    Iterable,\n",
    "    Iterator,\n",
    "    List,\n",
    "    Mapping,\n",
    "    Optional,\n",
    "    Sequence,\n",
    "    Tuple,\n",
    "    Union,\n",
    "    Any,\n",
    "    Dict,\n",
    ")\n",
    "from io import BytesIO\n",
    "from pprint import pprint\n",
    "import copy\n",
    "from PIL import Image\n",
    "from PyPDF2 import PdfFileReader\n",
    "import io\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import concurrent.futures\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import json_folder_comparison\n",
    "import openpyxl\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Alignment\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.worksheet.worksheet import Worksheet\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.styles import Font\n",
    "from utilities import file_names, store_document_as_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110046b1-ebea-4a2b-990d-5acda4a3b66c",
   "metadata": {},
   "source": [
    "### 2.Setup the inputs\n",
    "\n",
    "* `project_id` : This is the unique identifier for the Google Cloud project.\n",
    "* `location` : This specifies the location or region where the resources are located.\n",
    "* `processor_id` : This is the unique CDE identifier for a processor in Google Cloud.\n",
    "* `processor_version_id` : This identifies the specific version of the processor or model you are using.\n",
    "* `gcs_input_path` : This is the path to a Google Cloud Storage (GCS) bucket and folder where input documents are stored.\n",
    "* `mime_type` : This specifies the MIME type of the files being processed.\n",
    "* `gcs_sync_output_path` : This is the GCS path where output from synchronous processing will be stored.\n",
    "* `gcs_async_output_path` : This is the GCS path where output from asynchronous (batch) processing will be stored.\n",
    "* `gcs_result_path` : This is the path to store the results, likely to be Excel sheets or similar, that summarize or process the output from both synchronous and asynchronous processing.\n",
    "* `process_type` : This is a list specifying which types of processes to use for the task. \"async\" refers to asynchronous processing, \"sync\" refers to synchronous processing, and both can be chosen if you want to use both types.\n",
    "* `timeout` : This defines the timeout period (in seconds) for the process to complete. If the process takes longer than this time, it will be stopped or failed.\n",
    "* `iteration` : This specifies the number of iterations to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41618163-fafe-438a-a8d4-25571fc600b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = \"project_id\"\n",
    "location = \"us\"\n",
    "processor_id = \"processor_id\"\n",
    "processor_version_id = \"pretrained-foundation-model-v1.2-2024-05-10\"\n",
    "gcs_input_path = \"gs://bucket_name/pdf_files_path/\"  # Contains Input PDF Files\n",
    "mime_type = \"application/pdf\"\n",
    "gcs_sync_output_path = \"gs://bucket_name/output_sync_process_files_path/\"  # Stores the gcs sync output files\n",
    "gcs_async_output_path = \"gs://bucket_name/output_async_process_files_path/\"  # Stores the gcs sync output files\n",
    "\n",
    "gcs_result_path = \"gs://bucket_name/sheets_output/\"  # Stores the resulted sheets\n",
    "\n",
    "process_type = [\"async\", \"sync\"]  # the values are sync or async or both.\n",
    "\n",
    "timeout = 6000\n",
    "iteration = 2  # No of iteration required\n",
    "sync_runtime = {}\n",
    "batch_runtime = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846011bb-eb92-454d-9ea9-6c9763490bce",
   "metadata": {},
   "source": [
    "### 3.Run the required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6127e1d9-6a45-4827-a10c-481343697abc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_current_ist_time() -> str:\n",
    "    \"\"\"\n",
    "    Get the current time in Indian Standard Time (IST).\n",
    "\n",
    "    Returns:\n",
    "        str: The current IST time formatted as 'YYYY-MM-DD HH:MM:SS IST'.\n",
    "    \"\"\"\n",
    "    ist_offset = timedelta(hours=5, minutes=30)  # IST is UTC+5:30\n",
    "    ist_time = datetime.now(timezone.utc) + ist_offset\n",
    "    return ist_time.strftime(\"%Y-%m-%d %H:%M:%S IST\")\n",
    "\n",
    "\n",
    "def online_process(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    processor_id: str,\n",
    "    processor_version_id: str,\n",
    "    gcs_bucket_name: str,\n",
    "    gcs_blob_name: str,\n",
    "    mime_type: str,\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Processes a document using the Document AI Online Processing API with a file from a GCS bucket.\n",
    "\n",
    "    Args:\n",
    "        project_id (str): The Google Cloud project ID.\n",
    "        location (str): The Google Cloud location (e.g., \"us\").\n",
    "        processor_id (str): The ID of the Document AI processor.\n",
    "        processor_version_id (str): The version of the Document AI processor to use.\n",
    "        gcs_bucket_name (str): The name of the Google Cloud Storage bucket where the file is stored.\n",
    "        gcs_blob_name (str): The name of the file (blob) in the GCS bucket.\n",
    "        mime_type (str): The MIME type of the document (e.g., \"application/pdf\").\n",
    "\n",
    "    Returns:\n",
    "        Any: The processed document result returned by Document AI, which contains the extracted information.\n",
    "    \"\"\"\n",
    "    opts = {\"api_endpoint\": f\"{location}-documentai.googleapis.com\"}\n",
    "\n",
    "    # Instantiates a client for Document AI\n",
    "    documentai_client = documentai.DocumentProcessorServiceClient(client_options=opts)\n",
    "\n",
    "    # Instantiates a client for GCS\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Access the bucket and blob\n",
    "    bucket = storage_client.bucket(gcs_bucket_name)\n",
    "    blob = bucket.blob(gcs_blob_name)\n",
    "\n",
    "    # Download the file content as bytes\n",
    "    image_content = blob.download_as_bytes()\n",
    "\n",
    "    # The full resource name of the processor\n",
    "    resource_name = documentai_client.processor_version_path(\n",
    "        project_id, location, processor_id, processor_version_id\n",
    "    )\n",
    "\n",
    "    # Load Binary Data into Document AI RawDocument Object\n",
    "    raw_document = documentai.RawDocument(content=image_content, mime_type=mime_type)\n",
    "\n",
    "    # Configure the process request\n",
    "    request = documentai.ProcessRequest(name=resource_name, raw_document=raw_document)\n",
    "\n",
    "    # Use the Document AI client to process the sample form\n",
    "    result = documentai_client.process_document(request=request)\n",
    "\n",
    "    document = result.document\n",
    "\n",
    "    # Uncomment to print the extracted text\n",
    "    # print(\"Text.......\")\n",
    "    # print(document.text)\n",
    "\n",
    "    return document\n",
    "\n",
    "\n",
    "def batch_process_documents(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    processor_id: str,\n",
    "    processor_version_id: str,\n",
    "    gcs_input_path: str,\n",
    "    gcs_async_output_path: str,\n",
    "    gcs_async_output_path_prefix: str,\n",
    "    timeout: int = 6000,\n",
    ") -> operations_pb2.Operation:\n",
    "    \"\"\"\n",
    "    Batch process documents using Document AI's asynchronous processing API.\n",
    "\n",
    "    Args:\n",
    "        project_id (str): The Google Cloud project ID.\n",
    "        location (str): The location of the Document AI processor (e.g., \"us\" or \"eu\").\n",
    "        processor_id (str): The ID of the Document AI processor.\n",
    "        processor_version_id (str): The version of the Document AI processor to use.\n",
    "        gcs_input_path (str): The GCS URI for the input files (e.g., \"gs://bucket/path/\").\n",
    "        gcs_async_output_path (str): The GCS URI where the output should be stored.\n",
    "        gcs_async_output_path_prefix (str): The prefix for the output files in the GCS bucket.\n",
    "        timeout (int, optional): The maximum wait time (in seconds) for the batch processing to finish. Default is 6000 seconds (100 minutes).\n",
    "\n",
    "    Returns:\n",
    "        documentai.types.operations_pb2.Operation: The operation object that tracks the status of the batch processing request.\n",
    "    \"\"\"\n",
    "    from google.cloud import documentai_v1beta3 as documentai\n",
    "\n",
    "    # You must set the api_endpoint if you use a location other than 'us', e.g.:\n",
    "    opts = {}\n",
    "    if location == \"eu\":\n",
    "        opts = {\"api_endpoint\": \"eu-documentai.googleapis.com\"}\n",
    "    elif location == \"us\":\n",
    "        opts = {\"api_endpoint\": \"us-documentai.googleapis.com\"}\n",
    "        # opts = {\"api_endpoint\": \"us-autopush-documentai.sandbox.googleapis.com\"}\n",
    "    client = documentai.DocumentProcessorServiceClient(client_options=opts)\n",
    "\n",
    "    destination_uri = f\"{gcs_async_output_path}{gcs_async_output_path_prefix}/\"\n",
    "    input_config = documentai.BatchDocumentsInputConfig(\n",
    "        gcs_prefix=documentai.GcsPrefix(gcs_uri_prefix=gcs_input_path)\n",
    "    )\n",
    "\n",
    "    # Where to write results\n",
    "    output_config = documentai.DocumentOutputConfig(\n",
    "        gcs_output_config={\"gcs_uri\": destination_uri}\n",
    "    )\n",
    "\n",
    "    # Location can be 'us' or 'eu'\n",
    "    name = f\"projects/{project_id}/locations/{location}/processors/{processor_id}/processorVersions/{processor_version_id}\"\n",
    "    request = documentai.types.document_processor_service.BatchProcessRequest(\n",
    "        name=name,\n",
    "        input_documents=input_config,\n",
    "        document_output_config=output_config,\n",
    "    )\n",
    "\n",
    "    operation = client.batch_process_documents(request)\n",
    "\n",
    "    # Wait for the operation to finish\n",
    "    operation.result(timeout=timeout)\n",
    "    return operation\n",
    "\n",
    "\n",
    "def process_file_wrapper(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    processor_id: str,\n",
    "    processor_version_id: str,\n",
    "    gcs_bucket_name: str,\n",
    "    gcs_blob_name: str,\n",
    "    mime_type: str,\n",
    "    run_id: int,\n",
    "    gcs_sync_output_path: str,\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Wrapper function to call the `online_process` function for document processing.\n",
    "\n",
    "    It logs the start and end times for the processing, calculates the runtime,\n",
    "    and stores the result as a JSON file in the specified GCS path.\n",
    "\n",
    "    Args:\n",
    "        project_id (str): The ID of the Google Cloud project.\n",
    "        location (str): The location (region) where the processor is hosted.\n",
    "        processor_id (str): The ID of the document processor.\n",
    "        processor_version_id (str): The version ID of the processor.\n",
    "        gcs_bucket_name (str): The name of the GCS bucket containing the input file.\n",
    "        gcs_blob_name (str): The name of the input file in the GCS bucket.\n",
    "        mime_type (str): The MIME type of the document (e.g., \"application/pdf\").\n",
    "        run_id (int): The iteration run ID for tracking multiple runs.\n",
    "        gcs_sync_output_path (str): The GCS path to store the output JSON file.\n",
    "\n",
    "    Returns:\n",
    "        Any: The processed document (likely a `documentai.Document` object).\n",
    "    \"\"\"\n",
    "    start_time = datetime.now()\n",
    "    # print(f\"Run {run_id} started at {start_time}\")\n",
    "\n",
    "    # Simulate processing\n",
    "    document = online_process(\n",
    "        project_id=project_id,\n",
    "        location=location,\n",
    "        processor_id=processor_id,\n",
    "        processor_version_id=processor_version_id,\n",
    "        gcs_bucket_name=gcs_bucket_name,\n",
    "        gcs_blob_name=gcs_blob_name,\n",
    "        mime_type=mime_type,\n",
    "    )\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    # print(f\"Run {run_id} ended at {end_time}\")\n",
    "    # print(f\"Run {run_id} duration: {end_time - start_time}\")\n",
    "\n",
    "    file = gcs_blob_name.split(\"/\")[-1].split(\".\")[0] + \".json\"\n",
    "\n",
    "    if file not in sync_runtime.keys():\n",
    "        sync_runtime[file] = [0] * iteration\n",
    "    sync_runtime[file][int(run_id) - 1] = (end_time - start_time).total_seconds()\n",
    "\n",
    "    json_data = documentai.Document.to_dict(document)\n",
    "\n",
    "    file_name = (\n",
    "        \"/\".join(gcs_sync_output_path.split(\"/\")[3:])\n",
    "        + \"iteration_\"\n",
    "        + str(run_id)\n",
    "        + \"/\"\n",
    "        + gcs_blob_name.split(\"/\")[-1].split(\".\")[0]\n",
    "        + \".json\"\n",
    "    )\n",
    "    # print(gcs_sync_output_path.split(\"/\")[2],file_name)\n",
    "    store_document_as_json(\n",
    "        json.dumps(json_data), gcs_sync_output_path.split(\"/\")[2], file_name\n",
    "    )\n",
    "    # print(sync_runtime)\n",
    "    return document\n",
    "\n",
    "\n",
    "def run_parallel_processing() -> None:\n",
    "    \"\"\"\n",
    "    Run the processing of the same file multiple times in parallel and confirm parallel execution.\n",
    "\n",
    "    This function executes the `process_file_wrapper` in parallel for each file in `gcs_input_path`,\n",
    "    and runs the processing `iteration` times in parallel for each file.\n",
    "\n",
    "    Assumes that the following global variables are defined:\n",
    "        gcs_input_path (str): The GCS URI where the input files are stored.\n",
    "        iteration (int): Number of parallel runs for each file.\n",
    "        project_id (str): The Google Cloud project ID.\n",
    "        location (str): The location of the Document AI processor (e.g., 'us').\n",
    "        processor_id (str): The processor ID to be used for processing.\n",
    "        processor_version_id (str): The processor version ID to be used.\n",
    "        mime_type (str): The MIME type of the input file (e.g., 'application/pdf').\n",
    "        gcs_sync_output_path (str): The GCS URI to store the processed output.\n",
    "\n",
    "    Returns:\n",
    "        None: This function does not return any value, but it will print status for each processed document.\n",
    "    \"\"\"\n",
    "\n",
    "    gcs_bucket_name = gcs_input_path.split(\"/\")[2]\n",
    "\n",
    "    for gcs_blob_names in file_names(gcs_input_path)[1].values():\n",
    "        gcs_blob_name = gcs_blob_names\n",
    "\n",
    "        # Number of parallel runs\n",
    "        num_runs = iteration\n",
    "\n",
    "        # Use ThreadPoolExecutor for parallel execution\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            # Submit tasks for parallel execution\n",
    "            futures = [\n",
    "                executor.submit(\n",
    "                    process_file_wrapper,\n",
    "                    project_id,\n",
    "                    location,\n",
    "                    processor_id,\n",
    "                    processor_version_id,\n",
    "                    gcs_bucket_name,\n",
    "                    gcs_blob_name,\n",
    "                    mime_type,\n",
    "                    run_id,\n",
    "                    gcs_sync_output_path,\n",
    "                )\n",
    "                for run_id in range(1, num_runs + 1)\n",
    "            ]\n",
    "\n",
    "            # Collect results\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                try:\n",
    "                    document = future.result()  # Get the result of the task\n",
    "                    print(f\"\\t\\tSuccessfully processed document.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Task failed with error: {e}\")\n",
    "\n",
    "\n",
    "def batch_process_file_wrapper(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    processor_id: str,\n",
    "    processor_version_id: str,\n",
    "    gcs_input_path: str,\n",
    "    gcs_async_output_path: str,\n",
    "    gcs_async_output_path_prefix: str,\n",
    "    timeout: int,\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Wrapper function to call the `batch_process_documents` function for batch processing of documents.\n",
    "\n",
    "    It logs the start and end times for the processing, calculates the runtime,\n",
    "    and stores the results in the specified GCS output path prefix.\n",
    "\n",
    "    Args:\n",
    "        project_id (str): The ID of the Google Cloud project.\n",
    "        location (str): The location (region) where the processor is hosted.\n",
    "        processor_id (str): The ID of the document processor.\n",
    "        processor_version_id (str): The version ID of the processor.\n",
    "        gcs_input_path (str): The GCS path containing the input documents for batch processing.\n",
    "        gcs_async_output_path (str): The GCS path to store the output files.\n",
    "        gcs_async_output_path_prefix (str): The prefix for the output files in the GCS path.\n",
    "        timeout (int): The timeout duration for the operation in seconds.\n",
    "\n",
    "    Returns:\n",
    "        Any: The operation result from `batch_process_documents`, typically an operation object.\n",
    "    \"\"\"\n",
    "    start_time = datetime.now()\n",
    "    # print(f\"Run {run_id} started at {start_time}\")\n",
    "\n",
    "    # Simulate processing\n",
    "    operation = batch_process_documents(\n",
    "        project_id=project_id,\n",
    "        location=location,\n",
    "        processor_id=processor_id,\n",
    "        processor_version_id=processor_version_id,\n",
    "        gcs_input_path=gcs_input_path,\n",
    "        gcs_async_output_path=gcs_async_output_path,\n",
    "        gcs_async_output_path_prefix=gcs_async_output_path_prefix,\n",
    "        timeout=timeout,\n",
    "    )\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    # print(f\"Run {run_id} ended at {end_time}\")\n",
    "    # print(f\"Run {run_id} duration: {end_time - start_time}\")\n",
    "    key = gcs_async_output_path_prefix.replace(\"iteration_\", \"Iteration \")\n",
    "    if key not in batch_runtime.keys():\n",
    "        batch_runtime[key] = [(end_time - start_time).total_seconds()]\n",
    "\n",
    "    return operation\n",
    "\n",
    "\n",
    "def run_parallel_batch_processing() -> None:\n",
    "    \"\"\"\n",
    "    Run the batch processing of the same file multiple times in parallel,\n",
    "    confirming parallel execution and processing efficiency.\n",
    "\n",
    "    This function will submit tasks for batch processing (with `batch_process_file_wrapper`)\n",
    "    and execute them concurrently using a ThreadPoolExecutor. The results (success or failure)\n",
    "    will be printed once the tasks complete.\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of parallel runs\n",
    "    num_runs = iteration\n",
    "    # print(gcs_async_output_path)\n",
    "    # Use ThreadPoolExecutor for parallel execution\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Submit tasks for parallel execution\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                batch_process_file_wrapper,\n",
    "                project_id,\n",
    "                location,\n",
    "                processor_id,\n",
    "                processor_version_id,\n",
    "                gcs_input_path,\n",
    "                gcs_async_output_path,\n",
    "                \"iteration_\" + str(run_id),\n",
    "                timeout,\n",
    "            )\n",
    "            for run_id in range(1, num_runs + 1)\n",
    "        ]\n",
    "\n",
    "        # Collect results\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                document = future.result()  # Get the result of the task\n",
    "                print(f\"\\t\\tSuccessfully processed document.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Task failed with error: {e}\")\n",
    "\n",
    "\n",
    "def get_xlsx_from_gcp(file_path: str) -> pd.ExcelFile:\n",
    "    \"\"\"\n",
    "    Downloads an Excel file from GCP storage and returns it as a pandas ExcelFile object.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The GCS path to the Excel file (e.g., 'gs://bucket_name/path/to/file.xlsx').\n",
    "\n",
    "    Returns:\n",
    "    - pd.ExcelFile: The loaded Excel file object.\n",
    "\n",
    "    Raises:\n",
    "    - FileNotFoundError: If the specified file is not found in the GCS bucket.\n",
    "    - Exception: For any other errors during the file retrieval process.\n",
    "    \"\"\"\n",
    "    bucket_name = file_path.split(\"/\")[2]\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    file_path = \"/\".join(file_path.split(\"/\")[3:])\n",
    "    # print(file_path)\n",
    "    content = bucket.blob(file_path).download_as_bytes()\n",
    "    return pd.ExcelFile(BytesIO(content))\n",
    "\n",
    "\n",
    "def compare_latencies(\n",
    "    previous_file: str,\n",
    "    previous_file_name: str,\n",
    "    new_file: BytesIO,  # Assuming new_file is passed as BytesIO (from the `save` method)\n",
    "    new_file_name: str,\n",
    "    gcs_result_path: str,\n",
    "    date: str,\n",
    "    process_type: List[str] = [\"sync\", \"async\"],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Compares the latencies from two Excel files and generates a comparison report.\n",
    "\n",
    "    Args:\n",
    "        previous_file (str): The path to the previous Excel file.\n",
    "        previous_file_name (str): The name of the previous file.\n",
    "        new_file (BytesIO): The file object for the new Excel file.\n",
    "        new_file_name (str): The name of the new file.\n",
    "        gcs_result_path (str): The GCS path where the output file will be uploaded.\n",
    "        date (str): The date for file naming.\n",
    "        process_type (List[str]): List specifying the process types to compare (\"sync\" and/or \"async\").\n",
    "\n",
    "    Returns:\n",
    "        None: Saves the comparison file locally and uploads it to GCS.\n",
    "    \"\"\"\n",
    "    excel_buffer = BytesIO()\n",
    "    new_file.save(excel_buffer)\n",
    "    excel_buffer.seek(0)\n",
    "    output_file = \"latency_comparison.xlsx\"\n",
    "\n",
    "    valid_types = {\"sync\", \"async\"}\n",
    "    if not set(process_type).issubset(valid_types):\n",
    "        raise ValueError(\n",
    "            \"process_type must be a list containing any of 'sync' and 'async'\"\n",
    "        )\n",
    "\n",
    "    comparisons = {}\n",
    "    file_names = []\n",
    "\n",
    "    def compare_values(prev: float, new: float) -> Tuple[str, float]:\n",
    "        \"\"\"\n",
    "        Compares two values (previous and new) and classifies the change as\n",
    "        \"Increased\", \"Decreased\", or \"No Change\". Also calculates the difference.\n",
    "\n",
    "        Args:\n",
    "            prev (float or None): The previous latency value.\n",
    "            new (float or None): The new latency value.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[str, float or None]: A tuple containing the classification of the change\n",
    "            (\"Increased\", \"Decreased\", \"No Change\") and the difference (or None if missing data).\n",
    "        \"\"\"\n",
    "        if prev is None or new is None:\n",
    "            return \"Missing Data\", None\n",
    "        difference = new - prev\n",
    "        return (\n",
    "            \"Increased\" if new > prev else \"Decreased\" if new < prev else \"No Change\"\n",
    "        ), difference\n",
    "\n",
    "    if \"sync\" in process_type:\n",
    "        previous_sync = pd.read_excel(previous_file, sheet_name=\"Sync_RunTime\")\n",
    "        new_sync = pd.read_excel(excel_buffer, sheet_name=\"Sync_RunTime\")\n",
    "        sync_comparison = pd.merge(\n",
    "            previous_sync[[\"FileName\", \"Average\"]],\n",
    "            new_sync[[\"FileName\", \"Average\"]],\n",
    "            on=\"FileName\",\n",
    "            suffixes=(\"_Previous\", \"_New\"),\n",
    "            how=\"outer\",\n",
    "        )\n",
    "\n",
    "        sync_comparison[[\"Sync_Comparison\", \"Sync_Difference\"]] = sync_comparison.apply(\n",
    "            lambda row: compare_values(row[\"Average_Previous\"], row[\"Average_New\"]),\n",
    "            axis=1,\n",
    "            result_type=\"expand\",\n",
    "        )\n",
    "        comparisons[\"Sync_Comparison\"] = sync_comparison\n",
    "        file_names = sync_comparison[\"FileName\"].tolist()\n",
    "\n",
    "    if \"async\" in process_type:\n",
    "        previous_async = pd.read_excel(previous_file, sheet_name=\"Batch_RunTime\")\n",
    "        new_async = pd.read_excel(excel_buffer, sheet_name=\"Batch_RunTime\")\n",
    "\n",
    "        previous_batch_latency = (\n",
    "            previous_async[\"Average\"].values[0] if not previous_async.empty else None\n",
    "        )\n",
    "        new_batch_latency = (\n",
    "            new_async[\"Average\"].values[0] if not new_async.empty else None\n",
    "        )\n",
    "\n",
    "        if not file_names:\n",
    "            file_names = (\n",
    "                previous_async[\"Filename\"].tolist()\n",
    "                if \"Filename\" in previous_async\n",
    "                else []\n",
    "            )\n",
    "            file_names = (\n",
    "                new_async[\"Filename\"].tolist()\n",
    "                if not file_names and \"Filename\" in new_async\n",
    "                else file_names\n",
    "            )\n",
    "\n",
    "        async_comparison = pd.DataFrame(\n",
    "            {\n",
    "                \"Filename\": file_names,\n",
    "                \"Previous_Batch_Average\": [previous_batch_latency] * len(file_names),\n",
    "                \"New_Batch_Average\": [new_batch_latency] * len(file_names),\n",
    "                \"Batch_Comparison\": [\n",
    "                    compare_values(previous_batch_latency, new_batch_latency)[0]\n",
    "                ]\n",
    "                * len(file_names),\n",
    "                \"Batch_Difference\": [\n",
    "                    compare_values(previous_batch_latency, new_batch_latency)[1]\n",
    "                ]\n",
    "                * len(file_names),\n",
    "            }\n",
    "        )\n",
    "        comparisons[\"Batch_Comparison\"] = async_comparison\n",
    "\n",
    "    with pd.ExcelWriter(output_file, engine=\"openpyxl\") as writer:\n",
    "        for sheet_name, df in comparisons.items():\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    wb = load_workbook(output_file)\n",
    "    if \"Batch_Comparison\" in comparisons:\n",
    "        batch_sheet = wb[\"Batch_Comparison\"]\n",
    "        if len(file_names) > 1:\n",
    "\n",
    "            def merge_cells(\n",
    "                sheet, start_row: int, end_row: int, cols_to_merge: List[int]\n",
    "            ) -> None:\n",
    "                \"\"\"\n",
    "                Merges cells in the specified columns for the given row range.\n",
    "\n",
    "                Args:\n",
    "                    sheet (Worksheet): The Excel worksheet object.\n",
    "                    start_row (int): The starting row index.\n",
    "                    end_row (int): The ending row index.\n",
    "                    cols_to_merge (List[int]): List of columns to merge.\n",
    "\n",
    "                Returns:\n",
    "                    None: Merges the specified cells in the worksheet.\n",
    "                \"\"\"\n",
    "                for col in cols_to_merge:\n",
    "                    col_letter = get_column_letter(col)\n",
    "                    sheet.merge_cells(f\"{col_letter}{start_row}:{col_letter}{end_row}\")\n",
    "\n",
    "            merge_cells(batch_sheet, 2, len(file_names) + 1, [2, 3, 4, 5])\n",
    "\n",
    "    input_sheet = wb.create_sheet(title=\"Input_Details\", index=0)\n",
    "    input_details = {\n",
    "        \"Previous File\": previous_file_name,\n",
    "        \"New File\": f\"{'/'.join(gcs_result_path.split('/')[:3])}/{new_file_name}\",\n",
    "        \"Process Type\": \", \".join(process_type),\n",
    "    }\n",
    "    for row_index, (key, value) in enumerate(input_details.items(), start=1):\n",
    "        input_sheet.cell(row=row_index, column=1, value=key).font = Font(bold=True)\n",
    "        input_sheet.cell(row=row_index, column=2, value=value)\n",
    "\n",
    "    wb.save(output_file)\n",
    "    json_folder_comparison.upload_xlsx_to_gcs(\n",
    "        gcs_result_path.split(\"/\")[2],\n",
    "        output_file,\n",
    "        \"/\".join(gcs_result_path.split(\"/\")[3:])\n",
    "        + f\"latency_comparison/{output_file.split('.')[0]}_{date}.xlsx\",\n",
    "    )\n",
    "\n",
    "    print(f\"Comparison file saved as {output_file} with merged batch columns!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb2b3e0-8d10-4a57-83ab-330b041fa187",
   "metadata": {},
   "source": [
    "### 4.Run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b255b0-cb06-4f4e-8f0e-75c2d56a52bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Create a new workbook\n",
    "    valid_types = {\"sync\", \"async\"}\n",
    "    if not set(process_type).issubset(valid_types):\n",
    "        raise ValueError(\n",
    "            \"process_type must be a list containing any of 'sync' and 'async'\"\n",
    "        )\n",
    "    workbook = openpyxl.Workbook()\n",
    "\n",
    "    # Add Input_Details Sheet as the first sheet\n",
    "    input_details_sheet = workbook.active\n",
    "    input_details_sheet.title = \"Input_Details\"\n",
    "\n",
    "    # Input details dictionary with current date and time\n",
    "    input_details = {\n",
    "        \"Processor ID\": processor_id,\n",
    "        \"Processor Version ID\": processor_version_id,\n",
    "        \"Date\": get_current_ist_time(),\n",
    "    }\n",
    "\n",
    "    # Write the input details to the sheet\n",
    "    for row_index, (key, value) in enumerate(input_details.items(), start=1):\n",
    "        input_details_sheet.cell(row=row_index, column=1, value=key)\n",
    "        input_details_sheet.cell(row=row_index, column=2, value=value)\n",
    "\n",
    "    print(\"Done Input Details Processing\")\n",
    "\n",
    "    if \"sync\" in process_type:\n",
    "        print(\"Running Online or Sync Processing\")\n",
    "        run_parallel_processing()\n",
    "\n",
    "        # Add Sync Runtime Sheet\n",
    "        sync_sheet = workbook.create_sheet(title=\"Sync_RunTime\")\n",
    "\n",
    "        # Determine the number of iterations and set headers\n",
    "        num_iters = max(len(values) for values in sync_runtime.values())\n",
    "        sync_headers = [\"FileName\"] + [f\"Iteration {i+1}\" for i in range(num_iters)]\n",
    "\n",
    "        # Write the sync headers\n",
    "        for col_index, header in enumerate(sync_headers, start=1):\n",
    "            sync_sheet.cell(row=1, column=col_index, value=header)\n",
    "\n",
    "        # Write the sync data rows\n",
    "        for row_index, (filename, values) in enumerate(sync_runtime.items(), start=2):\n",
    "            sync_sheet.cell(\n",
    "                row=row_index, column=1, value=filename\n",
    "            )  # Write the filename\n",
    "            for col_index, value in enumerate(values, start=2):\n",
    "                sync_sheet.cell(\n",
    "                    row=row_index, column=col_index, value=value\n",
    "                )  # Write iteration values\n",
    "        # calculate Average and add to new column\n",
    "        sync_process_average = []\n",
    "        for index, values in enumerate(sync_runtime):\n",
    "            sync_process_average.append(\n",
    "                sum(sync_runtime[values]) / len(sync_runtime[values])\n",
    "            )\n",
    "        sync_max_column = sync_sheet.max_column + 1\n",
    "\n",
    "        for i, value in enumerate(sync_process_average, start=1):\n",
    "            sync_sheet.cell(row=i + 1, column=sync_max_column, value=value)\n",
    "        sync_sheet.cell(row=1, column=sync_max_column, value=\"Average\")\n",
    "\n",
    "        print(\"Done Online or Sync Processing\")\n",
    "        json_folder_comparison.main(\n",
    "            gcs_sync_output_path,\n",
    "            iteration,\n",
    "            \"sync\",\n",
    "            gcs_result_path,\n",
    "            input_details[\"Date\"],\n",
    "        )\n",
    "\n",
    "    if \"async\" in process_type:\n",
    "        batch_runtime[\"Filename\"] = file_names(gcs_input_path)[0]\n",
    "        print(\"Running Batch or Async Processing\")\n",
    "        run_parallel_batch_processing()\n",
    "\n",
    "        # Add Batch Runtime Sheet\n",
    "        batch_sheet = workbook.create_sheet(title=\"Batch_RunTime\")\n",
    "\n",
    "        batch_headers = [\"Filename\"] + list(batch_runtime.keys())[1:]\n",
    "\n",
    "        for col_index, header in enumerate(batch_headers, start=1):\n",
    "            batch_sheet.cell(row=1, column=col_index, value=header)\n",
    "\n",
    "        # Write filenames\n",
    "        for row_index, filename in enumerate(batch_runtime[\"Filename\"], start=2):\n",
    "            batch_sheet.cell(row=row_index, column=1, value=filename)\n",
    "\n",
    "        # Write and merge iter values\n",
    "        for col_index, (key, value) in enumerate(batch_runtime.items()):\n",
    "            if key != \"Filename\":\n",
    "                iter_value = value[0]\n",
    "                start_row = 2\n",
    "                end_row = start_row + len(batch_runtime[\"Filename\"]) - 1\n",
    "                # Merge cells for the iter column\n",
    "                batch_sheet.merge_cells(\n",
    "                    start_row=start_row,\n",
    "                    start_column=col_index + 1,\n",
    "                    end_row=end_row,\n",
    "                    end_column=col_index + 1,\n",
    "                )\n",
    "                merged_cell = batch_sheet.cell(row=start_row, column=col_index + 1)\n",
    "                merged_cell.value = iter_value\n",
    "                merged_cell.alignment = Alignment(\n",
    "                    horizontal=\"center\", vertical=\"center\"\n",
    "                )\n",
    "\n",
    "        batch_process_average = []\n",
    "        batch_process_total_time = 0\n",
    "        count = 0\n",
    "        for index, values in enumerate(batch_runtime):\n",
    "            if index != 0:\n",
    "                count += 1\n",
    "                batch_process_total_time += batch_runtime[values][0]\n",
    "        batch_process_total_time = batch_process_total_time / count\n",
    "        batch_process_average.append(batch_process_total_time)\n",
    "        batch_max_column = batch_sheet.max_column + 1\n",
    "        for i, value in enumerate(batch_process_average, start=1):\n",
    "            batch_sheet.cell(row=i + 1, column=batch_max_column, value=value)\n",
    "        batch_sheet.merge_cells(\n",
    "            start_row=2,\n",
    "            start_column=batch_max_column,\n",
    "            end_row=batch_sheet.max_row,\n",
    "            end_column=batch_max_column,\n",
    "        )\n",
    "        batch_sheet.cell(row=1, column=batch_max_column, value=\"Average\")\n",
    "\n",
    "        print(\"Done Batch or Async Processing\")\n",
    "        json_folder_comparison.main(\n",
    "            gcs_async_output_path,\n",
    "            iteration,\n",
    "            \"async\",\n",
    "            gcs_result_path,\n",
    "            input_details[\"Date\"],\n",
    "        )\n",
    "\n",
    "    # Save the workbook\n",
    "    output_file = \"runtime_data.xlsx\"\n",
    "    workbook.save(output_file)\n",
    "    if len(process_type) == 2:\n",
    "        latency_file_path = f\"{'/'.join(gcs_result_path.split('/')[3:])}latency/both/\"\n",
    "        latency_file_name = (\n",
    "            f\"{latency_file_path}latency_data_{input_details['Date']}.xlsx\"\n",
    "        )\n",
    "    elif len(process_type) == 1:\n",
    "        latency_file_path = (\n",
    "            f\"{'/'.join(gcs_result_path.split('/')[3:])}latency/{process_type[0]}/\"\n",
    "        )\n",
    "        latency_file_name = (\n",
    "            f\"{latency_file_path}latency_data_{input_details['Date']}.xlsx\"\n",
    "        )\n",
    "    json_folder_comparison.upload_xlsx_to_gcs(\n",
    "        gcs_result_path.split(\"/\")[2], output_file, latency_file_name\n",
    "    )\n",
    "    print(f\"Excel file '{output_file}' created successfully\")\n",
    "\n",
    "    old_files_to_compare = file_names(\n",
    "        f\"{'/'.join(gcs_result_path.split('/')[:3])}/{latency_file_path}\"\n",
    "    )[0]\n",
    "    old_files_to_compare.remove(latency_file_name.split(\"/\")[-1])\n",
    "    clean_old_files_to_compare = [\n",
    "        \"\".join(i.split(\"_\")[2:]).replace(\" IST.xlsx\", \"\") for i in old_files_to_compare\n",
    "    ]\n",
    "    if len(clean_old_files_to_compare) != 0:\n",
    "        sorted_date = sorted(\n",
    "            clean_old_files_to_compare,\n",
    "            key=lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\"),\n",
    "            reverse=True,\n",
    "        )[0]\n",
    "        for i in old_files_to_compare:\n",
    "            if sorted_date in i:\n",
    "                previous_sheet_name = (\n",
    "                    f\"{'/'.join(gcs_result_path.split('/')[:3])}/{latency_file_path}{i}\"\n",
    "                )\n",
    "                previous_sheet = get_xlsx_from_gcp(previous_sheet_name)\n",
    "                compare_latencies(\n",
    "                    previous_sheet,\n",
    "                    previous_sheet_name,\n",
    "                    workbook,\n",
    "                    latency_file_name,\n",
    "                    gcs_result_path,\n",
    "                    input_details[\"Date\"],\n",
    "                    process_type,\n",
    "                )\n",
    "                break\n",
    "    else:\n",
    "        print(\n",
    "            \"Only One Latency File Generated Present Now, So we can't compare with the previous sheet.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21c9da6-5333-4b95-90dc-7f78425b597e",
   "metadata": {},
   "source": [
    "### 5.Output\n",
    "\n",
    "The updated JSONs containing line information will be saved to the specified output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5608512-51ff-4eab-972e-254869de86af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
