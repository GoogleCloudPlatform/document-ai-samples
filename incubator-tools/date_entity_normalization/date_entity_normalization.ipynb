{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05597417-c7d3-4132-99d9-76591e72b58b",
   "metadata": {},
   "source": [
    "# Date Entity Normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad09b57c-8166-4662-b33d-dd883bcd4b2e",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45325f56-279d-480d-af14-f138181d3efa",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the DocAI Incubator Team. No guarantees of performance are implied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cc7ea6-79b7-4905-a77f-6f5dec1de471",
   "metadata": {},
   "source": [
    "## Purpose and Description\n",
    "\n",
    "This tool updates the values of normalized dates in entities within the Document AI JSON output. It aids in identifying the actual date format, such as MM/DD/YYYY or DD/MM/YYYY, through a heuristic approach. Upon successful identification, the tool updates all date values in the JSON to maintain a consistent format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70169fe9-1709-4050-8f1c-3050cef19566",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Vertex AI Notebook or Google Colab\n",
    "2. GCS bucket for processing of  the input json and output json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174b2cf3-04ad-45e4-b0b0-b279990ea211",
   "metadata": {},
   "source": [
    "## Step by Step procedure "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8983f13d-91d8-48cc-8f79-87c1cf309cc3",
   "metadata": {},
   "source": [
    "### 1. Install the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1fa99b-07ba-427c-8c03-520efbb81161",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install google-cloud-storage\n",
    "%pip install google-cloud-documentai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bd0830-3222-4300-b63b-fd21c7b9aa01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8371a9df-8297-4424-9be0-bfa7cfb9485f",
   "metadata": {},
   "source": [
    "### 2. Import the required libraries/Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb6b464-fa6e-42ba-8e1a-feade406fdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path \n",
    "from typing import Dict, List, Union, Optional, Tuple\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from google.cloud import storage\n",
    "from utilities import file_names,documentai_json_proto_downloader,store_document_as_json\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70e438f-c525-48ef-8de3-fdeda1294a7f",
   "metadata": {},
   "source": [
    "### 3. Input Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a948aa-b3fb-4b5e-bfd3-058f5e9a38cb",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li><b>input_path :</b> GCS Storage name. It should contain DocAI processed output json files. This bucket is used for processing input files and saving output files in the folders.</li>\n",
    "    <li><b>output_path:</b> GCS URI of the folder, where the Output Json files will store.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74083216-d1d6-44a6-9b29-967593fb53c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"gs://{bucket_name}/{folder_path}\" # Path to your Document AI input JSON files.\n",
    "output_path = \"gs://{bucket_name}/{folder_path}\" # Path where Vertex AI output merged JSON files will be saved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28716e4e-210e-4f42-9a0f-fc7d97310567",
   "metadata": {},
   "source": [
    "### 4.Execute the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe73f8a-19ff-4025-a082-685ddeb54fc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_storage_bucket_name = input_path.split(\"/\")[2]\n",
    "input_bucket_path_prefix = \"/\".join(input_path.split(\"/\")[3:])\n",
    "output_storage_bucket_name = output_path.split(\"/\")[2]\n",
    "output_bucket_path_prefix = \"/\".join(output_path.split(\"/\")[3:])\n",
    "\n",
    "def identify_and_convert_date_format(mention_text: str, known_format: Optional[str] = None) -> Tuple[Optional[datetime], str] :\n",
    "    \n",
    "    \"\"\"\n",
    "    This function attempts to identify and convert a date string to a datetime object.\n",
    "\n",
    "    Args:\n",
    "      mention_text: The text string potentially containing a date.\n",
    "      known_format: (Optional) A specific date format string to try first (e.g., \"%Y-%m-%d\").\n",
    "\n",
    "    Returns:\n",
    "      A tuple containing two elements:\n",
    "          - The converted datetime object (or None if not successful).\n",
    "          - The identified date format string (or \"N/A\" if not found).\n",
    "    \"\"\"\n",
    "        \n",
    "    formats = [\"%d/%m/%Y\", \"%m/%d/%Y\"]\n",
    "    if known_format:\n",
    "        formats.insert(0, known_format)\n",
    "\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            date_obj = datetime.strptime(mention_text, fmt)\n",
    "            return date_obj, fmt\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return None, \"N/A\"\n",
    "\n",
    "def process_json_files(list_of_files : List[str], input_storage_bucket_name : str, output_storage_bucket_name : str, output_bucket_path_prefix : str) -> None:\n",
    "    \"\"\"\n",
    "    Processes a list of JSON files, converting dates within entities to ISO 8601 format and storing the updated JSON data in a specified output bucket.\n",
    "\n",
    "    Args:\n",
    "        list_of_files: List of file paths for the JSON files to process (type: List[str]).\n",
    "        input_storage_bucket_name: Name of the input storage bucket (type: str).\n",
    "        output_storage_bucket_name: Name of the output storage bucket (type: str).\n",
    "        output_bucket_path_prefix: Prefix for the output file paths (type: str).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    all_json_data = []\n",
    "\n",
    "    for k in tqdm(range(0, len(list_of_files))):\n",
    "        print(\"***************\")\n",
    "        file_name = list_of_files[k].split('/')[-1]  \n",
    "        print(f\"File Name {file_name}\")\n",
    "        json_proto_data = documentai_json_proto_downloader(input_storage_bucket_name, list_of_files[k])\n",
    "        for ind,ent in enumerate(json_proto_data.entities):\n",
    "            if \"date\" in ent.type:\n",
    "                print('---------------')\n",
    "                mention_text = ent.mention_text if hasattr(ent, 'mention_text') else \"\"\n",
    "                normalized_value = ent.normalized_value if hasattr(ent, 'normalized_value') else \"\"\n",
    "                type_ = ent.type if hasattr(ent, 'type') else \"\"\n",
    "                print(f\"Type: {type_}\")\n",
    "                print(f\"Mention Text: {mention_text}\")\n",
    "                print(f\"Old Normalized Value: {normalized_value}\")\n",
    "            \n",
    "                date_obj, identified_format = identify_and_convert_date_format(mention_text)\n",
    "                \n",
    "                json_data = json.loads(documentai.Document.to_json(json_proto_data))\n",
    "\n",
    "                ent_json = json_data['entities'][ind]\n",
    "                    \n",
    "                if date_obj:\n",
    "                    new_date_text_iso = date_obj.strftime(\"%Y-%m-%d\")\n",
    "                    ent_json['normalizedValue']['text'] = new_date_text_iso\n",
    "                    ent_json['normalizedValue']['dateValue'] = {\n",
    "                        'day': date_obj.day,\n",
    "                        'month': date_obj.month,\n",
    "                        'year': date_obj.year\n",
    "                    }\n",
    "                    if identified_format != \"N/A\":\n",
    "                        ent_json['identified_format'] = identified_format\n",
    "                else:\n",
    "                    if identified_format == \"N/A\" and any(e for e in json_data['entities'] if e['type'] == 'date' and 'identified_format' in e):\n",
    "                        known_format = next((e['identified_format'] for e in json_data['entities'] if e['type'] == 'date' and 'identified_format' in e), None)\n",
    "                        date_obj, identified_format = identify_and_convert_date_format(mention_text, known_format=known_format)\n",
    "                        if date_obj:\n",
    "                            new_date_text_iso = date_obj.strftime(\"%Y-%m-%d\")\n",
    "                            ent_json['normalizedValue']['text'] = new_date_text_iso\n",
    "                            ent_json['normalizedValue']['dateValue'] = {\n",
    "                                'day': date_obj.day,\n",
    "                                'month': date_obj.month,\n",
    "                                'year': date_obj.year\n",
    "                            }\n",
    "        \n",
    "   \n",
    "        output_file_name = f\"{output_bucket_path_prefix}{file_name}\"\n",
    "        store_document_as_json(json.dumps(json_data), output_storage_bucket_name, output_file_name)\n",
    "    \n",
    "    print(\"--------------------\")\n",
    "    print(\"All files processed.\")\n",
    "\n",
    "json_files = file_names(input_path)[1].values()\n",
    "list_of_files = [i for i in list(json_files) if i.endswith(\".json\")]\n",
    "process_json_files(list_of_files, input_storage_bucket_name, output_storage_bucket_name, output_bucket_path_prefix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8baad3-4bd5-4626-81b8-b3b5587a5056",
   "metadata": {},
   "source": [
    "### 5.Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cda194-9d54-4d42-9eb0-de225422e17f",
   "metadata": {},
   "source": [
    "The post processed json field can be found in the storage path provided by the user during the script execution that is output_bucket_path. <br><hr>\n",
    "<b>Comparison Between Input and Output File</b><br><br>\n",
    "<i><h4>Post processing results<h4><i><br>\n",
    "Upon running the post processing script against input data. The resultant output json data is obtained. The following image will show the difference date formate in the date filed <br>\n",
    "    \n",
    "\n",
    "<img src= \"./images/output_image1.png\" width=800 height=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace1b20b-6417-420e-b0dd-3841ecac2773",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
