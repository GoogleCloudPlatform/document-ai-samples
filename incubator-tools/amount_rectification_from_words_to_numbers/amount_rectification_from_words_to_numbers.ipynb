{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6589fc93-39d1-4d10-be1f-e7eb33fe4087",
   "metadata": {},
   "source": [
    "# Amount rectification from words to numbers\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5bf22bf7-4a47-4f3a-9eef-6f19348a5250",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "361f188e-fe11-4a49-b7c8-080e0e69ce7a",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the DocAI Incubator Team. No guarantees of performance are implied. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1036937a-0221-48eb-862e-3fa0b8e646a8",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "This document provides instructions for rectifying the amount in cheques using the words with entity types and parsed jsons as input.\n",
    "\n",
    "<b>Note: This tool was developed to address output produced by a processor trained on a dataset of cheques.</b>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "115a4e82-5e83-468a-b0e5-097ca14f15d5",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "* Vertex AI Notebook Or Colab (If using Colab, use authentication)\n",
    "* Storage Bucket for storing input and output json files\n",
    "* Permission For Google Storage and Vertex AI Notebook.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe81de40-5c62-4c0b-adea-937f957b1a6e",
   "metadata": {},
   "source": [
    "## Step by Step procedure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "142123d3-37b1-4aa8-841c-40c3bd52d70c",
   "metadata": {},
   "source": [
    "### 1. Importing Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0643c5f9-29fe-4252-9e6c-e2afc8c2f2b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pandas numpy google-cloud-storage google-cloud-documentai==2.16.0 PyPDF2 configparser\n",
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7588c13e-0e09-4a76-8c21-85a68ee262c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from word2number import w2n\n",
    "import difflib\n",
    "from fuzzywuzzy import fuzz\n",
    "import re\n",
    "from indian_word2number import indian_w2n\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from google.cloud import storage\n",
    "from pathlib import Path\n",
    "from utilities import *\n",
    "from typing import Any, List, Dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd7c8c4c-68b8-413c-b4bc-c66f044d3b7a",
   "metadata": {},
   "source": [
    "### 2. Input and Output Paths"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f5812eb",
   "metadata": {},
   "source": [
    "<b>Alter the `amount_in_words_entity_name` and `amount_in_figures_entity_name` list values to modify to fit your use case.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d9b3ca3-e486-4614-81c2-da8e1f695666",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"gs://xxxx/xxxx/xxx/\"  # GCS parsed jsons path\n",
    "output_path = \"gs://xxxx/xxxx/xxx/\"  # GCS path to save the updated docs\n",
    "project_id = \"xxxx-xxxx-xxx\"  # Project ID\n",
    "amount_in_words_entity_name = [\n",
    "    \"amount_in_words\",\n",
    "    \"amountInWords1\",\n",
    "    \"amountInWords2\",\n",
    "    \"amountInWords\",\n",
    "]  # entity name for letters\n",
    "amount_in_figures_entity_name = [\n",
    "    \"amount_in_figures\",\n",
    "    \"amount\",\n",
    "]  # entity name for figures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da0d4909-e00c-4704-a43b-6534f7403872",
   "metadata": {},
   "source": [
    "* `input_path` : GCS Input Path. It should contain DocAI processed output json files. \n",
    "* `output_path` : GCS Output Path. The updated jsons will be saved in output path. \n",
    "* `project_id` : It should contains the project id of your current project.\n",
    "* `amount_in_words_entity_name` :  entity name for letters\n",
    "* `amount_in_figures_entity_name` : entity name for figures\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "737d1c70-fef5-49e3-a266-695bf8076a54",
   "metadata": {},
   "source": [
    "### 3. Run the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c22bfdd-abdc-4d1c-8f7c-86164e7c4103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "\n",
    "def translate_text(target: str, text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Translates text into the target language.\n",
    "\n",
    "    Args:\n",
    "        target (str): The target language, specified as an ISO 639-1 language code.\n",
    "                     See https://g.co/cloud/translate/v2/translate-reference#supported_languages\n",
    "        text (str): The text to be translated.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing translation information.\n",
    "    \"\"\"\n",
    "    from google.cloud import translate_v2 as translate\n",
    "\n",
    "    translate_client = translate.Client()\n",
    "\n",
    "    if isinstance(text, bytes):\n",
    "        text = text.decode(\"utf-8\")\n",
    "\n",
    "    # Text can also be a sequence of strings, in which case this method\n",
    "    # will return a sequence of results for each text.\n",
    "    result = translate_client.translate(text, target_language=target)\n",
    "\n",
    "    # print(\"Text: {}\".format(result[\"input\"]))\n",
    "    # print(\"Translation: {}\".format(result[\"translatedText\"]))\n",
    "    # print(\"Detected source language: {}\".format(result[\"detectedSourceLanguage\"]))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def convert_amount_in_words_to_numbers(text: str) -> int:\n",
    "    \"\"\"\n",
    "    Convert amounts written in words to numerical values.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text containing the amount written in words.\n",
    "\n",
    "    Returns:\n",
    "        int: The numerical value corresponding to the input text.\n",
    "    \"\"\"\n",
    "    # Check if the text contains any Indian numbering system words\n",
    "    indian_numbering_words = [\"lakh\", \"crore\"]\n",
    "    is_indian_system = any(word in text for word in indian_numbering_words)\n",
    "\n",
    "    if is_indian_system:\n",
    "        # return text_to_number_indian(text)\n",
    "        return indian_w2n.word_to_num(text)\n",
    "    else:\n",
    "        return w2n.word_to_num(text)\n",
    "\n",
    "\n",
    "def remove_repetitive_words(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove repetitive consecutive words from the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text containing potentially repetitive consecutive words.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with repetitive consecutive words removed.\n",
    "    \"\"\"\n",
    "    # Use regular expression to find and replace repetitive words\n",
    "    cleaned_text = re.sub(r\"\\b(\\w+)\\s+\\1\\b\", r\"\\1\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def words_nearest_match(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Find the nearest matches for words in the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text containing words to find matches for.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with replaced words that are nearest matches to the predefined dictionary.\n",
    "    \"\"\"\n",
    "    word_to_number = {\n",
    "        \"one\": 1,\n",
    "        \"two\": 2,\n",
    "        \"three\": 3,\n",
    "        \"four\": 4,\n",
    "        \"five\": 5,\n",
    "        \"six\": 6,\n",
    "        \"seven\": 7,\n",
    "        \"eight\": 8,\n",
    "        \"nine\": 9,\n",
    "        \"ten\": 10,\n",
    "        \"eleven\": 11,\n",
    "        \"twelve\": 12,\n",
    "        \"thirteen\": 13,\n",
    "        \"fourteen\": 14,\n",
    "        \"fifteen\": 15,\n",
    "        \"sixteen\": 16,\n",
    "        \"seventeen\": 17,\n",
    "        \"eighteen\": 18,\n",
    "        \"nineteen\": 19,\n",
    "        \"twenty\": 20,\n",
    "        \"thirty\": 30,\n",
    "        \"forty\": 40,\n",
    "        \"fifty\": 50,\n",
    "        \"sixty\": 60,\n",
    "        \"seventy\": 70,\n",
    "        \"eighty\": 80,\n",
    "        \"ninety\": 90,\n",
    "        \"lakh\": 100000,\n",
    "        \"crore\": 10000000,\n",
    "        \"hundred\": 100,\n",
    "        \"thousand\": 1000,\n",
    "        \"million\": 1000000,\n",
    "        \"billion\": 1000000000,\n",
    "    }\n",
    "    single_digit = {\n",
    "        \"one\": 1,\n",
    "        \"two\": 2,\n",
    "        \"three\": 3,\n",
    "        \"four\": 4,\n",
    "        \"five\": 5,\n",
    "        \"six\": 6,\n",
    "        \"seven\": 7,\n",
    "        \"eight\": 8,\n",
    "        \"nine\": 9,\n",
    "        \"ten\": 10,\n",
    "    }\n",
    "    multi_digit = {\n",
    "        \"twenty\": 20,\n",
    "        \"thirty\": 30,\n",
    "        \"forty\": 40,\n",
    "        \"fifty\": 50,\n",
    "        \"sixty\": 60,\n",
    "        \"seventy\": 70,\n",
    "        \"eighty\": 80,\n",
    "        \"ninety\": 90,\n",
    "        \"lakh\": 100000,\n",
    "        \"crore\": 10000000,\n",
    "        \"hundred\": 100,\n",
    "        \"thousand\": 1000,\n",
    "        \"million\": 1000000,\n",
    "        \"billion\": 1000000000,\n",
    "    }\n",
    "    unwanted_list = [\"only\", \"Rupees\", \"and\"]\n",
    "    text = re.sub(r\"[,.\\-_]\", \" \", text)\n",
    "\n",
    "    def find_nearest_match(word: str, word_list: list) -> str:\n",
    "        \"\"\"\n",
    "        Find the nearest match for a given word in a list of words.\n",
    "\n",
    "        Args:\n",
    "            word (str): The word for which to find the nearest match.\n",
    "            word_list (list): The list of words to search for a match.\n",
    "\n",
    "        Returns:\n",
    "            str: The nearest match, or None if no match is found.\n",
    "        \"\"\"\n",
    "        closest_match = difflib.get_close_matches(\n",
    "            word.lower(), word_list, n=1, cutoff=0.6\n",
    "        )\n",
    "        # print(closest_match)\n",
    "        if closest_match:\n",
    "            if closest_match[0] == \"one\":\n",
    "                if \"one\" not in word.lower():\n",
    "                    return None\n",
    "                else:\n",
    "                    return closest_match[0]\n",
    "            else:\n",
    "                return closest_match[0]\n",
    "\n",
    "        return None\n",
    "\n",
    "    def matching_ratio(unwanted_list: list, text: str) -> int:\n",
    "        \"\"\"\n",
    "        Calculate the similarity ratio between the given text and a list of unwanted strings.\n",
    "\n",
    "        Args:\n",
    "            unwanted_list (list): The list of unwanted strings.\n",
    "            text (str): The text for which to calculate the similarity ratio.\n",
    "\n",
    "        Returns:\n",
    "            int: The similarity index (1 if a match is found, 0 otherwise).\n",
    "        \"\"\"\n",
    "        threshold = 75\n",
    "        similarity_index = 0\n",
    "        similarity_ratio = 0\n",
    "        for string in unwanted_list:\n",
    "            similarity = fuzz.ratio(text, string)\n",
    "            if similarity >= threshold:\n",
    "                similarity_index = 1\n",
    "                similarity_ratio = similarity\n",
    "        return similarity_index\n",
    "\n",
    "    matching_text = \"\"\n",
    "    for i in text.split():\n",
    "        # print(i,matching_ratio(unwanted_list,i.lower()))\n",
    "        if (\n",
    "            i.lower() not in unwanted_list\n",
    "            and matching_ratio(unwanted_list, i.lower()) != 1\n",
    "        ):\n",
    "            # print(i)\n",
    "            match = find_nearest_match(i.lower(), word_to_number.keys())\n",
    "            if match != None:\n",
    "                matching_text = matching_text + \" \" + match\n",
    "\n",
    "    split_match = matching_text.split()\n",
    "    if split_match[-1] in single_digit.keys():\n",
    "        right_match_2 = find_nearest_match(split_match[-2], multi_digit.keys())\n",
    "        split_match[-2] = right_match_2\n",
    "\n",
    "    matching_text = (\" \").join(split_match)\n",
    "\n",
    "    return matching_text\n",
    "\n",
    "\n",
    "def get_amount_in_text(json_data: Any, amount_in_words_entity_name: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Extract and sort text entities based on their position on a page, specifically focusing on amounts written in words.\n",
    "\n",
    "    Args:\n",
    "        json_data (Any): The input JSON data containing information about text entities.\n",
    "        amount_in_words_entity_name (List[str]): A list of entity types corresponding to amounts in words.\n",
    "\n",
    "    Returns:\n",
    "        str: The merged and sorted text representing amounts in words.\n",
    "    \"\"\"\n",
    "\n",
    "    ent_type = {}\n",
    "    n = 0\n",
    "    merged_text = \"\"\n",
    "    for entity in json_data.entities:\n",
    "        if entity.type in amount_in_words_entity_name:\n",
    "            bound_poly = entity.page_anchor.page_refs\n",
    "            coordinates_xy = bound_poly[0].bounding_poly.normalized_vertices\n",
    "            x_1 = [xy.x for xy in coordinates_xy]\n",
    "            y_1 = [xy.y for xy in coordinates_xy]\n",
    "            temp_text_anc = []\n",
    "            for t1 in entity.text_anchor.text_segments:\n",
    "                temp_text_anc.append(\n",
    "                    {\"start_index_temp\": t1.start_index, \"end_index_temp\": t1.end_index}\n",
    "                )\n",
    "            for an2 in temp_text_anc:\n",
    "                text_sorted = text_sequenced(\n",
    "                    json_data,\n",
    "                    entity.mention_text,\n",
    "                    an2[\"start_index_temp\"],\n",
    "                    an2[\"end_index_temp\"],\n",
    "                )\n",
    "                ent_type[n] = text_sorted\n",
    "                n += 1\n",
    "    # print(ent_type)\n",
    "    if len(ent_type) > 1:\n",
    "        # sorted_data = dict(sorted(ent_type.items(), key=lambda item: item[1]['ent_y']))\n",
    "        sorted_data = sorted(\n",
    "            ent_type.items(),\n",
    "            key=lambda item: (item[1][\"ent_y\"], item[1][\"ent_x\"])\n",
    "            if item[1][\"ent_y\"]\n",
    "            - min(ent_type.values(), key=lambda x: x[\"ent_y\"])[\"ent_y\"]\n",
    "            > 0.05\n",
    "            else (item[1][\"ent_x\"], item[1][\"ent_y\"]),\n",
    "        )\n",
    "        sorted_dict = {key: value for key, value in sorted_data}\n",
    "        merged_text = \" \".join([item[\"text\"] for item in sorted_dict.values()])\n",
    "    elif len(ent_type) == 1:\n",
    "        merged_text = ent_type[0][\"text\"]\n",
    "\n",
    "    return merged_text\n",
    "\n",
    "\n",
    "def get_min_max_xy(\n",
    "    json_temp: Any, start_index_1: str, end_index_1: str\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Get the minimum and maximum coordinates (x, y) based on the provided text indices.\n",
    "\n",
    "    Args:\n",
    "        json_temp (Any): The input JSON data containing information about text entities.\n",
    "        start_index_1 (str): The start index of the text segment.\n",
    "        end_index_1 (str): The end index of the text segment.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: A dictionary containing the minimum and maximum coordinates (x, y).\n",
    "    \"\"\"\n",
    "    x_2 = \"\"\n",
    "    y_2 = \"\"\n",
    "    for page in json_temp.pages:\n",
    "        for token in page.tokens:\n",
    "            if (\n",
    "                abs(\n",
    "                    int(start_index_1)\n",
    "                    - int(token.layout.text_anchor.text_segments[0].start_index)\n",
    "                )\n",
    "                <= 2\n",
    "                and abs(\n",
    "                    int(end_index_1)\n",
    "                    - int(token.layout.text_anchor.text_segments[0].end_index)\n",
    "                )\n",
    "                <= 2\n",
    "            ):\n",
    "                # print(token.layout.bounding_poly.normalized_vertices)\n",
    "                # # bound_poly = entity.page_anchor.page_refs\n",
    "                coordinates_x_y = token.layout.bounding_poly.normalized_vertices\n",
    "                x_2 = [xy.x for xy in coordinates_x_y]\n",
    "                y_2 = [xy.y for xy in coordinates_x_y]\n",
    "\n",
    "            elif int(token.layout.text_anchor.text_segments[0].start_index) >= int(\n",
    "                start_index_1\n",
    "            ) and int(token.layout.text_anchor.text_segments[0].end_index) <= int(\n",
    "                end_index_1\n",
    "            ):\n",
    "                coordinates_x_y = token.layout.bounding_poly.normalized_vertices\n",
    "                x_2 = [xy.x for xy in coordinates_x_y]\n",
    "                y_2 = [xy.y for xy in coordinates_x_y]\n",
    "\n",
    "    return {\"min_x\": min(x_2), \"min_y\": min(y_2), \"max_x\": max(x_2), \"max_y\": min(y_2)}\n",
    "\n",
    "\n",
    "def text_sequenced(\n",
    "    json_temp: Any, temp_text: str, start_index_temp: int, end_index_temp: int\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process text data, extract information about matched text sequences, and organize the data based on coordinates.\n",
    "\n",
    "    Args:\n",
    "        json_temp (Any): The input JSON data containing information about text entities.\n",
    "        temp_text (str): The text to process and sequence.\n",
    "        start_index_temp (int): The start index of the text segment.\n",
    "        end_index_temp (int): The end index of the text segment.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: A dictionary containing the sorted and organized text information.\n",
    "    \"\"\"\n",
    "    temp_list = temp_text.split()\n",
    "    text_correction_dict = {}\n",
    "    k = 0\n",
    "    for t1 in temp_list:\n",
    "        matches = []\n",
    "        unique_matches = []\n",
    "        matches = [\n",
    "            {\n",
    "                \"start_index\": match.start() + start_index_temp,\n",
    "                \"end_index\": match.start() + start_index_temp + len(t1),\n",
    "            }\n",
    "            for match in re.finditer(\n",
    "                t1.lower(), json_temp.text[start_index_temp:end_index_temp].lower()\n",
    "            )\n",
    "        ]\n",
    "        unique_data = set(tuple(d.items()) for d in matches)\n",
    "        unique_matches = [dict(t) for t in unique_data]\n",
    "        try:\n",
    "            for match in unique_matches:\n",
    "                dict_xy = get_min_max_xy(\n",
    "                    json_temp, match[\"start_index\"], match[\"end_index\"]\n",
    "                )\n",
    "                text_correction_dict[k] = {\n",
    "                    \"mt\": t1,\n",
    "                    \"min_x\": dict_xy[\"min_x\"],\n",
    "                    \"min_y\": dict_xy[\"min_y\"],\n",
    "                }\n",
    "                k += 1\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    seen_values = set()\n",
    "    unique_data = {}\n",
    "\n",
    "    for key, value in text_correction_dict.items():\n",
    "        value_tuple = frozenset(value.items())\n",
    "        if value_tuple not in seen_values:\n",
    "            seen_values.add(value_tuple)\n",
    "            unique_data[key] = dict(value_tuple)\n",
    "\n",
    "    unique_data_2 = {}\n",
    "    for key, value in unique_data.items():\n",
    "        min_x_2 = value[\"min_x\"]\n",
    "        min_y_2 = value[\"min_y\"]\n",
    "\n",
    "        if (min_x_2, min_y_2) in unique_data_2:\n",
    "            if len(value[\"mt\"]) > len(unique_data_2[(min_x_2, min_y_2)][\"mt\"]):\n",
    "                unique_data_2[(min_x_2, min_y_2)] = value\n",
    "        else:\n",
    "            unique_data_2[(min_x_2, min_y_2)] = value\n",
    "\n",
    "    unique_data_mod = dict(enumerate(unique_data_2.values()))\n",
    "\n",
    "    sorted_dict_1 = sorted(\n",
    "        unique_data_mod.values(),\n",
    "        key=lambda x: (x[\"min_y\"], x[\"min_x\"])\n",
    "        if x[\"min_y\"] - unique_data_mod[0][\"min_y\"] > 0.05\n",
    "        else (x[\"min_x\"], x[\"min_y\"]),\n",
    "    )\n",
    "    sorted_dict_1 = {i: data for i, data in enumerate(sorted_dict_1)}\n",
    "\n",
    "    mt_values = [item[\"mt\"] for item in sorted_dict_1.values()]\n",
    "    joined_mt = \" \".join(mt_values)\n",
    "    min_x_value = min(sorted_dict_1.values(), key=lambda item: item.get(\"min_x\"))[\n",
    "        \"min_x\"\n",
    "    ]\n",
    "    min_y_value = min(sorted_dict_1.values(), key=lambda item: item.get(\"min_y\"))[\n",
    "        \"min_y\"\n",
    "    ]\n",
    "    joined_mt = \" \".join(mt_values)\n",
    "    mt_sorted_1 = {\"text\": joined_mt, \"ent_y\": min_y_value, \"ent_x\": min_x_value}\n",
    "\n",
    "    return mt_sorted_1\n",
    "\n",
    "\n",
    "def change_mt_amount(\n",
    "    json_data: Any,\n",
    "    amount_in_figures_entity_name: List[str],\n",
    "    cheque_amount_predicted: float,\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Change the mention text of an entity in the provided JSON data with the specified cheque amount.\n",
    "\n",
    "    Args:\n",
    "        json_data (Any): The input JSON data containing information about text entities.\n",
    "        amount_in_figures_entity_name (List[str]): A list of entity types corresponding to amounts in figures.\n",
    "        cheque_amount_predicted (float): The predicted cheque amount to set for the specified entity.\n",
    "\n",
    "    Returns:\n",
    "        Any: The updated JSON data with the mention text changed.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(str(cheque_amount_predicted)) != 0:\n",
    "        for en in json_data.entities:\n",
    "            if en.type in amount_in_figures_entity_name:\n",
    "                en.mention_text = str(cheque_amount_predicted)\n",
    "                # print(en)\n",
    "                return json_data\n",
    "\n",
    "\n",
    "file_name_list, file_path_dict = file_names(input_path)\n",
    "import pandas as pd\n",
    "\n",
    "# df=pd.DataFrame(columns=['file_name','nearest_match','amount_in_words','cheque_amount_updated','cheque_amount_previous'])\n",
    "for i in range(len(file_name_list)):\n",
    "    file_path = (\n",
    "        \"gs://\" + input_path.split(\"/\")[2] + \"/\" + file_path_dict[file_name_list[i]]\n",
    "    )\n",
    "    json_data = documentai_json_proto_downloader(\n",
    "        file_path.split(\"/\")[2], (\"/\").join(file_path.split(\"/\")[3:])\n",
    "    )\n",
    "    print(file_path)\n",
    "    cheque_amount = \"\"\n",
    "    cheque_amount_predicted = \"\"\n",
    "    amount_in_words = \"\"\n",
    "    nearest_match = \"\"\n",
    "    updated_amount = \"\"\n",
    "    for en in json_data.entities:\n",
    "        if en.type in amount_in_figures_entity_name:\n",
    "            cheque_amount = en.mention_text\n",
    "    if cheque_amount == \"\":\n",
    "        new_entity = documentai.Document.Entity()\n",
    "        new_entity.type_ = amount_in_figures_entity_name[0]\n",
    "        new_entity.mention_text = \"None\"\n",
    "        json_data.entities.append(new_entity)\n",
    "    try:\n",
    "        list_type = []\n",
    "        amount_in_words = get_amount_in_text(json_data, amount_in_words_entity_name)\n",
    "        amount_pred_letters = translate_text(\"en-US\", amount_in_words)[\"translatedText\"]\n",
    "        nearest_match = words_nearest_match(amount_pred_letters)\n",
    "        clean_text = remove_repetitive_words(nearest_match)\n",
    "        cheque_amount_predicted = convert_amount_in_words_to_numbers(clean_text)\n",
    "        json_data = change_mt_amount(\n",
    "            json_data, amount_in_figures_entity_name, cheque_amount_predicted\n",
    "        )\n",
    "        store_document_as_json(\n",
    "            documentai.Document.to_json(json_data),\n",
    "            output_path.split(\"/\")[2],\n",
    "            (\"/\").join(output_path.split(\"/\")[3:]) + \"/\" + file_name_list[i],\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Exception:  \", e)\n",
    "        # df.loc[len(df.index)] = [file_name_list[i],nearest_match,amount_in_words,cheque_amount_predicted,cheque_amount]\n",
    "        store_document_as_json(\n",
    "            documentai.Document.to_json(json_data),\n",
    "            output_path.split(\"/\")[2],\n",
    "            (\"/\").join(output_path.split(\"/\")[3:]) + \"/\" + file_name_list[i],\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62aa873d-f9be-46dc-b188-23e0d4d95dd0",
   "metadata": {},
   "source": [
    "### 4.Output\n",
    "\n",
    "The corrected jsons will be saved in output path\n",
    "\n",
    "Issue file\n",
    "\n",
    "<img src=\"./Images/issue.png\" width=800 height=400></img>\n",
    "\n",
    "Corrected file will be as below\n",
    "\n",
    "<img src=\"./Images/corrected.png\" width=800 height=400></img>"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
