{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9b561af-dfd4-43b5-9baa-36ba937ff124",
   "metadata": {},
   "source": [
    "# Enrich Address for Invoice and Expense Documents\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb81020b-6cf8-486e-98d7-def30ce0490a",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455354e7-a77e-44ca-b44e-0425d8b59a3d",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the **DocAI Incubator Team**. No guarantees of performance are implied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9962c16-5cc6-4642-93d5-11d11b6b8de1",
   "metadata": {},
   "source": [
    "# Objective\n",
    "The tool facilitates a more detailed and accurate address parsing process. Detected addresses are broken down into their constituent parts, such as city, country, and ZIP code. The address data is enriched with additional relevant information, enhancing its overall usability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb319168-3b6d-40e5-8681-fb47ad709556",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "* Python : Jupyter notebook (Vertex AI).\n",
    "\n",
    "NOTE : \n",
    " * The version of Python currently running in the Jupyter notebook should be greater than 3.8\n",
    "  * The normalizedValue attribute will be accessible exclusively in JSON file and is not visible in the processor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52acc46e-5e4c-4daa-ac9e-7031283f10a0",
   "metadata": {},
   "source": [
    "# Step-by-Step Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389c1532-21cb-4950-8446-7cabc0394332",
   "metadata": {},
   "source": [
    "## Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d8a5b9-f90e-4575-87ff-95810f84ccd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412f3332-6473-457c-922a-d6eb26b395c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this cell to download utilities module\n",
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad36024-52db-4628-9a87-a1fcbdfdc66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from google.cloud import storage\n",
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "from utilities import file_names, store_document_as_json, blob_downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6ea2a0-d3ea-4b2b-a2c1-0af6ec85f4d9",
   "metadata": {},
   "source": [
    "## 2. Input Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052c4c0b-3872-40dd-8a4e-17d5e05f01b7",
   "metadata": {},
   "source": [
    "* **PROJECT_ID** : It contains the project ID of the working project.\n",
    "* **LOCATION** : It contains the location.\n",
    "* **GCS_INPUT_PATH** : It contains the input jsons bucket path. \n",
    "* **GCS_OUTPUT_PATH** : It contains the output bucket path where the updated jsons after adding the attribute will be stored.\n",
    "* **ENTITY_NAME** : It contains the names of the entities which the user wants to split. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b83232-34e7-4ff1-abab-ab68235ad29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"rand-automl-project\"  # Your Google Cloud project ID.\n",
    "LOCATION = \"us-central1\"\n",
    "# '/' should be provided at the end of the path.\n",
    "GCS_INPUT_PATH = \"gs://bucket_name/path/to/jsons/\"\n",
    "# '/' should be provided at the end of the path.\n",
    "GCS_OUTPUT_PATH = \"gs://bucket_name/path/to/jsons/\"\n",
    "# Name of the entities in a list format.\n",
    "ENTITY_NAME = [\"receiver_address\", \"remit_to_address\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b658d0-d777-4bc6-8a94-2d7452b03db6",
   "metadata": {},
   "source": [
    "## 3. Run Below Code-Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98d18d7-9a44-4d0b-b743-25bc26010ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_address_to_json(address: str, project_id: str, location: str) -> dict:\n",
    "    \"\"\"\n",
    "    Split an address into JSON format with specific keys using a text generation model.\n",
    "\n",
    "    This function splits an address into JSON format with keys for streetAddress,\n",
    "    city, state, zipcode, and country\n",
    "    using a text generation model.\n",
    "\n",
    "    Args:\n",
    "        address (str): The input address string to be split.\n",
    "        project_id (str): The project ID for the Vertex AI project.\n",
    "        location (str): The location of the Vertex AI project.\n",
    "\n",
    "    Returns:\n",
    "        dict or None: A dictionary containing the JSON-formatted address if successful, else None.\n",
    "    \"\"\"\n",
    "    vertexai.init(project=project_id, location=location)\n",
    "    parameters = {\n",
    "        \"max_output_tokens\": 1024,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 0.8,\n",
    "        \"top_k\": 40,\n",
    "    }\n",
    "    model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "    response = model.predict(\n",
    "        f\"\"\"Please split the address into Json format with keys\n",
    "        streetAddress, city, state, zipcode, country\n",
    "\n",
    "        input: {address}\n",
    "        output:\n",
    "        \"\"\",\n",
    "        **parameters,\n",
    "    )\n",
    "\n",
    "    # Extracting JSON response from the model\n",
    "    json_response = response.text\n",
    "\n",
    "    try:\n",
    "        json_output = json.loads(json_response)\n",
    "        print(\"JSON OUTPUT\", json_output)\n",
    "        return json_output\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON response: {e}\")\n",
    "        print(\"Response from Model:\", response.text)\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_json_files(\n",
    "    list_of_files: list,\n",
    "    input_storage_bucket_name: str,\n",
    "    output_storage_bucket_name: str,\n",
    "    output_bucket_path_prefix: str,\n",
    "    project_id: str,\n",
    "    location: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Process JSON files containing address entities, split the addresses,\n",
    "    and store the updated JSON files in Google Cloud Storage.\n",
    "\n",
    "    This function iterates over a list of JSON files containing address entities,\n",
    "    splits the addresses into JSON format with keys for streetAddress,\n",
    "    city, state, zipcode, and country,\n",
    "    and stores the updated JSON files in a specified Google Cloud Storage bucket.\n",
    "\n",
    "    Args:\n",
    "        list_of_files (list): A list of JSON file paths to be processed.\n",
    "        input_storage_bucket_name (str): The name of the input Google Cloud Storage bucket.\n",
    "        output_storage_bucket_name (str): The name of the output Google Cloud Storage bucket.\n",
    "        output_bucket_path_prefix (str): The prefix path within the output bucket\n",
    "                                         where the processed files will be stored.\n",
    "        project_id (str): The project ID for the Vertex AI project.\n",
    "        location (str): The location of the Vertex AI project.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    for k, _ in enumerate(list_of_files):\n",
    "        print(\"***************\")\n",
    "        file_name = list_of_files[k].split(\"/\")[\n",
    "            -1\n",
    "        ]  # Extracting the file name from the path\n",
    "        print(f\"File Name {file_name}\")\n",
    "        json_data = blob_downloader(input_storage_bucket_name, list_of_files[k])\n",
    "        for ent in json_data[\"entities\"]:\n",
    "            for name in ENTITY_NAME:\n",
    "                if name in ent[\"type\"]:\n",
    "                    print(\"---------------\")\n",
    "                    mention_text = ent.get(\"mentionText\", \"\")\n",
    "                    # normalized_value = ent.get('normalizedValue', \"\")\n",
    "                    type_ = ent.get(\"type\", \"\")\n",
    "                    print(f\"Type: {type_}\")\n",
    "                    print(f\"Mention Text: {mention_text}\")\n",
    "\n",
    "                    # Try splitting the address\n",
    "                    output_json = split_address_to_json(\n",
    "                        mention_text.replace(\"\\n\", \" \").strip(), project_id, location\n",
    "                    )\n",
    "                    # If address was successfully split, update the entity\n",
    "                    if output_json is not None:\n",
    "                        ent[\"normalizedValue\"] = output_json\n",
    "                        ent[\"identified_format\"] = \"Address split\"\n",
    "                    else:\n",
    "                        print(\"Address couldn't be split.\")\n",
    "\n",
    "                    print(f\"New Normalized Value: {ent['normalizedValue']}\")\n",
    "\n",
    "        # save to Google Cloud Storage\n",
    "        output_file_name = f\"{output_bucket_path_prefix}{file_name}\"\n",
    "        store_document_as_json(\n",
    "            json.dumps(json_data), output_storage_bucket_name, output_file_name\n",
    "        )\n",
    "\n",
    "    print(\"--------------------\")\n",
    "    print(\"All files processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f6e420-8b24-4773-b069-771869be758f",
   "metadata": {},
   "source": [
    "## Run the main functions after executing the above functions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6aa350-f4af-440d-8185-dbcc030b73b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(project_id: str, location: str, input_path: str, output_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Main function to process JSON files containing address entities and\n",
    "    store the updated JSON files in Google Cloud Storage.\n",
    "\n",
    "    This function serves as the main entry point for processing JSON files\n",
    "    containing address entities, splitting the addresses,\n",
    "    and storing the updated JSON files in a specified Google Cloud Storage bucket.\n",
    "\n",
    "    Args:\n",
    "        project_id (str): The project ID for the Vertex AI project.\n",
    "        location (str): The location of the Vertex AI project.\n",
    "        input_path (str): The path to the input directory containing JSON files.\n",
    "        output_path (str): The path to the output directory\n",
    "                           where the processed files will be stored.\n",
    "\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    input_storage_bucket_name = input_path.split(\"/\")[2]\n",
    "    # input_bucket_path_prefix = \"/\".join(input_path.split(\"/\")[3:])\n",
    "    output_storage_bucket_name = output_path.split(\"/\")[2]\n",
    "    output_bucket_path_prefix = \"/\".join(output_path.split(\"/\")[3:])\n",
    "\n",
    "    json_files = file_names(input_path)[1].values()\n",
    "    list_of_files = [i for i in list(json_files) if i.endswith(\".json\")]\n",
    "    process_json_files(\n",
    "        list_of_files,\n",
    "        input_storage_bucket_name,\n",
    "        output_storage_bucket_name,\n",
    "        output_bucket_path_prefix,\n",
    "        project_id,\n",
    "        location,\n",
    "    )\n",
    "\n",
    "\n",
    "main(PROJECT_ID, LOCATION, GCS_INPUT_PATH, GCS_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e01349-3898-4160-adcd-715a9b83b5c7",
   "metadata": {},
   "source": [
    "## Output\n",
    "The new attribute 'normalizedValue' will be added to each address entity in the newly generated json file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a46bcb-4b12-4c6a-8e5f-b56469d8a4a5",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <b>Pre-processed data</b>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src='./images/input_image.png' width=600 height=600></img>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1847ab37-c050-4fa4-bd7c-ac3c826e9622",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f5c823-b08d-4a9b-a5b2-0235b3067f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
