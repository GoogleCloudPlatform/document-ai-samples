{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72fd064f-24f5-4d61-b0ad-2b2f3fe9427d",
   "metadata": {},
   "source": [
    "# DocAI Splitting Overlapping Entities\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5756f1a-631f-4c8a-bba0-98c6821d31a9",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b1d12ef-55dd-4fbd-8389-db14ed038eb1",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the DocAI Incubator Team. No guarantees of performance are implied."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94527514-1ae2-470b-96e2-0f48e4aa5e81",
   "metadata": {},
   "source": [
    "## Purpose and Description\n",
    "This tool uses exported labeled json to separate a pair of entities that are overlapped due to labeling into two individual entities. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8783f52-627b-4efa-b5d9-664ae2ca2564",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Vertex AI Notebook\n",
    "2. Parsed json files in GCS Folder.\n",
    "3. Output folder to upload the updated json files."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55cc5540-deb1-4449-8278-716488c54e5c",
   "metadata": {},
   "source": [
    "## Step by Step procedure "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d49f5b2d-f7fd-4403-a175-b95cc804f6ba",
   "metadata": {},
   "source": [
    "### 1. Input details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89120af5-c5f4-4897-a640-4ad9c5ce4739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input details\n",
    "# INPUT : storage bucket name\n",
    "input_path = \"gs://xxxxx/xxxxxxxx\"\n",
    "# OUTPUT : storage bucket's path\n",
    "output_path = \"gs://xxxxxx/xxxxxxxx\"\n",
    "\n",
    "list_of_pair_of_entities = [\n",
    "    (\"currency\", \"invoice_id\"),\n",
    "    (\"purchase_order\", \"delivery_date\"),\n",
    "]  # List of pair of entities that needs to be splitted.\n",
    "# Also, the entity name should be mentioned like this (small_entity,large_entity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf6a140e-bdd2-4014-9e10-2a11a6f5a0ef",
   "metadata": {},
   "source": [
    "input_path : GCS Path for input json files<br>\n",
    "output_path: GCS Path for output json files<br>\n",
    "list_of_pair_of_entities: [('customer_account_name','ship_to_address')]\n",
    "\n",
    "<div><i><b>Note:</b> List of pairs of entities that need to be splitted. Also, the entity name should be mentioned like this (small_entity,large_entity)</i><div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84356940-95a8-4489-bfc3-b85611f9558a",
   "metadata": {},
   "source": [
    "### 2. Output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55b50561-6cd5-433b-88db-b1bcfbbaaacb",
   "metadata": {},
   "source": [
    "The output json after execution of the code have individual entities.\n",
    "<img src=\"./Images/overlapping_split_output_1.png\" width=800 height=400 alt=\"Overlapping entity split output\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6fa4328-3d99-4de4-a5eb-0f2033d78b79",
   "metadata": {},
   "source": [
    "### 3. Run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9925065-1d70-47e8-97b7-0c65e45ec0d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install tqdm\n",
    "%pip install google.cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646350ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to download utilities module\n",
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26de90e6-2f77-49b1-800d-ed5b1e57b3d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import json, copy\n",
    "from google.cloud import storage\n",
    "from tqdm.notebook import tqdm\n",
    "from utilities import (\n",
    "    file_names,\n",
    "    documentai_json_proto_downloader,\n",
    "    bb_intersection_over_union,\n",
    "    store_document_as_json,\n",
    ")\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from typing import Any, Dict, List, Optional, Sequence, Tuple, Union\n",
    "\n",
    "\n",
    "input_storage_bucket_name = input_path.split(\"/\")[2]\n",
    "input_bucket_path_prefix = \"/\".join(input_path.split(\"/\")[3:])\n",
    "output_storage_bucket_name = output_path.split(\"/\")[2]\n",
    "output_bucket_path_prefix = \"/\".join(output_path.split(\"/\")[3:])\n",
    "\n",
    "json_files = file_names(input_path)[1].values()\n",
    "list_of_files = [i for i in list(json_files) if i.endswith(\".json\")]\n",
    "\n",
    "\n",
    "def get_coordinate(entity: documentai.Document.Entity, arg: str) -> int:\n",
    "    \"\"\"\n",
    "    To get the coordinates according to the entity coordinate name\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    entity : documentai.Document.Entity\n",
    "        The entity object from the input document\n",
    "    arg : str\n",
    "        The coordinate name of the entity bounding box\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        Returns the bounding box coordinate according to the name of the coordinates.\n",
    "    \"\"\"\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    for i in entity.page_anchor.page_refs:\n",
    "        for j in i.bounding_poly.normalized_vertices:\n",
    "            x_list.append(j.x)\n",
    "            y_list.append(j.y)\n",
    "\n",
    "    if arg == \"x_min\":\n",
    "        return min(x_list)\n",
    "    if arg == \"y_min\":\n",
    "        return min(y_list)\n",
    "    if arg == \"x_max\":\n",
    "        return max(x_list)\n",
    "    if arg == \"y_max\":\n",
    "        return max(y_list)\n",
    "\n",
    "\n",
    "def get_entity_coordinates(\n",
    "    entity1: documentai.Document.Entity, entity2: documentai.Document.Entity\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Calculate the Intersection over Union (IoU) of two bounding boxes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    entity1 : documentai.Document.Entity\n",
    "            The first entity from the elements of list_of_pair_of_entities\n",
    "    entity2 : documentai.Document.Entity\n",
    "            The second entity from the elements of list_of_pair_of_entities\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List\n",
    "        entity_1_coordinates : coordinates of first entity\n",
    "        entity_2_coordinates : coordinates of second entity\n",
    "    \"\"\"\n",
    "    entity_1_coordinates = [\n",
    "        get_coordinate(entity1, \"x_max\"),\n",
    "        get_coordinate(entity1, \"x_min\"),\n",
    "        get_coordinate(entity1, \"y_max\"),\n",
    "        get_coordinate(entity1, \"y_min\"),\n",
    "    ]\n",
    "    entity_2_coordinates = [\n",
    "        get_coordinate(entity2, \"x_max\"),\n",
    "        get_coordinate(entity2, \"x_min\"),\n",
    "        get_coordinate(entity2, \"y_max\"),\n",
    "        get_coordinate(entity2, \"y_min\"),\n",
    "    ]\n",
    "\n",
    "    return entity_1_coordinates, entity_2_coordinates\n",
    "\n",
    "\n",
    "def get_token(token: documentai.Document.PageAnchor.PageRef, param: str) -> int:\n",
    "    \"\"\"\n",
    "    To get the coordinates of the token by the coordinate name.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    token : documentai.Document.PageAnchor.PageRef\n",
    "            The token object from the entity.\n",
    "    param : str\n",
    "            The coordinates name to fetch the value.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int :\n",
    "        Coordinates of thr token according to the param(parameter).\n",
    "    \"\"\"\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    for j in token.layout.bounding_poly.normalized_vertices:\n",
    "        x_list.append(j[\"x\"])\n",
    "        y_list.append(j[\"y\"])\n",
    "    if param == \"x_min\":\n",
    "        return min(x_list)\n",
    "    if param == \"x_max\":\n",
    "        return max(x_list)\n",
    "    if param == \"y_min\":\n",
    "        return min(y_list)\n",
    "    if param == \"y_max\":\n",
    "        return max(y_list)\n",
    "\n",
    "\n",
    "def find_textSegment_list(\n",
    "    x_min: int, y_min: int, x_max: int, y_max: int, js: documentai.Document, page: int\n",
    ") -> List[documentai.Document.TextAnchor.TextSegment]:\n",
    "    \"\"\"\n",
    "    To get the text segment list.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_min : The minimum value of x coordinate (top left).\n",
    "    y_min : The minimum value of y coordinate (bottom left).\n",
    "    x_max : The maximum value of x coordinate (top right).\n",
    "    y_max : The maximum value of y coordinate (bottom right).\n",
    "    js    : documentai.Document\n",
    "            The Document proto object from the entities.\n",
    "    page : int\n",
    "            The page number.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[documentai.Document.TextAnchor.TextSegment] :\n",
    "        List of text segment.\n",
    "    \"\"\"\n",
    "    textSegments_list = []\n",
    "    for token in js.pages[page].tokens:\n",
    "        token_xMin = get_token(token, \"x_min\")\n",
    "        token_xMax = get_token(token, \"x_max\")\n",
    "        token_yMin = get_token(token, \"y_min\")\n",
    "        token_yMax = get_token(token, \"y_max\")\n",
    "        if (\n",
    "            token_xMin >= x_min\n",
    "            and token_xMax <= x_max\n",
    "            and token_yMin >= y_min\n",
    "            and token_yMax <= y_max\n",
    "        ):\n",
    "            textSegments_list.extend(token.layout.text_anchor.text_segments)\n",
    "\n",
    "    return textSegments_list\n",
    "\n",
    "\n",
    "def split_overlapping_entities(\n",
    "    large_entity: documentai.Document.Entity,\n",
    "    small_entity: documentai.Document.Entity,\n",
    "    js: documentai.Document,\n",
    "    page: str,\n",
    ") -> documentai.Document.Entity:\n",
    "    \"\"\"It will append new entities to Document Proto by replacing the text of small entitiy from large entity and also the coordinates\n",
    "\n",
    "    Args:\n",
    "        doc (documentai.Document): It is Document proto object\n",
    "        total_amount_type (str): Its value is set as type for an entity, here for all properties in an entity\n",
    "        list_total_amount (List[str]): It is a list of header words which will be used to identity and the values under those headers will be tagged with child type `total_amount_type`\n",
    "\n",
    "    Returns:\n",
    "        documentai.Document: It is Document proto object, which contains newly added entities as well\n",
    "    \"\"\"\n",
    "\n",
    "    new_entity = {}\n",
    "    new_entity[\"type\"] = large_entity.type\n",
    "    new_entity[\"mention_text\"] = large_entity.mention_text.replace(\n",
    "        small_entity.mention_text, \"\"\n",
    "    )\n",
    "    text_anchor = {}\n",
    "    text_anchor[\"content\"] = large_entity.mention_text.replace(\n",
    "        small_entity.mention_text, \"\"\n",
    "    )\n",
    "    A = {\n",
    "        \"x\": get_coordinate(small_entity, \"x_min\"),\n",
    "        \"y\": get_coordinate(small_entity, \"y_max\"),\n",
    "    }\n",
    "    B = {\n",
    "        \"x\": get_coordinate(small_entity, \"x_max\"),\n",
    "        \"y\": get_coordinate(small_entity, \"y_max\"),\n",
    "    }\n",
    "    C = {\n",
    "        \"x\": get_coordinate(large_entity, \"x_max\"),\n",
    "        \"y\": get_coordinate(large_entity, \"y_max\"),\n",
    "    }\n",
    "    D = {\n",
    "        \"x\": get_coordinate(large_entity, \"x_min\"),\n",
    "        \"y\": get_coordinate(large_entity, \"y_max\"),\n",
    "    }\n",
    "    new_entity[\"page_anchor\"] = large_entity.page_anchor\n",
    "    new_entity[\"page_anchor\"][\"page_refs\"][0][\"bounding_poly\"][\n",
    "        \"normalized_vertices\"\n",
    "    ] = [A, B, C, D]\n",
    "    new_entity[\"page_anchor\"][\"page_refs\"][0][\"page\"] = str(page)\n",
    "    text_anchor[\"text_segments\"] = find_textSegment_list(\n",
    "        A[\"x\"] - 0.005, A[\"y\"] - 0.005, C[\"x\"] + 0.005, C[\"y\"] + 0.005, js, page\n",
    "    )\n",
    "    new_entity[\"text_anchor\"] = text_anchor\n",
    "    print()\n",
    "    return new_entity\n",
    "\n",
    "\n",
    "for k in tqdm(range(0, len(list_of_files))):\n",
    "    new_entities = []\n",
    "    print(\"\\nProcessing >>> \", list_of_files[k])\n",
    "    document = documentai_json_proto_downloader(\n",
    "        input_storage_bucket_name, list_of_files[k]\n",
    "    )\n",
    "    try:\n",
    "        for j in list_of_pair_of_entities:\n",
    "            print(j)\n",
    "            small_entity = j[0]\n",
    "            large_entity = j[1]\n",
    "            list_of_small_entity = []\n",
    "            for entity in document.entities:\n",
    "                if entity.type == small_entity:  # or entity['type']==\"ship_to_address\"\n",
    "                    list_of_small_entity.append(entity)\n",
    "            for i in list_of_small_entity:\n",
    "                page_i = 0\n",
    "                if i.page_anchor.page_refs[0].page:\n",
    "                    page_i = int(i.page_anchor.page_refs[0].page)\n",
    "                for entity in document.entities:\n",
    "                    if entity.type == large_entity:\n",
    "                        page = 0\n",
    "                        if entity.page_anchor.page_refs[0].page:\n",
    "                            page = int(entity.page_anchor.page_refs[0].page)\n",
    "                        if page == page_i:\n",
    "                            (\n",
    "                                entity1_coordinate,\n",
    "                                entity2_coordinate,\n",
    "                            ) = get_entity_coordinates(entity, i)\n",
    "                            iou = bb_intersection_over_union(\n",
    "                                entity1_coordinate, entity2_coordinate\n",
    "                            )\n",
    "                            if iou > 0.0:\n",
    "                                new_entity = split_overlapping_entities(\n",
    "                                    entity, i, document, page\n",
    "                                )\n",
    "                                new_entities.append(new_entity)\n",
    "                                document.entities.remove(entity)\n",
    "                                document.entities.append(new_entity)\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            \">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\"\n",
    "            + list_of_files[k]\n",
    "            + \" was not processed successfully!!!\"\n",
    "        )\n",
    "        print(e)\n",
    "        continue\n",
    "    store_document_as_json(\n",
    "        documentai.Document.to_json(document),\n",
    "        output_storage_bucket_name,\n",
    "        output_bucket_path_prefix + \"/\" + list_of_files[k].split(\"/\")[-1],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e643bb-6273-40dd-9cf7-db03502dacd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
