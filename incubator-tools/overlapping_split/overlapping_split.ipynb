{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72fd064f-24f5-4d61-b0ad-2b2f3fe9427d",
   "metadata": {},
   "source": [
    "# DocAI Splitting Overlapping Entities\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5756f1a-631f-4c8a-bba0-98c6821d31a9",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b1d12ef-55dd-4fbd-8389-db14ed038eb1",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the DocAI Incubator Team. No guarantees of performance are implied."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94527514-1ae2-470b-96e2-0f48e4aa5e81",
   "metadata": {},
   "source": [
    "## Purpose and Description\n",
    "This tool uses exported labeled json to separate a pair of entities that are overlapped due to labeling into two individual entities. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8783f52-627b-4efa-b5d9-664ae2ca2564",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Vertex AI Notebook\n",
    "2. Parsed json files in GCS Folder.\n",
    "3. Output folder to upload the updated json files."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55cc5540-deb1-4449-8278-716488c54e5c",
   "metadata": {},
   "source": [
    "## Step by Step procedure "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d49f5b2d-f7fd-4403-a175-b95cc804f6ba",
   "metadata": {},
   "source": [
    "### 1. Input details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89120af5-c5f4-4897-a640-4ad9c5ce4739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input details\n",
    "# INPUT : storage bucket name\n",
    "input_path = \"gs://xxxxx/xxxxxxxx\"\n",
    "# OUTPUT : storage bucket's path\n",
    "output_path = \"gs://xxxxxx/xxxxxxxx\"\n",
    "\n",
    "list_of_pair_of_entities = [\n",
    "    (\"currency\", \"invoice_id\"),\n",
    "    (\"purchase_order\", \"delivery_date\"),\n",
    "]  # List of pair of entities that needs to be splitted.\n",
    "# Also, the entity name should be mentioned like this (small_entity,large_entity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf6a140e-bdd2-4014-9e10-2a11a6f5a0ef",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li><b>input_path :</b> GCS Path for input json files</li>\n",
    "    <li><b>output_path:</b> GCS Path for output json files</li>\n",
    "    <li><b>list_of_pair_of_entities:</b> [('customer_account_name','ship_to_address')]</li>\n",
    "</ul>\n",
    "<div style=\"background-color:#f5f569\" ><i><b>Note:</b> List of pairs of entities that need to be splitted. Also, the entity name should be mentioned like this (small_entity,large_entity)</i><div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84356940-95a8-4489-bfc3-b85611f9558a",
   "metadata": {},
   "source": [
    "### 2. Output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55b50561-6cd5-433b-88db-b1bcfbbaaacb",
   "metadata": {},
   "source": [
    "The output json after execution of the code have individual entities.\n",
    "<img src=\"./Images/overlapping_split_output_1.png\" width=800 height=400 alt=\"Overlapping entity split output\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6fa4328-3d99-4de4-a5eb-0f2033d78b79",
   "metadata": {},
   "source": [
    "### 3. Run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9925065-1d70-47e8-97b7-0c65e45ec0d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install tqdm\n",
    "%pip install google.cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646350ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to download utilities module\n",
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26de90e6-2f77-49b1-800d-ed5b1e57b3d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import json, copy\n",
    "from google.cloud import storage\n",
    "from tqdm.notebook import tqdm\n",
    "from utilities import (\n",
    "    file_names,\n",
    "    documentai_json_proto_downloader,\n",
    "    bb_intersection_over_union,\n",
    "    store_document_as_json,\n",
    "    bbox_maker,\n",
    ")\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from typing import Any, Dict, List, Optional, Sequence, Tuple, Union\n",
    "\n",
    "\n",
    "input_storage_bucket_name = input_path.split(\"/\")[2]\n",
    "input_bucket_path_prefix = \"/\".join(input_path.split(\"/\")[3:])\n",
    "output_storage_bucket_name = output_path.split(\"/\")[2]\n",
    "output_bucket_path_prefix = \"/\".join(output_path.split(\"/\")[3:])\n",
    "\n",
    "json_files = file_names(input_path)[1].values()\n",
    "list_of_files = [i for i in list(json_files) if i.endswith(\".json\")]\n",
    "\n",
    "\n",
    "def get_entity_coordinates(\n",
    "    entity1: documentai.Document.Entity, entity2: documentai.Document.Entity\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Calculate the Intersection over Union (IoU) of two bounding boxes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    entity1 : documentai.Document.Entity\n",
    "            The first entity from the elements of list_of_pair_of_entities\n",
    "    entity2 : documentai.Document.Entity\n",
    "            The second entity from the elements of list_of_pair_of_entities\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List\n",
    "        entity_1_coordinates : coordinates of first entity\n",
    "        entity_2_coordinates : coordinates of second entity\n",
    "    \"\"\"\n",
    "    entity1_coordinates_list = []\n",
    "    for i in entity1.page_anchor.page_refs[0].bounding_poly.normalized_vertices:\n",
    "        entity1_coordinates_list.append({\"x\": i.x, \"y\": i.y})\n",
    "    entity2_coordinates_list = []\n",
    "    for i in entity2.page_anchor.page_refs[0].bounding_poly.normalized_vertices:\n",
    "        entity2_coordinates_list.append({\"x\": i.x, \"y\": i.y})\n",
    "    entity1_coordinates_list = bbox_maker(entity1_coordinates_list)\n",
    "    entity2_coordinates_list = bbox_maker(entity2_coordinates_list)\n",
    "\n",
    "    return entity1_coordinates_list, entity2_coordinates_list\n",
    "\n",
    "\n",
    "def find_textSegment_list(\n",
    "    x_min: int, y_min: int, x_max: int, y_max: int, js: documentai.Document, page: int\n",
    ") -> List[documentai.Document.TextAnchor.TextSegment]:\n",
    "    \"\"\"\n",
    "    To get the text segment list.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_min : The minimum value of x coordinate (top left).\n",
    "    y_min : The minimum value of y coordinate (bottom left).\n",
    "    x_max : The maximum value of x coordinate (top right).\n",
    "    y_max : The maximum value of y coordinate (bottom right).\n",
    "    js    : documentai.Document\n",
    "            The Document proto object from the entities.\n",
    "    page : int\n",
    "            The page number.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[documentai.Document.TextAnchor.TextSegment] :\n",
    "        List of text segment.\n",
    "    \"\"\"\n",
    "    textSegments_list = []\n",
    "    for token in js.pages[page].tokens:\n",
    "        token_coordinates_list = []\n",
    "        for i in token.layout.bounding_poly.normalized_vertices:\n",
    "            token_coordinates_list.append({\"x\": i.x, \"y\": i.y})\n",
    "        token_coordinates_list = bbox_maker(token_coordinates_list)\n",
    "        token_xMin = token_coordinates_list[0]\n",
    "        token_xMax = token_coordinates_list[2]\n",
    "        token_yMin = token_coordinates_list[1]\n",
    "        token_yMax = token_coordinates_list[3]\n",
    "        if (\n",
    "            token_xMin >= x_min\n",
    "            and token_xMax <= x_max\n",
    "            and token_yMin >= y_min\n",
    "            and token_yMax <= y_max\n",
    "        ):\n",
    "            textSegments_list.extend(token.layout.text_anchor.text_segments)\n",
    "\n",
    "    return textSegments_list\n",
    "\n",
    "\n",
    "def split_overlapping_entities(\n",
    "    large_entity: documentai.Document.Entity,\n",
    "    small_entity: documentai.Document.Entity,\n",
    "    js: documentai.Document,\n",
    "    page: str,\n",
    ") -> documentai.Document.Entity:\n",
    "    \"\"\"It will append new entities to Document Proto, whose token segments falls with in range of Header token\n",
    "\n",
    "    Args:\n",
    "        doc (documentai.Document): It is Document proto object\n",
    "        total_amount_type (str): Its value is set as type for an entity, here for all properties in an entity\n",
    "        list_total_amount (List[str]): It is a list of header words which will be used to identity and the values under those headers will be tagged with child type `total_amount_type`\n",
    "\n",
    "    Returns:\n",
    "        documentai.Document: It is Document proto object, which contains newly added entities as well\n",
    "    \"\"\"\n",
    "\n",
    "    new_entity = documentai.Document.Entity()\n",
    "    new_entity.type = large_entity.type\n",
    "    new_entity.mention_text = large_entity.mention_text.replace(\n",
    "        small_entity.mention_text, \"\"\n",
    "    )\n",
    "    text_anchor = documentai.Document.TextAnchor()\n",
    "    text_anchor.content = large_entity.mention_text.replace(\n",
    "        small_entity.mention_text, \"\"\n",
    "    )\n",
    "    small_entity_coordinates_list = []\n",
    "    for i in small_entity.page_anchor.page_refs[0].bounding_poly.normalized_vertices:\n",
    "        small_entity_coordinates_list.append({\"x\": i.x, \"y\": i.y})\n",
    "    large_entity_coordinates_list = []\n",
    "    for i in large_entity.page_anchor.page_refs[0].bounding_poly.normalized_vertices:\n",
    "        large_entity_coordinates_list.append({\"x\": i.x, \"y\": i.y})\n",
    "    small_entity_coordinates_list = bbox_maker(small_entity_coordinates_list)\n",
    "    large_entity_coordinates_list = bbox_maker(large_entity_coordinates_list)\n",
    "    A = {\"x\": small_entity_coordinates_list[0], \"y\": small_entity_coordinates_list[3]}\n",
    "    B = {\"x\": small_entity_coordinates_list[2], \"y\": small_entity_coordinates_list[3]}\n",
    "    C = {\"x\": large_entity_coordinates_list[2], \"y\": large_entity_coordinates_list[3]}\n",
    "    D = {\"x\": large_entity_coordinates_list[0], \"y\": large_entity_coordinates_list[3]}\n",
    "    new_entity.page_anchor = large_entity.page_anchor\n",
    "    new_entity.page_anchor.page_refs[0].bounding_poly.normalized_vertices = [A, B, C, D]\n",
    "    new_entity.page_anchor.page_refs[0].page = str(page)\n",
    "    text_anchor.text_segments = find_textSegment_list(\n",
    "        A[\"x\"] - 0.005, A[\"y\"] - 0.005, C[\"x\"] + 0.005, C[\"y\"] + 0.005, js, page\n",
    "    )\n",
    "    new_entity.text_anchor = text_anchor\n",
    "    return new_entity\n",
    "\n",
    "\n",
    "def update_document(\n",
    "    document: documentai.Document, list_of_small_entity: List[str], large_entity: str\n",
    ") -> documentai.Document:\n",
    "    \"\"\"This function will take list of overlapping entities and match the iou of both the entities and if found iou more than 0.0\n",
    "        then it will call split_overlapping_entities() function and add the newly splitted entities to the original document object\n",
    "\n",
    "    Args:\n",
    "        document (documentai.Document): It is Document proto object which will have all the entities including overlapped entities.\n",
    "        list_of_small_entity: It is the list of all small entites which can be overlapped on large entity.\n",
    "        large_entity (str): It is the name of entity on which the small entitiy will be overlapped in original document.\n",
    "\n",
    "    Returns:\n",
    "        documentai.Document: It is Document proto object, which contains newly added entities as well\n",
    "    \"\"\"\n",
    "    for i in list_of_small_entity:\n",
    "        page_i = 0\n",
    "        if i.page_anchor.page_refs[0].page:\n",
    "            page_i = int(i.page_anchor.page_refs[0].page)\n",
    "        for entity in document.entities:\n",
    "            if entity.type == large_entity:\n",
    "                page = 0\n",
    "                if entity.page_anchor.page_refs[0].page:\n",
    "                    page = int(entity.page_anchor.page_refs[0].page)\n",
    "                if page == page_i:\n",
    "                    entity1_coordinate, entity2_coordinate = get_entity_coordinates(\n",
    "                        entity, i\n",
    "                    )\n",
    "                    iou = bb_intersection_over_union(\n",
    "                        entity1_coordinate, entity2_coordinate\n",
    "                    )\n",
    "                    if iou > 0.0:\n",
    "                        new_entity = split_overlapping_entities(\n",
    "                            entity, i, document, page\n",
    "                        )\n",
    "                        new_entities.append(new_entity)\n",
    "                        document.entities.remove(entity)\n",
    "\n",
    "                        document.entities.append(new_entity)\n",
    "    return document\n",
    "\n",
    "\n",
    "for k in tqdm(range(0, len(list_of_files))):\n",
    "    new_entities = []\n",
    "    print(\"\\nProcessing >>> \", list_of_files[k])\n",
    "    document = documentai_json_proto_downloader(\n",
    "        input_storage_bucket_name, list_of_files[k]\n",
    "    )\n",
    "    try:\n",
    "        for j in list_of_pair_of_entities:\n",
    "            small_entity = j[0]\n",
    "            large_entity = j[1]\n",
    "            list_of_small_entity = []\n",
    "            for entity in document.entities:\n",
    "                if entity.type == small_entity:\n",
    "                    list_of_small_entity.append(entity)\n",
    "\n",
    "            document = update_document(document, list_of_small_entity, large_entity)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            \">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\"\n",
    "            + list_of_files[k]\n",
    "            + \" was not processed successfully!!!\"\n",
    "        )\n",
    "        print(e)\n",
    "        continue\n",
    "    store_document_as_json(\n",
    "        documentai.Document.to_json(document),\n",
    "        output_storage_bucket_name,\n",
    "        output_bucket_path_prefix + \"/\" + list_of_files[k].split(\"/\")[-1],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e643bb-6273-40dd-9cf7-db03502dacd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
