{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2234486-f07d-4fd6-85c7-2aad9e49247f",
   "metadata": {},
   "source": [
    "# Doc AI Synonyms Entity Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2aaf3d8-7924-43ec-937b-ab51cd92a26c",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420260ad-cfe9-4bc0-9d74-8b568212fa77",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the **DocAI Incubator Team**. No guarantees of performance are implied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658013b6-09bb-4d9a-87e1-b4fcd35689f8",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "This tool uses parsed json files and a dictionary with key as entity names and values as synonyms for which the entity has to be tagged. New entities added to the json. \n",
    "\n",
    "Approach: The values of the dictionary are searched in the OCR text and tagged with entity name based on key. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ada8352-3053-4fa9-9247-c0306faf3a78",
   "metadata": {},
   "source": [
    "## Prerequisite\n",
    "\n",
    "* Vertex AI Notebook\n",
    "* Parsed json files in GCS Folder\n",
    "* Output folder to upload the updated json files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc7bbaa-7608-441e-95e4-01435e9db80c",
   "metadata": {},
   "source": [
    "## Step by Step procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea47bbc-8b14-49cc-9972-34c18eca4d3c",
   "metadata": {},
   "source": [
    "### 1.Importing Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aea6ef8-7875-49b5-8416-d3e64005dde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "345710ab-8453-43d7-876e-a286068c462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from tqdm import tqdm\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "import json\n",
    "import utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c700b5e-ca96-40c3-8030-aa8d32cb38c2",
   "metadata": {},
   "source": [
    "### 2.Setup the Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fddb2dd-cdf7-4dcc-a557-666e6e83a30c",
   "metadata": {},
   "source": [
    "* `project_id`: It is the project id of the project.\n",
    "* `gcs_input_path`: GCS Storage name. It should contain DocAI processed output json files. This bucket is used for processing input files and saving output files in the folders.\n",
    "* `gcs_output_path`: GCS URI of the folder, where the output is stored.\n",
    "* `synonyms_entities`:A dictionary with key as entity names and values as synonyms for which the entity has to be tagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45c3613-41b5-4342-bb4e-b3651678fec6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# input details\n",
    "project_id = \"xxxx-xxxx-xxxx\"\n",
    "gcs_input_path = \"gs://xxxx/xxxx/xxx/\"\n",
    "gcs_output_path = \"gs://xxxx/xxxx/xxx/\"\n",
    "synonyms_entities = {\n",
    "    \"cust_name\": [\"ROsweLL PARK MEMORIAL\", \"inst\"],\n",
    "    \"Name\": [\"name\", \"firstname\", \"lastname\", \"middlename\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54de84e0-6d1d-4cbc-b5e4-6b57dd8c325b",
   "metadata": {},
   "source": [
    "### 3.Run the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "062cefe8-84c9-46e8-b00e-f4e9badb7903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalizedvertices(normalized_vertices: object) -> tuple:\n",
    "    \"\"\"\n",
    "    Get the minimum and maximum coordinates from a list of normalized vertices.\n",
    "\n",
    "    Args:\n",
    "        normalized_vertices (object) : List of normalized vertices.\n",
    "    Returns:\n",
    "        tuple: Minimum x, Minimum y, Maximum x, Maximum y coordinates.\n",
    "    \"\"\"\n",
    "\n",
    "    min_x = min(vertex.x for vertex in normalized_vertices.normalized_vertices)\n",
    "    min_y = min(vertex.y for vertex in normalized_vertices.normalized_vertices)\n",
    "    max_x = max(vertex.x for vertex in normalized_vertices.normalized_vertices)\n",
    "    max_y = max(vertex.y for vertex in normalized_vertices.normalized_vertices)\n",
    "\n",
    "    return min_x, min_y, max_x, max_y\n",
    "\n",
    "\n",
    "def get_token(json_dict: object, page: str, text_anchors_check: list) -> tuple:\n",
    "    \"\"\"THIS FUNCTION USED LOADED JSON, PAGE NUMBER AND TEXT ANCHORS AS INPUT AND GIVES THE X AND Y COORDINATES\n",
    "\n",
    "     Args:\n",
    "         json_dict (object) : The document object containing entities.\n",
    "         page (str) : The page number as a string where these entities are found.\n",
    "         text_anchors_check (list) : The list contains text anchors information which need to be checked.\n",
    "    Returns:\n",
    "         A tuple with three elements : A dictionary with keys 'min_x', 'min_y', 'max_x', and 'max_y' ; list containing textanchors ; confidence\n",
    "    \"\"\"\n",
    "    min_x = \"\"\n",
    "    temp_text_anc = []\n",
    "    temp_confidence = []\n",
    "    temp_ver = {\"x\": [], \"y\": []}\n",
    "    for token in json_dict.pages[page].tokens:\n",
    "        if not token.layout.text_anchor.text_segments[0].start_index:\n",
    "            token.layout.text_anchor.text_segments[0].start_index = 0\n",
    "        token_anc = token.layout.text_anchor.text_segments[0]\n",
    "        if token.layout.text_anchor.text_segments == text_anchors_check:\n",
    "            normalized_vertices = token.layout.bounding_poly\n",
    "            min_x, min_y, max_x, max_y = get_normalizedvertices(normalized_vertices)\n",
    "            text_anc_token = token.layout.text_anchor.text_segments\n",
    "            confidence = token.layout.confidence\n",
    "        elif (\n",
    "            int(token_anc.start_index) >= int(text_anchors_check[0][\"start_index\"]) - 2\n",
    "            and int(token_anc.end_index) <= int(text_anchors_check[0][\"end_index\"]) + 2\n",
    "            and abs(int(token_anc.start_index) - int(token_anc.end_index)) > 2\n",
    "        ):\n",
    "            normalized_vertices = token.layout.bounding_poly\n",
    "            min_x, min_y, max_x, max_y = get_normalizedvertices(normalized_vertices)\n",
    "            temp_ver[\"x\"].extend([min_x, max_x])\n",
    "            temp_ver[\"y\"].extend([min_y, max_y])\n",
    "            text_anc_token = token.layout.text_anchor.text_segments\n",
    "            for an1 in text_anc_token:\n",
    "                temp_text_anc.append(an1)\n",
    "            confidence = token.layout.confidence\n",
    "            temp_confidence.append(confidence)\n",
    "\n",
    "    if min_x == \"\":\n",
    "        for token in json_dict.pages[page].tokens:\n",
    "            if not token.layout.text_anchor.text_segments[0].start_index:\n",
    "                token.layout.text_anchor.text_segments[0].start_index = 0\n",
    "\n",
    "            if (\n",
    "                abs(\n",
    "                    int(token.layout.text_anchor.text_segments[0].start_index)\n",
    "                    - int(text_anchors_check[0][\"start_index\"])\n",
    "                )\n",
    "                <= 2\n",
    "                and abs(\n",
    "                    int(token.layout.text_anchor.text_segments[0].end_index)\n",
    "                    - int(text_anchors_check[0][\"end_index\"])\n",
    "                )\n",
    "                <= 2\n",
    "            ):\n",
    "                normalized_vertices = token.layout.bounding_poly\n",
    "                min_x, min_y, max_x, max_y = get_normalizedvertices(normalized_vertices)\n",
    "                text_anc_token = token.layout.text_anchor.text_segments\n",
    "                confidence = token.layout.confidence\n",
    "    if len(temp_text_anc) != 0:\n",
    "        final_ver = {\n",
    "            \"min_x\": min(temp_ver[\"x\"]),\n",
    "            \"min_y\": min(temp_ver[\"y\"]),\n",
    "            \"max_x\": max(temp_ver[\"x\"]),\n",
    "            \"max_y\": max(temp_ver[\"y\"]),\n",
    "        }\n",
    "        final_confidence = min(temp_confidence)\n",
    "        final_text_anc = sorted(temp_text_anc, key=lambda x: x.end_index)\n",
    "        return final_ver, final_text_anc, final_confidence\n",
    "    else:\n",
    "        return (\n",
    "            {\"min_x\": min_x, \"min_y\": min_y, \"max_x\": max_x, \"max_y\": max_y},\n",
    "            text_anc_token,\n",
    "            confidence,\n",
    "        )\n",
    "\n",
    "\n",
    "def synonym_entities(json_dict: object, Synonyms_entities: dict) -> object:\n",
    "    \"\"\"\n",
    "    Find synonym entities in the loaded JSON and add them to the entities list.\n",
    "\n",
    "    Args:\n",
    "        json_dict (object): Loaded JSON dictionary.\n",
    "        Synonyms_entities (dict): Dictionary of synonym entities.\n",
    "    Returns:\n",
    "        object: Updated JSON dictionary with added entities.\n",
    "    \"\"\"\n",
    "\n",
    "    def find_substring_indexes(text: str, substring: str) -> list:\n",
    "        \"\"\"\n",
    "        Find the start and end indices of all occurrences of a substring in the given text.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to search in.\n",
    "            substring (str): The substring to find.\n",
    "\n",
    "        Returns:\n",
    "            List: A list of tuples containing start and end indices of substring occurrences.\n",
    "        \"\"\"\n",
    "        import re\n",
    "\n",
    "        if \" \" or \"\\n\" not in substring:\n",
    "            pattern = re.compile(re.escape(substring), re.IGNORECASE)\n",
    "            matches = [(match.start(), match.end()) for match in pattern.finditer(text)]\n",
    "        else:\n",
    "            pattern = re.compile(\n",
    "                r\"{}.*{}\".format(\n",
    "                    re.escape(substring.split(\" \")[0]),\n",
    "                    re.escape(substring.split(\" \")[-1]),\n",
    "                ),\n",
    "                re.IGNORECASE,\n",
    "            )\n",
    "            matches = [\n",
    "                (match.start(), match.end())\n",
    "                for match in pattern.finditer(json_dict.text)\n",
    "            ]\n",
    "\n",
    "        return matches\n",
    "\n",
    "    def create_ent(\n",
    "        ent_type: str, min_xy: dict, text_anc: list, page: str, confidence: float\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Create an entity dictionary.\n",
    "\n",
    "        Args:\n",
    "            ent_type (str): The type of the entity.\n",
    "            min_xy (Dict[str, int]): Dictionary containing minimum x, y coordinates of the bounding box.\n",
    "            text_anc (List): List of text segments.\n",
    "            page (str): Page number.\n",
    "            confidence (float): Confidence score.\n",
    "\n",
    "        Returns:\n",
    "            Dict: The created entity dictionary.\n",
    "        \"\"\"\n",
    "        final_mention_text = \"\"\n",
    "        for index1 in text_anc:\n",
    "            final_mention_text += json_dict.text[\n",
    "                int(index1.start_index) : int(index1.end_index)\n",
    "            ]\n",
    "        min_x = min_xy[\"min_x\"]\n",
    "        min_y = min_xy[\"min_y\"]\n",
    "        max_x = min_xy[\"max_x\"]\n",
    "        max_y = min_xy[\"max_y\"]\n",
    "        new_ent = {\n",
    "            \"confidence\": confidence,\n",
    "            \"mention_text\": final_mention_text,\n",
    "            \"page_anchor\": {\n",
    "                \"page_refs\": [\n",
    "                    {\n",
    "                        \"bounding_poly\": {\n",
    "                            \"normalized_vertices\": [\n",
    "                                {\"x\": min_x, \"y\": min_y},\n",
    "                                {\"x\": min_x, \"y\": max_y},\n",
    "                                {\"x\": max_x, \"y\": min_y},\n",
    "                                {\"x\": max_x, \"y\": max_y},\n",
    "                            ]\n",
    "                        },\n",
    "                        \"page\": page,\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \"text_anchor\": {\"text_segments\": text_anc},\n",
    "            \"type\": ent_type,\n",
    "        }\n",
    "        return new_ent\n",
    "\n",
    "    new_entities = []\n",
    "    for key, value in Synonyms_entities.items():\n",
    "        for syn in value:\n",
    "            match_indexes = find_substring_indexes(json_dict.text, syn)\n",
    "            for match in match_indexes:\n",
    "                print(match)\n",
    "                if len(match) > 1:\n",
    "                    for page in range(len(json_dict.pages)):\n",
    "                        temp = json_dict.pages[page].layout.text_anchor.text_segments\n",
    "                        if not temp[0].start_index:\n",
    "                            temp[0].start_index = 0\n",
    "                        if match[0] >= int(temp[0].start_index) and match[1] < int(\n",
    "                            temp[0].end_index\n",
    "                        ):\n",
    "                            try:\n",
    "                                min_xy, text_anc, confidence = get_token(\n",
    "                                    json_dict,\n",
    "                                    page,\n",
    "                                    [{\"start_index\": match[0], \"end_index\": match[1]}],\n",
    "                                )\n",
    "                                new_ent = create_ent(\n",
    "                                    key, min_xy, text_anc, page, confidence\n",
    "                                )\n",
    "                                new_entities.append(new_ent)\n",
    "                            except Exception as e:\n",
    "                                print(e)\n",
    "    if len(new_entities) > 0:\n",
    "        for ent1 in new_entities:\n",
    "            json_dict.entities.append(ent1)\n",
    "\n",
    "    return json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71679fb2-e09b-41cb-80e0-1ee9b9059ec6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_names_list, file_dict = utilities.file_names(gcs_input_path)\n",
    "for filename, filepath in tqdm(file_dict.items(), desc=\"Progress\"):\n",
    "    print(\">>>>>>>>>>>>>>> Processing File : \", filename)\n",
    "    input_bucket_name = gcs_input_path.split(\"/\")[2]\n",
    "    if \".json\" in filepath:\n",
    "        json_dict = utilities.documentai_json_proto_downloader(\n",
    "            input_bucket_name, filepath\n",
    "        )\n",
    "        json_dict_updated = synonym_entities(json_dict, synonyms_entities)\n",
    "        output_bucket_name = gcs_output_path.split(\"/\")[2]\n",
    "        output_path_within_bucket = \"/\".join(gcs_output_path.split(\"/\")[3:]) + filename\n",
    "        utilities.store_document_as_json(\n",
    "            documentai.Document.to_json(json_dict_updated),\n",
    "            output_bucket_name,\n",
    "            output_path_within_bucket,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4910c5-2270-49ea-a585-3ce3023e9038",
   "metadata": {},
   "source": [
    "# 4.Output Details\n",
    "\n",
    "The output jsons files will be stored in the given output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc29ee0-7621-4ef1-9788-23c639a18fff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
