{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea6835d4-cda8-44c5-873f-5687dc564ff9",
   "metadata": {},
   "source": [
    "# PRE - POST HITL Bounding Box Mismatch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcf8e7b-1374-4a4a-9b63-2f04e76d63cd",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b31e10-66da-4e9c-bea3-22be7c232ad6",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the DocAI Incubator Team. No guarantees of performance are implied.\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790ad613-2fe3-4e0f-a5f5-2217497f5329",
   "metadata": {},
   "source": [
    "## Purpose of the script\n",
    "\n",
    "Pre and POST HITL comparison tool which detect two issues - Parser issue and OCR issue.\n",
    "And the result output contains a summary json file which shows basic stats, count of the OCR and Parser issues for entities present in each document and corresponding analysis csv files.\n",
    "\n",
    " * **Parser issue :** This issue is identified with the parser when the bounding box is not covering the text region completely and hence the required text was not captured completely. The user accesses HITL worker UI and adjusts the bounding box to include the text region and save. The script highlight such cases\n",
    "\n",
    " * **OCR issue :** This issue is identified with the parser when the bounding box covers the whole text region and as result the expected text was not captured completely. The script highlight such cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1352d4-1a7a-4232-9f1c-c3e4fa47ab7a",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    " * Vertex AI Notebook\n",
    " * Google Cloud Storage bucket\n",
    " * Pre HITL and Post HITL Json files (filename should be same) in GCS Folders\n",
    " * DocumentAI and HITL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f1381d-f69a-4dca-a65b-eae04761c40d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step by Step procedure \n",
    "### 1. Setup the required inputs\n",
    "#### Execute the below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec15989-4a5e-4882-a9cb-bae8b5480361",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = \"<Project-ID>\"\n",
    "pre_HITL_output_URI = \"gs://<bucket-name>/<folder_pre>\"\n",
    "post_HITL_output_URI = \"gs://<bucket-name>/<folder_post>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2035fbe-b6fa-4c51-9b76-40e87c38c850",
   "metadata": {},
   "source": [
    " * **project_id**: provide the project id \n",
    " * **Pre_HITL_Output_URI:** provide the gcs path of pre HITL jsons (processed jsons) \n",
    " * **Post_HITL_Output_URI:** provide the gcs path of post HITL jsons (Jsons processed through HITL) \n",
    "\n",
    "**NOTE:** The Name of Post-HITL Json will not be the same as the original file name by default. This has to be updated manually before using this tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e98fc7d-4e80-4e49-a2ba-60851864ff0b",
   "metadata": {},
   "source": [
    "##  2. Output\n",
    "Result summary table is obtained which highlight the count of parser and ocr issues for each file. The result table contain details related to pre and post HITL entity changes, whether there were bounding box coordinates mismatched upon post HITL processing. The below screenshots showcases the parser or ocr issue.\n",
    "\n",
    "![](https://screenshot.googleplex.com/6S47qFm5SjP8eMC.png)\n",
    "![](https://screenshot.googleplex.com/6HyQwucSQPZR4ii.png)\n",
    "\n",
    "Summary json file is generated which highlight count of bounding box mismatches, OCR and Parser errors and analysis path to result table for each of the processed files.\n",
    "\n",
    "![](https://screenshot.googleplex.com/55R5NKSuVYmyP9H.png)\n",
    "\n",
    "Entity wise analysis for each file can be observed in the following csv files under analysis/ folder.\n",
    "\n",
    "![](https://screenshot.googleplex.com/BKd5QCidEJac9Jy.png)\n",
    "\n",
    "**Table columns:**\n",
    "\n",
    "The result output table has following columns and its details are as follows:\n",
    " * File Name : name of the file\n",
    " * Entity Type : type of the entity \n",
    " * Pre_HITL_Output : entity text before HITL \n",
    " * Pre_HITL_bbox : entity bounding box coordinates before HITL\n",
    " * Post_HITL_Output : entity text before HITL \n",
    " * Hitl_update : if there was HITL update for that particular entity\n",
    " * Post_HITL_bbox : entity bounding box coordinates after HITL\n",
    " * Fuzzy Ratio : text match %\n",
    " * Bbox_mismatch : if the bounding box coordinates are mismatched\n",
    " * OCR issue : represents if its classified as OCR Issue\n",
    " * Parser issue : represents if its classified as Parser Issue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe86372-45ba-443b-818c-0b4a75fee96c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Notebook Script\n",
    "\n",
    "**Install the below libraries before executing the script** \\\n",
    "If you encounter an error while importing libraries, please verify that you have installed them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1daa5fc-1259-4c5c-a0d3-e81d81b78637",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-cloud-documentai\n",
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a18e66-dd30-4f7e-9236-cc04a371fd6b",
   "metadata": {},
   "source": [
    "**Script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8751be86-e5d3-49e2-a38c-bf362cb5dab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import configparser\n",
    "import difflib\n",
    "import io\n",
    "import json\n",
    "import operator\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from collections.abc import Container, Iterable, Iterator, Mapping, Sequence\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Import the libraries\n",
    "import pandas as pd\n",
    "from google.cloud import documentai_v1beta3, storage\n",
    "from PIL import Image\n",
    "from PyPDF2 import PdfFileReader\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import utilities\n",
    "\n",
    "\n",
    "now = str(datetime.datetime.now())\n",
    "now = re.sub(r\"\\W+\", \"\", now)\n",
    "\n",
    "print(\"Creating temporary buckets\")\n",
    "pre_HITL_bucket_name_temp = \"pre_hitl_output\" + \"_\" + now\n",
    "post_HITL_bucket_name_temp = \"post_hitl_output_temp\" + \"_\" + now\n",
    "# bucket name and prefix\n",
    "pre_HITL_bucket = pre_HITL_output_URI.split(\"/\")[2]\n",
    "post_HITL_bucket = post_HITL_output_URI.split(\"/\")[2]\n",
    "# getting all files and copying to temporary folder\n",
    "\n",
    "try:\n",
    "    utilities.check_create_bucket(pre_HITL_bucket_name_temp)\n",
    "    utilities.check_create_bucket(post_HITL_bucket_name_temp)\n",
    "except Exception as e:\n",
    "    print(\"unable to create bucket because of exception : \", e)\n",
    "\n",
    "try:\n",
    "    pre_HITL_output_files, pre_HITL_output_dict = utilities.file_names(\n",
    "        pre_HITL_output_URI\n",
    "    )\n",
    "    # print(pre_HITL_output_files,pre_HITL_output_dict)\n",
    "    post_HITL_output_files, post_HITL_output_dict = utilities.file_names(\n",
    "        post_HITL_output_URI\n",
    "    )\n",
    "    # print(post_HITL_output_files,post_HITL_output_dict)\n",
    "    print(\"copying files to temporary bucket\")\n",
    "    for i in pre_HITL_output_files:\n",
    "        utilities.copy_blob(\n",
    "            pre_HITL_bucket, pre_HITL_output_dict[i], pre_HITL_bucket_name_temp, i\n",
    "        )\n",
    "    for i in post_HITL_output_files:\n",
    "        utilities.copy_blob(\n",
    "            post_HITL_bucket, post_HITL_output_dict[i], post_HITL_bucket_name_temp, i\n",
    "        )\n",
    "    pre_HITL_files_list = utilities.list_blobs(pre_HITL_bucket_name_temp)\n",
    "    post_HITL_files_list = utilities.list_blobs(post_HITL_bucket_name_temp)\n",
    "except Exception as e:\n",
    "    print(\"unable to get list of files in buckets because : \", e)\n",
    "# processing the files and saving the files in temporary GCP bucket\n",
    "relation_dict, non_relation_dict = utilities.matching_files_two_buckets(\n",
    "    pre_HITL_bucket_name_temp, post_HITL_bucket_name_temp\n",
    ")\n",
    "\n",
    "time_stamp = datetime.datetime.now().strftime(\"%d_%m_%y-%H%M%S\")\n",
    "filename_error_count_dict = {}\n",
    "\n",
    "compare_merged = pd.DataFrame()\n",
    "accuracy_docs = []\n",
    "print(\"comparing the PRE-HITL Jsons and POST-HITL jsons ....Wait for Summary \")\n",
    "for i in relation_dict:\n",
    "    # print(\"***** i : \", i)\n",
    "    pre_HITL_json = utilities.documentai_json_proto_downloader(\n",
    "        pre_HITL_bucket_name_temp, i\n",
    "    )\n",
    "    post_HITL_json = utilities.documentai_json_proto_downloader(\n",
    "        post_HITL_bucket_name_temp, relation_dict[i]\n",
    "    )\n",
    "    # print('pre_HITL_json : ', pre_HITL_json)\n",
    "    # print('post_HITL_json : ', post_HITL_json)\n",
    "    compare_output = utilities.compare_pre_hitl_and_post_hitl_output(\n",
    "        pre_HITL_json, post_HITL_json\n",
    "    )[0]\n",
    "    # Rename columns\n",
    "    compare_output = compare_output.rename(\n",
    "        columns={\"pre_bbox\": \"Pre_HITL_bbox\", \"post_bbox\": \"Post_HITL_bbox\"}\n",
    "    )\n",
    "\n",
    "    # Drop unwanted columns\n",
    "    compare_output = compare_output.drop([\"page1\", \"page2\"], axis=1)\n",
    "\n",
    "    # print('compare_output :',compare_output)\n",
    "    # display(compare_output)\n",
    "    column = [relation_dict[i]] * compare_output.shape[0]\n",
    "    # print(\"++++column++++\")\n",
    "    # print(column)\n",
    "    compare_output.insert(loc=0, column=\"File Name\", value=column)\n",
    "\n",
    "    compare_output.insert(loc=5, column=\"hitl_update\", value=\" \")\n",
    "    for j in range(len(compare_output)):\n",
    "        if compare_output[\"Fuzzy Ratio\"][j] != 1.0:  # strict\n",
    "            if (\n",
    "                compare_output[\"Pre_HITL_Output\"][j] == \"Entity not found.\"\n",
    "                and compare_output[\"Post_HITL_Output\"][j] == \"Entity not found.\"\n",
    "            ):\n",
    "                compare_output[\"hitl_update\"][j] = \"NO\"\n",
    "            else:\n",
    "                compare_output[\"hitl_update\"][j] = \"YES\"\n",
    "        else:\n",
    "            compare_output[\"hitl_update\"][j] = \"NO\"\n",
    "    for k in range(len(compare_output)):\n",
    "        if compare_output[\"Fuzzy Ratio\"][k] != 1.0:  # strict\n",
    "            hitl_update = \"HITL UPDATED\"\n",
    "            break\n",
    "        else:\n",
    "            compare_output[\"hitl_update\"][k] = \"NO\"\n",
    "\n",
    "    ##\n",
    "    compare_output[\"bbox_mismatch\"] = (\n",
    "        compare_output[\"Pre_HITL_bbox\"] != compare_output[\"Post_HITL_bbox\"]\n",
    "    )\n",
    "\n",
    "    # OCR Issue\n",
    "    compare_output[\"OCR Issue\"] = \"No\"\n",
    "    # compare_output.loc[(compare_output['Pre_HITL_Output'] != compare_output['Post_HITL_Output']), 'OCR Issue']  = 'Yes' # & cordinates are same\n",
    "    compare_output.loc[\n",
    "        (compare_output[\"Pre_HITL_Output\"] != compare_output[\"Post_HITL_Output\"])\n",
    "        & (compare_output[\"Pre_HITL_bbox\"] == compare_output[\"Post_HITL_bbox\"]),\n",
    "        \"OCR Issue\",\n",
    "    ] = \"Yes\"\n",
    "\n",
    "    # Parser Issue\n",
    "    compare_output[\"Parser Issue\"] = \"No\"\n",
    "    compare_output.loc[\n",
    "        (compare_output[\"hitl_update\"] == \"YES\")\n",
    "        & (compare_output[\"bbox_mismatch\"] == True),\n",
    "        \"Parser Issue\",\n",
    "    ] = \"Yes\"  # & cordinates are different\n",
    "    try:\n",
    "        compare_merged.loc[\n",
    "            (compare_merged[\"Post_HITL_Output\"] == \"Entity not found.\")\n",
    "            | (compare_merged[\"Pre_HITL_Output\"] == \"Entity not found.\"),\n",
    "            \"Parser Issue\",\n",
    "        ] = \"Yes\"\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    ## global dict : no of parser error / file\n",
    "    temp = {}\n",
    "    temp[\"bbox_mismatch\"] = len(compare_output[compare_output[\"bbox_mismatch\"] == True])\n",
    "\n",
    "    temp[\"OCR_issue\"] = len(\n",
    "        compare_output.loc[\n",
    "            (compare_output[\"Pre_HITL_Output\"] != compare_output[\"Post_HITL_Output\"])\n",
    "            & (compare_output[\"Pre_HITL_bbox\"] == compare_output[\"Post_HITL_bbox\"])\n",
    "        ]\n",
    "    )\n",
    "    temp[\"Parser_issue\"] = len(\n",
    "        compare_output.loc[\n",
    "            (compare_output[\"hitl_update\"] == \"YES\")\n",
    "            & (compare_output[\"bbox_mismatch\"] == True)\n",
    "        ]\n",
    "    )\n",
    "    temp[\"output_file\"] = \"analysis_\" + time_stamp + \"/\" + i.replace(\"json\", \"csv\")\n",
    "\n",
    "    filename_error_count_dict[i] = temp\n",
    "\n",
    "    new_row = pd.Series(\n",
    "        [\n",
    "            i,\n",
    "            \"Entities\",\n",
    "            \"are updated\",\n",
    "            \"by HITL\",\n",
    "            \":\",\n",
    "            np.nan,\n",
    "            hitl_update,\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "        ],\n",
    "        index=compare_output.columns,\n",
    "    )\n",
    "    compare_output = compare_output.append(new_row, ignore_index=True)\n",
    "    frames = [compare_merged, compare_output]\n",
    "    compare_merged = pd.concat(frames)\n",
    "\n",
    "with open(\"summary_\" + time_stamp + \".json\", \"w\") as ofile:\n",
    "    ofile.write(json.dumps(filename_error_count_dict))\n",
    "\n",
    "for x in relation_dict:\n",
    "    # print(x)\n",
    "    file_out = compare_merged[compare_merged[\"File Name\"] == x]\n",
    "    try:\n",
    "        os.mkdir(\"analysis_\" + time_stamp)\n",
    "    except:\n",
    "        pass\n",
    "    file_out.to_csv(\"analysis_\" + time_stamp + \"/\" + x.replace(\"json\", \"csv\"))\n",
    "\n",
    "utilities.bucket_delete(pre_HITL_bucket_name_temp)\n",
    "utilities.bucket_delete(post_HITL_bucket_name_temp)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m104",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m104"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
