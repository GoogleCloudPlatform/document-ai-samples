{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "707aac26-5f83-4dfe-a9c3-73f9eb34dea4",
   "metadata": {},
   "source": [
    "# Document AI Parser Result Merger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff479c9-a573-48c7-bd26-d9c13cf5be7f",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5efc16-e389-48af-90d6-5d99019a1059",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the DocAI Incubator Team. No guarantees of performance are implied.\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47eed16a-e66d-4c76-860f-16f08ae45867",
   "metadata": {},
   "source": [
    "## Objective\n",
    "Document AI Parser Result Merger is a tool built using Python programming language. Its purpose is to address the issue of merging the two or more resultant json files of Document AI processors. This document highlights the working of the tool(script) and its requirements. The documents usually contain multiple pages. There are 2 use cases by which this solution can be operated. \n",
    "### Case 1: Different documents, parser results  json merger (Default).\n",
    " * Case 1 deals when we are using two or multiple parser output Jsons are from different documents\n",
    " * To Enable this case the flag should be ‘1’\n",
    "### Case 2: Same document, different parsers json merger(Added functionality).\n",
    " * Case 2 deals when we are using two or multiple parser outputs from the same document.\n",
    " * To Enable this case the flag should be ‘2’"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ef6ea3-79e0-42a0-a2bb-84ece51bff74",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64bc02d-ec9b-42b6-b3b0-ed70fa6c3808",
   "metadata": {},
   "source": [
    "This tool requires the following services:\n",
    "\n",
    " * Google Jupyter Notebook or Colab.\n",
    " * Google Cloud Storage \n",
    " * DocumentAI processor and JSON files\n",
    " \n",
    "Google Jupyter Notebook or Colab is used for running the python notebook file. Cloud Storage Buckets have the input files to this script. The multiple input files are the json files which are the result of a Document AI processor (for eg., Bank Statement Parser). These json files include multiple pages in its document. After the script executes, the output file is a single merged json file stored in the output bucket path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b4964c-d228-4aad-a6fb-346465791fe7",
   "metadata": {},
   "source": [
    "## Workflow overview\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a6f212-d41a-4ae7-b374-1c97ffb03931",
   "metadata": {},
   "source": [
    "![](https://screenshot.googleplex.com/9F5qLEtZJ4Kdj8m.png)\n",
    "\n",
    "The above diagram shows the flow diagram of the tool. As highlighted there are input and output GCP buckets and there is a python script which processes the request. The input bucket holds the multiple json files which need to be merged into a single file and this is achieved by the python script. This script accepts the input json files and prompts users to switch between the default case-1 or the case-2 mode as highlighted in the previous sections.  Finally there is an output GCP bucket to store the single merged file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3155aca4-1aeb-4a22-a0b7-a3e9b43e69c0",
   "metadata": {},
   "source": [
    "## Script walkthrough\n",
    "Insights and details about the script are explained in detail as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dba5180-f972-4d5e-9802-974885efe2d4",
   "metadata": {},
   "source": [
    "## 1. Import Modules/Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a73b8fd9-547b-4db9-8a00-5628ceac6034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from google.cloud import storage\n",
    "from google.cloud.documentai_v1beta3 import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758afb38-cede-4042-b9e1-9c847eef818f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Input Details : Entering Project details in below variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444df062-df9e-48c0-9f96-fe2789f3e1f0",
   "metadata": {},
   "source": [
    "\n",
    " * **PROJECT_ID:** provide your GCP project ID (Optional)\n",
    " * **INPUT_MULTIPLE_JSONS_URI:** provide the uri link of folder containing the input files (ends with \"/\")\n",
    " * **JSON_DIRECTORY_PATH_OUTPUT:** provide the folder name of the output file(ends with \"/\") which gets generated post execution of the script.\n",
    " * **OUTPUT_FILE_NAME:** enter a name for the generated file which is saved in the output bucket.\n",
    " * **MERGER_TYPE_FLAG:** based on user need, values 1 or 2 can be provided as mentioned in the earlier part of this document.\n",
    "\n",
    "     - Case 1 deals when we are using two or multiple parser output Jsons are from different documents\n",
    "\n",
    "     - Case 2 deals when we are using two or multiple parser outputs from the same document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17bdf742-c286-4735-bae5-91eb5c1a1ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"xxxx-xxxx\"  # Optional\n",
    "INPUT_MULTIPLE_JSONS_URI = \"gs://xxxx/xxxx/\"  # ends with \"/\"\n",
    "JSON_DIRECTORY_PATH_OUTPUT = \"gs://xxxx/xxxx/\"  # ends with \"/\"\n",
    "OUTPUT_FILE_NAME = \"merged_file.json\"\n",
    "MERGER_TYPE_FLAG = 1  # 1-for different docs, 2-same doc default=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79be40da-86a7-46c3-8135-6b1e384438b6",
   "metadata": {},
   "source": [
    "## 3. Run the below code.\n",
    "\n",
    "Use the below code and Run all the cells (Update the Path parameter if it is not available in the current working directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ffdf37c-332f-4ba5-a55f-349d2cccb432",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_gcs_folder(path: str) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    This function splits gcs uri to 2 parts\n",
    "        1. gcs bucket\n",
    "        2. file path after bucket\n",
    "    \"\"\"\n",
    "\n",
    "    pattern = re.compile(\"gs://(?P<bucket>.*?)/(?P<files_dir>.*)\")\n",
    "    uri = re.match(pattern, path)\n",
    "    return uri.group(\"bucket\"), uri.group(\"files_dir\")\n",
    "\n",
    "\n",
    "def file_names(bucket: str, files_dir_prefix: str) -> Tuple[List[str], Dict[str, str]]:\n",
    "    \"\"\"This Function will load the bucket and get the list of files\n",
    "    in the gs path given\n",
    "    \"\"\"\n",
    "\n",
    "    filenames_list = []\n",
    "    filenames_dict = {}\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket)\n",
    "    blobs = bucket.list_blobs(prefix=files_dir_prefix)\n",
    "    filenames = [blob.name for blob in blobs]\n",
    "    for filename in filenames:\n",
    "        file = filename.split(\"/\")[-1]\n",
    "        if file:\n",
    "            filenames_list.append(file)\n",
    "            filenames_dict[file] = filename\n",
    "    return filenames_list, filenames_dict\n",
    "\n",
    "\n",
    "def list_json_files(filenames: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Takes filenames and return JSON files only as list\n",
    "    \"\"\"\n",
    "\n",
    "    json_files = []\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".json\"):\n",
    "            json_files.append(filename)\n",
    "    return json_files\n",
    "\n",
    "\n",
    "def assign_indexes(\n",
    "    layout: Union[Document.Entity, Document.Page.Layout], text: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    It will assign new index values to start_index and end_index for respective class object\n",
    "    \"\"\"\n",
    "\n",
    "    for text_segment in layout.text_anchor.text_segments:\n",
    "        text_segment.end_index = int(text_segment.end_index) + len(text)\n",
    "        text_segment.start_index = int(text_segment.start_index) + len(text)\n",
    "\n",
    "\n",
    "def assign_page_ref_page(\n",
    "    entity: documentai.Document.Entity, doc_first: documentai.Document\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    It will accumulate page count for entities-page_anchor-page_refs\n",
    "    \"\"\"\n",
    "\n",
    "    for page_ref in entity.page_anchor.page_refs:\n",
    "        page_ref.page = str(int(page_ref.page) + len(doc_first.pages))\n",
    "\n",
    "\n",
    "### CASE - 1\n",
    "def different_doc_merger(\n",
    "    doc_first: documentai.Document, doc_second: documentai.Document\n",
    ") -> documentai.Document:\n",
    "    \"\"\"\n",
    "    This function takes two documentai.Document objects and merges them as one\n",
    "    \"\"\"\n",
    "\n",
    "    doc_merged = documentai.Document()\n",
    "\n",
    "    ### Entities ###\n",
    "    for entity in doc_second.entities:\n",
    "        assign_indexes(entity, doc_first.text)  # entity-textanchors\n",
    "        assign_page_ref_page(entity, doc_first)  # entity-pageanchors\n",
    "        for prop in entity.properties:  # entity properties\n",
    "            assign_indexes(prop, doc_first.text)\n",
    "            assign_page_ref_page(prop, doc_first)\n",
    "    doc_merged.entities = list(doc_first.entities) + list(doc_second.entities)\n",
    "\n",
    "    # Pages\n",
    "    for page in doc_second.pages:\n",
    "        print(page.page_number, end=\" \")\n",
    "        page.page_number = int(page.page_number) + len(\n",
    "            doc_first.pages\n",
    "        )  # Page Number increment in second doc\n",
    "        print(\" \", page.page_number)\n",
    "\n",
    "        # page . layout . textanchor . textsegment\n",
    "        assign_indexes(page.layout, doc_first.text)\n",
    "\n",
    "        for block in page.blocks:\n",
    "            assign_indexes(block.layout, doc_first.text)\n",
    "\n",
    "        for paragraph in page.paragraphs:\n",
    "            assign_indexes(paragraph.layout, doc_first.text)\n",
    "\n",
    "        for line in page.lines:\n",
    "            assign_indexes(line.layout, doc_first.text)\n",
    "\n",
    "        for token in page.tokens:\n",
    "            assign_indexes(token.layout, doc_first.text)\n",
    "\n",
    "    doc_merged.pages = list(doc_first.pages) + list(doc_second.pages)\n",
    "    doc_merged.text = doc_first.text + doc_second.text\n",
    "    doc_merged.shard_info = doc_second.shard_info\n",
    "    doc_merged.uri = doc_second.uri\n",
    "    return doc_merged\n",
    "\n",
    "\n",
    "### CASE -2\n",
    "def same_doc_diff_parser_merger(\n",
    "    doc_first: documentai.Document, doc_second: documentai.Document\n",
    ") -> documentai.Document:\n",
    "    \"\"\"\n",
    "    This function merges the entities of two documentai.Document object as one\n",
    "    \"\"\"\n",
    "\n",
    "    doc_first.entities = list(doc_first.entities) + list(doc_second.entities)\n",
    "    doc_first.uri = doc_second.uri\n",
    "    doc_first.text = doc_second.text\n",
    "    doc_first.pages = doc_second.pages\n",
    "    doc_first.shard_info = doc_second.shard_info\n",
    "    return doc_first\n",
    "\n",
    "\n",
    "def iter_json_files(\n",
    "    bucket_obj: storage.Bucket,\n",
    "    input_bucket_files: List[str],\n",
    "    file_dict: Dict[str, str],\n",
    "    doc_merged: documentai.Document,\n",
    "    MERGER_TYPE_FLAG: int = 1,\n",
    ") -> documentai.Document:\n",
    "    \"\"\"\n",
    "    It will iterate through all json files and merges each file to doc_merged parameter based on MERGER_TYPE_FLAG\n",
    "    \"\"\"\n",
    "\n",
    "    func = (\n",
    "        different_doc_merger if (MERGER_TYPE_FLAG == 1) else same_doc_diff_parser_merger\n",
    "    )\n",
    "    for file in input_bucket_files:\n",
    "        print(file)\n",
    "        doc_second = load_document_from_gcs(bucket_obj, file_dict[file])\n",
    "        doc_merged = func(doc_merged, doc_second)\n",
    "    return doc_merged\n",
    "\n",
    "\n",
    "def delete_id(doc_merged: documentai.Document) -> documentai.Document:\n",
    "    \"\"\"\n",
    "    It will assign empty string to id property of Entity object\n",
    "    \"\"\"\n",
    "\n",
    "    for entity in doc_merged.entities:\n",
    "        entity.id = \"\"\n",
    "        for prop in entity.properties:\n",
    "            prop.id = \"\"\n",
    "    return doc_merged\n",
    "\n",
    "\n",
    "def load_document_from_gcs(\n",
    "    bucket_obj: storage.Bucket, filepath: str\n",
    ") -> documentai.Document:\n",
    "    \"\"\"\n",
    "    It will load json file from GCS filepath and returns documentai.Document object\n",
    "    \"\"\"\n",
    "\n",
    "    data_str = bucket_obj.blob(filepath).download_as_string().decode(\"utf-8\")\n",
    "    document = documentai.Document.from_json(data_str)\n",
    "    return document\n",
    "\n",
    "\n",
    "def merge_document_objects(\n",
    "    MERGER_TYPE_FLAG: int,\n",
    "    input_bucket: str,\n",
    "    input_bucket_files: List[str],\n",
    "    file_dict: Dict[str, str],\n",
    ") -> documentai.Document:\n",
    "    \"\"\"\n",
    "    It will merges all json files from gcs folder based on MERGER_TYPE_FLAG and return merged documentai.Document object\n",
    "    \"\"\"\n",
    "\n",
    "    if MERGER_TYPE_FLAG not in (2, \"2\"):\n",
    "        print(\"\\t\" * 5, \"Using Default Merger\")\n",
    "        MERGER_TYPE_FLAG = 1\n",
    "    else:\n",
    "        print(\"\\t\" * 5, \"Using Different Processor Result jsons merger\")\n",
    "        MERGER_TYPE_FLAG = 2\n",
    "    storage_client = storage.Client()\n",
    "    bucket_obj = storage_client.get_bucket(input_bucket)\n",
    "    if len(input_bucket_files) < 2:\n",
    "        raise AssertionError(\n",
    "            \"minimum number of files required are >= 2 to perform Merging.\"\n",
    "        )\n",
    "    print(\n",
    "        \"....more than 2 JSON files detected....\",\n",
    "        \"Process Started...\",\n",
    "        sep=\"\\n\",\n",
    "    )\n",
    "    doc_merged = documentai.Document()\n",
    "    if int(MERGER_TYPE_FLAG) == 1:\n",
    "        doc_merged = iter_json_files(\n",
    "            bucket_obj, input_bucket_files, file_dict, doc_merged, MERGER_TYPE_FLAG=1\n",
    "        )\n",
    "    elif int(MERGER_TYPE_FLAG) == 2:\n",
    "        doc_merged = iter_json_files(\n",
    "            bucket_obj, input_bucket_files, file_dict, doc_merged, MERGER_TYPE_FLAG=2\n",
    "        )\n",
    "    return doc_merged\n",
    "\n",
    "\n",
    "def upload_doc_obj_to_gcs(\n",
    "    doc_merged: documentai.Document, output_bucket: str, merged_json_path: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    It will convert documentai.Document object to JSON and uploads to specified GCS uri path as JSON.\n",
    "    \"\"\"\n",
    "\n",
    "    storage_client = storage.Client(output_bucket)\n",
    "    bucket_obj = storage_client.get_bucket(output_bucket)\n",
    "    blob = bucket_obj.blob(merged_json_path)\n",
    "    print(f\"Uploading file to gs://{output_bucket}/{merged_json_path} ...\")\n",
    "    blob.upload_from_string(\n",
    "        documentai.Document.to_json(\n",
    "            doc_merged,\n",
    "            use_integers_for_enums=False,\n",
    "            including_default_value_fields=False,\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Entities count After Merging - \",\n",
    "        len(doc_merged.entities),\n",
    "    )\n",
    "    print(\n",
    "        \"Pages count After Merging - \",\n",
    "        len(doc_merged.pages),\n",
    "    )\n",
    "    blob.content_type = \"application/json\"\n",
    "    blob.update()\n",
    "    print(f\"Successfully uploaded Merged Documnet Object as JSON to GCS\")\n",
    "\n",
    "\n",
    "def main(\n",
    "    INPUT_MULTIPLE_JSONS_URI: str,\n",
    "    JSON_DIRECTORY_PATH_OUTPUT: str,\n",
    "    OUTPUT_FILE_NAME: str,\n",
    "    MERGER_TYPE_FLAG: int,\n",
    "    PROJECT_ID: str = \"\",\n",
    ") -> None:\n",
    "    print(\"Merging JSON's tool started\")\n",
    "    input_bucket, input_files_dir = split_gcs_folder(INPUT_MULTIPLE_JSONS_URI)\n",
    "    output_bucket, output_files_dir = split_gcs_folder(JSON_DIRECTORY_PATH_OUTPUT)\n",
    "    output_files_dir = output_files_dir.strip(\"/\")\n",
    "    file_names_list, file_dict = file_names(input_bucket, input_files_dir)\n",
    "    print(\n",
    "        f\"Pulling list of JSON files from source GCS Path - {INPUT_MULTIPLE_JSONS_URI}\"\n",
    "    )\n",
    "    input_bucket_files = list_json_files(file_names_list)\n",
    "    doc_merged = merge_document_objects(\n",
    "        MERGER_TYPE_FLAG, input_bucket, input_bucket_files, file_dict\n",
    "    )\n",
    "    print(\"Merging process completed...\")\n",
    "    print(\"Deleting id under Entities & Properties of Document Object...\")\n",
    "    delete_id(doc_merged)\n",
    "    merged_json_path = (\n",
    "        (output_files_dir + \"/\" + OUTPUT_FILE_NAME)\n",
    "        if output_files_dir\n",
    "        else (OUTPUT_FILE_NAME)\n",
    "    )\n",
    "    upload_doc_obj_to_gcs(doc_merged, output_bucket, merged_json_path)\n",
    "    print(\"Process Completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5a4589a-f4e2-4d45-b89e-0b5f1b691bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging JSON's tool started\n",
      "Pulling list of JSON files from source GCS Path - gs://siddam_bucket_test/cde_processor_test/test/\n",
      "\t\t\t\t\t Using Default Merger\n",
      "....more than 2 JSON files detected....\n",
      "Process Started...\n",
      "InsuranceCard-7.json\n",
      "1   1\n",
      "InsuranceCard_24.json\n",
      "1   2\n",
      "InsuranceCard_21.json\n",
      "1   3\n",
      "InsuranceCard_20.json\n",
      "1   4\n",
      "InsuranceCard_26.json\n",
      "1   5\n",
      "InsuranceCard_22.json\n",
      "1   6\n",
      "InsuranceCard_23.json\n",
      "1   7\n",
      "InsuranceCard_25.json\n",
      "1   8\n",
      "InsuranceCard-6.json\n",
      "1   9\n",
      "InsuranceCard-10.json\n",
      "1   10\n",
      "Merging process completed...\n",
      "Deleting id under Entities & Properties of Document Object...\n",
      "Uploading file to gs://siddam_bucket_test/cde_processor_test/merged_file.json ...\n",
      "Entities count After Merging -  40\n",
      "Pages count After Merging -  10\n",
      "Successfully uploaded Merged Documnet Object as JSON to GCS\n",
      "Process Completed.\n"
     ]
    }
   ],
   "source": [
    "main(\n",
    "    INPUT_MULTIPLE_JSONS_URI,\n",
    "    JSON_DIRECTORY_PATH_OUTPUT,\n",
    "    OUTPUT_FILE_NAME,\n",
    "    MERGER_TYPE_FLAG,\n",
    "    PROJECT_ID,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051b1c28-c998-4788-92e6-e9b50c671043",
   "metadata": {},
   "source": [
    "## 4. Output \n",
    "\n",
    "The output of the tool is a **single json file**. Let's examine the outputs for each of the case types. We’ll consider 3 json docs for our experiment and examine the output formats.\n",
    "\n",
    "Consider following 3 input json files residing the input GCS Bucket: \n",
    "\n",
    "json_doc_merge / 0 / doc-0.json\n",
    "json_doc_merge / 1 / doc-1.json\n",
    "json_doc_merge / 2 / doc-2.json\n",
    "\n",
    "Upon running the script for both the cases, the below output details are observed as follows.\n",
    "\n",
    "### CASE - 1 Output \n",
    "Let's suppose the three json files are from different documents (The parser used may be same or different )\n",
    "In Case - 1, we observe in the output that the Pages and Entities count increases with the number of pages and entities present in the input files upon merging. The same applies for the and Text, the value is changed and texts are concatenated and stored as a single value for the Text key of the output file.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4288801-7881-4c5f-9ead-c7a55f298120",
   "metadata": {},
   "source": [
    "| Input json files | Screenshot highlighting the number of entities and number of pages in each of the input json files | The output single merged json file                         |\n",
    "|:----------------:|----------------------------------------------------------------------------------------------------|------------------------------------------------------------|\n",
    "|    **doc-0.json**    | ![](https://screenshot.googleplex.com/7Cn7bf5HKA62omx.png)                                         | ![](https://screenshot.googleplex.com/7zWP7zPZkLeZSra.png) |\n",
    "|    **doc-1.json**    | ![](https://screenshot.googleplex.com/BMGMEcW3EFxWrRc.png)                                         |                                                            |\n",
    "|    **doc-2.json**    | ![](https://screenshot.googleplex.com/3wCEqP9i3Bm9dqB.png)                                         |                                                            |\n",
    "\n",
    "**For example :** each json has  2 pages and 21 entities , the final output merged json has 6 pages and 63 entities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80cd3e7-ee35-4003-ab4a-094a7a935f16",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CASE - 2 Output \n",
    "\n",
    "Let's suppose the three json files are from the single document and from different parser results.\n",
    "\n",
    "In Case - 2, we observe the pages count remains the same and there is only an increase in the count of Entities upon merging the multiple input json files. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b055a42-20dd-4b34-b624-89218224e7ea",
   "metadata": {},
   "source": [
    "| Input json files | Screenshot highlighting the number of entities and number of pages in each of the input json files | The output single merged json file                         |\n",
    "|:----------------:|----------------------------------------------------------------------------------------------------|------------------------------------------------------------|\n",
    "|    **doc-0.json**    | ![](https://screenshot.googleplex.com/ZofmvdULKVFvZ9w.png)                                         | ![](https://screenshot.googleplex.com/Bx2WNCxdcv3pN8p.png) |\n",
    "|    **doc-1.json**    | ![](https://screenshot.googleplex.com/6fgDDEEtRaxNJ2N.png)                                         |                                                            |\n",
    "|    **doc-2.json**    | ![](https://screenshot.googleplex.com/BwYcWwMuT6byLTm.png)                                         |                                                            |\n",
    "\n",
    "**For example :** each json has  2 pages and 21 entities , the final output merged json has 2 pages and 63 entities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089d2d80-0eb5-41a8-9af6-763559607999",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m104",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m104"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
