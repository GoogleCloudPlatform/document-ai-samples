{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Poor Performing Documents\n",
    "\n",
    "- Author: docai-incubator@google.com\n",
    "\n",
    "## Purpose and Description\n",
    "\n",
    "The goal is to automate identifying poorly performing documents for uptraining. The metric of \n",
    "poorly performing documents by the number of missed critical fields. The script will work based on the following conditions :\n",
    "\n",
    "### 1. Input provided to the script\n",
    "\n",
    "    a. Input bucket of labeled documents\n",
    "    \n",
    "    b. Output bucket for poorly performing documents\n",
    "    \n",
    "    c. Project and processor ID (and version) to call the specified processor\n",
    "    \n",
    "    c. List of critical fields. Script should validate before running that critical field names match schema names. If it does not match then          the script should throw an error and request you to update input for critical fields to match schema.\n",
    "    \n",
    "    d. Threshold needed for a document to be sent output bucket for poorly performing documents\n",
    "    \n",
    "### 2. Numerical substring matching condition\n",
    "\n",
    "     a. Script runs documents through a specified processor and identifies poorly performing docs by looking at critical fields of each               document and comparing it to Ground Truth(GT).\n",
    "     \n",
    "    b. Optional numerical substring matching that can be set by entity. If enabled then as long as the numerical subset is correct then it is        not counted as a miss by the processor. For Example, the ground truth is “ID 5123” and the model predicts “5123”. It is not counted as        a miss by the script, as long as it picks up the substring containing all the correct numerical digits then would be correct.\n",
    "    \n",
    "### 3. Threshold logic to move poor performance documents to bucket\n",
    "\n",
    "    a. Script outputs the worst poorly performing documents (by some custom set threshold). For example, documents that got more than 50% of          critical fields wrong are in the output.  Should also accept integers, such as any document with more than 5 missed critical fields  v        sent to the output bucket.\n",
    "    \n",
    "### 4. Output summary and stats file\n",
    "\n",
    "    a. Output a list of misses by critical fields in sheets/CSV for each document that is being sent to the output bucket.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output CSV of missed critical fields:\n",
    "\n",
    "| Document Names | Invoice_1 | Invoice_2 |\n",
    "| --- | --- | --- |\n",
    "| # of mIssed Invoice_ID | 2 | 0 |\n",
    "| # of missed Address | 1 | 1 |\n",
    "| # of missed Taxes | 1 | 3 |\n",
    "\n",
    "Example Input critical fields: \n",
    "| Critical Fields | Numerical substring matching |\n",
    "| --- | --- | \n",
    "| Invoice_ID | Yes | \n",
    "| Address | No | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites \n",
    "1. Access to a Google Cloud project to create Document AI processors.\n",
    "   - Permission to Google project is needed to access Document AI processors.\n",
    "2. Python: Jupyter notebook (Vertex AI) or Google Colab.\n",
    "3. Critical fields list in csv file\n",
    "4. Ground truth Json files in GCS Folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE ON INPUT DETAILS\n",
    "\n",
    "### The possible values for pctg_or_count_flag are one of the below:\n",
    "\n",
    "#### pctg_or_count_flag = count\n",
    "#### pctg_or_count_flag = pctg\n",
    "\n",
    "1. If the pctg_or_count_flag = count, then value for threshold_count must be provided and threshold_pctg should be 0. And if the error count is greater than the value of input threshold_count then the predicted document is moved to a poor performance storage path.\n",
    "\n",
    "2. If the pctg_or_count_flag = pctg, then value for threshold_pctg must be provided and threshold_count = 0. And if the error percentage is greater than the value of input threshold_pctg then the predicted document is moved to a poor performance storage path.\n",
    "\n",
    "### Critical Fields File: This file contains the required list entities of documents along with the flag having values Yes/No. This file determines if the processing substring matches with the schema if enabled. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Operation Procedure\n",
    "\n",
    "### 1. Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install google-cloud-documentai\n",
    "%pip install google-cloud-storage\n",
    "%pip install google-api-core\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install operator\n",
    "%pip install difflib\n",
    "%pip install json\n",
    "%pip install gcsfs\n",
    "%pip install PyPDF2\n",
    "%pip install ast\n",
    "%pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download incubator-tools utilities module to present-working-directory\n",
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "import difflib\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import gcsfs\n",
    "from google.cloud import storage\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from PIL import Image\n",
    "from typing import (\n",
    "    Container,\n",
    "    Iterable,\n",
    "    Iterator,\n",
    "    List,\n",
    "    Mapping,\n",
    "    Optional,\n",
    "    Sequence,\n",
    "    Tuple,\n",
    "    Union,\n",
    ")\n",
    "from PyPDF2 import PdfFileReader\n",
    "import ast\n",
    "import io\n",
    "import re\n",
    "import datetime\n",
    "import utilities  # --> DOWNLOAD THIS AND IMPORT ACCORDINGLY\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Input Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUT DETAILS\n",
    "processor_ID = \"7fbb1ccb4dff7b3c\"  # processor ID based on which documents performance has to be checked\n",
    "project_number = \"514064100333\"  # GCP Project number\n",
    "processor_versionID = (\n",
    "    \"pretrained-invoice-v1.3-2022-07-15\"  # Processor version ID to use for testing\n",
    ")\n",
    "location = \"us\"  # location of processor created\n",
    "project_id = \"rand-automl-project\"  # GCP project ID\n",
    "GT_Output_URI = \"gs://scb_line_item_exp/SCB_Samples/groundtruth/\"  # GCS Bucket where the ground truth files are saved\n",
    "output_folder_path_name = \"poor_performance_doc/\"  # Name of the folder which has to be created and poor performance docs has to be\n",
    "pctg_or_count_FLAG = \"count\"  # criteria to decide the poor performance documents\n",
    "threshold_count = 1  # Threshold count\n",
    "threshold_pctg = 0  # Threshold percentage\n",
    "critical_fields_csv = (\n",
    "    \"CriticalFields.csv\"  # path to csv file containing the list of critical fields\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run the functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_poor_perfoming_docs(\n",
    "    master_df,\n",
    "    pctg_or_count_FLAG,\n",
    "    threshold_count,\n",
    "    threshold_pctg,\n",
    "    cf,\n",
    "    output_folder_path_name,\n",
    "    processed_predected_documents,\n",
    "):\n",
    "    \"\"\"\n",
    "    Identify and upload poorly performing documents based on specified criteria.\n",
    "\n",
    "    Args:\n",
    "        master_df (DataFrame): The master dataframe containing document data.\n",
    "        pctg_or_count_FLAG (str): The flag indicating whether to use 'count' or 'pctg' as the performance criterion.\n",
    "        threshold_count (int): The threshold count value for 'count' criterion.\n",
    "        threshold_pctg (float): The threshold percentage value for 'pctg' criterion.\n",
    "        cf (dict): A dictionary of Critical fields given in csv.\n",
    "        output_folder_path_name (str): The path where the output files will be stored.\n",
    "        processed_predected_documents (dict): A dictionary containing processed predicted documents.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing performance statistics and file paths.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the 'pctg_or_count_FLAG' is not 'count' or 'pctg'.\n",
    "    \"\"\"\n",
    "\n",
    "    def blob_upload(output_folder_path_name, file_name, json_upload):\n",
    "        \"\"\"\n",
    "        Upload a JSON document to a cloud storage bucket.\n",
    "\n",
    "        Args:\n",
    "            output_folder_path_name (str): The path where the document will be stored.\n",
    "            file_name (str): The name of the file to be uploaded.\n",
    "            json_upload (Document): The JSON document to be uploaded.\n",
    "        \"\"\"\n",
    "        output_json_file = (\n",
    "            output_folder_path_name + \"poor_performance-\" + str(file_name)\n",
    "        )\n",
    "        json_poor = documentai.Document.to_json(json_upload)\n",
    "        storage_client = storage.Client()\n",
    "        source_bucket = storage_client.bucket(GT_Output_URI.split(\"/\")[2])\n",
    "        blob = source_bucket.blob(output_json_file)\n",
    "        blob.upload_from_string(\n",
    "            data=bytes(json.dumps(json_poor), \"utf-8\"), content_type=\"application/json\"\n",
    "        )\n",
    "        stats_json[\"poor_performed_doc\"].append(output_json_file)\n",
    "\n",
    "    stats_json = {}\n",
    "    stats_json[\"GT_file_path\"] = GT_Output_URI\n",
    "\n",
    "    time_stamp = datetime.datetime.now().strftime(\"%d_%m_%y-%H%M%S\")\n",
    "    df_filtered = master_df.loc[\n",
    "        (master_df[\"GTvsPredictedDifference\"] == \"YES\"),\n",
    "        (\"File Name\", \"GT Entity Type\", \"GT_Output\", \"Predicted_Output\"),\n",
    "    ]\n",
    "    df_filtered = df_filtered[df_filtered[\"GT Entity Type\"].isin(cf.keys())]\n",
    "    df_group = (\n",
    "        df_filtered.groupby(\"GT Entity Type\")[\"File Name\"]\n",
    "        .value_counts()\n",
    "        .unstack()\n",
    "        .fillna(0)\n",
    "    )\n",
    "    summary_df = pd.DataFrame()\n",
    "    summary_df[\"GT Entity Type\"] = cf.keys()\n",
    "    summary_df.reset_index(drop=True, inplace=True)\n",
    "    x = summary_df.join(df_group, on=\"GT Entity Type\").fillna(0)\n",
    "    x.iloc[:, 1:] = x.iloc[:, 1:].applymap(lambda x: int(x) if not pd.isnull(x) else x)\n",
    "    x.to_csv(\"analysis/summary_\" + time_stamp + \".csv\")\n",
    "    stats_json[\"analysis_summary_csv\"] = \"analysis/summary_stats_\" + time_stamp + \".csv\"\n",
    "    stats_json[\"pctg_or_count_FLAG\"] = pctg_or_count_FLAG\n",
    "    stats_json[\"threshold_count\"] = threshold_count\n",
    "    stats_json[\"threshold_pctg\"] = threshold_pctg\n",
    "    stats_json[\"poor_performed_doc\"] = []\n",
    "    if (pctg_or_count_FLAG == \"count\") and (threshold_pctg == 0):\n",
    "        for predicted_file in list(processed_predected_documents.keys()):\n",
    "            try:\n",
    "                if sum((x[predicted_file])) >= threshold_count:\n",
    "                    blob_upload(\n",
    "                        output_folder_path_name,\n",
    "                        predicted_file,\n",
    "                        processed_predected_documents[predicted_file],\n",
    "                    )\n",
    "                else:\n",
    "                    print(\"not meeting the threshold count value\")\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "    elif (pctg_or_count_FLAG == \"pctg\") and (threshold_count == 0):\n",
    "        for predicted_file in list(processed_predected_documents.keys()):\n",
    "            for val in x[predicted_file].values:\n",
    "                try:\n",
    "                    if (val / sum(x[predicted_file].values) * 100) >= threshold_pctg:\n",
    "                        blob_upload(\n",
    "                            output_folder_path_name,\n",
    "                            predicted_file,\n",
    "                            processed_predected_documents[predicted_file],\n",
    "                        )\n",
    "                        break\n",
    "                    else:\n",
    "                        print(\"not meeting the threshold pctg value\")\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    continue\n",
    "    else:\n",
    "        print(\n",
    "            \"Please check input 'pctg_or_count_flag'. Value should be either 'count' or 'pctg'.\"\n",
    "        )\n",
    "    from pprint import pprint\n",
    "\n",
    "    pprint(stats_json)\n",
    "    with open(\"summary_run_\" + time_stamp + \".json\", \"w\") as fo:\n",
    "        fo.write(json.dumps(stats_json))\n",
    "\n",
    "    return stats_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Getting data from Critical fields.csv\n",
    "    with open(critical_fields_csv, \"r\") as cf_file:\n",
    "        cf_data = cf_file.read()\n",
    "\n",
    "    cf = {}\n",
    "    for field in cf_data.split(\"\\n\"):\n",
    "        data = field.split(\",\")\n",
    "        cf[data[0].strip()] = data[1].strip()\n",
    "        # cf.append((field.split(',')))\n",
    "    num_substring_entities_list = []\n",
    "    for k, v in cf.items():\n",
    "        if v.lower() == \"yes\":\n",
    "            num_substring_entities_list.append(k)\n",
    "\n",
    "    GT_bucket = GT_Output_URI.split(\"/\")[2]\n",
    "    try:\n",
    "        os.mkdir(\"analysis\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    source_bucket = storage_client.bucket(GT_bucket)  # storage bucket name\n",
    "    source_blob = source_bucket.list_blobs(\n",
    "        prefix=\"/\".join(GT_Output_URI.split(\"/\")[3:-1])\n",
    "    )\n",
    "\n",
    "    list_of_files = []\n",
    "    for blob in source_blob:\n",
    "        if blob.name.endswith(\".json\"):\n",
    "            list_of_files.append(\"gs://\" + GT_bucket + \"/\" + blob.name)\n",
    "\n",
    "    document_schema = utilities.get_document_schema(\n",
    "        location, project_number, processor_ID, processor_versionID\n",
    "    )\n",
    "\n",
    "    # Checking whether critical entities are available in schema\n",
    "    list_of_entities = []\n",
    "    for entity_type in document_schema.entity_types:\n",
    "        for entity in entity_type.properties:\n",
    "            list_of_entities.append(entity.name)\n",
    "    for ent in cf.keys():\n",
    "        if ent not in list_of_entities:\n",
    "            print(\n",
    "                \"Stop! Critical Field Entity {} NOT FOUND in  Ground Truth Entities \\n Check the entities in each files and correct...\".format(\n",
    "                    ent\n",
    "                )\n",
    "            )\n",
    "            sys.exit(1)\n",
    "\n",
    "    master_df = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"File Name\",\n",
    "            \"GT Entity Type\",\n",
    "            \"GT_Output\",\n",
    "            \"GT_bbox\",\n",
    "            \"Predicted_Output\",\n",
    "            \"GTvsPredictedDifference\",\n",
    "            \"Predicted_bbox\",\n",
    "            \"Match\",\n",
    "            \"Fuzzy Ratio\",\n",
    "            \"bbox_mismatch\",\n",
    "        ]\n",
    "    )\n",
    "    processed_predected_documents = {}\n",
    "\n",
    "    for prcsd_file in list_of_files:\n",
    "        compare_merged = pd.DataFrame()\n",
    "        GT_json = utilities.documentai_json_proto_downloader(\n",
    "            GT_bucket, (\"/\").join(prcsd_file.split(\"/\")[3:])\n",
    "        )\n",
    "        pdf_bytes, synthesiz_images = utilities.create_pdf_bytes_from_json(\n",
    "            documentai.Document.to_dict(GT_json)\n",
    "        )\n",
    "        processor_result = utilities.process_document_sample(\n",
    "            project_id=project_number,\n",
    "            location=location,\n",
    "            processor_id=processor_ID,\n",
    "            pdf_bytes=pdf_bytes,\n",
    "            processor_version=processor_versionID,\n",
    "        ).document\n",
    "        temp_parsed_json = documentai.Document(processor_result)\n",
    "        compare_output_1, score = utilities.compare_pre_hitl_and_post_hitl_output(\n",
    "            GT_json, temp_parsed_json\n",
    "        )\n",
    "        compare_output_1.rename(\n",
    "            columns={\n",
    "                \"Entity Type\": \"GT Entity Type\",\n",
    "                \"Pre_HITL_Output\": \"GT_Output\",\n",
    "                \"Post_HITL_Output\": \"Predicted_Output\",\n",
    "                \"pre_bbox\": \"GT_bbox\",\n",
    "                \"post_bbox\": \"Predicted_bbox\",\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "        compare_output = compare_output_1.loc[\n",
    "            :,\n",
    "            [\n",
    "                \"GT Entity Type\",\n",
    "                \"GT_Output\",\n",
    "                \"GT_bbox\",\n",
    "                \"Predicted_Output\",\n",
    "                \"Predicted_bbox\",\n",
    "                \"Fuzzy Ratio\",\n",
    "            ],\n",
    "        ]\n",
    "        compare_output.to_csv(\"2.csv\")\n",
    "        column = [prcsd_file.split(\"/\")[-1]] * compare_output.shape[0]\n",
    "        compare_output.insert(loc=0, column=\"File Name\", value=column)\n",
    "        compare_output.insert(loc=5, column=\"GTvsPredictedDifference\", value=\" \")\n",
    "        for j in range(len(compare_output)):\n",
    "            if compare_output[\"Fuzzy Ratio\"][j] != 1.0:  # strict\n",
    "                # if logic - check if the entity value has numeric and update the column to No/Yes\n",
    "                for x in num_substring_entities_list:\n",
    "                    GTo = compare_output[\n",
    "                        (compare_output[\"GT Entity Type\"] == x)\n",
    "                        & (\n",
    "                            compare_output[\"GT_Output\"]\n",
    "                            != compare_output[\"Predicted_Output\"]\n",
    "                        )\n",
    "                    ][\"GT_Output\"]\n",
    "                    Labo = compare_output[\n",
    "                        (compare_output[\"GT Entity Type\"] == x)\n",
    "                        & (\n",
    "                            compare_output[\"GT_Output\"]\n",
    "                            != compare_output[\"Predicted_Output\"]\n",
    "                        )\n",
    "                    ][\"Predicted_Output\"]\n",
    "                    if str(Labo).isdigit() and Labo in GTo:\n",
    "                        compare_output.loc[\n",
    "                            (compare_output[\"GT Entity Type\"] == x),\n",
    "                            \"GTvsPredictedDifference\",\n",
    "                        ] = \"NO\"\n",
    "                if (\n",
    "                    compare_output[\"GT_Output\"][j] == \"Entity not found.\"\n",
    "                    and compare_output[\"Predicted_Output\"][j] == \"Entity not found.\"\n",
    "                ):\n",
    "                    compare_output[\"GTvsPredictedDifference\"][j] = \"NO\"\n",
    "                else:\n",
    "                    compare_output[\"GTvsPredictedDifference\"][j] = \"YES\"\n",
    "            else:\n",
    "                compare_output[\"GTvsPredictedDifference\"][j] = \"NO\"\n",
    "        for k in range(len(compare_output)):\n",
    "            if compare_output[\"Fuzzy Ratio\"][k] != 1.0:  # strict\n",
    "                change_GT_and_parsed = \"parsed json is diff from GT\"\n",
    "                break\n",
    "            else:\n",
    "                compare_output[\"GTvsPredictedDifference\"][k] = \"NO\"\n",
    "        processed_predected_documents[prcsd_file.split(\"/\")[-1]] = temp_parsed_json\n",
    "        # compare_output['bbox_mismatch'] = compare_output['GT_bbox'] != compare_output['Predicted_bbox']\n",
    "        new_row = pd.Series(\n",
    "            [\n",
    "                prcsd_file.split(\"/\")[-1],\n",
    "                \"parsed json\",\n",
    "                \"is updated\",\n",
    "                \"compared to GT\",\n",
    "                \":\",\n",
    "                np.nan,\n",
    "                change_GT_and_parsed,\n",
    "                \"\",\n",
    "            ],\n",
    "            index=compare_output.columns,\n",
    "        )\n",
    "        compare_output = compare_output.append(new_row, ignore_index=True)\n",
    "        frames = [compare_merged, compare_output]\n",
    "        compare_merged = pd.concat(frames)\n",
    "        master_df = pd.concat([master_df, compare_merged], ignore_index=True)\n",
    "\n",
    "    stats_json = get_poor_perfoming_docs(\n",
    "        master_df,\n",
    "        pctg_or_count_FLAG,\n",
    "        threshold_count,\n",
    "        threshold_pctg,\n",
    "        cf,\n",
    "        output_folder_path_name,\n",
    "        processed_predected_documents,\n",
    "    )\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m104",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m104"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
