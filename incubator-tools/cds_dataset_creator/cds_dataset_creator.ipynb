{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a231ea55-c01f-46d4-942e-64a8037b753a",
   "metadata": {},
   "source": [
    "# Creating CDS Dataset from Various GCP Folders"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de5feda9-1dca-47ab-8329-83f4a5a8480b",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e13e8d25-a697-413c-a7fc-71454b2af568",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the DocAI Incubator Team. No guarantees of performance are implied.\t"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3dc70906-4d71-424a-8f85-3aca07c055f2",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "This document highlights the solution for generating shoeboxes (PDF files with multiple documents inside - a composite document - in random order) automatically from a collection of folders (each with a document type) present in a GCP Storage bucket. Each folder should contain a dataset of a specific document type like W2, 1030, Paystub Forms etc. \n",
    "\n",
    "The python script provided in this document helps to create the set of â€œshoebox (composite PDF) documents  based on the number of documents that should be pulled from each folder for each shoebox. The flexibility is given to you to configure\n",
    "\n",
    "* the number of shoebox files to be created, and \n",
    "* the weight distribution for each type of document per shoebox. \n",
    "\n",
    "The order of the documents in the PDF are random and not repeated for every shoebox file that is created. \n",
    "\n",
    "Lastly, two final steps: \n",
    "1. the DocumentAI OCR processor type process documents, API is triggered for the files to generate Google Document JSONs, and \n",
    "2. the JSON has CDS-compliant entities injected that reflect how the PDF document was composed: which document type for what pages. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8bff14ef-d02f-4173-89e8-8af28be87536",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "* Vertex AI Notebook.\n",
    "* Document AI\n",
    "* Storage Bucket for storing input PDF files and output JSON files.\n",
    "* Permission For Google Storage and Vertex AI Notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1120efdc-5f17-4b2c-9aaa-5e3d31005c54",
   "metadata": {},
   "source": [
    "## Step by Step procedure \n",
    "\n",
    "### 1. Project Folder structure\n",
    "\n",
    "In the GCS bucket, store the different types of documents similar to shown in the below image. For example, 1040, paystub, w2 PDF forms are placed in their respective paths.\n",
    "\n",
    "**NOTE:** There should be a sufficient number of documents for the execution of the script.\n",
    "The folder naming conventions follow lower case names.\n",
    "\n",
    "<img src=\"./images/shoebox_1.png\" width=500 height=400></img>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a48f96b-33fb-47c3-9b72-d0cd0c00ce9e",
   "metadata": {},
   "source": [
    "### 2. Install the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9dd6bf-ab07-4399-8c0d-be9b06cdec13",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyPDF2\n",
    "!pip install google-cloud-storage\n",
    "!pip install google-cloud-documentai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05e7fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to download utilities module\n",
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa96dcaf-02bd-49f2-ae51-70fa433d55d8",
   "metadata": {},
   "source": [
    "### 3. Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfb9180-b5ab-42d7-be8d-1b07fa583c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, PyPDF2\n",
    "from io import BytesIO\n",
    "from pprint import pprint\n",
    "import json, copy\n",
    "from collections import OrderedDict\n",
    "from google.cloud import storage\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "import utilities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f8cf4f8-ea3b-4d71-8687-63c76ca19612",
   "metadata": {},
   "source": [
    "### 4. Setup the required inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8f60ff-c6b5-4ce2-98de-7887e2df8b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "input_number_of_shoeboxes = 5\n",
    "min_number_variation = 1\n",
    "input_min_docs = 4\n",
    "input_max_docs = 7\n",
    "weights = {\"form_w2\": 0.2, \"form_1040\": 0.5, \"form_paystub\": 0.3}\n",
    "project_id = \"<your-project-id>\"\n",
    "location = \"us\"\n",
    "processor_id = \"<your-processor-id>\"\n",
    "processor_version = \"<your-processor-id>\"\n",
    "\n",
    "bucket_name = \"<your-bucket-name>\"\n",
    "bucket_folder_prefix_name = \"<your-bucket-folder-prefix>\"\n",
    "output_folder_pdf_name = \"<path-for-output-pdf>\"\n",
    "output_folder_json_name = \"<path-for-output-json>\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "278608f4-93d4-4777-96a9-3e6d3799166e",
   "metadata": {},
   "source": [
    "## Configuration for Document Processing\n",
    "\n",
    "- `input_number_of_shoeboxes`: Number of shoeboxes to be created.\n",
    "\n",
    "- `min_number_variation`: Minimum number of different document types in each shoebox.\n",
    "\n",
    "- `input_min_docs`: Minimum number of documents in a shoebox.\n",
    "\n",
    "- `input_max_docs`: Maximum number of documents in a shoebox.\n",
    "\n",
    "- `bucket_name`: Name of the Google Cloud Storage bucket.\n",
    "  - Example: 'my-bucket-name'\n",
    "\n",
    "- `bucket_folder_prefix_name`: Folder prefix in the GCS bucket for documents.\n",
    "  - Example: '2024_tax_documents'\n",
    "\n",
    "- `weights`: Weights for each document type, dictating selection probability. The keys in `weights` should correspond to the folder names inside `bucket_folder_prefix_name`.\n",
    "  - Example: \n",
    "    - If you have folders named 'w2_docs', '1040_docs', and 'paystub_docs' inside '2024_tax_documents', the `weights` might be:\n",
    "    - {'w2_docs': 0.2, '1040_docs': 0.5, 'paystub_docs': 0.3}\n",
    "\n",
    "- `project_id`: Google Cloud project ID.\n",
    "  - Example: 'your-project-id'\n",
    "\n",
    "- `location`: Location of the DocumentAI processor (e.g., 'us', 'eu').\n",
    "  - Example: 'us'\n",
    "\n",
    "- `processor_id`: ID of the DocumentAI processor.\n",
    "  - Example: 'your-processor-id'\n",
    "\n",
    "- `processor_version`: Version of the DocumentAI processor.\n",
    "  - Example: 'your-processor-version'\n",
    "\n",
    "- `output_folder_pdf_name`: Folder in the GCS bucket for storing merged PDFs. This folder should be within the main bucket defined in `bucket_name`.\n",
    "  - Example: 'my-bucket-name/2024_tax_documents/output_pdf/'\n",
    "\n",
    "- `output_folder_json_name`: Folder in the GCS bucket for storing processed JSON data. This folder should also be within the main bucket defined in `bucket_name`.\n",
    "  - Example: 'my-bucket-name/2024_tax_documents/output_json/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2101da19-e496-4302-9e39-9c1a05b735b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_condition(tracker: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if the number of non-zero values in the tracker dictionary exceeds a predefined minimum number.\n",
    "\n",
    "    This function iterates through the values of the input dictionary. It counts the number of values that are not zero.\n",
    "    If this count exceeds the global variable `min_number_variation`, the function returns True, indicating that the\n",
    "    condition is met. Otherwise, it returns False.\n",
    "\n",
    "    Args:\n",
    "        tracker (dict): A dictionary with values that are to be checked. The values are expected to be numeric.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the count of non-zero values is greater than `min_number_variation`, otherwise False.\n",
    "    \"\"\"\n",
    "    non_zero_count = 0\n",
    "    for value in tracker.values():\n",
    "        if value != 0:\n",
    "            non_zero_count += 1\n",
    "    return non_zero_count > min_number_variation\n",
    "\n",
    "\n",
    "def make_entities(shoebox_dict: list) -> list:\n",
    "    \"\"\"\n",
    "    Constructs a list of entities based on the given shoebox dictionary. Each entity is formed with a type and\n",
    "    a corresponding list of page references.\n",
    "\n",
    "    The function iterates through each item in the shoebox dictionary. For each item, it creates an entity dictionary\n",
    "    with a 'type' key (based on the item's key) and a 'pageAnchor' key containing a list of page references. The number\n",
    "    of page references is determined by the value associated with each key in the shoebox dictionary. A global page\n",
    "    counter is used to assign unique page numbers to each page reference.\n",
    "\n",
    "    Args:\n",
    "        shoebox_dict (list): A list of dictionaries, each having a single key-value pair where the key represents the\n",
    "        type of the entity and the value represents the number of page references to be created for that entity.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each representing an entity with its type and associated page references.\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    global_page = 0\n",
    "    for i in shoebox_dict:\n",
    "        entity = {}\n",
    "        entity[\"type\"] = list(i.keys())[0]\n",
    "        temp_pageRef_list = []\n",
    "        for j in range(0, int(i[list(i.keys())[0]])):\n",
    "            temp_page = {}\n",
    "            temp_page[\"page\"] = global_page\n",
    "            global_page = global_page + 1\n",
    "            temp_pageRef_list.append(temp_page)\n",
    "        temp_pageRef = {}\n",
    "        temp_pageRef[\"pageRefs\"] = temp_pageRef_list\n",
    "        entity[\"pageAnchor\"] = temp_pageRef\n",
    "        entities.append(entity)\n",
    "\n",
    "    return entities\n",
    "\n",
    "\n",
    "def convert_blob_content_type(blob_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Changes the content type of a specified blob in a Google Cloud Storage bucket to 'application/pdf'.\n",
    "\n",
    "    This function first retrieves a reference to the specified source bucket using the global `storage_client`.\n",
    "    It then fetches the blob (file) specified by `blob_name` within this bucket. Once the blob is obtained,\n",
    "    the function updates its content type to 'application/pdf' and applies this change using the `patch` method.\n",
    "\n",
    "    Note: This function relies on a globally available `storage_client` which should be an instance of\n",
    "    `google.cloud.storage.Client`, and `source_bucket` which should be the name of the bucket as a string.\n",
    "\n",
    "    Args:\n",
    "        blob_name (str): The name of the blob (file) within the bucket whose content type needs to be updated.\n",
    "\n",
    "    Returns:\n",
    "        None: This function does not return anything. It updates the content type of the blob in-place.\n",
    "    \"\"\"\n",
    "    bucket = storage_client.get_bucket(source_bucket)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.content_type = \"application/pdf\"\n",
    "    blob.patch()\n",
    "\n",
    "\n",
    "storage_client = storage.Client()\n",
    "source_bucket = storage_client.bucket(bucket_name)  # storage bucket name\n",
    "source_blob = source_bucket.list_blobs(prefix=bucket_folder_prefix_name)\n",
    "\n",
    "all_documents_dict = {}\n",
    "\n",
    "list_of_files = []\n",
    "for blob in source_blob:\n",
    "    if blob.name.endswith(\".pdf\"):\n",
    "        list_of_files.append(blob.name)\n",
    "\n",
    "for k, v in weights.items():\n",
    "    all_documents_dict[k] = []\n",
    "\n",
    "for docType in list_of_files:\n",
    "    doc_type = \"form_\" + docType.split(\"/\")[-2]\n",
    "    if doc_type in weights.keys():  # all_documents_dict:\n",
    "        all_documents_dict[doc_type].append(docType)\n",
    "\n",
    "sb_count = 0\n",
    "while True:\n",
    "    if min_number_variation == 0:\n",
    "        break\n",
    "\n",
    "    if sum(weights.values()) > 1.0:\n",
    "        print(\n",
    "            \" Total Weights should not exceed 1.0 : you provided : \",\n",
    "            sum(weights.values()),\n",
    "        )\n",
    "        break\n",
    "\n",
    "    print(\"==== \" + str(sb_count) + \" ====\")\n",
    "\n",
    "    combine_number = random.randint(input_min_docs, input_max_docs)\n",
    "    print(\"combine_number : \", combine_number)\n",
    "\n",
    "    number_of_docs = {}\n",
    "    for doc_type, weight in weights.items():\n",
    "        number_of_docs[doc_type] = round(combine_number * weight)\n",
    "    print(number_of_docs)\n",
    "\n",
    "    tracker = {}\n",
    "    for y in number_of_docs.keys():\n",
    "        tracker[y] = len(all_documents_dict[y])\n",
    "    min_flag = min_condition(tracker)\n",
    "    if not min_flag:\n",
    "        print(\" >>>>>>> minimum condition is met\")\n",
    "        break\n",
    "\n",
    "    shoebox = []\n",
    "    for docType, doc_list in all_documents_dict.items():\n",
    "        unique_random_docs = []\n",
    "        try:\n",
    "            for x in range(number_of_docs[docType]):\n",
    "                rand = random.choice(doc_list)\n",
    "                if rand not in unique_random_docs:\n",
    "                    unique_random_docs.append(rand)\n",
    "                    all_documents_dict[docType].remove(rand)\n",
    "        except:\n",
    "            pass\n",
    "        shoebox.extend(unique_random_docs)\n",
    "    print(\"Shoebox : \", shoebox)\n",
    "\n",
    "    random.shuffle(shoebox)\n",
    "\n",
    "    blob_shoebox = []\n",
    "    for x in shoebox:\n",
    "        blob = source_bucket.blob(x)\n",
    "        blob_shoebox.append(blob)\n",
    "\n",
    "    output_pdf_file = output_folder_pdf_name + \"shoebox-\" + str(sb_count) + \".pdf\"\n",
    "    print(\"output : \", output_pdf_file)\n",
    "    blob_out = source_bucket.blob(output_pdf_file)\n",
    "\n",
    "    merged_pdf = PyPDF2.PdfMerger()\n",
    "    shoebox_dict_list = []\n",
    "    for pdf_file in blob_shoebox:\n",
    "        shoebox_dict = OrderedDict()  # ordered_dict\n",
    "        with pdf_file.open(\"rb\") as file:\n",
    "            merged_pdf.append(PyPDF2.PdfReader(file))\n",
    "            page_per_doc = len(PyPDF2.PdfReader(file).pages)\n",
    "            shoebox_dict[\n",
    "                \"form_\" + pdf_file.name.split(\"/\")[-2]\n",
    "            ] = page_per_doc  # find correct key\n",
    "            shoebox_dict_list.append(shoebox_dict)\n",
    "    try:\n",
    "        entities = make_entities(shoebox_dict_list)\n",
    "    except:\n",
    "        print(\"Exception occured... unable to make entities\")\n",
    "        break\n",
    "    merged_pdf2 = copy.deepcopy(merged_pdf)\n",
    "    with blob_out.open(\"wb\") as file:\n",
    "        merged_pdf.write(file)\n",
    "    convert_blob_content_type(output_pdf_file)\n",
    "    try:\n",
    "        with BytesIO() as f:\n",
    "            merged_pdf2.write(f)\n",
    "            f.seek(0)\n",
    "            result = utilities.process_document_sample(\n",
    "                project_id, location, processor_id, f.read(), processor_version\n",
    "            )\n",
    "        doc_dict = documentai.Document.to_dict(result.document)\n",
    "        doc_dict[\"entities\"] = entities\n",
    "    except Exception as e:\n",
    "        print(\"Exception occurred during document processing:\", e)\n",
    "        break\n",
    "    output_json_file = output_folder_json_name + \"shoebox-\" + str(sb_count) + \".json\"\n",
    "    json_string = json.dumps(doc_dict)\n",
    "    utilities.store_document_as_json(json_string, bucket_name, output_json_file)\n",
    "    sb_count += 1\n",
    "    if sb_count == input_number_of_shoeboxes:\n",
    "        break\n",
    "print(\"Dataset Created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c8a699-301f-4a20-846e-08fd484bdb8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
