{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66a897de-24dc-428a-afb9-21eede2c8545",
   "metadata": {},
   "source": [
    "# Export and Import Document schema using Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1d66f7-145a-4865-bfd2-644d8bb87c4f",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048272f7-2c36-49ce-96ef-af2ac250a7e3",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the **DocAI Incubator Team**. No guarantees of performance are implied.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e292fb-27fd-4cb6-959b-aa1c1f8add5e",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "This document guides how to extract and export a schema from a sample document to a spreadsheet (.xlsx extension) using Gemini and import a schema from a spreadsheet to a processor. This approach considers 3 level nesting as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56373ed-9505-4389-b66a-70d7252d4f34",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* Vertex AI Notebook Or Colab (If using Colab, use authentication) \n",
    "* Vertex AI API enabled for Gemini API calls\n",
    "* Processor details to import to the processor\n",
    "* Permission For Google Storage, Vertex AI and Vertex AI Notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87224b66-9c71-4246-bfad-e0c6dd580256",
   "metadata": {},
   "source": [
    "## Step by Step procedure "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbd50d3-a8ec-4082-8803-6793c92b7793",
   "metadata": {},
   "source": [
    "## Exporting Document schema to a spreadsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8909a50-01f7-452e-bc28-1b718cfa32a5",
   "metadata": {},
   "source": [
    "### 1.Importing Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd376493-75d9-4ad7-9d9d-527807d65d16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fa38e8-fa31-4eb7-a61a-3dd586f3c62b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import vertexai\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "import os\n",
    "from vertexai.generative_models import GenerativeModel, Part, SafetySetting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d317056e-bf75-4285-b26f-77da6472dc87",
   "metadata": {},
   "source": [
    "### 2.Setup the inputs\n",
    "* `project_id` : Provide GCP project Number\n",
    "* `location` : The region where the resources or services are hosted\n",
    "* `mime_type` : The format type of the file\n",
    "* `input_uri` : The URI of the input file stored in Google Cloud Storage (GCS).\n",
    "* `output_gcs_bucket` : The GCS bucket where the output will be stored.\n",
    "* `output_gcs_folder` : The folder path within the GCS bucket for storing output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64300ddd-8b3b-4b6d-908f-8ee66306c58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = \"xxxxxxxxxxxxxx\"  # Project ID of the project\n",
    "location = \"us-central1\"  # Location of Gemini\n",
    "mime_type = \"application/pdf\"  # Mime type of input document\n",
    "input_uri = (\n",
    "    \"gs://your-bucket/your-folder/your-document.pdf\"  # GCS uri of input document\n",
    ")\n",
    "output_gcs_bucket = \"destination-bucket\"\n",
    "output_gcs_folder = \"destination-folder/sub-folder\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df84544-02b0-4467-80fe-53aecd1cfcb8",
   "metadata": {},
   "source": [
    "### 3.Run the required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a765651-6b65-4dc4-a46c-7177aca64fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_gcs(\n",
    "    bucket_name: str, destination_blob_name: str, source_file_path: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Uploads a local file to a Google Cloud Storage (GCS) bucket.\n",
    "\n",
    "    Args:\n",
    "        bucket_name (str): The name of the GCS bucket where the file will be uploaded.\n",
    "        destination_blob_name (str): The name of the destination file (blob) in the GCS bucket.\n",
    "        source_file_path (str): The local path to the file that needs to be uploaded.\n",
    "\n",
    "    Returns:\n",
    "        None: This function does not return a value but prints a message upon successful upload.\n",
    "    \"\"\"\n",
    "    # Initialize a client\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Get the bucket\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # Create a new blob (file) in the bucket\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    # Upload the file to the blob\n",
    "    blob.upload_from_filename(source_file_path)\n",
    "\n",
    "    print(f\"File {source_file_path} uploaded to {destination_blob_name}\")\n",
    "\n",
    "\n",
    "def remove_local_file(file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Removes a file from local storage.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The full path to the file that needs to be deleted.\n",
    "\n",
    "    Returns:\n",
    "        None: This function does not return a value but prints a message indicating whether\n",
    "              the file was successfully deleted or if an error occurred.\n",
    "\n",
    "    Exceptions:\n",
    "        FileNotFoundError: Raised if the specified file is not found.\n",
    "        PermissionError: Raised if the file cannot be deleted due to insufficient permissions.\n",
    "        Exception: Catches and prints any other exceptions that occur during the deletion process.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.remove(file_path)\n",
    "        print(f\"File {file_path} has been removed from local storage.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} not found.\")\n",
    "    except PermissionError:\n",
    "        print(f\"Permission denied: Unable to delete {file_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while trying to delete {file_path}: {e}\")\n",
    "\n",
    "\n",
    "def export_schema(\n",
    "    document_schema: dict, output_bucket: str, output_folder: str, source_filename: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Exports a document schema to an Excel file and uploads it to a Google Cloud Storage (GCS) bucket.\n",
    "\n",
    "    Args:\n",
    "        document_schema (dict): The document schema that needs to be exported. It should be a dictionary that can be converted to a pandas DataFrame.\n",
    "        output_bucket (str): The name of the GCS bucket where the exported schema will be uploaded.\n",
    "        output_folder (str): The folder path within the GCS bucket where the schema file will be stored.\n",
    "        source_filename (str): The name of the source file used to name the exported schema file.\n",
    "\n",
    "    Returns:\n",
    "        None: This function does not return a value but performs the following tasks:\n",
    "            1. Exports the schema to an Excel file.\n",
    "            2. Uploads the Excel file to the specified GCS bucket.\n",
    "            3. Removes the local copy of the schema file after uploading.\n",
    "\n",
    "    Exceptions:\n",
    "        Exception: Catches any exception during the upload process and prints an error message.\n",
    "    \"\"\"\n",
    "    schema_filename = f\"{source_filename}_schema_exported.xlsx\"\n",
    "\n",
    "    df = pd.DataFrame(document_schema)\n",
    "    df.to_excel(schema_filename, index=False)\n",
    "\n",
    "    try:\n",
    "        upload_to_gcs(\n",
    "            output_bucket,\n",
    "            f\"{output_folder.rstrip('/')}/{schema_filename}\",\n",
    "            schema_filename,\n",
    "        )\n",
    "    except e:\n",
    "        print(\"Error occured while uploading schema to bucket\")\n",
    "        print(e)\n",
    "\n",
    "    remove_local_file(schema_filename)\n",
    "\n",
    "\n",
    "def generate(project_id: str, location: str, mime_type: str, input_uri: str) -> dict:\n",
    "    \"\"\"\n",
    "    Generates a schema from a document using a generative AI model and returns the schema in JSON format.\n",
    "\n",
    "    Args:\n",
    "        project_id (str): The Google Cloud project ID where the AI model is deployed.\n",
    "        location (str): The region where the model is hosted.\n",
    "        mime_type (str): The MIME type of the input document (e.g., 'application/pdf').\n",
    "        input_uri (str): The URI of the input document that is used to generate the schema.\n",
    "\n",
    "    Returns:\n",
    "        dict: The generated schema in JSON format.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If an error occurs during schema generation or JSON parsing.\n",
    "    \"\"\"\n",
    "\n",
    "    schema = \"\"\n",
    "\n",
    "    try:\n",
    "        # Initialize Vertex AI project and location\n",
    "        vertexai.init(project=project_id, location=location)\n",
    "\n",
    "        # Load the generative model\n",
    "        model = GenerativeModel(\"gemini-1.5-pro-001\")\n",
    "\n",
    "        print(f\"Generating schema using the document {input_uri}...\")\n",
    "\n",
    "        # Generate schema content\n",
    "        responses = model.generate_content(\n",
    "            [text1, document1],\n",
    "            generation_config=generation_config,\n",
    "            safety_settings=safety_settings,\n",
    "            stream=True,\n",
    "        )\n",
    "\n",
    "        # Concatenate generated text\n",
    "        for response in responses:\n",
    "            schema += response.text\n",
    "\n",
    "        # Convert generated text to JSON\n",
    "        schema_json = json.loads(schema)\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing generated schema into JSON: {e}\")\n",
    "        raise\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while generating the schema: {e}\")\n",
    "        raise\n",
    "\n",
    "    return schema_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb5eaf4-672d-41e2-a903-63a3b256aa60",
   "metadata": {},
   "source": [
    "### 4.Run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52242db-5122-4a88-96c0-44f33dc4ddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    text1 = \"\"\"Please analyze the structure of the attached document and provide me with a schema definition as an object array. Return object array in a single line.\n",
    "\n",
    "    The schema should include:\n",
    "    *Fields and their data types (e.g., text, number, date)\n",
    "    *Any relationships between fields (e.g., nested objects, arrays)\n",
    "    *Each object in the object array will have key names: name, value_type, occurrence_type and display_name\n",
    "    *All key names should be less than 64 characters and should be snake cased. If there are multiple words in key name, replace space with underscore and join the words.\n",
    "    *The \\\\\\\"name\\\\\\\" key is the title of the field. This should be a semantically named field. \n",
    "    *The \\\\\\\"value_type\\\\\\\" describe the type of field and is either string, number, currency, money, datetime, address or checkbox. If the field is a parent entity, then the value_type should be equal to the \\\\\\\"name\\\\\\\"\n",
    "    *The \\\\\\\"occurrence_type\\\\\\\" describes the number of times the field is expected and can either be REQUIRED_ONCE, REQUIRED_MULTIPLE, OPTIONAL_ONCE or OPTIONAL_MULTIPLE. In most of the cases, it will either be OPTIONAL_ONCE or OPTIONAL_MULTIPLE. When the field is an identifier or you feel is a mandatory field, use REQUIRED_ONCE or REQUIRED_MULTIPLE.\n",
    "    *The \\\\\\\"display_name\\\\\\\" is blank unless the entity is a child entity. In that case, the display_name should be equal to the parent entity\\\\\\'s name.\n",
    "\n",
    "    Make sure the schema you provide is in accordance with the Document AI schema format. Retain the language of label names as in original documents\"\"\"\n",
    "\n",
    "    document1 = Part.from_uri(\n",
    "        mime_type=mime_type,\n",
    "        uri=input_uri,\n",
    "    )\n",
    "\n",
    "    generation_config = {\n",
    "        \"max_output_tokens\": 8192,\n",
    "        \"temperature\": 1,\n",
    "        \"top_p\": 0.95,\n",
    "    }\n",
    "\n",
    "    safety_settings = [\n",
    "        SafetySetting(\n",
    "            category=SafetySetting.HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
    "            threshold=SafetySetting.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "        ),\n",
    "        SafetySetting(\n",
    "            category=SafetySetting.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
    "            threshold=SafetySetting.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "        ),\n",
    "        SafetySetting(\n",
    "            category=SafetySetting.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
    "            threshold=SafetySetting.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "        ),\n",
    "        SafetySetting(\n",
    "            category=SafetySetting.HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
    "            threshold=SafetySetting.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # Generating the schema\n",
    "    schema_json = generate(project_id, location, mime_type, input_uri)\n",
    "\n",
    "    # Exporting the Schema to the GCS Destination Bucket Folder\n",
    "    export_schema(\n",
    "        schema_json,\n",
    "        output_gcs_bucket,\n",
    "        output_gcs_folder,\n",
    "        input_uri.split(\"/\")[-1].split(\".\")[0],\n",
    "    )\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f35804f-db9a-473f-8709-744f312fc186",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.Output\n",
    "\n",
    "The output will be the schema saved in \"<input_document_name>_schema_exported.xlsx\" file as shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbe5b1e-c8e2-40fa-b46b-e14b9045408a",
   "metadata": {},
   "source": [
    "#### Exported Schema in the SpreadSheet\n",
    "<img src=\"./Images/Exported_Schema.png\" width=800 height=400 ></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9813ac99-58ef-4e18-975e-21c4672ce55b",
   "metadata": {},
   "source": [
    "## Importing Document schema from a spreadsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e320d2-8b66-439c-869a-be6e8e21dd67",
   "metadata": {},
   "source": [
    "### 1.Importing Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49da68d-9f89-4f72-ba18-fca662b002cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from google.cloud import documentai_v1beta3\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e08d66-5f61-4a99-94d1-426f72357d9d",
   "metadata": {},
   "source": [
    "### 2.Setup the inputs\n",
    "* `project_id` : Provide GCP project Number\n",
    "* `new_location` : The region where the new resources or services are hosted.\n",
    "* `new_processor_id` : The unique identifier of the newly created processor.\n",
    "* `schema_bucket_name` : The name of the Google Cloud Storage bucket where the schema file is stored.\n",
    "* `schema_file_path` : The file path within the GCS bucket for the schema file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dad54e-4889-4d0b-8423-5a13400d3a65",
   "metadata": {},
   "source": [
    "#### Note : Importing the schema can be done in new processor or else processor with empty schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c0a4ed-4834-4e01-a26a-5f7434d1f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = \"xxxxxxxxxxxxxxxx\"  # Project ID of the project\n",
    "new_location = \"us\"  # location of the processor\n",
    "new_processor_id = (\n",
    "    \"xxxxxxxxxxxxx\"  # Processor id of processor to which the schema has to be imported\n",
    ")\n",
    "schema_bucket_name = \"your-bucket\"  # Bucket name where exported schema file is stored\n",
    "schema_file_path = \"your-folder/<document_name>_schema_exported.xlsx\"  # exported schema file path in GCS bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd9e96c-6538-4b67-9427-c37e1d06e06a",
   "metadata": {},
   "source": [
    "### 3.Run the required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3d78bb-f5af-4c5b-aef5-2ea56a698ed2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_from_gcs(\n",
    "    bucket_name: str, source_blob_name: str, destination_file_path: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Downloads a file from a Google Cloud Storage (GCS) bucket to local storage.\n",
    "\n",
    "    Args:\n",
    "        bucket_name (str): The name of the GCS bucket.\n",
    "        source_blob_name (str): The name of the file (blob) in the GCS bucket to be downloaded.\n",
    "        destination_file_path (str): The local file path where the downloaded file will be saved.\n",
    "\n",
    "    Returns:\n",
    "        None: This function does not return any value. It downloads the file from GCS to local storage.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize a client\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Get the bucket\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # Get the blob (file) in the bucket\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "\n",
    "    # Download the file from the bucket to the local storage\n",
    "    blob.download_to_filename(destination_file_path)\n",
    "\n",
    "    print(f\"File {source_blob_name} downloaded to {destination_file_path}.\")\n",
    "\n",
    "\n",
    "def get_dataset_schema(processor_name: str) -> documentai_v1beta3.DatasetSchema:\n",
    "    \"\"\"\n",
    "    Retrieves the dataset schema for a specific processor in Document AI.\n",
    "\n",
    "    Args:\n",
    "        processor_name (str): The full resource name of the processor, typically in the format:\n",
    "                              'projects/{project_id}/locations/{location}/processors/{processor_id}'.\n",
    "\n",
    "    Returns:\n",
    "        documentai_v1beta3.DatasetSchema: The schema of the dataset associated with the specified processor.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a client\n",
    "    client = documentai_v1beta3.DocumentServiceClient()\n",
    "\n",
    "    # Initialize request argument(s)\n",
    "    request = documentai_v1beta3.GetDatasetSchemaRequest(\n",
    "        name=processor_name + \"/dataset/datasetSchema\",\n",
    "    )\n",
    "\n",
    "    # Make the request\n",
    "    response = client.get_dataset_schema(request=request)\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def update_dataset_schema(\n",
    "    schema: documentai_v1beta3.DatasetSchema,\n",
    ") -> documentai_v1beta3.DatasetSchema:\n",
    "    \"\"\"\n",
    "    Updates the dataset schema for a specified processor in Document AI.\n",
    "\n",
    "    Args:\n",
    "        schema (documentai_v1beta3.DatasetSchema): The dataset schema object that includes the\n",
    "                                                   updated schema details (name and document schema).\n",
    "\n",
    "    Returns:\n",
    "        documentai_v1beta3.DatasetSchema: The updated dataset schema object returned by the API.\n",
    "    \"\"\"\n",
    "\n",
    "    from google.cloud import documentai_v1beta3\n",
    "\n",
    "    # Create a client\n",
    "    client = documentai_v1beta3.DocumentServiceClient()\n",
    "\n",
    "    # Initialize request argument(s)\n",
    "    request = documentai_v1beta3.UpdateDatasetSchemaRequest(\n",
    "        dataset_schema={\"name\": schema.name, \"document_schema\": schema.document_schema}\n",
    "    )\n",
    "\n",
    "    # Make the request\n",
    "    response = client.update_dataset_schema(request=request)\n",
    "\n",
    "    # Handle the response\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b871b920-a4ab-42b3-ac70-27277e24fcae",
   "metadata": {},
   "source": [
    "### 4.Run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6240b31-abe3-486b-ac4f-3da96d8bafcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Download the schema excel file from GCS\n",
    "    local_excel_path = schema_file_path.split(\"/\")[-1]\n",
    "    download_from_gcs(schema_bucket_name, schema_file_path, local_excel_path)\n",
    "\n",
    "    # time.sleep(10)\n",
    "\n",
    "    # Import the Excel file back into a data frame\n",
    "    imported_df = pd.read_excel(local_excel_path, engine=\"openpyxl\")\n",
    "\n",
    "    # Convert the data frame back to a list of dictionaries\n",
    "    imported_data = imported_df.to_dict(orient=\"records\")\n",
    "\n",
    "    parent_entities = []\n",
    "    nested_entities = {}\n",
    "    for data in imported_data:\n",
    "        temp_data = {key: value for key, value in data.items() if key != \"display_name\"}\n",
    "        if isinstance(data[\"display_name\"], float) and math.isnan(data[\"display_name\"]):\n",
    "            parent_entities.append(temp_data)\n",
    "        else:\n",
    "            if data[\"display_name\"] in nested_entities.keys():\n",
    "                nested_entities[data[\"display_name\"]].append(temp_data)\n",
    "            else:\n",
    "                nested_entities[data[\"display_name\"]] = [temp_data]\n",
    "\n",
    "    schema_line = []\n",
    "\n",
    "    for line, properties in nested_entities.items():\n",
    "        client = documentai_v1beta3.types.DocumentSchema.EntityType()\n",
    "        client.name = line\n",
    "        client.base_types = [\"object\"]\n",
    "        client.properties = properties\n",
    "        client.display_name = line\n",
    "        schema_line.append(client)\n",
    "\n",
    "    new_processor_name = (\n",
    "        f\"projects/{project_id}/locations/{new_location}/processors/{new_processor_id}\"\n",
    "    )\n",
    "\n",
    "    response_newprocessor = get_dataset_schema(new_processor_name)\n",
    "    # updating into the processor\n",
    "    for i in response_newprocessor.document_schema.entity_types:\n",
    "        for e3 in parent_entities:\n",
    "            i.properties.append(e3)\n",
    "\n",
    "    for e4 in schema_line:\n",
    "        response_newprocessor.document_schema.entity_types.append(e4)\n",
    "\n",
    "    response_update = update_dataset_schema(response_newprocessor)\n",
    "    print(\"Schema Imported Successfully...\")\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f92afd-9b58-4fbb-bad2-cb197479e21d",
   "metadata": {},
   "source": [
    "### 5.Output\n",
    "\n",
    "The schema will be saved in new processor version id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f19cc9c-e858-4d85-abca-50558c983252",
   "metadata": {},
   "source": [
    "#### Exported Schema in the DocAI Processor UI\n",
    "\n",
    "<img src=\"./Images/DocAI_Processor_Schema.png\" width=800 height=400 ></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff40f1c4-17fc-4080-99fc-579c2e87a58a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
