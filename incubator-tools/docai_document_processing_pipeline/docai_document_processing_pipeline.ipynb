{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9ad8788-654c-4e38-9d6d-42158d95c6ee",
   "metadata": {},
   "source": [
    "# DocAI Document Processing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08400e10-6639-43a6-8e2a-0b4f28ba08e2",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc2d117-9e2a-4ebe-8aef-94ad4f2b46b1",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the **DocAI COS Team**. No guarantees of performance are implied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f1d1c0-823d-46d4-9a4a-e5ffa56c6b55",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "This tool helps to:\n",
    "- Automate Document AI processing based on the number of pages in PDF documents. This includes both online and batch processing.\n",
    "- Implement a queue for concurrent batch processing using a first-in, first-out (FIFO) approach.\n",
    "- Monitor batches for their completion and trigger new batch processing as needed.\n",
    "- Handle quota hit issues in online processing by automatically switching to batch processing for affected documents.\n",
    "- Manage failed documents separately, ensuring they are processed successfully or flagged for manual review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe00a75-567e-4f33-b4b4-aa72f9f4fe50",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* GCP Project ID with billing setup\n",
    "* DocumentAI Processor ID\n",
    "* Cloud Storage(GCS)\n",
    "* Cloud Firestore\n",
    "* Cloud Functions\n",
    "* Cloud Pub/Sub\n",
    "* Service Account with following permissions:\n",
    "    * Cloud Datastore User\n",
    "    * Cloud Run Invoker\n",
    "    * Document AI API User\n",
    "    * Pub/Sub Editor\n",
    "    * Storage Object User\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a33b88-01fe-420c-908f-b64675d0bef7",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3bcdb8-412f-434a-9f0d-b4c0b6d6eb78",
   "metadata": {
    "tags": []
   },
   "source": [
    "* This tool provides a DocAI processing pipeline designed for both synchronous and asynchronous (batch) processing of documents. \n",
    "* The pipeline begins with a setup.sh script that initializes necessary GCP resources, including Firestore indexes and a Pub/Sub topic. \n",
    "* Documents are loaded into a queue_collection via an HTTP trigger. Documents with 15 or fewer pages are processed synchronously. Larger documents, or those exceeding quota limitations for synchronous processing, are marked for batch processing. \n",
    "* A batch process is triggered manually or automatically upon completion of an asynchronous task by publishing a message to the document-processing-trigger topic.\n",
    "* The process_batch cloud function then processes documents in batches (FIFO), updating document statuses within the queue. \n",
    "* Both sync and batch processes handle failures by copying failed documents to a separate folder for retry, and both update the queue_collection to reflect processing status. \n",
    "* This system combines flexibility for smaller documents with efficient batch processing for larger workloads, ensuring robust and scalable document processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ef8359-7786-4f3e-9e72-868f91cf3602",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Flowchart\n",
    "\n",
    "<img src=\"images/docai-processing-pipeline-flowchart.png\" alt=\"docai-processing-pipeline-flowchart.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b33b7f0-baf8-421c-b476-2d25b1ee9b02",
   "metadata": {},
   "source": [
    "### Step by Step Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff643e9c-eabd-4933-bbca-184a81984644",
   "metadata": {},
   "source": [
    "### 1. Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6ac1de-4335-4991-9b9b-e599a21208f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install json requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadfe192-5691-4053-8670-32ef3e5bea8c",
   "metadata": {},
   "source": [
    "### 2. Create required variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4e5ed8-2ef3-48db-8419-00ec2eb8d2de",
   "metadata": {},
   "source": [
    "##### Configuration Variables for Document AI Processing Pipeline\n",
    "\n",
    "* <span style=\"color: green\">PROJECT_ID: </span>\n",
    "       The Google Cloud Project ID where resources will be deployed.\n",
    "* <span style=\"color: green\">SERVICE_ACCOUNT: </span>\n",
    "       The service account email with necessary IAM permissions. Must have permissions for Cloud Functions, Document AI, GCS, and Pub/Sub.\n",
    "* <span style=\"color: green\">FIRESTORE_COLLECTION: </span>\n",
    "       Name of the Firestore collection used for queue management. This collection will store document processing status and metadata.\n",
    "* <span style=\"color: green\">PUBSUB_TOPIC_PROCESS: </span>\n",
    "       Name of the Pub/Sub topic for triggering batch processing. Will be created if it doesn't exist.\n",
    "* <span style=\"color: green\">REGION: </span>\n",
    "       The GCP region where Cloud Functions will be deployed. Must be a valid GCP region with Cloud Functions support.\n",
    "* <span style=\"color: green\">PROCESSOR_LOCATION: </span>\n",
    "       Location of the Document AI processor. Valid values are \"us\" or \"eu\".\n",
    "* <span style=\"color: green\">PROCESSOR_ID: </span>\n",
    "       The ID of your Document AI processor. Can be found in the Document AI console.\n",
    "* <span style=\"color: green\">MAX_CONCURRENT_BATCHES: </span>\n",
    "       Maximum number of batch processes that can run simultaneously. Controls pipeline throughput and resource usage.\n",
    "* <span style=\"color: green\">INPUT_MIME_TYPE: </span>\n",
    "       MIME type of the input documents. Must match the document type supported by your Document AI processor.\n",
    "* <span style=\"color: green\">GCS_OUTPUT_BUCKET: </span>\n",
    "       Name of the GCS bucket for processed outputs. Do not include \"gs://\" prefix.\n",
    "* <span style=\"color: green\">GCS_OUTPUT_PREFIX: </span>\n",
    "       Folder path prefix within the output bucket. Do not include bucket name or \"gs://\" prefix.\n",
    "* <span style=\"color: green\">GCS_FAILED_FILES_BUCKET: </span>\n",
    "       Name of the GCS bucket for failed documents. Do not include \"gs://\" prefix.\n",
    "* <span style=\"color: green\">GCS_FAILED_FILES_PREFIX: </span>\n",
    "       Folder path prefix for failed files. Do not include bucket name or \"gs://\" prefix.\n",
    "\n",
    "### Note:\n",
    "* All bucket names should be globally unique\n",
    "* Service account must have appropriate IAM roles assigned\n",
    "* All GCS paths should end with a forward slash (/)\n",
    "* Review and update all values before deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d1fe6a0-92b3-4c25-a6ee-11090eff39c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TO-DO: Change the variables values before run the next cells\n",
    "PROJECT_ID = \"your-project-id\"\n",
    "SERVICE_ACCOUNT = \"your-service-account\"\n",
    "FIRESTORE_COLLECTION = \"queue_collection\"\n",
    "PUBSUB_TOPIC_PROCESS = \"document-processing-trigger\"\n",
    "REGION = \"us-central1\"\n",
    "PROCESSOR_LOCATION = \"us\"\n",
    "PROCESSOR_ID = \"your-processor-id\"\n",
    "MAX_CONCURRENT_BATCHES = 5\n",
    "INPUT_MIME_TYPE = \"application/pdf\"\n",
    "GCS_OUTPUT_BUCKET = \"output-bucket-name\"\n",
    "GCS_OUTPUT_PREFIX = \"some/folder/\"\n",
    "GCS_FAILED_FILES_BUCKET = \"bucket-name\"\n",
    "GCS_FAILED_FILES_PREFIX = \"some/folder/failed/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eec50ed-c789-4b9f-bedd-6cc6b8a3e0dc",
   "metadata": {},
   "source": [
    "### 3. Create indexes for Firestore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b95a0f-fecc-40f2-9e19-db255a0c2b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud firestore indexes composite create --file \"src/firestore.indexes.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff94bed-43a5-4dfc-983d-1b5c48b5c3e5",
   "metadata": {},
   "source": [
    "### 4. Create Pub/Sub topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0a6687-6ee0-470a-ae54-4dd8e4b63a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud pubsub topics create {PUBSUB_TOPIC_PROCESS} --project={PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b519dc2f-4d97-4719-be2e-ca6fa56ef7d3",
   "metadata": {},
   "source": [
    "### 5. Deploy Cloud Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2636688-0b7d-413c-9e7f-3afe82a2ba61",
   "metadata": {},
   "source": [
    "### Deploy load_queue cloud function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5503ee33-ea2f-45ba-a60c-ac48e4fad0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud functions deploy load_queue \\\n",
    "    --gen2 \\\n",
    "    --runtime python311 \\\n",
    "    --trigger-http \\\n",
    "    --allow-unauthenticated \\\n",
    "    --region {REGION} \\\n",
    "    --source load_queue_cf \\\n",
    "    --set-env-vars PROJECT_ID={PROJECT_ID},PROCESSOR_LOCATION={PROCESSOR_LOCATION},PROCESSOR_ID={PROCESSOR_ID},FIRESTORE_COLLECTION={FIRESTORE_COLLECTION},PUBSUB_TOPIC_PROCESS={PUBSUB_TOPIC_PROCESS},INPUT_MIME_TYPE={INPUT_MIME_TYPE},GCS_OUTPUT_BUCKET={GCS_OUTPUT_BUCKET},GCS_OUTPUT_PREFIX={GCS_OUTPUT_PREFIX},GCS_FAILED_FILES_BUCKET={GCS_FAILED_FILES_BUCKET},GCS_FAILED_FILES_PREFIX={GCS_FAILED_FILES_PREFIX} \\\n",
    "    --service-account {SERVICE_ACCOUNT} \\\n",
    "    --run-service-account {SERVICE_ACCOUNT} \\\n",
    "    --entry-point src/load_queue \\\n",
    "    --memory 512MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9f68ba-94fa-4748-a865-e5df457edffc",
   "metadata": {},
   "source": [
    "### Deploy process_batch cloud function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c9a0f8-0fd0-4955-b3df-6f7fd5ddec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud functions deploy process_batch \\\n",
    "    --gen2 \\\n",
    "    --runtime python311 \\\n",
    "    --trigger-topic {PUBSUB_TOPIC_PROCESS} \\\n",
    "    --entry-point process_batch_documents \\\n",
    "    --set-env-vars MAX_CONCURRENT_BATCHES={MAX_CONCURRENT_BATCHES},PROJECT_ID={PROJECT_ID},PROCESSOR_LOCATION={PROCESSOR_LOCATION},PROCESSOR_ID={PROCESSOR_ID},FIRESTORE_COLLECTION={FIRESTORE_COLLECTION},PUBSUB_TOPIC_PROCESS={PUBSUB_TOPIC_PROCESS},INPUT_MIME_TYPE={INPUT_MIME_TYPE},GCS_OUTPUT_BUCKET={GCS_OUTPUT_BUCKET},GCS_OUTPUT_PREFIX={GCS_OUTPUT_PREFIX},GCS_FAILED_FILES_BUCKET={GCS_FAILED_FILES_BUCKET},GCS_FAILED_FILES_PREFIX={GCS_FAILED_FILES_PREFIX} \\\n",
    "    --region {REGION} \\\n",
    "    --source src/process_batch_cf \\\n",
    "    --service-account {SERVICE_ACCOUNT} \\\n",
    "    --run-service-account {SERVICE_ACCOUNT} \\\n",
    "    --memory 512MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe7ca1a-6c57-4561-b0b2-99b7af663d87",
   "metadata": {},
   "source": [
    "### 6. Trigger Document processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00974992-839d-478c-b857-25e77ac71c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: Currently code considers that batch is triggered for 1 file at a time for each file in the provided file/folder paths\n",
    "\n",
    "# TO-DO: Update the \"file_paths\" in the below payload to inlcude the files/folders to be processed. It is a list of GCS files/folders to process. Starts with gs://\n",
    "\n",
    "# First get the auth token\n",
    "auth_token = !gcloud auth print-identity-token\n",
    "token = auth_token[0]  # Get the actual token string\n",
    "\n",
    "# Construct the URL\n",
    "url = f\"https://{REGION}-{PROJECT_ID}.cloudfunctions.net/load_queue_test\"\n",
    "\n",
    "# Define the payload\n",
    "payload = {\n",
    "    \"file_paths\": [\n",
    "        \"gs://example-test-bucket/folder/subfolder/\",\n",
    "        \"gs://example-bucket/folder/test-file.pdf\",\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaf183a-51c3-45cf-81fd-371f28dd3d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Python's requests library is more reliable than curl in Jupyter\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "headers = {\"Authorization\": f\"bearer {token}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=payload, timeout=70)\n",
    "\n",
    "print(f\"Status Code: {response.status_code}\")\n",
    "print(\"Response:\")\n",
    "print(\n",
    "    json.dumps(response.json(), indent=2)\n",
    "    if response.status_code == 200\n",
    "    else response.text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2716d5fb-f7f8-410d-a5ef-a7e77d227c55",
   "metadata": {},
   "source": [
    "### 7. Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bf9e1a-cf47-44b4-9a33-32d2a7f43081",
   "metadata": {},
   "source": [
    "The processed document JSONs will be stored in <span style=\"color: green\">GCS_OUTPUT_PREFIX</span> inside <span style=\"color: green\">GCS_OUTPUT_BUCKET</span> bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6443fd3f-c9a7-4b67-bf70-d66b203eef2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
