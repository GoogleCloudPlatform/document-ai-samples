{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Reorder Document Page Based On Unique Strings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the **DocAI Incubator Team**. No guarantees of performance are implied."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tool is to reorder the pages of the provided pdf-pages based on the unique strings provided as list of strings(i.e, Logical Identifiers) that appear in a logical order. Based on the provided logical-order of strings pages are shuffled in output file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisite\n",
    "\n",
    "* Vertex AI Notebook\n",
    "* Documents in GCS Folder\n",
    "* Output folder to upload fixed documents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step By Step Procedure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Modules/Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install PyPDF2\n",
    "%pip install google-cloud-storage\n",
    "%pip install google-cloud-documentai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to download utilities module\n",
    "# !wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from google.cloud import storage\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "\n",
    "import utilities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Input Details"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **PROJECT_ID**: Provide GCP Project id\n",
    "* **LOCATION**: Provide processor location `us` or `eu`\n",
    "* **PROCESSOR_ID**: Provide DocumentAI processor id\n",
    "* **PROCESSOR_VERSION_ID**: Provide DocumentAI processor version id\n",
    "* **GCS_INPUT_FOLDER**: Provide GCS folder name of the input PDF's which needs to be processed.\n",
    "* **GCS_OUTPUT_FOLDER**: Provide GCS folder name to store processed results as PDF's.\n",
    "    * **NOTE**: Both input and output bucket are different, If both buckets same then this tool will overwrite input sample with output data.\n",
    "* **UNIQUE_STRINGS**: Provide list of unique strings in logical-order, whose order is used to sort the pdf-pages. Output pages order is based on the index-order of this data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"xx-xx-xx\"\n",
    "LOCATION = \"us\"\n",
    "PROCESSOR_ID = \"xx-xx-xx\"\n",
    "PROCESSOR_VERSION_ID = \"pretrained-ocr-v2.0-2023-06-02\"\n",
    "GCS_INPUT_FOLDER = \"gs://bucket_1/reorder_document_page_based_on_unique_strings/input\"\n",
    "GCS_OUTPUT_FOLDER = \"gs://bucket_2\"\n",
    "# Define the order of unique strings\n",
    "UNIQUE_STRINGS = [\n",
    "    \"Your Contact Information\",\n",
    "    \"if you have one\",\n",
    "    \"fill in their information here\",\n",
    "    \"person doesn't want coverage\",\n",
    "    \"person is filing a joint return\",\n",
    "    \"How many babies are expected\",\n",
    "    \"If hourly, average number\",\n",
    "    \"income will this person get\",\n",
    "    \"Veterans Administration\",\n",
    "    \"Deduction type\",\n",
    "    \"receive any information about their\",\n",
    "    \"as amended by the Health Care\",\n",
    "    \"information from these outside sources\",\n",
    "    \"you are signing as the Authorized\",\n",
    "    \"may also check your information\",\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run the Below Code-Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layout_to_text(layout: documentai.Document.Page.Layout, text: str) -> str:\n",
    "    \"\"\"Document AI identifies text in different parts of the document by their\n",
    "    offsets in the entirety of the document\"s text. This function converts\n",
    "    offsets to a string.\n",
    "\n",
    "    Args:\n",
    "        layout (documentai.Document.Page.Layout): It is Layout proto of DocAI Document Object\n",
    "        text (str): It is a text-data detected by DocAI Processor (i.e, Document.text)\n",
    "\n",
    "    Returns:\n",
    "        str: It returns text-data based on offset-indexes of Layout Proto\n",
    "    \"\"\"\n",
    "\n",
    "    # If a text segment spans several lines, it will\n",
    "    # be stored in different text segments.\n",
    "    return \"\".join(\n",
    "        text[segment.start_index : segment.end_index]\n",
    "        for segment in layout.text_anchor.text_segments\n",
    "    )\n",
    "\n",
    "\n",
    "def store_blob(pdf_bytes: bytes, bucket_name: str, file_name: str) -> None:\n",
    "    \"\"\"Store PDF in GCS\n",
    "\n",
    "    Args:\n",
    "        pdf_bytes (bytes): Binary Format of pdf data\n",
    "        bucket_name (str): GCS bucket name\n",
    "        file_name (str): filename to store in specified GCS bucket\n",
    "    \"\"\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    process_result_bucket = storage_client.get_bucket(bucket_name)\n",
    "    document_blob = storage.Blob(\n",
    "        name=str(Path(file_name)), bucket=process_result_bucket\n",
    "    )\n",
    "    document_blob.upload_from_string(pdf_bytes, content_type=\"application/pdf\")\n",
    "\n",
    "\n",
    "def sort_pdf(\n",
    "    pdf_bytes: bytes,\n",
    "    output_pdf_path: str,\n",
    "    unique_strings: List[str],\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    processor_id: str,\n",
    "    processor_version: str,\n",
    ") -> None:\n",
    "    \"\"\"It will shuffle the pdf-pages based on provided list of strings i.e,`unique_strings`\n",
    "\n",
    "    Args:\n",
    "        pdf_bytes (bytes): Binary Format of pdf data\n",
    "        output_pdf_path (str): Output filename to store in specified GCS bucket\n",
    "        unique_strings (List[str]): List of unique identifier strings\n",
    "        project_id (str): GCP Project id\n",
    "        location (str): Processor Location `us` or `eu`\n",
    "        processor_id (str): DocumentAI Processor Id\n",
    "        processor_version (str): Processor version id\n",
    "    \"\"\"\n",
    "\n",
    "    # Call the Document AI service to process the PDF\n",
    "    document = utilities.process_document_sample(\n",
    "        project_id, location, processor_id, pdf_bytes, processor_version\n",
    "    )\n",
    "    document = document.document\n",
    "    # Dictionary to hold page number and its position based on unique strings\n",
    "    page_order = {}\n",
    "\n",
    "    # Loop through each page in the document\n",
    "    for page in document.pages:\n",
    "        # Extract text from each page using Document OCR\n",
    "        text = \"\"\n",
    "        for paragraph in page.paragraphs:\n",
    "            text += layout_to_text(paragraph.layout, document.text)\n",
    "\n",
    "        # For each unique string, determine if it's in the content\n",
    "        for order, unique_string in enumerate(unique_strings):\n",
    "            if unique_string in text:\n",
    "                page_order[page.page_number - 1] = order\n",
    "                break\n",
    "\n",
    "    # Sort the pages by the order determined\n",
    "    sorted_pages = sorted(page_order, key=page_order.get)\n",
    "\n",
    "    # Initialize PDF reader and writer\n",
    "    reader = PdfReader(io.BytesIO(pdf_bytes))\n",
    "    writer = PdfWriter()\n",
    "\n",
    "    # Add pages to the writer in the sorted order\n",
    "    for page_num in sorted_pages:\n",
    "        writer.add_page(reader.pages[page_num])\n",
    "    pdf_buffer = io.BytesIO()\n",
    "    writer.write(pdf_buffer)\n",
    "    output_pdf_bytes = pdf_buffer.getvalue()\n",
    "    store_blob(output_pdf_bytes, output_storage_bucket_name, output_pdf_path)\n",
    "\n",
    "\n",
    "input_storage_bucket_name = GCS_INPUT_FOLDER.split(\"/\")[2]\n",
    "input_bucket_path_prefix = \"/\".join(GCS_INPUT_FOLDER.split(\"/\")[3:])\n",
    "output_storage_bucket_name = GCS_OUTPUT_FOLDER.split(\"/\")[2]\n",
    "output_bucket_path_prefix = \"/\".join(GCS_OUTPUT_FOLDER.split(\"/\")[3:])\n",
    "\n",
    "storage_client = storage.Client()\n",
    "source_bucket = storage_client.bucket(input_storage_bucket_name)\n",
    "_, file_name_dict = utilities.file_names(GCS_INPUT_FOLDER)\n",
    "\n",
    "for filename, filepath in file_name_dict.items():\n",
    "    print(f\"Process Started for {filename}\")\n",
    "    blob = source_bucket.blob(filepath)\n",
    "    pdf_bytes = blob.download_as_bytes()\n",
    "    sort_pdf(\n",
    "        pdf_bytes,\n",
    "        filepath,\n",
    "        UNIQUE_STRINGS,\n",
    "        PROJECT_ID,\n",
    "        LOCATION,\n",
    "        PROCESSOR_ID,\n",
    "        PROCESSOR_VERSION_ID,\n",
    "    )\n",
    "print(\"Process Completed Successfully for all files\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Output Details"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running script, pages were shuffled in pdf based on the logical-order of provided `UNIQUE_STRINGS`\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td> Pre-processing</td>\n",
    "<td> Post-processing</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./images/pre_processing_sample.png\" width=400 height=800></td>\n",
    "<td><img src=\"./images/post_processing_sample.png\" width=400 height=800></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m104",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m104"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
