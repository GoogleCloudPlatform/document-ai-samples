{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55646894-baf1-4c1c-9790-5fc16468a282",
   "metadata": {},
   "source": [
    "# Identity Document Proofing Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1e5f0d-91ea-4766-8ab3-f66429e66e1b",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528c5a36-a860-468f-a30a-08d051880aee",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the DocAI Incubator Team. No guarantees of performance are implied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c195555a-cb6d-4896-af89-3e97b3312cc1",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "The purpose of this document is to provide instructions and a Python script for evaluating identity document proofing . The script should parse the document with **Identity Document Proofing processor**, and fetch all the entities storing it in a csv file along with the percentage of Total Fraudulent and Non Fraudulent documents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5734e7-8944-485a-9412-ae063ef57318",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* Python : Jupyter notebook (Vertex AI) \n",
    "* Service account permissions in projects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce627c16-65b6-4870-b0b7-28d5fe599905",
   "metadata": {},
   "source": [
    "## Step by Step procedure \n",
    "\n",
    "### 1.Importing Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1964d8-b1e4-42b7-a399-40861581c371",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1928e8-b562-443d-b17d-88f42cebda45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import utilities\n",
    "\n",
    "from google.cloud import storage\n",
    "from io import BytesIO\n",
    "from google.cloud import documentai_v1beta3 as documentai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdcdd55-89b2-4b67-838e-d82fc5c2d957",
   "metadata": {},
   "source": [
    "### 2.Setup the required inputs\n",
    "* `project_id` : Your Google project id or name\n",
    "* `processor_id` : Processor id with can be found on processor detail tab in gcp UI.\n",
    "* `input_dir` - The path of the folder containing the image files to be processed, with the bucket name ending with slash(/).              \n",
    "(Eg : gs://bucket_name/folder_name/)\n",
    "* `processor_output_dir` -The path of the output folder of the processor with the bucket name and without ending with slash(/).              \n",
    "(Eg : gs://bucket_name/folder_name)\n",
    "* `location_processor` - Your Processor location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d717d6b4-4bdd-4d04-b9e4-da57f4dd008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = \"xxxxxxxxxxxxx\"\n",
    "processor_id = \"xxxxxxxxxxx\"\n",
    "input_dir = \"gs://xxxxxxx/xxxxxxx/xxxxxxx/\"\n",
    "processor_output_dir = \"gs://xxxxxxxxx/xxxxxxxx\"\n",
    "location_processor = \"us\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d320b8-c2c8-46b9-8c10-33c5147347a6",
   "metadata": {},
   "source": [
    "### 3.Execute the First part of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e49ea6-f84c-47e5-a562-1632f8e6e2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = utilities.batch_process_documents_sample(\n",
    "    project_id=project_id,\n",
    "    location=location_processor,\n",
    "    processor_id=processor_id,\n",
    "    gcs_input_uri=input_dir,\n",
    "    gcs_output_uri=processor_output_dir,\n",
    "    timeout=700,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ed47fe-cea8-4577-9b22-526d6f7642b4",
   "metadata": {},
   "source": [
    "#### This First part of code should generate json files which get stored in a random folder name  generated inside the **processor_output_dir**.\n",
    "\n",
    "#### Provide the same random folder name in `parser_output_folder_name` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7349143-6b5b-48e9-9845-078d9aa365af",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser_output_folder_name = \"xxxxxxxxxxxxxx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5df0d3b-69da-4982-9b76-386e9781aeb6",
   "metadata": {},
   "source": [
    "<img src=\"./Images/parser_output_filename.png\" width=800 height=400></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c53305e-368e-4005-9146-772ddd77292b",
   "metadata": {},
   "source": [
    "### 4.Execute the Second part of the code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62d0a79-9250-4405-98ab-589e3be0767a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(parser_output_folder_name, processor_output_dir):\n",
    "    processor_output_dir = processor_output_dir.replace(\"gs://\", \"\")\n",
    "    processor_output_dir = (\n",
    "        \"gs://\" + processor_output_dir + \"/\" + parser_output_folder_name + \"/\"\n",
    "    )\n",
    "    file_names_list, file_dict = utilities.file_names(processor_output_dir)\n",
    "    bucket_name = processor_output_dir.split(\"/\")[2]\n",
    "    df = pd.DataFrame(columns=[\"FileName\", \"FraudulentDocument\", \"SignalsDetected\"])\n",
    "    for key, value in file_dict.items():\n",
    "        content = utilities.documentai_json_proto_downloader(bucket_name, value)\n",
    "        file_name = key.replace(\"-0.json\", \"\")\n",
    "        fraud_list = []\n",
    "        for entity in content.entities:\n",
    "            if not entity.mention_text == \"PASS\":\n",
    "                str_ = f\"{entity.type_} : {entity.mention_text}\"\n",
    "                fraud_list.append(str_)\n",
    "\n",
    "        if fraud_list:\n",
    "            row = {\n",
    "                \"FileName\": file_name,\n",
    "                \"FraudulentDocument\": \"Y\",\n",
    "                \"SignalsDetected\": \", \".join(fraud_list),\n",
    "            }\n",
    "            df = df.append(row, ignore_index=True)\n",
    "        else:\n",
    "            row = {\n",
    "                \"FileName\": file_name,\n",
    "                \"FraudulentDocument\": \"N\",\n",
    "                \"SignalsDetected\": \"\",\n",
    "            }\n",
    "            df = df.append(row, ignore_index=True)\n",
    "    fraudulent_document_count = df[\"FraudulentDocument\"].value_counts()\n",
    "\n",
    "    if \"Y\" not in fraudulent_document_count.keys():\n",
    "        total_Fraudulent_documents_count = 0\n",
    "    else:\n",
    "        total_Fraudulent_documents_count = fraudulent_document_count[\"Y\"]\n",
    "    if \"N\" not in fraudulent_document_count.keys():\n",
    "        total_NonFradulent_documents_count = 0\n",
    "    else:\n",
    "        total_NonFradulent_documents_count = fraudulent_document_count[\"N\"]\n",
    "\n",
    "    total_document = (\n",
    "        total_Fraudulent_documents_count + total_NonFradulent_documents_count\n",
    "    )\n",
    "\n",
    "    total_Fraudulent_documents = round(\n",
    "        (total_Fraudulent_documents_count / total_document) * 100, 2\n",
    "    )\n",
    "    total_NonFradulent_documents = round(\n",
    "        (total_NonFradulent_documents_count / total_document * 100), 2\n",
    "    )\n",
    "    print(\"Total Fraudulent documents:\", total_Fraudulent_documents, \"%\")\n",
    "    print(\"Total NonFradulent documents:\", total_NonFradulent_documents, \"%\")\n",
    "\n",
    "    df.to_csv(\"output.csv\", index=False)\n",
    "\n",
    "\n",
    "main(parser_output_folder_name, processor_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955f1175-2fd4-490d-bd4c-1f7d5531eab5",
   "metadata": {},
   "source": [
    "### 5.Output\n",
    "The script after execution creates a CSV file containing a list of file names with all the fraud detected on the document . The script also generates the percentage of Total Fraudulent and Non Fraudulent documents at the end.\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
