{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0396799c-a7b9-4572-813e-f1940e332b80",
   "metadata": {},
   "source": [
    "# CS Decision Matrix Automation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05e4b88-d62c-41bb-9094-d448099bb2de",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d0d35f-1ca8-4b53-b508-2900e9e6fe58",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the **DocAI Incubator Team**. No guarantees of performance are implied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf8bebe-720c-4fb9-8449-ecdcf2b1c3de",
   "metadata": {},
   "source": [
    "# Objective\n",
    "The document will guide to get the CS decision matrix by comparing Ground truth files and predicted jsons which are taken as input. ECE(Expected Confidence Error) is also calculated in the tool that helps to measure the accuracy of confidence score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8782c76f-ca1f-4341-83e3-8d9fba967461",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "* Vertex AI Notebook\n",
    "* Ground truth and Predicted jsons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e554aae7-245f-4342-83fc-a7d7ead70049",
   "metadata": {},
   "source": [
    "# Step-by-Step Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c857c3-230b-4c8d-80ee-46cc44f8493f",
   "metadata": {},
   "source": [
    "## 1. Import Modules/Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c6d9e8-aa31-4613-9e3e-3ea52cbce4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to install required packages\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install google-cloud-documentai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca65467-4ed8-4521-945d-57ed5411c487",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this cell to download utilities module\n",
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64c5868-9809-4518-8d36-1b56eae9897c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import difflib\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from pandas import DataFrame\n",
    "\n",
    "from utilities import (\n",
    "    documentai_json_proto_downloader,\n",
    "    file_names,\n",
    "    find_match,\n",
    "    get_match_ratio,\n",
    "    json_to_dataframe,\n",
    "    remove_row,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c967e2-1b39-4335-906a-5135a613b503",
   "metadata": {},
   "source": [
    "## 2. Input Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096ad870-e76b-4190-8934-9b95f88c9865",
   "metadata": {},
   "source": [
    "* **INPUT_GCS_PATH** : It is input GCS folder path which contains DocumentAI processor JSON results\n",
    "* **OUTPUT_GCS_PATH** : It is a GCS folder path to store post-processing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e16b97-fce1-4593-8683-d23f3cd0092c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " # GCS folder where the ground truth files are saved\n",
    "GROUND_TRUTH_URI =\"gs://BUCKET/input_path_cs_decision_matrix_automation/GT/\"\"\n",
    "# GCS folder where the prediction files are saved\n",
    "PREDICTED_URI = \"gs://BUCKET/output_path_cs_decision_matrix_automation/PT/\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b448dc-2572-417e-b685-afe8db2bcfe0",
   "metadata": {},
   "source": [
    "## 3. Run Below Code-Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92155d09-86dc-4a08-bc19-791dd91a32c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "# Disable the specific PerformanceWarning\n",
    "# default='warn'\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "\n",
    "def modify_ground_truth_and_predictions(\n",
    "    df_file1: pd.DataFrame, df_file2: pd.DataFrame\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    It is helper function for remaining entities which are matched comparing\n",
    "    the area of IOU across them\n",
    "\n",
    "    Args:\n",
    "        df_file1 (pd.DataFrame): Dataframe containing ground truth data\n",
    "        df_file2 (pd.DataFrame): Dataframe containing predicted details\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame]: Modified dataframes\n",
    "    \"\"\"\n",
    "\n",
    "    mention_text2 = pd.Series(dtype=str)\n",
    "    bbox2 = pd.Series(dtype=object)\n",
    "    bbox1 = pd.Series(dtype=object)\n",
    "    page_1 = pd.Series(dtype=object)\n",
    "    page_2 = pd.Series(dtype=object)\n",
    "    confidence2 = pd.Series(dtype=object)\n",
    "    for index, row in enumerate(df_file1.values):\n",
    "        matched_index = find_match(row, df_file2)\n",
    "        if matched_index is not None:\n",
    "            mention_text2.loc[index] = df_file2.loc[matched_index][1]\n",
    "            confidence2.loc[index] = df_file2.loc[matched_index][4]\n",
    "            bbox2.loc[index] = df_file2.loc[matched_index][2]\n",
    "            bbox1.loc[index] = row[2]\n",
    "            page_2.loc[index] = df_file2.loc[matched_index][3]\n",
    "            page_1.loc[index] = row[3]\n",
    "            df_file2 = df_file2.drop(matched_index)\n",
    "        else:\n",
    "            mention_text2.loc[index] = \"Entity not found.\"\n",
    "            confidence2.loc[index] = \"Not found\"\n",
    "            bbox2.loc[index] = \"Entity not found.\"\n",
    "            bbox1.loc[index] = row[2]\n",
    "            page_1.loc[index] = row[3]\n",
    "            page_2.loc[index] = \"no\"\n",
    "\n",
    "    df_file1[\"mention_text2\"] = mention_text2.values\n",
    "    df_file1[\"bbox2\"] = bbox2.values\n",
    "    df_file1[\"bbox1\"] = bbox1.values\n",
    "    df_file1[\"page_1\"] = page_1.values\n",
    "    df_file1[\"page_2\"] = page_2.values\n",
    "    df_file1[\"confidence2\"] = confidence2.values\n",
    "    df_file1 = df_file1.drop([\"bbox\"], axis=1)\n",
    "    df_file1 = df_file1.drop([\"page\"], axis=1)\n",
    "    df_file1.rename(\n",
    "        columns={\n",
    "            \"type_\": \"Entity Type\",\n",
    "            \"mention_text\": \"GT_Output\",\n",
    "            \"mention_text2\": \"prediction_Output\",\n",
    "            \"bbox1\": \"pre_bbox\",\n",
    "            \"bbox2\": \"post_bbox\",\n",
    "            \"page_1\": \"page1\",\n",
    "            \"page_2\": \"page2\",\n",
    "            \"confidence\": \"confidence1\",\n",
    "            \"confidence2\": \"confidence2\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "    return df_file1, df_file2\n",
    "\n",
    "\n",
    "def compare_gt_and_prediction_output(\n",
    "    file1: documentai.Document, file2: documentai.Document\n",
    ") -> Tuple[DataFrame, list]:\n",
    "    \"\"\"\n",
    "    Compares the entities between two files and returns the results in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        file1 (documentai.Document): DocumentAI Object  the first file Ground Truth.\n",
    "        file2 (documentai.Document): DocumentAI Object from the second file Prediction.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[DataFrame, list]:\n",
    "            A tuple where the first element is a DataFrame based on the\n",
    "            comparison,and the second element is a list of dicts.\n",
    "    \"\"\"\n",
    "\n",
    "    df_file1 = json_to_dataframe(file1)\n",
    "    df_file2 = json_to_dataframe(file2)\n",
    "\n",
    "    file1_entities = [entity[0] for entity in df_file1.values]\n",
    "    file2_entities = [entity[0] for entity in df_file2.values]\n",
    "    # find entities which are present only once in both files\n",
    "    # these entities will be matched directly\n",
    "    common_entities = set(file1_entities).intersection(set(file2_entities))\n",
    "    exclude_entities = []\n",
    "    for entity in common_entities:\n",
    "        if file1_entities.count(entity) > 1 or file2_entities.count(entity) > 1:\n",
    "            exclude_entities.append(entity)\n",
    "    for entity in exclude_entities:\n",
    "        common_entities.remove(entity)\n",
    "    df_compare = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"Entity Type\",\n",
    "            \"GT_Output\",\n",
    "            \"prediction_Output\",\n",
    "            \"pre_bbox\",\n",
    "            \"post_bbox\",\n",
    "            \"page1\",\n",
    "            \"page2\",\n",
    "            \"confidence1\",\n",
    "            \"confidence2\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for entity in common_entities:\n",
    "        confidences = (\n",
    "            df_file1[df_file1[\"type_\"] == entity].iloc[0][\"confidence\"],\n",
    "            df_file2[df_file2[\"type_\"] == entity].iloc[0][\"confidence\"],\n",
    "        )\n",
    "        _values = (\n",
    "            df_file1[df_file1[\"type_\"] == entity].iloc[0][\"mention_text\"],\n",
    "            df_file2[df_file2[\"type_\"] == entity].iloc[0][\"mention_text\"],\n",
    "        )\n",
    "        bboxes = (\n",
    "            df_file1[df_file1[\"type_\"] == entity].iloc[0][\"bbox\"],\n",
    "            df_file2[df_file2[\"type_\"] == entity].iloc[0][\"bbox\"],\n",
    "        )\n",
    "        pages = (\n",
    "            df_file1[df_file1[\"type_\"] == entity].iloc[0][\"page\"],\n",
    "            df_file2[df_file2[\"type_\"] == entity].iloc[0][\"page\"],\n",
    "        )\n",
    "\n",
    "        df_compare.loc[len(df_compare.index)] = [\n",
    "            entity,\n",
    "            _values[0],\n",
    "            _values[1],\n",
    "            bboxes[0],\n",
    "            bboxes[1],\n",
    "            pages[0],\n",
    "            pages[1],\n",
    "            confidences[0],\n",
    "            confidences[1],\n",
    "        ]\n",
    "        # common entities are removed from df_file1 and df_file2\n",
    "        df_file1 = remove_row(df_file1, entity)\n",
    "        df_file2 = remove_row(df_file2, entity)\n",
    "\n",
    "    df_file1, df_file2 = modify_ground_truth_and_predictions(df_file1, df_file2)\n",
    "    # df_compare = df_compare._append(df_file1, ignore_index=True)\n",
    "    df_compare = pd.concat([df_compare, df_file1], ignore_index=True)\n",
    "    # adding entities which are present in file2 but not in file1\n",
    "    for entity in df_file2.values:\n",
    "        df_compare.loc[len(df_compare.index)] = [\n",
    "            entity[0],\n",
    "            \"Entity not found.\",\n",
    "            entity[1],\n",
    "            \"[]\",\n",
    "            entity[2],\n",
    "            \"[]\",\n",
    "            entity[3],\n",
    "            \"\",\n",
    "            entity[4],\n",
    "        ]\n",
    "\n",
    "    match_array_entity_data = get_match_array_ent_data(df_compare)\n",
    "    df_compare[\"Match\"] = match_array_entity_data[0]\n",
    "\n",
    "    df_compare[\"Fuzzy Ratio\"] = df_compare.apply(get_match_ratio, axis=1)\n",
    "\n",
    "    return df_compare, match_array_entity_data[1]\n",
    "\n",
    "\n",
    "def get_match_array_ent_data(\n",
    "    df_compare: pd.DataFrame,\n",
    ") -> Tuple[List[str], List[Dict[str, Dict[str, str]]]]:\n",
    "    \"\"\"\n",
    "    It is helper function to return matched array and prediction confidence as dict\n",
    "\n",
    "    Args:\n",
    "        df_compare (pd.DataFrame): Dataframe which hold both ground truth and prediction details\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], List[Dict[str, Dict[str, str]]]]:\n",
    "            Returns match strings and dict containing prediction scores\n",
    "    \"\"\"\n",
    "    match_array = []\n",
    "    entity_data = []\n",
    "    for df_i in range(0, len(df_compare)):\n",
    "        temp_dict = {}\n",
    "        match_string = \"\"\n",
    "        not_found_string = \"Entity not found.\"\n",
    "        pre_output = df_compare.iloc[df_i][\"GT_Output\"]\n",
    "        post_output = df_compare.iloc[df_i][\"prediction_Output\"]\n",
    "\n",
    "        if pre_output == not_found_string and post_output == not_found_string:\n",
    "            match_string = \"TN\"\n",
    "        elif pre_output != not_found_string and post_output == not_found_string:\n",
    "            match_string = \"FN\"\n",
    "            temp_dict[df_compare.iloc[df_i][\"Entity Type\"]] = {\n",
    "                \"Match\": \"N\",\n",
    "                \"Confidence_pred\": \"0\",\n",
    "            }\n",
    "            entity_data.append(temp_dict)\n",
    "        elif pre_output == not_found_string and post_output != not_found_string:\n",
    "            match_string = \"FP\"\n",
    "            temp_dict[df_compare.iloc[df_i][\"Entity Type\"]] = {\n",
    "                \"Match\": \"N\",\n",
    "                \"Confidence_pred\": df_compare.iloc[df_i][\"confidence2\"],\n",
    "            }\n",
    "            entity_data.append(temp_dict)\n",
    "        elif pre_output != not_found_string and not post_output is not_found_string:\n",
    "            if pre_output == post_output:\n",
    "                match_string = \"TP\"\n",
    "                temp_dict[df_compare.iloc[df_i][\"Entity Type\"]] = {\n",
    "                    \"Match\": \"Y\",\n",
    "                    \"Confidence_pred\": df_compare.iloc[df_i][\"confidence2\"],\n",
    "                }\n",
    "                entity_data.append(temp_dict)\n",
    "            else:\n",
    "                match_string = \"TP\"\n",
    "                temp_dict[df_compare.iloc[df_i][\"Entity Type\"]] = {\n",
    "                    \"Match\": \"N\",\n",
    "                    \"Confidence_pred\": df_compare.iloc[df_i][\"confidence2\"],\n",
    "                }\n",
    "                entity_data.append(temp_dict)\n",
    "        else:\n",
    "            match_string = \"Something went Wrong.\"\n",
    "\n",
    "        match_array.append(match_string)\n",
    "    return match_array, entity_data\n",
    "\n",
    "\n",
    "# Function to assign values based on ranges\n",
    "def assign_bin(value: float) -> Union[float, None]:\n",
    "    \"\"\"\n",
    "    Assign a bin value based on a given input value.\n",
    "\n",
    "    This function assigns a bin value to a given input value based on predefined ranges.\n",
    "\n",
    "    Args:\n",
    "        value (float): The input value.\n",
    "\n",
    "    Returns:\n",
    "        float or None:\n",
    "        The assigned bin value if the input value falls within the defined ranges, else None.\n",
    "    \"\"\"\n",
    "    ranges = [\n",
    "        (0.0, 0.1),\n",
    "        (0.1, 0.2),\n",
    "        (0.2, 0.3),\n",
    "        (0.3, 0.4),\n",
    "        (0.4, 0.5),\n",
    "        (0.5, 0.6),\n",
    "        (0.6, 0.7),\n",
    "        (0.7, 0.8),\n",
    "        (0.8, 0.9),\n",
    "        (0.9, 1.0),\n",
    "    ]\n",
    "\n",
    "    for start, end in ranges:\n",
    "        if start <= value < end:\n",
    "            return start\n",
    "    if value == 1.0:\n",
    "        return 1.0\n",
    "\n",
    "    # Add more conditions for other ranges if needed\n",
    "    return None  # Handle values outside specified ranges\n",
    "\n",
    "\n",
    "def get_cs_mean(\n",
    "    dataframe_cs: pd.DataFrame, cs_name: str\n",
    ") -> Dict[str, Union[float, None]]:\n",
    "    \"\"\"It will returm mean score for provided cs_name(field) in dataframe\n",
    "\n",
    "    Args:\n",
    "        dataframe_cs (pd.DataFrame): A dataframe cotainig target columns in it.\n",
    "        cs_name (str): Column name\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Union[float, None]]: Mean score for provided column bason on bins\n",
    "    \"\"\"\n",
    "\n",
    "    cs_list = list(dataframe_cs[cs_name].values)\n",
    "    cs_list = [\n",
    "        float(value)\n",
    "        if isinstance(value, (int, float)) or value.replace(\".\", \"\", 1).isdigit()\n",
    "        else value\n",
    "        for value in cs_list\n",
    "    ]\n",
    "\n",
    "    # Remove NaN values from the list\n",
    "    cs_list_without_nan = [value for value in cs_list if pd.notna(value)]\n",
    "\n",
    "    # Define bin edges\n",
    "    bins = [i / 10.0 for i in range(11)]\n",
    "\n",
    "    cs_mean = {}\n",
    "    for bin_i in range(len(bins) - 1):\n",
    "        bin_range = f\"{bin_i/10:.1f}\"\n",
    "        values_in_bin = [\n",
    "            value\n",
    "            for value in cs_list_without_nan\n",
    "            if bins[bin_i] <= value < bins[bin_i + 1]\n",
    "        ]\n",
    "        mean_value = round(np.mean(values_in_bin), 3) if values_in_bin else np.nan\n",
    "        cs_mean[bin_range] = mean_value\n",
    "    return cs_mean\n",
    "\n",
    "\n",
    "# creating crosstab Dataframes function\n",
    "def get_cs_analysis(\n",
    "    entity_name: str, dataframe_cs: pd.DataFrame\n",
    ") -> Tuple[pd.DataFrame, float]:\n",
    "    \"\"\"\n",
    "    Perform analysis on confidence score (CS) data for a specific entity.\n",
    "\n",
    "    This function performs analysis on confidence score (CS) data for a specific entity.\n",
    "    It calculates various metrics including accuracy, cumulative error, weighted average,\n",
    "    etc., and returns a DataFrame with the analysis results.\n",
    "\n",
    "    Args:\n",
    "        entity_name (str): The name of the entity for which CS analysis is performed.\n",
    "        dataframe_cs (DataFrame): The DataFrame containing CS data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame, float: A DataFrame containing the CS analysis results and\n",
    "        the expected confidence error (ECE) as a float.\n",
    "    \"\"\"\n",
    "\n",
    "    # creating a pivot table\n",
    "    cs_bin_name = entity_name + \"_CS_bin\"\n",
    "    match_name = entity_name + \"_Match\"\n",
    "    cs_name = entity_name + \"_CS\"\n",
    "    cross_tab = pd.crosstab(\n",
    "        index=dataframe_cs[cs_bin_name], columns=dataframe_cs[match_name]\n",
    "    )\n",
    "    cross_tab_sorted = cross_tab.sort_values(by=cs_bin_name, ascending=False)\n",
    "\n",
    "    # Add grand total for 'Y' and 'N' in columns\n",
    "    # cross_tab_sorted.loc['Grand Total', :] = cross_tab_sorted.sum()\n",
    "    if \"N\" not in cross_tab_sorted.columns:\n",
    "        # Add column 'D' with NaN values\n",
    "        cross_tab_sorted[\"N\"] = 0\n",
    "    elif \"Y\" not in cross_tab_sorted.columns:\n",
    "        # Add column 'D' with NaN values\n",
    "        cross_tab_sorted[\"Y\"] = 0\n",
    "    # Add grand total for 'Y' and 'N' in rows\n",
    "    cross_tab_sorted.loc[:, \"Grand Total\"] = cross_tab_sorted.sum(axis=1)\n",
    "    cross_tab_sorted[\"F2CS Bins\"] = cross_tab_sorted.index\n",
    "\n",
    "    cross_tab_sorted[\"% Err Cum\"] = (\n",
    "        (cross_tab_sorted[\"N\"].cumsum() / cross_tab_sorted[\"Grand Total\"].sum()) * 100\n",
    "    ).round()\n",
    "    cross_tab_sorted[\"% Bypass\"] = (\n",
    "        (\n",
    "            cross_tab_sorted[\"Grand Total\"].cumsum()\n",
    "            / cross_tab_sorted[\"Grand Total\"].sum()\n",
    "        )\n",
    "        * 100\n",
    "    ).round(1)\n",
    "    cross_tab_sorted[\"Cum Acc\"] = 100 - cross_tab_sorted[\"% Err Cum\"]\n",
    "\n",
    "    cs_mean = get_cs_mean(dataframe_cs, cs_name)\n",
    "    cross_tab_sorted[\"AVERAGE of CS\"] = (\n",
    "        cross_tab_sorted[\"F2CS Bins\"].astype(str).map(cs_mean)\n",
    "    )\n",
    "    cross_tab_sorted[\"Accuracy in Bin\"] = (\n",
    "        cross_tab_sorted[\"Y\"] / cross_tab_sorted[\"Grand Total\"]\n",
    "    ).round(3)\n",
    "    cross_tab_sorted[\"Diff Acc-AvgCS\"] = abs(\n",
    "        cross_tab_sorted[\"Accuracy in Bin\"] - cross_tab_sorted[\"AVERAGE of CS\"]\n",
    "    )\n",
    "    cross_tab_sorted[\"Weighted Avg\"] = (\n",
    "        cross_tab_sorted[\"Diff Acc-AvgCS\"] * cross_tab_sorted[\"Grand Total\"]\n",
    "    ).round(3)\n",
    "    ece = (\n",
    "        (cross_tab_sorted[\"Weighted Avg\"].sum() / cross_tab_sorted[\"Grand Total\"].sum())\n",
    "        * 100\n",
    "    ).round(2)\n",
    "\n",
    "    return cross_tab_sorted, ece\n",
    "\n",
    "\n",
    "gt_files_list, gt_path_dict = file_names(GROUND_TRUTH_URI)\n",
    "predicted_files_list, predicted_path_dict = file_names(PREDICTED_URI)\n",
    "matched_files_dict = {}\n",
    "non_matched_files_dict = {}\n",
    "for i in gt_files_list:\n",
    "    for j in predicted_files_list:\n",
    "        matched_score = difflib.SequenceMatcher(None, i, j).ratio()\n",
    "\n",
    "        if matched_score >= 0.8:\n",
    "            matched_files_dict[i] = j\n",
    "        else:\n",
    "            non_matched_files_dict[i] = \"No parsed output available\"\n",
    "\n",
    "for i in matched_files_dict:\n",
    "    if i in non_matched_files_dict:\n",
    "        del non_matched_files_dict[i]\n",
    "\n",
    "file_wise_data = {}\n",
    "total_file_wise_data = {}\n",
    "merge_dataframe = pd.DataFrame()\n",
    "for gt_file, pred_file in matched_files_dict.items():\n",
    "    print(gt_file)\n",
    "    try:\n",
    "        gt_json = documentai_json_proto_downloader(\n",
    "            GROUND_TRUTH_URI.split(\"/\")[2], gt_path_dict[gt_file]\n",
    "        )\n",
    "        pred_json = documentai_json_proto_downloader(\n",
    "            PREDICTED_URI.split(\"/\")[2], predicted_path_dict[pred_file]\n",
    "        )\n",
    "\n",
    "        temp_compare, data_entity = compare_gt_and_prediction_output(gt_json, pred_json)\n",
    "        temp_compare.insert(0, \"filename\", gt_file)\n",
    "        merge_dataframe = pd.concat([merge_dataframe, temp_compare], ignore_index=True)\n",
    "        file_wise_data[str(gt_file)] = data_entity\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "transformed_data = []\n",
    "\n",
    "for filename, values in file_wise_data.items():\n",
    "    for entry in values:\n",
    "        key_name, sub_data = entry.popitem()\n",
    "\n",
    "        # Check if sub_data is a string and convert it to a dictionary\n",
    "        if isinstance(sub_data, str):\n",
    "            sub_data = {\"Match\": sub_data, \"Confidence_pred\": \"0\"}\n",
    "\n",
    "        row_data = {\"filename\": filename, \"Entity_name\": key_name}\n",
    "        row_data.update(sub_data)\n",
    "        transformed_data.append(row_data)\n",
    "\n",
    "# Create a DataFrame from the transformed data\n",
    "DF_DATA = pd.DataFrame(transformed_data)\n",
    "DF_DATA = DF_DATA.reset_index(drop=True)\n",
    "\n",
    "\n",
    "df_cs = pd.DataFrame(columns=[\"File_name\"])\n",
    "for item in transformed_data:\n",
    "    col_match = item[\"Entity_name\"] + \"_\" + \"Match\"\n",
    "    col_cs = item[\"Entity_name\"] + \"_\" + \"CS\"\n",
    "    new_row_data = {\n",
    "        \"File_name\": item[\"filename\"],\n",
    "        col_match: item[\"Match\"],\n",
    "        col_cs: item[\"Confidence_pred\"],\n",
    "    }\n",
    "    new_row = pd.DataFrame([new_row_data])\n",
    "    df_cs = pd.concat([df_cs, new_row], ignore_index=True)\n",
    "\n",
    "\n",
    "# Find columns containing \"_CS\" and apply the conditions\n",
    "cs_columns = [col for col in df_cs.columns if \"_CS\" in col]\n",
    "for col in cs_columns:\n",
    "    df_cs[col + \"_bin\"] = df_cs[col].apply(lambda x: assign_bin(float(x)))\n",
    "\n",
    "unique_columns = {\n",
    "    col.replace(\"_CS\", \"\")\n",
    "    .replace(\"_CS_bin\", \"\")\n",
    "    .replace(\"_Match\", \"\")\n",
    "    .replace(\"_bin\", \"\")\n",
    "    for col in df_cs.columns\n",
    "}\n",
    "considered_col = []\n",
    "cs_analysis = []\n",
    "cs_analysis_dict = {}\n",
    "\n",
    "for unique_col in unique_columns:\n",
    "    try:\n",
    "        x, y = get_cs_analysis(unique_col, df_cs)\n",
    "        temporary_dict = {unique_col: x, \"ece\": y}\n",
    "        cs_analysis.append(temporary_dict)\n",
    "        cs_analysis_dict[unique_col] = x\n",
    "        considered_col.append(unique_col)\n",
    "    except KeyError as e:\n",
    "        print(e)\n",
    "\n",
    "# exporting to excel\n",
    "# Create a Pandas Excel writer using the `with` statement\n",
    "EXCEL_FILE = \"output.xlsx\"\n",
    "with pd.ExcelWriter(EXCEL_FILE, engine=\"xlsxwriter\") as writer:\n",
    "    # Get the xlsxwriter workbook and worksheet objects\n",
    "    workbook = writer.book\n",
    "\n",
    "    worksheet = workbook.add_worksheet(name=\"CS_analysis_sheet\")\n",
    "\n",
    "    # Set the format for bold text\n",
    "    bold_format = workbook.add_format({\"bold\": True})\n",
    "    # Set the format with 3 empty lines between each DataFrame\n",
    "    format_with_gap = workbook.add_format({\"text_wrap\": True, \"valign\": \"top\"})\n",
    "    worksheet.set_row(0, None, format_with_gap)\n",
    "\n",
    "    # Add heading at the top of the sheet\n",
    "    worksheet.write(0, 0, \"CS ANALYSIS SHEET\", bold_format)\n",
    "\n",
    "    # Write each key-value pair to the Excel file with a gap of three empty lines\n",
    "    ROW = 3  # Starting row after the heading\n",
    "    for cs_data in cs_analysis:\n",
    "        for label, df in cs_data.items():\n",
    "            if label != \"ECE\":\n",
    "                # Write the label (text) to the sheet\n",
    "                worksheet.write(ROW, 0, label, bold_format)\n",
    "\n",
    "                # Add three empty lines\n",
    "                ROW += 1  # Skip three lines after writing the label\n",
    "\n",
    "                # Write the DataFrame to the sheet, starting from the next row\n",
    "                df.to_excel(\n",
    "                    writer, sheet_name=\"CS_analysis_sheet\", startrow=ROW + 1, index=True\n",
    "                )\n",
    "\n",
    "                # Calculate the new starting row for the next label\n",
    "                ROW += len(df) + 2  # Add three lines for the gap\n",
    "\n",
    "                # Add a new row with merged columns and the specified text (BOLD)\n",
    "                merged_col_format = workbook.add_format(\n",
    "                    {\n",
    "                        \"text_wrap\": True,\n",
    "                        \"align\": \"center\",\n",
    "                        \"valign\": \"vcenter\",\n",
    "                        \"bold\": True,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # Move to the next row\n",
    "            elif label == \"ECE\":\n",
    "                worksheet.write(ROW, 13, label, bold_format)\n",
    "                worksheet.write(ROW, 14, df, bold_format)\n",
    "                ROW += 2\n",
    "\n",
    "    DF_DATA.to_excel(\n",
    "        writer, sheet_name=\"Match_cs_data\", startrow=0, startcol=0, index=True\n",
    "    )\n",
    "\n",
    "    # merge_dataframe=merge_dataframe.drop(['pre_bbox','post_bbox', 'page1', 'page2'],axis=1)\n",
    "    merge_dataframe.to_excel(\n",
    "        writer, sheet_name=\"Full_comparision_data\", startrow=0, startcol=0, index=True\n",
    "    )\n",
    "\n",
    "    df_cs = df_cs[df_cs.columns[0:1].tolist() + sorted(df_cs.columns[1:].tolist())]\n",
    "    df_cs.to_excel(\n",
    "        writer, sheet_name=\"Entity_wise_data\", startrow=0, startcol=0, index=True\n",
    "    )\n",
    "\n",
    "print(f'Excel file \"{EXCEL_FILE}\" created successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8df1da-2ef2-424c-902e-e4cebc0515c6",
   "metadata": {},
   "source": [
    "# 4. Output Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db365cd5-6aa3-4645-8c36-3e7f25a230e7",
   "metadata": {},
   "source": [
    "The excel sheet `output.xlsx`  will be saved which contains entity wise confidence score based error analysis.\n",
    "Column names and description\n",
    "\n",
    "**line_item/quantity_CS_bin**: Bin created for confidence scores ( for confidence scores between 0.1-0.2 will be in bin 0.1 â€¦etc)  \n",
    "**N**: Number of entities are mismatching respective to the confidence score bins.  \n",
    "**Y**: Number of entities are matching respective to the confidence score bins.  \n",
    "**Grand Total**: Total number of entities in respective confidence score bins.  \n",
    "**%Err Cumm**: Cummamlative Error percentage respective to confidence score bins.  \n",
    "**% Bypass**: % of documents bypassed respective to confidence score bins.  \n",
    "**Cum Acc**: Cumulative accuracy respective to confidence score bins.  \n",
    "**AVERAGE of CS**:  Average of confidence scores in the bins.  \n",
    "**Accuracy in Bin**: Accuracy respective to confidence score bins.  \n",
    "**Diff Acc-AvgCS**: Accuracy in bin and Average confidence score difference.  \n",
    "**Weighted Avg**: Weighted average (Diff Acc-AvgCS X Grand Total) respective to confidence score bins.  \n",
    "\n",
    "**ECE(Expected Confidence Error)**: Sum of Weighted Avg /Sum of Grand Total.  \n",
    "ECE  helps us to determine the accuracy of the confidence score. The lower ECE is preferred for the better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83949d45-cac6-48c2-a067-0b67c0d36910",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src='./images/output_sample.png' width=1000 height=600 alt=\"Sample Output\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5d43a8-2a60-4b42-b79f-8b990d8a108b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
