{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9ad8788-654c-4e38-9d6d-42158d95c6ee",
   "metadata": {},
   "source": [
    "# OCR Upgrade Tool Using Enterprise OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08400e10-6639-43a6-8e2a-0b4f28ba08e2",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc2d117-9e2a-4ebe-8aef-94ad4f2b46b1",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the **DocAI Incubator Team**. No guarantees of performance are implied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f1d1c0-823d-46d4-9a4a-e5ffa56c6b55",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Objective\n",
    "\n",
    "This document is intended as a guide to help with the migration of datasets between older processors versions (before  Sept. 29th, 2024) and new (as of Sept. 29th, 2024) created processors.  \n",
    "To convert the OCR in a labeled dataset from an older version to an updated version in 3 steps: \n",
    "1. Exporting datasets from a processor\n",
    "2. Reprocessing exported OCR-labeled JSON data with an updated OCR engine which is using from the Enterprised OCR \n",
    "3. Importing the enhanced OCR JSON files into a new processor, while also migrating the schema from the original processor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe00a75-567e-4f33-b4b4-aa72f9f4fe50",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* Vertex AI Notebook.\n",
    "* Storage Bucket for storing exported json files and output JSON files.\n",
    "* Permission For Google DocAI Processors, Storage and Vertex AI Notebook.\n",
    "* A new DocAI Processor to receive the upgraded OCR Labeled documents in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b33b7f0-baf8-421c-b476-2d25b1ee9b02",
   "metadata": {},
   "source": [
    "## Step by Step Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadfe192-5691-4053-8670-32ef3e5bea8c",
   "metadata": {},
   "source": [
    "### 1. Import Modules/Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea066b24-5e3b-4c28-a56f-263fc6e9aa67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this cell to download utilities module\n",
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8e2a54-e250-4924-8f58-9a4642e6ddf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install google-cloud-documentai google-cloud-storage PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6495b161-ac1b-40b2-a668-fe835da2950a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from google.api_core.client_options import ClientOptions\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from google.cloud import storage\n",
    "from typing import (\n",
    "    Container,\n",
    "    Iterable,\n",
    "    Iterator,\n",
    "    List,\n",
    "    Mapping,\n",
    "    Optional,\n",
    "    Sequence,\n",
    "    Tuple,\n",
    "    Union,\n",
    "    Callable,\n",
    "    Any,\n",
    ")\n",
    "from io import BytesIO\n",
    "from pprint import pprint\n",
    "import copy\n",
    "from PIL import Image\n",
    "from PyPDF2 import PdfFileReader\n",
    "import io\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "import math\n",
    "import concurrent.futures\n",
    "\n",
    "from utilities import file_names, store_document_as_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1709ea5e-bf14-44ab-9963-cda14b092dc2",
   "metadata": {},
   "source": [
    "### 2. Input Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce030a1-663b-445e-a38e-24dc415942c2",
   "metadata": {},
   "source": [
    "* **project_id**: Project ID/Number of the Project\n",
    "* **location**: Processor Location \n",
    "* **processor_id**: Unique Processor ID/Number\n",
    "* **export_dataset_path**: GS Path where we need to export the procesor dataset.\n",
    "* **updated_ocr_files_path**: GS Path where the updated OCR dataset needs to be save.\n",
    "* **new_processor_id**: Unique Processor ID/Number of new Processor\n",
    "* **new_location**: New Processor Location \n",
    "* **ocr_location**: Processor Location of the OCR Processsor.\n",
    "* **ocr_processor_id**: Unique Processor ID/Number of the OCR Processor.\n",
    "* **ocr_version_id**: Unique Processor Version ID/Number of the OCR Processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eb0a45-7e29-41b4-a9e0-44dab8f55c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = \"xxx-xxx-xxx\"\n",
    "location = \"us\"\n",
    "processor_id = \"xxxx-xxxx-xxxx\"\n",
    "export_dataset_path = \"gs://bucket/path/to/export_dataset/\"  # empty folder\n",
    "updated_ocr_files_path = \"gs://bucket/path/to/updated_dataset/\"  # empty folder\n",
    "\n",
    "new_processor_id = \"xxxx-xxx-xxxx\"\n",
    "new_location = \"us\"\n",
    "\n",
    "ocr_location = \"us\"\n",
    "ocr_processor_id = \"xxxx-xxx-xxxx\"\n",
    "ocr_version_id = \"pretrained-ocr-v2.0-2023-06-02\"\n",
    "\n",
    "offset = 0.005  # To Expand the Existing bounding box in order to get all the tokens corrosponding to the entities. Can adjust with optimal value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a4b1ed-2e86-4a8e-b00a-ebf6e41921ef",
   "metadata": {},
   "source": [
    "### 3. Run the required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2afdc0-3873-4db9-a7b7-e64cf0dce00b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_document_sample(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    processor_id: str,\n",
    "    file_path: str,\n",
    "    processor_version_id: Optional[str] = None,\n",
    "    mime_type: Optional[str] = \"application/pdf\",\n",
    "    field_mask: Optional[str] = None,\n",
    ") -> documentai.ProcessResponse:\n",
    "    \"\"\"\n",
    "    Processes a document using a specified Document AI processor in Google Cloud and\n",
    "    returns the processed result. This function reads a file, processes it through a Document AI processor,\n",
    "    and retrieves the result which may include text extraction, form parsing, etc.\n",
    "\n",
    "    Args:\n",
    "        project_id (str): The Google Cloud project ID where the Document AI processor is located.\n",
    "        location (str): The location/region of the Document AI processor (e.g., 'us', 'eu').\n",
    "        processor_id (str): The ID of the Document AI processor to use for processing.\n",
    "        file_path (str): The local path or in-memory string content of the document to be processed.\n",
    "        processor_version_id (Optional[str], optional): The specific processor version to use, if any.\n",
    "            If not provided, the default processor version will be used. Defaults to None.\n",
    "        mime_type (Optional[str], optional): The MIME type of the document. Defaults to 'application/pdf'.\n",
    "        field_mask (Optional[str], optional): Field mask specifying the parts of the document to process.\n",
    "            If not provided, the entire document will be processed. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        documentai.ProcessResponse: The response object containing the processed document data from the processor.\n",
    "    \"\"\"\n",
    "    # You must set the `api_endpoint` if you use a location other than \"us\".\n",
    "    opts = ClientOptions(api_endpoint=f\"{location}-documentai.googleapis.com\")\n",
    "    client = documentai.DocumentProcessorServiceClient(client_options=opts)\n",
    "    if processor_version_id:\n",
    "        name = client.processor_version_path(\n",
    "            project_id, location, processor_id, processor_version_id\n",
    "        )\n",
    "    else:\n",
    "        name = client.processor_path(project_id, location, processor_id)\n",
    "    # Read the file into memory\n",
    "    image_content = file_path\n",
    "    # Load binary data\n",
    "    raw_document = documentai.RawDocument(content=image_content, mime_type=mime_type)\n",
    "    request = documentai.ProcessRequest(\n",
    "        name=name,\n",
    "        raw_document=raw_document,\n",
    "        field_mask=field_mask,\n",
    "        # process_options=process_options,\n",
    "    )\n",
    "    result = client.process_document(request=request)\n",
    "    # Read the text recognition output from the processor\n",
    "    return result\n",
    "\n",
    "\n",
    "def create_pdf_bytes(json: str) -> bytes:\n",
    "    \"\"\"\n",
    "    Creates PDF bytes from image content in a JSON document (typically ground truth data),\n",
    "    which is used for further processing of files. This function decodes image data and\n",
    "    combines them into a single PDF.\n",
    "\n",
    "    Args:\n",
    "        json (str): The JSON string representing the ground truth data, typically retrieved\n",
    "        from Google Cloud's Document AI output or other sources. The JSON should contain image data in\n",
    "        its content field.\n",
    "\n",
    "    Returns:\n",
    "        bytes: A byte representation of the generated PDF containing all images.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If no images are found in the input JSON or an invalid image format is encountered.\n",
    "\n",
    "    Example:\n",
    "        json_str = '{\"pages\": [{\"image\": {\"content\": \"<image_bytes_in_base64>\"}}]}'\n",
    "        pdf_bytes = create_pdf_bytes(json_str)\n",
    "    \"\"\"\n",
    "    from google.cloud import documentai_v1beta3\n",
    "\n",
    "    def decode_image(image_bytes: bytes) -> Image.Image:\n",
    "        \"\"\"Decodes image bytes into a PIL Image object.\"\"\"\n",
    "        with io.BytesIO(image_bytes) as image_file:\n",
    "            image = Image.open(image_file)\n",
    "            image.load()\n",
    "        return image\n",
    "\n",
    "    def create_pdf_from_images(images: Sequence[Image.Image]) -> bytes:\n",
    "        \"\"\"Creates a PDF from a sequence of images.\n",
    "\n",
    "        Args:\n",
    "            images: A sequence of images to be included in the PDF.\n",
    "\n",
    "        Returns:\n",
    "            bytes: The PDF bytes generated from the images.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If no images are provided.\n",
    "        \"\"\"\n",
    "        if not images:\n",
    "            raise ValueError(\"At least one image is required to create a PDF\")\n",
    "\n",
    "        # PIL PDF saver does not support RGBA images\n",
    "        images = [\n",
    "            image.convert(\"RGB\") if image.mode == \"RGBA\" else image for image in images\n",
    "        ]\n",
    "\n",
    "        with io.BytesIO() as pdf_file:\n",
    "            images[0].save(\n",
    "                pdf_file, save_all=True, append_images=images[1:], format=\"PDF\"\n",
    "            )\n",
    "            return pdf_file.getvalue()\n",
    "\n",
    "    d = documentai_v1beta3.Document\n",
    "    document = d.from_json(json)\n",
    "    synthesized_images = []\n",
    "    for i in range(len(document.pages)):\n",
    "        synthesized_images.append(decode_image(document.pages[i].image.content))\n",
    "    pdf_bytes = create_pdf_from_images(synthesized_images)\n",
    "\n",
    "    return pdf_bytes\n",
    "\n",
    "\n",
    "def get(entity: dict, arg: str) -> float:\n",
    "    \"\"\"\n",
    "    Extracts the specified bounding box coordinate (x_min, y_min, x_max, y_max) from the entity object.\n",
    "    This function calculates the minimum or maximum x and y coordinates of the bounding box around the\n",
    "    entity based on normalized vertices.\n",
    "\n",
    "    Args:\n",
    "        entity (dict): The entity dictionary that contains the bounding box information.\n",
    "            It may have different key formats (`pageAnchor` or `page_anchor`) depending on the format.\n",
    "        arg (str): The coordinate to extract. Valid values are 'x_min', 'y_min', 'x_max', 'y_max'.\n",
    "\n",
    "    Returns:\n",
    "        float: The value of the requested coordinate (minimum or maximum of x or y).\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an invalid argument is passed for `arg`.\n",
    "\n",
    "    Example:\n",
    "        entity = {\n",
    "            \"pageAnchor\": {\n",
    "                \"pageRefs\": [{\n",
    "                    \"boundingPoly\": {\n",
    "                        \"normalizedVertices\": [{\"x\": 0.1, \"y\": 0.2}, {\"x\": 0.4, \"y\": 0.5}]\n",
    "                    }\n",
    "                }]\n",
    "            }\n",
    "        }\n",
    "        x_min = get(entity, 'x_min')\n",
    "        y_max = get(entity, 'y_max')\n",
    "    \"\"\"\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    if \"pageAnchor\" in entity.keys():\n",
    "        for i in entity[\"pageAnchor\"][\"pageRefs\"]:\n",
    "            for j in i[\"boundingPoly\"][\"normalizedVertices\"]:\n",
    "                x_list.append(j.get(\"x\", 0))\n",
    "                y_list.append(j.get(\"y\", 0))\n",
    "\n",
    "        if arg == \"x_min\":\n",
    "            return min(x_list)\n",
    "        if arg == \"y_min\":\n",
    "            return min(y_list)\n",
    "        if arg == \"x_max\":\n",
    "            return max(x_list)\n",
    "        if arg == \"y_max\":\n",
    "            return max(y_list)\n",
    "    else:\n",
    "        for i in entity[\"page_anchor\"][\"page_refs\"]:\n",
    "            for j in i[\"bounding_poly\"][\"normalized_vertices\"]:\n",
    "                x_list.append(j.get(\"x\", 0))\n",
    "                y_list.append(j.get(\"y\", 0))\n",
    "\n",
    "        if arg == \"x_min\":\n",
    "            return min(x_list)\n",
    "        if arg == \"y_min\":\n",
    "            return min(y_list)\n",
    "        if arg == \"x_max\":\n",
    "            return max(x_list)\n",
    "        if arg == \"y_max\":\n",
    "            return max(y_list)\n",
    "\n",
    "\n",
    "def find_textSegment_list(\n",
    "    x_min: float, y_min: float, x_max: float, y_max: float, js: dict, page: int\n",
    ") -> List[dict]:\n",
    "    \"\"\"\n",
    "    Finds and returns a list of text segments within a specified bounding box (defined by x_min, y_min,\n",
    "    x_max, y_max) from the tokens on a given page of a Document AI JSON structure.\n",
    "\n",
    "    Args:\n",
    "        x_min (float): The minimum x-coordinate of the bounding box.\n",
    "        y_min (float): The minimum y-coordinate of the bounding box.\n",
    "        x_max (float): The maximum x-coordinate of the bounding box.\n",
    "        y_max (float): The maximum y-coordinate of the bounding box.\n",
    "        js (dict): The JSON data (in Document AI format) containing page and token information.\n",
    "        page (int): The page number from which to extract the text segments.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: A list of text segments (from text_anchor) that fall within the specified bounding box.\n",
    "\n",
    "    Example:\n",
    "        text_segments = find_textSegment_list(\n",
    "            x_min=0.1, y_min=0.2, x_max=0.4, y_max=0.5,\n",
    "            js=document_json, page=0\n",
    "        )\n",
    "    \"\"\"\n",
    "    textSegments_list = []\n",
    "    for token in js[\"pages\"][page][\"tokens\"]:\n",
    "        token_xMin = get_token(token, \"x_min\")\n",
    "        token_xMax = get_token(token, \"x_max\")\n",
    "        token_yMin = get_token(token, \"y_min\")\n",
    "        token_yMax = get_token(token, \"y_max\")\n",
    "        mid_point = ((token_xMin + token_xMax) / 2, (token_yMin + token_yMax) / 2)\n",
    "        if (\n",
    "            mid_point[0] >= x_min\n",
    "            and mid_point[0] <= x_max\n",
    "            and mid_point[1] >= y_min\n",
    "            and mid_point[1] <= y_max\n",
    "        ):\n",
    "            textSegments_list.extend(token[\"layout\"][\"text_anchor\"][\"text_segments\"])\n",
    "    return textSegments_list\n",
    "\n",
    "\n",
    "def get_token(token: dict, param: str) -> float:\n",
    "    \"\"\"\n",
    "    Retrieves the specified bounding box coordinate (x_min, y_min, x_max, y_max) from a token's layout information.\n",
    "\n",
    "    This function extracts the list of normalized vertices (x, y coordinates) from the bounding box of the token\n",
    "    and returns the minimum or maximum value based on the requested parameter.\n",
    "\n",
    "    Args:\n",
    "        token (dict): The token dictionary that contains the bounding box (in the 'layout' field).\n",
    "        param (str): The coordinate to extract. Valid values are 'x_min', 'x_max', 'y_min', 'y_max'.\n",
    "\n",
    "    Returns:\n",
    "        float: The value of the requested coordinate (minimum or maximum of x or y).\n",
    "    \"\"\"\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    for j in token[\"layout\"][\"bounding_poly\"][\"normalized_vertices\"]:\n",
    "        x_list.append(j[\"x\"])\n",
    "        y_list.append(j[\"y\"])\n",
    "    if param == \"x_min\":\n",
    "        return min(x_list)\n",
    "    if param == \"x_max\":\n",
    "        return max(x_list)\n",
    "    if param == \"y_min\":\n",
    "        return min(y_list)\n",
    "    if param == \"y_max\":\n",
    "        return max(y_list)\n",
    "\n",
    "\n",
    "def update_text_anchors_mention_text(\n",
    "    entity: dict,\n",
    "    js: dict,\n",
    "    new_js: dict,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Updates the text anchor of an entity with the corresponding text segments from a new JSON structure.\n",
    "\n",
    "    This function extracts text segments from the `new_js` based on the bounding box coordinates of the provided\n",
    "    `entity`. It constructs a new entity that includes a text anchor and the mention text derived from the text segments.\n",
    "\n",
    "    Args:\n",
    "        entity (dict): The original entity containing the bounding box and page anchor information.\n",
    "        js (dict): The original JSON structure containing page and token information.\n",
    "        new_js (dict): The new JSON structure containing text and token information to extract text segments from.\n",
    "        offset (float): An offset to be applied to the bounding box coordinates for expanding the search area.\n",
    "\n",
    "    Returns:\n",
    "        dict: A new entity with updated text anchor and mention text, including the corresponding page references.\n",
    "    \"\"\"\n",
    "    if \"pageAnchor\" not in entity.keys():\n",
    "        # print(f\"We're skipping the {entity['type']} because there's no PageAnchor.\") we are skipping entities\n",
    "        return None\n",
    "    new_entity = {}\n",
    "    text_anchor = {}\n",
    "    textAnchorList = []\n",
    "    x_min = get(entity, \"x_min\")\n",
    "    x_max = get(entity, \"x_max\")\n",
    "    y_min = get(entity, \"y_min\")\n",
    "    y_max = get(entity, \"y_max\")\n",
    "    page = 0\n",
    "    if \"page\" in entity[\"pageAnchor\"][\"pageRefs\"][0].keys():\n",
    "        page = int(entity[\"pageAnchor\"][\"pageRefs\"][0][\"page\"])\n",
    "    textSegmentList = find_textSegment_list(\n",
    "        x_min - offset, y_min - offset, x_max + offset, y_max + offset, new_js, page\n",
    "    )\n",
    "    for j in textSegmentList:\n",
    "        if \"start_index\" not in j.keys():\n",
    "            j[\"start_index\"] = str(0)\n",
    "    textSegmentList = sorted(textSegmentList, key=lambda x: int(x[\"start_index\"]))\n",
    "    text_anchor[\"text_segments\"] = textSegmentList\n",
    "    mentionText = \"\"\n",
    "    listOfIndex = []\n",
    "    for j in textSegmentList:\n",
    "        mentionText += new_js[\"text\"][int(j[\"start_index\"]) : int(j[\"end_index\"])]\n",
    "    text_anchor[\"content\"] = mentionText\n",
    "    new_entity[\"text_anchor\"] = text_anchor\n",
    "    new_entity[\"mention_text\"] = mentionText\n",
    "    temp_page_anchor = {}\n",
    "\n",
    "    list_of_page_refs = []\n",
    "    for i in entity[\"pageAnchor\"][\"pageRefs\"]:\n",
    "        temp = {}\n",
    "        temp2 = {}\n",
    "        temp3 = []\n",
    "        for j in i[\"boundingPoly\"][\"normalizedVertices\"]:\n",
    "            temp3.append(j)\n",
    "        temp2[\"normalized_vertices\"] = temp3\n",
    "        temp[\"bounding_poly\"] = temp2\n",
    "        if \"layoutType\" in i.keys():\n",
    "            temp[\"layout_type\"] = i[\"layoutType\"]\n",
    "        temp[\"page\"] = str(page)\n",
    "        list_of_page_refs.append(temp)\n",
    "    temp_page_anchor[\"page_refs\"] = list_of_page_refs\n",
    "    new_entity[\"page_anchor\"] = temp_page_anchor\n",
    "    new_entity[\"type\"] = entity[\"type\"]\n",
    "    return new_entity\n",
    "\n",
    "\n",
    "def make_parent_from_child_entities(temp_child: list, new_js: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Creates a parent entity from a list of child entities by combining their text anchors and bounding boxes.\n",
    "\n",
    "    This function checks the number of child entities. If there is one child, it returns that child directly.\n",
    "    If there are two or more children, it combines them into a single parent entity, merging their text segments,\n",
    "    mention text, and bounding box coordinates.\n",
    "\n",
    "    Args:\n",
    "        temp_child (list): A list of child entity dictionaries to be combined.\n",
    "        new_js (dict): The new JSON structure containing text data used for extracting text segments.\n",
    "\n",
    "    Returns:\n",
    "        dict: A parent entity that includes the merged text anchor, mention text, and bounding box.\n",
    "\n",
    "    Example:\n",
    "        parent_entity = make_parent_from_child_entities(child_entities, new_json)\n",
    "    \"\"\"\n",
    "\n",
    "    def combine_two_entities(entity1: dict, entity2: dict, js: dict) -> dict:\n",
    "        \"\"\"Combines two entities into one by merging their text anchors and bounding boxes.\"\"\"\n",
    "        new_entity = {}\n",
    "        new_entity[\"type\"] = entity1[\"type\"]\n",
    "        text_anchor = {}\n",
    "        # print(\"Entity1 : \"+entity1['mentionText'])\n",
    "        # print(\"Entity2 : \"+entity2['mentionText'])\n",
    "        textAnchorList = []\n",
    "\n",
    "        entity1[\"text_anchor\"][\"text_segments\"] = sorted(\n",
    "            entity1[\"text_anchor\"][\"text_segments\"], key=lambda x: int(x[\"start_index\"])\n",
    "        )\n",
    "        entity2[\"text_anchor\"][\"text_segments\"] = sorted(\n",
    "            entity2[\"text_anchor\"][\"text_segments\"], key=lambda x: int(x[\"start_index\"])\n",
    "        )\n",
    "        for j in entity1[\"text_anchor\"][\"text_segments\"]:\n",
    "            textAnchorList.append(j)\n",
    "            # print(js['text'][int(j['startIndex']):int(j['endIndex'])])\n",
    "        for j in entity2[\"text_anchor\"][\"text_segments\"]:\n",
    "            textAnchorList.append(j)\n",
    "        textAnchorList = sorted(textAnchorList, key=lambda x: int(x[\"start_index\"]))\n",
    "        mentionText = \"\"\n",
    "        for j in textAnchorList:\n",
    "            mentionText += js[\"text\"][int(j[\"start_index\"]) : int(j[\"end_index\"])]\n",
    "        new_entity[\"mention_text\"] = mentionText\n",
    "        text_anchor[\"content\"] = mentionText\n",
    "        temp_text_anchor_list = []\n",
    "        for i in range(len(entity1[\"text_anchor\"][\"text_segments\"])):\n",
    "            temp_text_anchor_list.append(entity1[\"text_anchor\"][\"text_segments\"][i])\n",
    "        for i in range(len(entity2[\"text_anchor\"][\"text_segments\"])):\n",
    "            temp_text_anchor_list.append(entity2[\"text_anchor\"][\"text_segments\"][i])\n",
    "        text_anchor[\"text_segments\"] = temp_text_anchor_list\n",
    "        new_entity[\"text_anchor\"] = text_anchor\n",
    "        min_x = min(get(entity1, \"x_min\"), get(entity2, \"x_min\"))\n",
    "        min_y = min(get(entity1, \"y_min\"), get(entity2, \"y_min\"))\n",
    "        max_x = max(get(entity1, \"x_max\"), get(entity2, \"x_max\"))\n",
    "        max_y = max(get(entity1, \"y_max\"), get(entity2, \"y_max\"))\n",
    "        A = {\"x\": min_x, \"y\": min_y}\n",
    "        B = {\"x\": max_x, \"y\": min_y}\n",
    "        C = {\"x\": max_x, \"y\": max_y}\n",
    "        D = {\"x\": min_x, \"y\": max_y}\n",
    "        new_entity[\"page_anchor\"] = entity1[\"page_anchor\"]\n",
    "        new_entity[\"page_anchor\"][\"page_refs\"][0][\"bounding_poly\"][\n",
    "            \"normalized_vertices\"\n",
    "        ] = [A, B, C, D]\n",
    "        return new_entity\n",
    "\n",
    "    if len(temp_child) == 1:\n",
    "        return temp_child[0]\n",
    "    if len(temp_child) == 2:\n",
    "        parent_entity = combine_two_entities(temp_child[0], temp_child[1], new_js)\n",
    "        return parent_entity\n",
    "    parent_entity = combine_two_entities(temp_child[0], temp_child[1], new_js)\n",
    "    for i in range(2, len(temp_child)):\n",
    "        parent_entity = combine_two_entities(parent_entity, temp_child[i], new_js)\n",
    "    return parent_entity\n",
    "\n",
    "\n",
    "def list_documents(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    processor: str,\n",
    "    page_size: int = 100,\n",
    "    page_token: str = \"\",\n",
    ") -> documentai.types.ListDocumentsResponse:\n",
    "    \"\"\"\n",
    "    Lists documents in a specified Document AI processor.\n",
    "\n",
    "    This function retrieves a list of documents from the specified Document AI processor dataset.\n",
    "    It supports pagination through the `page_size` and `page_token` parameters.\n",
    "\n",
    "    Args:\n",
    "        project_id (str): The ID of the Google Cloud project.\n",
    "        location (str): The location of the Document AI processor (e.g., 'us', 'eu').\n",
    "        processor (str): The ID of the Document AI processor.\n",
    "        page_size (int, optional): The maximum number of documents to return per page. Default is 100.\n",
    "        page_token (str, optional): A token for pagination; it indicates the next page of results. Default is an empty string.\n",
    "\n",
    "    Returns:\n",
    "        documentai.types.ListDocumentsResponse: The response containing a list of documents and additional metadata.\n",
    "\n",
    "    Example:\n",
    "        response = list_documents('my-project-id', 'us', 'my-processor-id')\n",
    "        for document in response.documents:\n",
    "            print(document.name)\n",
    "    \"\"\"\n",
    "    opts = ClientOptions(api_endpoint=f\"{location}-documentai.googleapis.com\")\n",
    "    client = documentai.DocumentServiceClient(client_options=opts)\n",
    "    # client = documentai.DocumentServiceClient()\n",
    "    dataset = (\n",
    "        f\"projects/{project_id}/locations/{location}/processors/{processor}/dataset\"\n",
    "    )\n",
    "    request = documentai.types.ListDocumentsRequest(\n",
    "        dataset=dataset,\n",
    "        page_token=page_token,\n",
    "        page_size=page_size,\n",
    "        return_total_size=True,\n",
    "    )\n",
    "    operation = client.list_documents(request)\n",
    "    return operation\n",
    "\n",
    "\n",
    "def get_document(\n",
    "    project_id: str, location: str, processor: str, doc_id: str\n",
    ") -> documentai.types.Document:\n",
    "    \"\"\"\n",
    "    Retrieves a specific document from a Document AI processor by its document ID.\n",
    "\n",
    "    This function fetches the details of a document stored in the specified Document AI processor\n",
    "    dataset using the document's unique ID.\n",
    "\n",
    "    Args:\n",
    "        project_id (str): The ID of the Google Cloud project.\n",
    "        location (str): The location of the Document AI processor (e.g., 'us', 'eu').\n",
    "        processor (str): The ID of the Document AI processor.\n",
    "        doc_id (str): The unique identifier of the document to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        documentai.types.Document: The document object containing the requested document's details.\n",
    "\n",
    "    Example:\n",
    "        document = get_document('my-project-id', 'us', 'my-processor-id', 'my-document-id')\n",
    "        print(document.name)\n",
    "    \"\"\"\n",
    "    opts = ClientOptions(api_endpoint=f\"{location}-documentai.googleapis.com\")\n",
    "    client = documentai.DocumentServiceClient(client_options=opts)\n",
    "    # client = documentai.DocumentServiceClient()\n",
    "    dataset = (\n",
    "        f\"projects/{project_id}/locations/{location}/processors/{processor}/dataset\"\n",
    "    )\n",
    "    request = documentai.types.GetDocumentRequest(dataset=dataset, document_id=doc_id)\n",
    "    operation = client.get_document(request)\n",
    "    return operation.document\n",
    "\n",
    "\n",
    "def get_dataset_schema(\n",
    "    project_id: str, processor_id: str, location: str\n",
    ") -> documentai.types.DatasetSchema:\n",
    "    \"\"\"\n",
    "    Retrieves the dataset schema for a specified Document AI processor.\n",
    "\n",
    "    This function fetches the schema of the dataset associated with a given Document AI processor,\n",
    "    which describes the structure and organization of the dataset.\n",
    "\n",
    "    Args:\n",
    "        project_id (str): The ID of the Google Cloud project.\n",
    "        processor_id (str): The ID of the Document AI processor.\n",
    "        location (str): The location of the Document AI processor (e.g., 'us', 'eu').\n",
    "\n",
    "    Returns:\n",
    "        documentai.types.DatasetSchema: The schema of the dataset associated with the processor.\n",
    "\n",
    "    Example:\n",
    "        schema = get_dataset_schema('my-project-id', 'my-processor-id', 'us')\n",
    "        print(schema)\n",
    "    \"\"\"\n",
    "    # Create a client\n",
    "    opts = ClientOptions(api_endpoint=f\"{location}-documentai.googleapis.com\")\n",
    "    client = documentai.DocumentServiceClient(client_options=opts)\n",
    "    processor_name = (\n",
    "        f\"projects/{project_id}/locations/{location}/processors/{processor_id}\"\n",
    "    )\n",
    "    # client = documentai.DocumentServiceClient()\n",
    "    request = documentai.GetDatasetSchemaRequest(\n",
    "        name=processor_name + \"/dataset/datasetSchema\"\n",
    "    )\n",
    "    # Make the request\n",
    "    response = client.get_dataset_schema(request=request)\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def upload_dataset_schema(\n",
    "    schema: documentai.types.DatasetSchema, location: str\n",
    ") -> documentai.types.DatasetSchema:\n",
    "    \"\"\"\n",
    "    Uploads a new or updated dataset schema to a Document AI processor.\n",
    "\n",
    "    This function sends the provided dataset schema to the Document AI processor, allowing\n",
    "    for the schema to be updated or created as necessary.\n",
    "\n",
    "    Args:\n",
    "        schema (documentai.types.DatasetSchema): The dataset schema to be uploaded.\n",
    "\n",
    "    Returns:\n",
    "        documentai.types.DatasetSchema: The updated dataset schema returned by the service.\n",
    "\n",
    "    Example:\n",
    "        from google.cloud import documentai_v1beta3 as documentai\n",
    "\n",
    "        schema = documentai.DatasetSchema(\n",
    "            # populate schema fields as necessary\n",
    "        )\n",
    "        updated_schema = upload_dataset_schema(schema)\n",
    "        print(updated_schema)\n",
    "    \"\"\"\n",
    "    opts = ClientOptions(api_endpoint=f\"{location}-documentai.googleapis.com\")\n",
    "    client = documentai.DocumentServiceClient(client_options=opts)\n",
    "    # client = documentai.DocumentServiceClient()\n",
    "    request = documentai.UpdateDatasetSchemaRequest(dataset_schema=schema)\n",
    "    res = client.update_dataset_schema(request=request)\n",
    "    return res\n",
    "\n",
    "\n",
    "def import_documents(\n",
    "    project_id: str, processor_id: str, location: str, gcs_path: str\n",
    ") -> documentai.types.ImportDocumentsResponse:\n",
    "    \"\"\"\n",
    "    Imports documents from Google Cloud Storage (GCS) into a Document AI processor's dataset.\n",
    "\n",
    "    This function imports documents into the dataset associated with a specified Document AI\n",
    "    processor, organizing them into training, testing, and unassigned splits based on the\n",
    "    provided GCS path.\n",
    "\n",
    "    Args:\n",
    "        project_id (str): The ID of the Google Cloud project.\n",
    "        processor_id (str): The ID of the Document AI processor.\n",
    "        location (str): The location of the Document AI processor (e.g., 'us', 'eu').\n",
    "        gcs_path (str): The GCS path prefix where the documents are stored. It should include\n",
    "                        the base path with trailing slash.\n",
    "\n",
    "    Returns:\n",
    "        documentai.types.ImportDocumentsResponse: The response from the import operation.\n",
    "\n",
    "    Example:\n",
    "        response = import_documents('my-project-id', 'my-processor-id', 'us', 'gs://my-bucket/documents/')\n",
    "        print(response)\n",
    "    \"\"\"\n",
    "    opts = ClientOptions(api_endpoint=f\"{location}-documentai.googleapis.com\")\n",
    "    client = documentai.DocumentServiceClient(client_options=opts)\n",
    "    # client = documentai.DocumentServiceClient()\n",
    "    dataset = (\n",
    "        f\"projects/{project_id}/locations/{location}/processors/{processor_id}/dataset\"\n",
    "    )\n",
    "    request = documentai.ImportDocumentsRequest(\n",
    "        dataset=dataset,\n",
    "        batch_documents_import_configs=[\n",
    "            {\n",
    "                \"dataset_split\": \"DATASET_SPLIT_TRAIN\",\n",
    "                \"batch_input_config\": {\n",
    "                    \"gcs_prefix\": {\"gcs_uri_prefix\": gcs_path + \"train/\"}\n",
    "                },\n",
    "            },\n",
    "            {\n",
    "                \"dataset_split\": \"DATASET_SPLIT_TEST\",\n",
    "                \"batch_input_config\": {\n",
    "                    \"gcs_prefix\": {\"gcs_uri_prefix\": gcs_path + \"test/\"}\n",
    "                },\n",
    "            },\n",
    "            {\n",
    "                \"dataset_split\": \"DATASET_SPLIT_UNASSIGNED\",\n",
    "                \"batch_input_config\": {\n",
    "                    \"gcs_prefix\": {\"gcs_uri_prefix\": gcs_path + \"unassigned/\"}\n",
    "                },\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    response = client.import_documents(request=request)\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def retry_function_with_internal_error_handling(\n",
    "    func: Callable[..., Any],\n",
    "    max_retries: int = 3,\n",
    "    wait_time: int = 10,\n",
    "    *args: Any,\n",
    "    **kwargs: Any,\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Runs a function with retry logic if a 500 Internal Server Error is encountered.\n",
    "\n",
    "    Args:\n",
    "        func (function): The function to be called.\n",
    "        max_retries (int): Maximum number of retries (default is 3).\n",
    "        wait_time (int): Time to wait between retries in seconds (default is 10 seconds).\n",
    "        *args: Positional arguments to pass to the function.\n",
    "        **kwargs: Keyword arguments to pass to the function.\n",
    "\n",
    "    Returns:\n",
    "        Result of the function if it succeeds within the retry attempts.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If the function fails after max retries.\n",
    "    \"\"\"\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            # Call the function with passed arguments\n",
    "            result = func(*args, **kwargs)\n",
    "            return result  # If successful, return the result immediately\n",
    "        except Exception as e:\n",
    "            error = e\n",
    "            attempt += 1\n",
    "            time.sleep(wait_time)  # Wait for a short period before retrying\n",
    "\n",
    "    # If the function fails after all retries\n",
    "    raise Exception(\n",
    "        f\"Function failed after {max_retries} attempts due to repeated {error} errors.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1d85cf-7529-469b-be7e-3e00a778af87",
   "metadata": {},
   "source": [
    "### 4. Run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dab42f-6c98-4aac-b804-cd3ec81127f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_file(document_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Processes a document by downloading it from Cloud Storage, running OCR, and updating entities.\n",
    "\n",
    "    Args:\n",
    "        document_path (str): The Cloud Storage path of the document to be processed.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    source_bucket = storage_client.bucket(export_dataset_path.split(\"/\")[2])\n",
    "    try:\n",
    "        file_name = (\"/\").join(document_path.split(\"/\")[-2:])\n",
    "        print(document_path)\n",
    "        js_json = source_bucket.blob(document_path).download_as_string().decode(\"utf-8\")\n",
    "        merged_pdf = create_pdf_bytes(js_json)\n",
    "        js = json.loads(js_json)\n",
    "        res = retry_function_with_internal_error_handling(\n",
    "            process_document_sample,\n",
    "            project_id=project_id,\n",
    "            location=ocr_location,\n",
    "            processor_id=ocr_processor_id,\n",
    "            file_path=merged_pdf,\n",
    "            processor_version_id=ocr_version_id,\n",
    "        )\n",
    "        # print(res.document.entities)\n",
    "        if res.document.entities:\n",
    "            del res.document.entities\n",
    "        new_js = documentai.Document.to_dict(res.document)\n",
    "        updated_entities = []\n",
    "        for entity in js[\"entities\"]:\n",
    "            # print(entity)\n",
    "            temp_child = []\n",
    "            ent = {}\n",
    "            if \"properties\" in entity.keys() and len(entity[\"properties\"]) != 0:\n",
    "                for child_item in entity[\"properties\"]:\n",
    "                    temp_grand_child = []\n",
    "                    child_ent = {}\n",
    "                    if (\n",
    "                        \"properties\" in child_item.keys()\n",
    "                        and len(child_item[\"properties\"]) != 0\n",
    "                    ):\n",
    "                        for grand_child_item in child_item[\"properties\"]:\n",
    "                            ent_gch = update_text_anchors_mention_text(\n",
    "                                grand_child_item, js, new_js\n",
    "                            )\n",
    "                            if ent_gch is not None:\n",
    "                                temp_grand_child.append(ent_gch)\n",
    "                        child_ent = make_parent_from_child_entities(\n",
    "                            copy.deepcopy(temp_grand_child), new_js\n",
    "                        )\n",
    "                        child_ent[\"type\"] = child_item[\"type\"]\n",
    "                        child_ent[\"properties\"] = temp_grand_child\n",
    "                        temp_child.append(child_ent)\n",
    "                    else:\n",
    "                        ent_ch = update_text_anchors_mention_text(\n",
    "                            child_item, js, new_js\n",
    "                        )\n",
    "                        # print(ent_ch)\n",
    "                        if ent_ch is not None:\n",
    "                            temp_child.append(ent_ch)\n",
    "                ent = make_parent_from_child_entities(copy.deepcopy(temp_child), new_js)\n",
    "                ent[\"type\"] = entity[\"type\"]\n",
    "                ent[\"properties\"] = temp_child\n",
    "            else:\n",
    "                ent = update_text_anchors_mention_text(entity, js, new_js)\n",
    "            # pprint(ent)\n",
    "            if ent is not None:\n",
    "                updated_entities.append(ent)\n",
    "        # pprint(updated_entities)\n",
    "        new_js[\"entities\"] = updated_entities\n",
    "        d = documentai.Document.from_json(json.dumps(new_js))\n",
    "        output_bucket_path_prefix = \"/\".join(updated_ocr_files_path.split(\"/\")[3:])\n",
    "        output_file_name = f\"{output_bucket_path_prefix}{file_name}\"\n",
    "        # print(output_file_name)\n",
    "        store_document_as_json(\n",
    "            json.dumps(new_js), updated_ocr_files_path.split(\"/\")[2], output_file_name\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            \">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\"\n",
    "            + document_path\n",
    "            + \" was not processed successfully!!!\"\n",
    "        )\n",
    "        print(e)\n",
    "\n",
    "\n",
    "def process_files_concurrently(file_list: list, max_workers: int = 5) -> None:\n",
    "    \"\"\"\n",
    "    Processes a list of files concurrently using a process pool.\n",
    "\n",
    "    Args:\n",
    "        file_list (list): A list of file paths to process.\n",
    "        max_workers (int, optional): The maximum number of parallel workers. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # The current limit of 5 parallel processes can be increased by adjusting the max_workers parameter\n",
    "    results = []\n",
    "\n",
    "    # Use ProcessPoolExecutor for CPU-bound tasks\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for file in file_list:\n",
    "            executor.submit(process_file, file)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = list_documents(project_id, location, processor_id)\n",
    "    document_list = results.document_metadata\n",
    "    while len(document_list) != results.total_size:\n",
    "        page_token = results.next_page_token\n",
    "        results = list_documents(\n",
    "            project_id, location, processor_id, page_token=page_token\n",
    "        )\n",
    "        document_list.extend(results.document_metadata)\n",
    "    print(\"Exporting Dataset...\")\n",
    "    for doc in tqdm(document_list):\n",
    "        doc_id = doc.document_id\n",
    "        split_type = doc.dataset_type\n",
    "        if split_type == 3:\n",
    "            split = \"unassigned\"\n",
    "        elif split_type == 2:\n",
    "            split = \"test\"\n",
    "        elif split_type == 1:\n",
    "            split = \"train\"\n",
    "        else:\n",
    "            split = \"unknown\"\n",
    "        file_name = doc.display_name\n",
    "        res = get_document(project_id, location, processor_id, doc_id)\n",
    "        exported_path = (\"/\").join(export_dataset_path.split(\"/\")[3:])\n",
    "        # print(exported_path)\n",
    "        output_file_name = f\"{exported_path}/{split}/{file_name}.json\"\n",
    "        json_data = documentai.Document.to_json(res)\n",
    "        # print(export_dataset_path.split(\"/\")[2],output_file_name)\n",
    "        store_document_as_json(\n",
    "            json_data, export_dataset_path.split(\"/\")[2], output_file_name\n",
    "        )\n",
    "\n",
    "    print(\"Exporting Dataset is completed...\")\n",
    "    exported_schema = get_dataset_schema(project_id, processor_id, location)\n",
    "    exported_schema.name = f\"projects/{project_id}/locations/{new_location}/processors/{new_processor_id}/dataset/datasetSchema\"\n",
    "    import_schema = upload_dataset_schema(exported_schema, new_location)\n",
    "\n",
    "    document_paths = list(file_names(export_dataset_path)[1].values())\n",
    "    print(f\"updating the OCR . . . \")\n",
    "    process_files_concurrently(document_paths)\n",
    "    print(f\"imporing updated OCR documents to {new_processor_id}\")\n",
    "    res = import_documents(\n",
    "        project_id, new_processor_id, new_location, updated_ocr_files_path\n",
    "    )\n",
    "    print(f\"Waiting for {len(document_paths)*1} seconds to import all documents\")\n",
    "    time.sleep(len(document_paths) * 1)\n",
    "    print(\"All documents have been imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e3f538-c1ab-4ff1-a720-80d07ea9245b",
   "metadata": {},
   "source": [
    "### Output Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4badf9b-1b40-4b6a-9ac9-b7a0f233018f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Before OCR Upgrade\n",
    "<img src='./images/before_OCR.png' width=600 height=600 alt=\"Sample Output\"></img>\n",
    "### After OCR Upgrade\n",
    "<img src='./images/after_OCR.png' width=600 height=600 alt=\"Sample Output\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e83ee1-d2dc-48e1-8403-0e4a0b372626",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
