{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55646894-baf1-4c1c-9790-5fc16468a282",
   "metadata": {},
   "source": [
    "# Synonyms Based Splitter Document Labeling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1e5f0d-91ea-4766-8ab3-f66429e66e1b",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528c5a36-a860-468f-a30a-08d051880aee",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the DocAI Incubator Team. No guarantees of performance are implied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7374f665-15de-4292-b045-cf6acc14fa10",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "This notebook automates document labeling using a synonyms-based approach for a Custom Document Splitter Parser. By comparing a user-defined list of keywords against OCR-extracted text, it identifies and labels document segments, enhancing document organization and categorization.There is also an optional flag to split the pdfs and save into the GCS folders(named as labels).\n",
    "\n",
    "In this context, \"synonyms\" refer to a set of keywords that match text extracted from documents. By using these keywords, the tool searches the OCR text to create splitter entities, which are markers used to categorize and split the document based on identified keywords.\n",
    "\n",
    "### Practical Application\n",
    "\n",
    "The tool labels document sections by searching OCR text for user-provided synonyms, streamlining the process of splitting and categorizing documents based on their content.\n",
    "\n",
    "### Examples\n",
    "\n",
    "- **Example 1:** With `synonyms_list=['PART A','PART B','PART C','PART D']`, if both \"PART A\" and \"PART B\" are found, the page is labeled as \"PART A\".\n",
    "- **Example 2:** For `synonyms_list=['INTRODUCTION', 'EXECUTIVE SUMMARY', 'CONCLUSION']`, if \"EXECUTIVE SUMMARY\" and \"CONCLUSION\" are found, it labels the page as \"EXECUTIVE SUMMARY\".\n",
    "\n",
    "Priority is given to the synonym appearing first in the list when multiple matches occur on a page, ensuring consistent labeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5734e7-8944-485a-9412-ae063ef57318",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* Python : Jupyter notebook (Vertex AI) \n",
    "* Service account permissions in projects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce627c16-65b6-4870-b0b7-28d5fe599905",
   "metadata": {},
   "source": [
    "## Step by Step procedure \n",
    "\n",
    "### 1.Importing Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1964d8-b1e4-42b7-a399-40861581c371",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1928e8-b562-443d-b17d-88f42cebda45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import re\n",
    "from utilities import *\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Sequence, Tuple, Union\n",
    "from google.cloud import storage\n",
    "from google.cloud.exceptions import Conflict, NotFound\n",
    "from PIL import Image\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdcdd55-89b2-4b67-838e-d82fc5c2d957",
   "metadata": {},
   "source": [
    "### 2.Setup the required inputs\n",
    "* `project_id` : Your Google project id or name\n",
    "* `synonyms_list` : list of synonyms which has to be used to search in ocr for splitting documents\n",
    "* `gcs_input_uri` : OCR PARSED JSONS RESULTS PATH\n",
    "* `gcs_output_uri` : Path to save the updated jsons\n",
    "* `save_split_pdfs_flag` : flag whether to save the splitted pdfs in gcs bucket\n",
    "* `pdfs_ouput_path` : path to save the split files\n",
    "* `synonym_entity_name` : type of entity to view in cde \n",
    "* `label_unidentified_entity_name` : default label name in case first few pages no synonym found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d717d6b4-4bdd-4d04-b9e4-da57f4dd008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id='xxxx-xxxx-xxxx' \n",
    "synonyms_list=['PART A','PART B','PART C','PART D'] \n",
    "gcs_input_uri=\"gs://xxxx/xxxx/xxxx/\"\n",
    "gcs_output_uri='gs://xxxx/xxxx/xxx/' \n",
    "save_split_pdfs_flag='TRUE'\n",
    "pdfs_ouput_path='gs://xxxx/xxxx/xxx/' \n",
    "synonym_entity_name='synonym_entity' \n",
    "label_unidentified_entity_name=\"label_unidentified\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd83c5f9-8bf3-460e-ae3d-177ce642c415",
   "metadata": {},
   "source": [
    "### Function to parse the Raw pdfs upto 200 pages in a single json\n",
    "Use the below function to parse the documents which has more than 10 pages to get the output in a single json and provide the path in the `gcs_input_uri` as given in above cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d957fca3-7d5e-4a24-bd39-14750ac80130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BATCH PROCESSING FUNCTION WITH SHARDING TILL 200 pages\n",
    "def batch_process_documents(\n",
    "    project_id : str,\n",
    "    location : str,\n",
    "    processor_id : str,\n",
    "    gcs_input_uri : str,\n",
    "    gcs_output_uri : str,\n",
    "    timeout: int = 600,\n",
    ") -> Any:\n",
    "    \"\"\"It will perform Batch Process on raw input documents\n",
    "\n",
    "    Args:\n",
    "        project_id (str): GCP project ID\n",
    "        location (str): Processor location us or eu\n",
    "        processor_id (str): GCP DocumentAI ProcessorID\n",
    "        gcs_input_uri (str): GCS path which contains all input files\n",
    "        gcs_output_uri (str): GCS path to store processed JSON results\n",
    "        timeout (int, optional): Maximum waiting time for operation to complete.\n",
    "\n",
    "    Returns:\n",
    "        operation.Operation: LRO operation ID for current batch-job\n",
    "    \"\"\"\n",
    "\n",
    "    # You must set the api_endpoint if you use a location other than 'us', e.g.:\n",
    "    opts = {}\n",
    "    if location == \"eu\":\n",
    "        opts = {\"api_endpoint\": \"eu-documentai.googleapis.com\"}\n",
    "    elif location == \"us\":\n",
    "        opts = {\"api_endpoint\": \"us-documentai.googleapis.com\"}\n",
    "        #opts = {\"api_endpoint\": \"us-autopush-documentai.sandbox.googleapis.com\"}\n",
    "    client = documentai.DocumentProcessorServiceClient(client_options=opts)\n",
    "\n",
    "\n",
    "    input_config= documentai.BatchDocumentsInputConfig(gcs_prefix=documentai.GcsPrefix(gcs_uri_prefix=gcs_input_uri))\n",
    "    \n",
    "    sharding_config = documentai.DocumentOutputConfig.GcsOutputConfig.ShardingConfig(pages_per_shard=200)\n",
    "    gcs_output_config = documentai.DocumentOutputConfig.GcsOutputConfig(\n",
    "        gcs_uri=gcs_output_uri, sharding_config=sharding_config\n",
    "    )\n",
    "\n",
    "    output_config = documentai.DocumentOutputConfig(\n",
    "        gcs_output_config=gcs_output_config\n",
    "    )\n",
    "\n",
    "    # Location can be 'us' or 'eu'\n",
    "    name = f\"projects/{project_id}/locations/{location}/processors/{processor_id}\"\n",
    "    request = documentai.types.document_processor_service.BatchProcessRequest(\n",
    "        name=name,\n",
    "        input_documents=input_config,\n",
    "        document_output_config=output_config,\n",
    "    )\n",
    "\n",
    "    operation = client.batch_process_documents(request)\n",
    "\n",
    "    # Wait for the operation to finish\n",
    "    operation.result(timeout=timeout)\n",
    "    return operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d320b8-c2c8-46b9-8c10-33c5147347a6",
   "metadata": {},
   "source": [
    "### 3.Importing Required functions and calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e49ea6-f84c-47e5-a562-1632f8e6e2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_anchors_page_wise(json_ocr : object) -> Dict:\n",
    "    \"\"\"\n",
    "    Get text anchors for each page in the OCR result.\n",
    "\n",
    "    Args:\n",
    "        json_ocr (object): The OCR result in Document AI Document format.\n",
    "\n",
    "    Returns:\n",
    "        Dict : A dictionary where keys are page numbers (0-indexed),\n",
    "            and values are dictionaries with 'start_index' and 'end_index' for each page's text anchor.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Getting text anchors\n",
    "    p=0\n",
    "    text_anchors_page_wise={}\n",
    "    for page in json_ocr.pages:\n",
    "        for an in page.layout.text_anchor.text_segments:\n",
    "            start_index=an.start_index\n",
    "            end_index=an.end_index\n",
    "        text_anchors_page_wise[p]={'start_index':start_index,'end_index':end_index}\n",
    "        p+=1\n",
    "    return text_anchors_page_wise\n",
    "\n",
    "#getting text anchors of matches with synonyms\n",
    "def find_substring_indexes(text : str, substring : str) -> List[Union[int, int]]:\n",
    "    \"\"\"\n",
    "    Find the starting and ending indexes of occurrences of a substring in the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text where substring needs to be found.\n",
    "        substring (str): The substring to be searched in the text.\n",
    "\n",
    "    Returns:\n",
    "        List[Union[int, int]]: A list of tuples containing the starting and ending indexes of substring occurrences.\n",
    "    \"\"\"\n",
    "    \n",
    "    if ' ' or '\\n' not in substring:\n",
    "        pattern = re.compile(re.escape(substring), re.IGNORECASE)\n",
    "        matches = [(match.start(), match.end()) for match in pattern.finditer(text)]\n",
    "    else:\n",
    "        pattern = re.compile(r'{}.*{}'.format(re.escape(substring.split(' ')[0]),re.escape(substring.split(' ')[-1])), re.IGNORECASE)\n",
    "        matches = [(match.start(), match.end()) for match in pattern.finditer(json_dict['text'])]\n",
    "\n",
    "    return matches\n",
    "\n",
    "def get_synonyms_matches_pages(synonyms_list : List[str], text_anchors_page_wise : Dict[int, Dict[str, int]], json_ocr : object) -> Tuple:\n",
    "    \"\"\"\n",
    "    Find matches of synonyms in the OCR text and associate them with corresponding pages.\n",
    "\n",
    "    Args:\n",
    "        synonyms_list (List[str]): List of synonyms to be searched in the OCR text.\n",
    "        text_anchors_page_wise (Dict[int, Dict[str, int]]): Text anchors with start and end indexes for each page.\n",
    "        json_ocr (object): JSON representation of the OCR output.\n",
    "\n",
    "    Returns:\n",
    "        Tuple : A tuple containing:\n",
    "            - A dictionary with synonyms as keys and lists of pages where they are found, sorted in ascending order.\n",
    "            - A dictionary with synonym information, including text anchors and corresponding pages.\n",
    "    \"\"\"\n",
    "    \n",
    "    matches_synonyms={}\n",
    "    synonym_info={}\n",
    "    for synonym in synonyms_list:\n",
    "        pattern = re.compile('[^a-zA-Z0-9\\s]')\n",
    "        matches_list=find_substring_indexes(re.sub(pattern, ' ', json_ocr.text),re.sub(pattern, ' ', synonym))#find_substring_indexes(json_ocr.text, synonym)\n",
    "        # print(matches_list)\n",
    "        for match in matches_list:\n",
    "            for p1,anc in text_anchors_page_wise.items():\n",
    "                if match[0]>=anc['start_index'] and match[1]<=anc['end_index']:\n",
    "                    if synonym in matches_synonyms.keys():\n",
    "                        matches_synonyms[synonym].append(p1)\n",
    "                        synonym_info[synonym].append({'text_anchors':{'start_index':match[0],'end_index':match[1]},'page':p1})\n",
    "                    else:\n",
    "                        matches_synonyms[synonym]=[p1]\n",
    "                        synonym_info[synonym]=[{'text_anchors':{'start_index':match[0],'end_index':match[1]},'page':p1}]\n",
    "    matches_synonyms_updated = {key: sorted(list(set(value))) for key, value in matches_synonyms.items()}\n",
    "    synonym_wise_data={}\n",
    "    temp_pages=[]\n",
    "    temp_permanant=[]\n",
    "    temp_page=-1\n",
    "    asssigned_pages=list(set(value for values_list in matches_synonyms_updated.values() for value in values_list))\n",
    "    unassigned_pages=[]\n",
    "    synonym_assigned=''\n",
    "    for page_num in range(len(json_ocr.pages)):\n",
    "        asssigned_flag='NO'\n",
    "        for synonym_1,pages_available in matches_synonyms_updated.items():\n",
    "            if synonym_assigned=='':\n",
    "                synonym_assigned=synonym_1\n",
    "            if page_num in pages_available:\n",
    "                if temp_page<page_num:\n",
    "                    if len(temp_pages)==0:\n",
    "                        temp_pages=[page_num]\n",
    "                    else:\n",
    "                        temp_pages.append(page_num)\n",
    "                    temp_page=page_num\n",
    "                if synonym_1 in synonym_wise_data.keys() and len(temp_pages)>0:\n",
    "                    synonym_wise_data[synonym_1].append(temp_pages)\n",
    "                    temp_permanant.append(temp_pages)\n",
    "                elif len(temp_pages)>0:\n",
    "                    synonym_wise_data[synonym_1]=[temp_pages]\n",
    "                temp_pages=[]\n",
    "                asssigned_flag='YES'\n",
    "        if asssigned_flag=='NO':\n",
    "            unassigned_pages.append(page_num)\n",
    "\n",
    "    for unass_page in unassigned_pages:\n",
    "        closest_list=''\n",
    "        closest_synonym=''\n",
    "        min_diff=100\n",
    "        for syn_2,pagass_list in synonym_wise_data.items():\n",
    "            for pag_ass in pagass_list:\n",
    "                for p_n1 in pag_ass:\n",
    "                    if p_n1<unass_page:\n",
    "                        if min_diff>unass_page-p_n1:\n",
    "                            min_diff=unass_page-p_n1\n",
    "                            closest_list=pag_ass\n",
    "                            closest_synonym=syn_2\n",
    "                        else:\n",
    "                            continue\n",
    "        if closest_synonym!='':\n",
    "            #print(closest_list)\n",
    "            for syn_2,pagass_list in synonym_wise_data.items():\n",
    "                for pag_ass in pagass_list:\n",
    "                    if syn_2==closest_synonym and pag_ass==closest_list:\n",
    "                        pag_ass.append(unass_page)\n",
    "        else:\n",
    "            if label_unidentified_entity_name in synonym_wise_data.keys():\n",
    "                synonym_wise_data[label_unidentified_entity_name].append([unass_page])\n",
    "            else:\n",
    "                synonym_wise_data[label_unidentified_entity_name]=[[unass_page]]\n",
    "\n",
    "\n",
    "    data = {part: [list(set(sublist)) for sublist in lists] for part, lists in synonym_wise_data.items()}\n",
    "    for part, part_data in data.items():\n",
    "        for sublist in part_data:\n",
    "            sublist.sort()\n",
    "    return data,synonym_info\n",
    "\n",
    "def remove_repeated_pages(data : Dict[str, List[List[int]]]) -> Dict[str, List[List[int]]]:\n",
    "    \"\"\"\n",
    "    Remove repeated page numbers from the provided data.\n",
    "\n",
    "    Args:\n",
    "        data (Dict[str, List[List[int]]]): Dictionary containing part-wise data with lists of page numbers.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List[List[int]]]: Modified dictionary with repeated page numbers removed from the lists.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_numbers = []\n",
    "    unique_numbers = set()\n",
    "    repeated_numbers = set()\n",
    "\n",
    "    for part_data in data.values():\n",
    "        for item in part_data:\n",
    "            if isinstance(item, list):\n",
    "                flat_list = item\n",
    "            else:\n",
    "                flat_list = [item]\n",
    "            all_numbers.extend(flat_list)\n",
    "\n",
    "    for num in all_numbers:\n",
    "        if num in unique_numbers:\n",
    "            repeated_numbers.add(num)\n",
    "        else:\n",
    "            unique_numbers.add(num)\n",
    "    \n",
    "    for part, part_data in data.items():\n",
    "        for sublist in part_data:\n",
    "            if isinstance(sublist, list) and len(sublist) > 1 and sublist[0] in repeated_numbers:\n",
    "                sublist.pop(0)\n",
    "    return data\n",
    "\n",
    "def store_blob(bytes_stream: bytes, file: str ,BUCKET_NAME: str) -> None:\n",
    "    \"\"\"To store PDF files in GCS\n",
    "\n",
    "    Args:\n",
    "        bytes_stream (bytes): Binary Format of pdf data\n",
    "        file (str): filename to store in specified GCS bucket\n",
    "    \"\"\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    result_bucket = storage_client.get_bucket(BUCKET_NAME)\n",
    "    document_blob = storage.Blob(name=str(file), bucket=result_bucket)\n",
    "    document_blob.upload_from_string(bytes_stream, content_type=\"application/pdf\")\n",
    "\n",
    "def save_split_pdfs(json_ocr : object, pdfs_ouput_path : str, file_name : str, synonym_tag : Optional[str]) -> None:\n",
    "    \"\"\"\n",
    "    Save split PDFs based on OCR data.\n",
    "\n",
    "    Args:\n",
    "        json_ocr (object): JSON OCR data.\n",
    "        pdfs_ouput_path (str): Output path for storing the PDFs.\n",
    "        file_name (str): Base file name for the saved PDFs.\n",
    "        synonym_tag (Optional[str]): 'YES' if synonym tagging, 'NO' otherwise.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    if synonym_tag=='YES':\n",
    "        for entity in json_ocr.entities:\n",
    "            if entity.type!=synonym_entity_name:\n",
    "                pages_new=[]\n",
    "                for p_num in entity.page_anchor.page_refs:\n",
    "                    # print(p_num)\n",
    "                    pages_new.append(p_num.page)\n",
    "                pages_image=[]\n",
    "                for page_num1 in range(len(json_ocr.pages)):\n",
    "                    if page_num1 in pages_new:\n",
    "                        pages_image.append(json_ocr.pages[page_num1].image.content)\n",
    "                folder_name=entity.type\n",
    "                synthesized_images = [decode_image(page) for page in pages_image]\n",
    "                pdf_bytes = create_pdf_from_images(synthesized_images)\n",
    "                from datetime import datetime\n",
    "                current_time = datetime.now()\n",
    "                time_stamp_1=int(current_time.timestamp())\n",
    "                file_save_path=('/').join(pdfs_ouput_path.split('/')[3:])+str(folder_name)+'/'+file_name+'_'+str(time_stamp_1)+'.pdf'\n",
    "                BUCKET_NAME=pdfs_ouput_path.split('/')[2]\n",
    "                store_blob(bytes_stream= pdf_bytes, file=file_save_path ,BUCKET_NAME=BUCKET_NAME)\n",
    "\n",
    "    elif synonym_tag=='NO':\n",
    "        synthesized_images = [decode_image(page.image.content) for page in json_ocr.pages]\n",
    "        pdf_bytes = create_pdf_from_images(synthesized_images)\n",
    "        from datetime import datetime\n",
    "        current_time = datetime.now()\n",
    "        time_stamp_1=int(current_time.timestamp())\n",
    "        file_save_path=('/').join(pdfs_ouput_path.split('/')[3:])+label_unidentified_entity_name+'/'+file_name+'_'+str(time_stamp_1)+'.pdf'\n",
    "        BUCKET_NAME=pdfs_ouput_path.split('/')[2]\n",
    "        store_blob(bytes_stream= pdf_bytes, file=file_save_path ,BUCKET_NAME=BUCKET_NAME)\n",
    "\n",
    "def decode_image(image_bytes: bytes) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Decode image bytes into a Pillow Image object.\n",
    "\n",
    "    Args:\n",
    "        image_bytes (bytes): The image bytes to be decoded.\n",
    "\n",
    "    Returns:\n",
    "        Image.Image: The Pillow Image object.\n",
    "    \"\"\"\n",
    "    \n",
    "    with io.BytesIO(image_bytes) as image_file:\n",
    "        image = Image.open(image_file)\n",
    "        image.load()\n",
    "    return image\n",
    "\n",
    "def create_pdf_from_images(images: Sequence[Image.Image]) -> bytes:\n",
    "    \"\"\"Creates a PDF from a sequence of images.\n",
    "\n",
    "    The PDF will contain 1 page per image, in the same order.\n",
    "\n",
    "    Args:\n",
    "      images: A sequence of images.\n",
    "\n",
    "    Returns:\n",
    "      The PDF bytes.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not images:\n",
    "        raise ValueError(\"At least one image is required to create a PDF\")\n",
    "\n",
    "    # PIL PDF saver does not support RGBA images\n",
    "    images = [\n",
    "        image.convert(\"RGB\") if image.mode == \"RGBA\" else image for image in images\n",
    "    ]\n",
    "\n",
    "    with io.BytesIO() as pdf_file:\n",
    "        images[0].save(\n",
    "            pdf_file, save_all=True, append_images=images[1:], format=\"PDF\"\n",
    "        )\n",
    "        return pdf_file.getvalue()\n",
    "    \n",
    "def create_splitter_entities(json_ocr : object, synonyms_list : List[str], synonym_entity_name : str) -> Tuple[object, str]:\n",
    "    \"\"\"\n",
    "    Creates splitter entities based on the identified synonyms in the OCR output.\n",
    "\n",
    "    Args:\n",
    "        json_ocr (object): The OCR output in the form of a Document AI document.\n",
    "        synonyms_list (List[str]): List of synonyms to be identified in the OCR output.\n",
    "        synonym_entity_name (str): Name to be assigned to the entity representing identified synonyms.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[object, str]: A tuple containing the updated OCR document and a tag indicating\n",
    "        whether synonyms were found ('YES') or not ('NO').\n",
    "    \"\"\"\n",
    "    \n",
    "    text_anchors_page_wise=get_text_anchors_page_wise(json_ocr)\n",
    "    synonyms_pages,synonym_info=get_synonyms_matches_pages(synonyms_list,text_anchors_page_wise,json_ocr)\n",
    "    if len(synonyms_pages)>0:\n",
    "        data=remove_repeated_pages(synonyms_pages)\n",
    "        max_page=len(text_anchors_page_wise)\n",
    "        max_page_list = max([max(sublist, default=0) for sublist in sum(data.values(), [])])\n",
    "        entities_splitter=[]\n",
    "        for synonym,pages_nested_list in data.items():\n",
    "            for pages_list in pages_nested_list:\n",
    "                temp_splitter_entity={'type_':'','text_anchor':{'text_segments':[]},'page_anchor':{'page_refs':[]}}\n",
    "                # if max_page_list in pages_list:\n",
    "                #     pages_list.extend(range(max_page_list, max_page))\n",
    "                #     # print(pages_list)\n",
    "                if len(pages_list)>=1:\n",
    "                    sorted_pages=sorted(pages_list)\n",
    "                    start_index_ent=text_anchors_page_wise[sorted_pages[0]]['start_index']\n",
    "                    end_index_ent=text_anchors_page_wise[sorted_pages[-1]]['end_index']\n",
    "                for page_2 in pages_list:\n",
    "                    temp_splitter_entity['page_anchor']['page_refs'].append({'page':page_2})\n",
    "                temp_splitter_entity['text_anchor']['text_segments'].append({'start_index':start_index_ent,'end_index':end_index_ent})\n",
    "                temp_splitter_entity['type_']=''.join(['_' if c.isspace() or not c.isalnum() else c for c in synonym])\n",
    "                entities_splitter.append(temp_splitter_entity)\n",
    "        \n",
    "        json_ocr.entities=entities_splitter\n",
    "        json_ocr=add_ent_json(json_ocr,synonym_info,synonym_entity_name)\n",
    "        synonym_tag='YES'\n",
    "    else:\n",
    "        # print('NO OUTPUT')\n",
    "        synonym_tag='NO'\n",
    "    \n",
    "    return json_ocr,synonym_tag\n",
    "\n",
    "def get_new_entity(syn_data_1 : Dict[str, Any], json_ocr : object, synonym_entity_name : str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Creates a new entity based on the provided synonym data and the OCR output.\n",
    "\n",
    "    Args:\n",
    "        syn_data_1 (Dict[str, Any]): Information about the synonym data, including text anchors and page number.\n",
    "        json_ocr (object): The OCR output in the form of a Document AI document.\n",
    "        synonym_entity_name (str): Name to be assigned to the new entity.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: A dictionary representing the new entity with mention text, page anchors, text anchors, and type.\n",
    "    \"\"\"\n",
    "    \n",
    "    text_anchors_temp= syn_data_1['text_anchors']\n",
    "    page_num=syn_data_1['page']\n",
    "    # synonym_entity_name='synonym_entity'\n",
    "    new_ent={'mention_text':'','page_anchor':{'page_refs':[{'bounding_poly':{'normalized_vertices':[]},'page': page_num}]},'text_anchor':{'text_segments': []},'type_':''}\n",
    "    entity_text_anc=[]\n",
    "    page_anc={'x':[],'y':[]}\n",
    "    for page in json_ocr.pages:\n",
    "        # print(page.page_number)\n",
    "        if page_num==page.page_number-1:\n",
    "            # print(page.page_number)\n",
    "            for token in page.tokens:\n",
    "                # print(token)\n",
    "                token_seg=token.layout.text_anchor.text_segments\n",
    "                for seg in token_seg:\n",
    "                    token_start=seg.start_index\n",
    "                    token_end=seg.end_index\n",
    "                if token_start>=text_anchors_temp['start_index']-3 and token_end<=text_anchors_temp['end_index']+2:\n",
    "                    if json_ocr.text[token_start:token_end].replace(\" \", \"\") in json_ocr.text[text_anchors_temp['start_index']:text_anchors_temp['end_index']].replace(\" \", \"\"):\n",
    "                        vertices = token.layout.bounding_poly.normalized_vertices\n",
    "                        minx_token, miny_token = min(point.x for point in vertices), min(point.y for point in vertices)\n",
    "                        maxx_token, maxy_token = max(point.x for point in vertices), max(point.y for point in vertices)\n",
    "                        entity_text_anc.append({'start_index':token_start,'end_index':token_end})\n",
    "                        page_anc['x'].extend([minx_token,maxx_token])\n",
    "                        page_anc['y'].extend([miny_token,maxy_token])\n",
    "    new_ent['mention_text']=json_ocr.text[text_anchors_temp['start_index']:text_anchors_temp['end_index']]\n",
    "    page_anchors_ent=[{'x':min(page_anc['x']),'y':min(page_anc['y'])},{'x':min(page_anc['x']),'y':max(page_anc['y'])},\n",
    "                     {'x':max(page_anc['x']),'y':min(page_anc['y'])},{'x':max(page_anc['x']),'y':max(page_anc['y'])}]\n",
    "    new_ent['page_anchor']['page_refs'][0]['bounding_poly']['normalized_vertices']=page_anchors_ent\n",
    "    new_ent['text_anchor']['text_segments']=entity_text_anc\n",
    "    new_ent['type_']=synonym_entity_name\n",
    "    \n",
    "    return new_ent\n",
    "\n",
    "def create_cde_entities(synonym_info : Dict[str, List[Dict[str, Any]]], json_ocr : object, synonym_entity_name : str) ->List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Creates CDE entities based on the synonym information and OCR output.\n",
    "\n",
    "    Args:\n",
    "        synonym_info (Dict[str, List[Dict[str, Any]]]): Information about synonyms and their occurrences in the OCR output.\n",
    "        json_ocr (object): The OCR output in the form of a Document AI document.\n",
    "        synonym_entity_name (str): Name to be assigned to the synonym entity.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: A list of dictionaries representing CDE entities with mention text, page anchors, text anchors, and type.\n",
    "    \"\"\"\n",
    "    \n",
    "    cde_entities=[] \n",
    "    for syn,tag in synonym_info.items():\n",
    "        for item in tag:\n",
    "            try:\n",
    "                cde_ent=get_new_entity(item,json_ocr,synonym_entity_name)\n",
    "                cde_entities.append(cde_ent)\n",
    "            except:\n",
    "                continue\n",
    "    return cde_entities\n",
    "\n",
    "\n",
    "def add_ent_json(json_ocr : object, synonym_info : Dict[str, List[Dict[str, Any]]], synonym_entity_name : str) -> object:\n",
    "    \"\"\"\n",
    "    Adds CDE entities to the Document AI document based on synonym information.\n",
    "\n",
    "    Args:\n",
    "        json_ocr (object): The OCR output in the form of a Document AI document.\n",
    "        synonym_info (Dict[str, List[Dict[str, Any]]]): Information about synonyms and their occurrences in the OCR output.\n",
    "        synonym_entity_name (str): Name to be assigned to the synonym entity.\n",
    "\n",
    "    Returns:\n",
    "        object : The updated Document AI document with added CDE entities.\n",
    "    \"\"\"\n",
    "    \n",
    "    cde_entities=create_cde_entities(synonym_info,json_ocr,synonym_entity_name)\n",
    "    for ent_cde in cde_entities:\n",
    "        json_ocr.entities.append(ent_cde)\n",
    "    \n",
    "    return json_ocr\n",
    "\n",
    "def main():\n",
    "    files_name_list,files_path_dict=file_names(gcs_input_uri)\n",
    "    for i in range(len(files_name_list)):\n",
    "        #print(file_name_list[i])\n",
    "        file_path='gs://'+gcs_input_uri.split('/')[2]+'/'+files_path_dict[files_name_list[i]]\n",
    "        print(file_path)\n",
    "        json_ocr=documentai_json_proto_downloader(file_path.split('/')[2],('/').join(file_path.split('/')[3:]))\n",
    "        json_ocr,synonym_tag=create_splitter_entities(json_ocr,synonyms_list,synonym_entity_name)\n",
    "        if save_split_pdfs_flag=='TRUE':\n",
    "            save_split_pdfs(json_ocr,pdfs_ouput_path,files_name_list[i],synonym_tag)\n",
    "        store_document_as_json(documentai.Document.to_json(json_ocr),gcs_output_uri.split('/')[2],('/').join(gcs_output_uri.split('/')[3:])+files_name_list[i])\n",
    "        \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b571acbf-420d-4a05-bd7e-a0a4b3de211b",
   "metadata": {},
   "source": [
    "### Output\n",
    "\n",
    "Entities will be added to jsons and saved in the output gcs path\n",
    "\n",
    "* CDE format entities with entity type as ‘synonym_entity’ as shown below\n",
    "\n",
    "\n",
    "<img src=\"./Images/cde_entity.png\" width=800 height=400 alt=\"cde_entity\"></img>\n",
    "\n",
    "* Splitter format entities added with entity type same as labels or synonyms given\n",
    "\n",
    "<img src=\"./Images/splitter_entity.png\" width=800 height=400 alt=\"splitter_entity\"></img>\n",
    "\n",
    "\n",
    "* If save_split_pdfs_flag is TRUE , then the split pdfs will be saved in gcs path provided with folder names same as labels\n",
    "\n",
    "<img src=\"./Images/folders.png\" width=800 height=400 alt=\"folders_image\"></img>\n",
    "\n",
    "If the documents doesnt have any synonyms given , then it will be saved in label_unidentified folder.\n",
    "\n",
    "The names of files will be `filename’+timestamp.pdf`\n",
    "\n",
    "<img src=\"./Images/pdf_split.png\" width=800 height=400 alt=\"pdf_split\"></img>"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
