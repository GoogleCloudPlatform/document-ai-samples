{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cfe8b0f-ad69-425f-b353-1f2844e40198",
   "metadata": {},
   "source": [
    "# DocAI JSON Split Address Lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6c0612-e3d7-4c3f-8dbc-a14b03477c2d",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baed9602-e9d8-4c73-a48f-ad0bb66cd51d",
   "metadata": {},
   "source": [
    "## Disclaimer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55fa4c2-dc99-4231-9a29-886791506a2e",
   "metadata": {},
   "source": [
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the DocAI Incubator Team. No guarantees of performance are implied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea17170-6972-4681-a8de-38a1695ef0d0",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b533411e-4fa3-4020-abc7-80211d69e24f",
   "metadata": {},
   "source": [
    "This tool is a post processing script which splits the combined address into multiple addresses. In the parsed sample json file it is observed that there exists an address field ‘shipt_to_address’ and it needs to be split into multiple ‘ship_to_address_line’ address entities . This can be achieved by splitting the address lines into multiple address elements in the json. The json Entity keys Normalized Vertices and Text Segments indexes are to be updated properly with correct values when the address line is splitted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333c4db3-0528-4fe7-a956-6caadc9ad037",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdd2242-e96d-43c6-98a5-51cc74775b81",
   "metadata": {},
   "source": [
    "* Vertex AI Notebook.\n",
    "* Storage Bucket for storing input PDF files and output JSON files.\n",
    "* Permission For Google Storage and Vertex AI Notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e312c80b-64c1-4e4e-af46-8e9a9a0ed547",
   "metadata": {},
   "source": [
    "## Step by Step procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a04544-16f1-41bf-92e8-e62118f30321",
   "metadata": {},
   "source": [
    "### 1.Importing required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7272c657-c45b-4bc2-89d3-322456879dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download incubator-tools utilities module to present-working-directory\n",
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0b5aa0-5002-4a94-a2a2-f919d8548aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-cloud-storage google-cloud-documentai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c54b538-0f60-485d-a89b-b76ff320fffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utilities\n",
    "from io import BytesIO\n",
    "from pprint import pprint\n",
    "import json, copy\n",
    "from pprint import pprint\n",
    "from google.cloud import storage\n",
    "from google.cloud import documentai_v1beta3 as documentai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b80bd9b-63bb-410d-8276-f026fce6d681",
   "metadata": {},
   "source": [
    "### 2.Input and Output Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cf9e6f-3d46-480c-9ac6-942fdacbf2d5",
   "metadata": {},
   "source": [
    "Give the input and output gcs path.\n",
    "\n",
    "* **input_path**: GCS Storage name. It should contain DocAI processed output json files. This bucket is used for processing input files and saving output files in the folders.\n",
    "* **output_path**: GCS URI of the folder, where the dataset is exported from the processor.\n",
    "* **entity_name** : entity_name that needs to be splitted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e7f9cd-b232-4b5f-abe4-d5a5c012a912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT : storage bucket name\n",
    "input_path = \"gs://xxxx_xxxxxxx_xxxxxxxx/Processed/\"  # path should end with '/'\n",
    "# OUTPUT : storage bucket's path\n",
    "output_path = (\n",
    "    \"gs://xxxx_xxxxxxx_xxxxxxxx/Processed_Splitter/\"  # path should end with '/'\n",
    ")\n",
    "# Entity Name that needs to be splitted\n",
    "entity_name = \"ship_to_address_line\"\n",
    "\n",
    "\n",
    "input_storage_bucket_name = input_path.split(\"/\")[2]\n",
    "input_bucket_path_prefix = \"/\".join(input_path.split(\"/\")[3:])\n",
    "output_storage_bucket_name = output_path.split(\"/\")[2]\n",
    "output_bucket_path_prefix = \"/\".join(output_path.split(\"/\")[3:])\n",
    "\n",
    "\n",
    "storage_client = storage.Client()\n",
    "source_bucket = storage_client.bucket(input_storage_bucket_name)  # storage bucket name\n",
    "source_blob = source_bucket.list_blobs(\n",
    "    prefix=input_bucket_path_prefix\n",
    ")  # storage bucket's sub folders path\n",
    "destination_bucket = storage_client.bucket(output_storage_bucket_name)\n",
    "\n",
    "list_of_files = []\n",
    "for blob in source_blob:\n",
    "    if blob.name.endswith(\".json\"):\n",
    "        list_of_files.append(blob.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37e9832-78eb-4219-86bb-d66129eaba49",
   "metadata": {},
   "source": [
    "* **NOTE**: The output entity will be entity_name_line for e.g. for ship_to_address the o/p will be ship_to_address_line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e09366-e703-49a7-8416-40d74d021d15",
   "metadata": {},
   "source": [
    "### 3.Run the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0632a893-9a2c-489e-8de7-515960cc0379",
   "metadata": {},
   "source": [
    "Copy the code provided in this document, Enter the paths and list of entity type names as described in previous steps. The full code is mentioned in the last section of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d592bb-e98f-4b3c-9100-40b43f344b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_anchor_fix_new(document: object) -> object:\n",
    "    \"\"\"\n",
    "    This function takes document as input and splits the address into categories and returns document as output.\n",
    "\n",
    "    Args:\n",
    "         document (object): The document containing entities to be processed.\n",
    "\n",
    "    Returns:\n",
    "         document (object) : The updated document with address being splitted.\n",
    "    \"\"\"\n",
    "\n",
    "    token_range = {}\n",
    "    for page_number, page in enumerate(document.pages):\n",
    "        for token_number, token in enumerate(page.tokens):\n",
    "            start_index = (\n",
    "                int(token.layout.text_anchor.text_segments[0].start_index)\n",
    "                if token.layout.text_anchor.text_segments\n",
    "                else 0\n",
    "            )\n",
    "            end_index = (\n",
    "                int(token.layout.text_anchor.text_segments[0].end_index)\n",
    "                if token.layout.text_anchor.text_segments\n",
    "                else 0\n",
    "            )\n",
    "            token_range[(start_index, end_index)] = {\n",
    "                \"pageNumber\": page_number,\n",
    "                \"tokenNumber\": token_number,\n",
    "            }\n",
    "\n",
    "    for entity in document.entities:\n",
    "        if entity.type_ == entity_name:\n",
    "            start = int(entity.text_anchor.text_segments[0].start_index)\n",
    "            end = int(entity.text_anchor.text_segments[0].end_index) - 1\n",
    "\n",
    "            lower_token, upper_token = None, None\n",
    "            for start_rng, end_rng in token_range:\n",
    "                if start >= start_rng and start < end_rng:\n",
    "                    lower_token = token_range[(start_rng, end_rng)]\n",
    "                if end >= start_rng and end < end_rng:\n",
    "                    upper_token = token_range[(start_rng, end_rng)]\n",
    "\n",
    "            if lower_token and upper_token:\n",
    "                lower_token_data = (\n",
    "                    document.pages[lower_token[\"pageNumber\"]]\n",
    "                    .tokens[lower_token[\"tokenNumber\"]]\n",
    "                    .layout.bounding_poly.normalized_vertices\n",
    "                )\n",
    "                upper_token_data = (\n",
    "                    document.pages[upper_token[\"pageNumber\"]]\n",
    "                    .tokens[upper_token[\"tokenNumber\"]]\n",
    "                    .layout.bounding_poly.normalized_vertices\n",
    "                )\n",
    "                # print(\"----------------------\")\n",
    "                # print(lower_token_data)\n",
    "                # print(\"***********************\")\n",
    "                # print(upper_token_data)\n",
    "                # for A\n",
    "                xA = float(lower_token_data[0].x)\n",
    "                yA = float(lower_token_data[0].y)\n",
    "                xA_ = float(upper_token_data[0].x)\n",
    "                yA_ = float(upper_token_data[0].y)\n",
    "                # for B\n",
    "                xB = float(lower_token_data[1].x)\n",
    "                yB = float(lower_token_data[1].y)\n",
    "                xB_ = float(upper_token_data[1].x)\n",
    "                yB_ = float(upper_token_data[1].y)\n",
    "                # for C\n",
    "                xC = float(lower_token_data[2].x)\n",
    "                yC = float(lower_token_data[2].y)\n",
    "                xC_ = float(upper_token_data[2].x)\n",
    "                yC_ = float(upper_token_data[2].y)\n",
    "                # for D\n",
    "                xD = float(lower_token_data[3].x)\n",
    "                yD = float(lower_token_data[3].y)\n",
    "                xD_ = float(upper_token_data[3].x)\n",
    "                yD_ = float(upper_token_data[3].y)\n",
    "\n",
    "                A = {\"x\": min(xA, xA_), \"y\": min(yA, yA_)}\n",
    "                B = {\"x\": max(xB, xB_), \"y\": min(yB, yB_)}\n",
    "                C = {\"x\": max(xC, xC_), \"y\": max(yC, yC_)}\n",
    "                D = {\"x\": min(xD, xD_), \"y\": max(yD, yD_)}\n",
    "                entity.page_anchor.page_refs[0].bounding_poly.normalized_vertices = [\n",
    "                    A,\n",
    "                    B,\n",
    "                    C,\n",
    "                    D,\n",
    "                ]\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fd19f8-caa2-485e-b5cd-4625112514df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(list_of_files)):\n",
    "    print(\"\\nProcessing >>> \", list_of_files[i])\n",
    "    js = json.loads(\n",
    "        source_bucket.blob(list_of_files[i]).download_as_string().decode(\"utf-8\")\n",
    "    )\n",
    "    document1 = documentai.Document.from_json(json.dumps(js))\n",
    "\n",
    "    addr_content = \"\"\n",
    "    addr_splits = []\n",
    "\n",
    "    copy_enty = {}\n",
    "\n",
    "    start_idx_list = []\n",
    "    end_idx_list = []\n",
    "\n",
    "    for entity in document1.entities:\n",
    "        if entity.type_ == entity_name:\n",
    "            copy_enty = entity\n",
    "            addr_content = entity.text_anchor.content\n",
    "            addr_splits = addr_content.strip().split(\"\\n\")\n",
    "\n",
    "            ts = entity.text_anchor.text_segments\n",
    "            for start_end_idx in ts:\n",
    "                start_idx_list.append(start_end_idx.start_index)\n",
    "                end_idx_list.append(start_end_idx.end_index)\n",
    "\n",
    "    start_index1 = int(min(start_idx_list))\n",
    "    end_index1 = int(max(end_idx_list))\n",
    "\n",
    "    try:\n",
    "        for x in range(0, len(addr_splits)):\n",
    "            enty = copy.deepcopy(copy_enty)\n",
    "            del enty.id\n",
    "            enty.mention_text = addr_splits[x]\n",
    "            enty.text_anchor.content = addr_splits[x]\n",
    "            enty.type_ = enty.type_ + \"_line\"\n",
    "\n",
    "            s_idx = document1.text.find(addr_splits[x], start_index1, end_index1)\n",
    "            e_idx = s_idx + len(addr_splits[x]) + 1\n",
    "            start_index1 = start_index1 + len(addr_splits[x])\n",
    "            # start_idx = s_idx+int(min(start_idx_list))\n",
    "            # end_idx = e_idx+int(min(start_idx_list))\n",
    "            # print('\\n >>> start_idx , end_idx>>>> ',s_idx , e_idx)\n",
    "            enty.text_anchor.text_segments = [\n",
    "                {\"end_index\": e_idx, \"start_index\": s_idx}\n",
    "            ]\n",
    "            document1.entities.append(enty)\n",
    "\n",
    "        for entity in document1.entities:\n",
    "            if entity.type_ == entity_name:\n",
    "                document1.entities.remove(entity)\n",
    "\n",
    "        document2 = page_anchor_fix_new(document1)\n",
    "\n",
    "        utilities.store_document_as_json(\n",
    "            documentai.Document.to_json(document2),\n",
    "            output_storage_bucket_name,\n",
    "            output_bucket_path_prefix + \"_\" + list_of_files[i].split(\"/\")[-1],\n",
    "        )\n",
    "\n",
    "        print(\"\\nCompleted\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            \">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\"\n",
    "            + list_of_files[i]\n",
    "            + \" was not processed successfully!!!\"\n",
    "        )\n",
    "        # print(e)\n",
    "        traceback_str = traceback.format_exc()\n",
    "        # print(traceback_str)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca13e7fd-28e8-4d7e-a0c6-d3c7b08a3bd8",
   "metadata": {},
   "source": [
    "### 4.Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75973512-e2da-43f0-bbb3-020737e5c84f",
   "metadata": {},
   "source": [
    "The post processed json field can be found in the storage path provided by the user during the script execution that is **output_bucket_path**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66be6820-0d3a-4b81-9f0d-242bd74c460f",
   "metadata": {},
   "source": [
    "### 5.Comparison Between Input and Output File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e1f550-8cc4-4d97-afe7-4accd60328d0",
   "metadata": {},
   "source": [
    "#### Post processing results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0b1b22-9e49-4669-80c2-e8f10baae16f",
   "metadata": {},
   "source": [
    "Upon running the post processing script against input data. The resultant output json data is obtained. The following table highlights the differences for following elements in the json document.\n",
    "* Address\n",
    "* Normalized Vertices\n",
    "* Text Segment indexes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0630f8-514b-4973-bf45-a805dd673b51",
   "metadata": {},
   "source": [
    "<img src=\"./images/json_sample_2.png\" width=800 height=400></img>\n",
    "<img src=\"./images/json_sample_2.png\" width=800 height=400></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fbcd88-05b8-4f0d-82d5-7f44b32700e4",
   "metadata": {},
   "source": [
    "##### When the output json document is imported into the processor, it is observed that the address is now a multiple entity and has the bounding boxes as shown:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f607ad5e-4b90-4f62-b1ed-8035a7c7e2d0",
   "metadata": {},
   "source": [
    "<img src=\"./images/address_sample.png\" width=800 height=400></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c25a590-4d48-40c8-930f-e8687e51db02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m104",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m104"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
