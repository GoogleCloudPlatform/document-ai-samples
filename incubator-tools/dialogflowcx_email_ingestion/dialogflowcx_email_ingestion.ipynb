{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d47cdb1-faaa-440d-8d84-16b744f5a142",
   "metadata": {},
   "source": [
    "# DocAI Dialogflow CX & Email Ingestion Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90310798-0f96-40ad-b9c7-e2d5ed160e33",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf30071-b8eb-45d0-987b-cbe4b2c26549",
   "metadata": {},
   "source": [
    "# Disclaimer\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the **DocAI Incubator Team**. No guarantees of performance are implied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32ee87f-8924-4ac7-9835-68c04d4173cc",
   "metadata": {},
   "source": [
    "# Objective: \n",
    "\n",
    "1. Dialogflow Email Integration tool which can be used to fetch the mail, analyze the body to get the relevant gcp services  and send a response with the help of dialogflow agent as a mail. [DocAI Dialogflow CX]\n",
    "\n",
    "2. DocAI Email ingestion via Cloud function which can be used to ingest all the emails with their body, subject and the attachments and store it in the cloud bucket. [Email Ingestion Tool]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2aa424-25e5-4ff7-982f-57ab5bd6c9f1",
   "metadata": {},
   "source": [
    "# DocAI Dialogflow CX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0896ac-21c2-4467-ad1a-f04d30181f13",
   "metadata": {},
   "source": [
    "## Step 1 : Enable Email Communication with a Dialogflow CX Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3357ad6-84c4-4e78-b561-01e83b975b7d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "\n",
    "In this guide, you learn how to configure Google Cloud services for email communication with a Dialogflow CX Agent.\n",
    "\n",
    "This solution leverages Gmail API Push Notifications to watch a Gmail or Google Workspace inbox that has been created as an email receiver for   Dialogflow CX agent.  you deploy a Cloud Function to handle communication between Gmail & Dialogflow, and Pub/Sub are configured to trigger the Cloud Function when new emails arrive. Cloud Firebase in Datastore Mode stores information related to messages, conversation threads, and corresponding Dialogflow sessions.\n",
    "\n",
    "<img src='./images/dfcx_1.png' width=800 height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae541f58-18b9-40c4-8520-748ed10e2bb5",
   "metadata": {},
   "source": [
    "### Prequisites :\n",
    "\n",
    "* **Cloud Pub/Sub** : For triggering cloud function whenever new mail is received.\n",
    "* **Gmail API** : For reading and sending the emails. \n",
    "* **Cloud Scheduler** : To schedule the cloud function for refreshing the token.\n",
    "* **Cloud Functions (NodeJs)** : Using as backend to connect all the services \n",
    "* **Cloud Firestore** : A database storing hyperlinks for the gcp products and services which can be used when sending responses to the user.\n",
    "* **Dialogflow CX** : To send the response according to the user query sent over email.\n",
    "* **AutoML Natural Language - Entity Extraction** : Trained a model for the extraction of the signature of the sender from a mail.\n",
    "* **AutoML Natural Language - Text Classification** : Trained a model to determine which GCP products and services are relevant to the questions asked by the users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbda560d-0eb9-4523-b5cb-8c1111c5a139",
   "metadata": {},
   "source": [
    "## Step 2 : Create a GCP Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af498bcf-ab8d-48f6-9dee-8896dcfa48b6",
   "metadata": {},
   "source": [
    "Create a new GCP Project or use an existing project that contains the Dialogflow CX Agent to be enabled for email conversations. For this guide, an agent export is available in this repository that  you can upload to Dialogflow CX.\n",
    "### Enable Google Cloud APIs\n",
    "Enable the following APIs in the project.\n",
    "* Cloud Functions API\n",
    "* Cloud Build API\n",
    "* Cloud Firestore\n",
    "* Cloud Pub/Sub\n",
    "* AutoML Natural Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9df59b-67ef-40e3-9b29-09392ddc4111",
   "metadata": {},
   "source": [
    "## Step 3 : Download the Application Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba33e43c-7b2e-44d0-82ec-23920bc44bf1",
   "metadata": {},
   "source": [
    "Cloud Shell provides  us command-line access to   Google Cloud Platform resources directly from the browser.  you can use Cloud Shell to execute the terminal commands required to deploy this solution.\n",
    "\n",
    "To open Google Cloud Shell, click the Activate Cloud Shell button on the top blue horizontal bar. A new panel appears at the bottom of the screen:\n",
    "<img src='./images/dfcx_3.png' width=800 height=400>\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/GoogleCloudPlatform/dialogflow-email-agent-demo.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfeba5f-d1d7-4770-880e-e26110c1504c",
   "metadata": {},
   "source": [
    "## Step 4 : Restore Dialogflow CX Demo Support Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdfde4e-126c-4975-abd4-3aebc2e409b4",
   "metadata": {},
   "source": [
    "First, create a new Dialogflow CX agent by following the instructions here: Create an Agent. Then, follow the instructions to restore an agent using the agent export available in this repository: dialogflow-support-agent.blob. Name the agent something like \"GCP Support Agent\" and leave the defaults, using us-central1 as the location. Cloud Shell allows us to download files -  you can use this feature to download dialogflow-support-agent.blob and upload into Dialogflow CX.\n",
    "\n",
    "This is what the agent should look like after restore from dialogflow-support-agent.blob.\n",
    "\n",
    "<img src='./images/dfcx_4.png' width=800 height=400>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab166e24-8176-4236-a2e2-5f16456113b2",
   "metadata": {},
   "source": [
    "## Step 5 : Train ML Models Using AutoML Natural Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2dc379-36fb-40bb-8c7f-28ac1f154f8d",
   "metadata": {},
   "source": [
    "### Email Signature Extraction\n",
    "\n",
    "The BC3: British Columbia Conversation Corpora is a public dataset of emails containing signatures that you can use to train a basic machine learning model for email signature extraction. This model is for demo purposes only - ideally there would be a larger quantity of training data to improve the signature recognition capability.Incubator team already prepared annotated training data in a format that AutoML Natural Language Entity Extraction accept. Learn more about preparing data for AutoML here.\n",
    "\n",
    "If  you need to create our own json file using the raw .xml file provided by the University of British Columbia,  you can follow along with the Colab Notebook included in this repository: Training_Data_for_Signature_Extraction.ipynb. Once  you have the .json file uploaded to AutoML,  you need to use AutoML to annotate all of the signatures in the dataset or submit a data labeling request.\n",
    "\n",
    "Otherwise, continue with the instructions to use the annotated data you provided to us.\n",
    "1. Create a storage bucket and upload the email signature training data to Cloud Storage using the following commands. \n",
    "\n",
    "```bash\n",
    "export BUCKET_NAME=new-storage-bucket-name\n",
    "cd dialogflow-email-agent-demo\n",
    "unzip bc3_annotated_email_data.zip\n",
    "sed \"s/ -storage-bucket/$BUCKET_NAME/g\" ./bc3_annotated_email_data/text_extraction_template.csv > ./bc3_annotated_email_data/text_extraction.csv\n",
    "gsutil mb gs://$BUCKET_NAME\n",
    "gsutil -m cp -r bc3_annotated_email_data gs://$BUCKET_NAME/bc3_annotated_email_data\n",
    "```\n",
    "\n",
    "2. Navigate to Natural Language > AutoML Entity Extraction > Get Started using the Cloud Console navigation menu.\n",
    "<img src='./images/dfcx_5_2.png' width=800 height=400>\n",
    "\n",
    "3. Create a new dataset with a name like bc3_email_data for AutoML Entity Extraction. Select the bc3_annotated_email_data/text_extraction.csv file in the cloud storage location from the step above to import the training data. The csv file points to the individual json files that were also uploaded to the cloud storage bucket.\n",
    "<img src='./images/dfcx_5_3i.png' width=800 height=400>  \n",
    "<img src='./images/dfcx_5_3ii.png' width=800 height=400>  \n",
    "\n",
    "4. Once the dataset has been created, navigate to the Train tab and train a new model. Leave the box checked to deploy the model after training finishes.\n",
    "<img src='./images/dfcx_5_4.png' width=800 height=400>  \n",
    "5. When training completes, you able to test and deploy the model for integration into our email application using the Test & Deploy tab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6300bca2-5b84-4523-9ef9-2849b6e085f8",
   "metadata": {},
   "source": [
    "## Step 6 : Email Topic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865031d3-29c2-4164-890d-27fdd88223f0",
   "metadata": {},
   "source": [
    "\n",
    "Next you trains a model in our project to classify emails and determine which GCP products and services are relevant to the questions asked by your users. This is safe to do while the entity extraction model above is training. Training can take up to an hour to complete. Google Cloud provides public datasets that can be used to create training datasets for some problems. Here you can use the public StackOverflow dataset filtered on posts with Google or Dialogflow in the tag. These posts are similar in nature to support emails that you receive for Google Cloud products and services.\n",
    "\n",
    "If  you would like to see how our training data was formatted for AutoML Natural Language,  you can follow along with the Colab Notebook included in this repository: StackOverflow_Topic_Classification.ipynb. Labels that have less than 100 items needs to be removed.\n",
    "\n",
    "Otherwise, continue with the instructions to use the prelabeled dataset that you exported from our demo environment.\n",
    "\n",
    "1. Create a storage bucket and upload the StackOverflow training data to Cloud Storage using the following commands.\n",
    "```bash\n",
    "export BUCKET_NAME=new-storage-bucket-name\n",
    "cd dialogflow-email-agent-demo\n",
    "unzip stackoverflow_train_data.zip\n",
    "sed \"s/ -storage-bucket/$BUCKET_NAME/g\" ./stackoverflow_train_data/text_classification_template.csv > ./stackoverflow_train_data/text_classification.csv\n",
    "gsutil mb -l us-central1 gs://$BUCKET_NAME\n",
    "gsutil -m cp -r stackoverflow_train_data gs://$BUCKET_NAME/stackoverflow_train_data\n",
    "``` \n",
    "\n",
    "2. Navigate to Natural Language > AutoML Text & Document Classification using the Cloud Console navigation menu. Create a new dataset for Multi-label Classification and give it a name like stackoverflow_topic_classifier.  \n",
    "<img src='./images/dfcx_6_2.png' width=800 height=400>  \n",
    "\n",
    "3. Import the training data by browsing to the text_classification.csv file in the storage bucket that  you created in the prior step.  \n",
    "<img src='./images/dfcx_6_3.png' width=800 height=400>  \n",
    "\n",
    "4. Once the data has been imported, navigate to the Train tab and train a new model. Leave the box checked to deploy the model after training finishes.  \n",
    "<img src='./images/dfcx_6_4.png' width=800 height=400>  \n",
    "\n",
    "5. When training completes,  you are able to test and deploy the model for integration into our email application using the Test & Deploy tab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4593a779-386e-4d5f-bad9-88f361ea6271",
   "metadata": {},
   "source": [
    "## Step 7 : Configure Pub/Sub and Gmail Push Notifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7b87c9-c6de-45dc-b293-c695fd158536",
   "metadata": {},
   "source": [
    "### Create a PubSub Topic\n",
    "\n",
    "In this section,  you create a PubSub Topic to receive the Gmail API Push Notifications.\n",
    "\n",
    "1. In the GCP Project, use the Navigation menu to locate Pub/Sub and create a new Topic. For this guide,you can use gmail-inbox-watch. Uncheck the box to “Add a default subscription.”  you can create a subscription for Cloud Functions in a later step.  \n",
    "<img src='./images/dfcx_7_1.png' width=800 height=400>  \n",
    "\n",
    "2. Additionally,  you must give the Gmail API permission to send messages to   Pub/Sub topic: click the context menu of the topic  you just created (three vertical dots), and choose View permissions.  \n",
    "<img src='./images/dfcx_7_2.png' width=800 height=400>  \n",
    "\n",
    "3. Click Add members, specify gmail-api-push@system.gserviceaccount.com as a new member, and give it the role of Pub/Sub > Pub/Sub Publisher; lastly, click Save to apply the changes.  \n",
    "<img src='./images/dfcx_7_3.png' width=800 height=400>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dac1e42-cd13-4622-8550-b118aae99b70",
   "metadata": {},
   "source": [
    "## Step 8 : Create or Use Existing Gmail or Google Workspace Account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464b7602-9371-42e2-9d0f-0d3a0fe7bfda",
   "metadata": {},
   "source": [
    "Create or use an existing Gmail or Google Workspace account to act on behalf of our Dialogflow CX Agent. You can  communicate with our agent by sending an email to this address, and our application sends a response using this account and the Gmail API.\n",
    "\n",
    "In the next section,  you see how to configure access for the application to this Gmail or Google Workspace account inbox.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bcb86d-453c-469b-ad7b-fad2f0866a97",
   "metadata": {},
   "source": [
    "## Step 9 : Enable Gmail API & Create an OAuth 2.0 Client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bb1fb2-438a-4d70-b67d-a96727010ab8",
   "metadata": {},
   "source": [
    "In this section,  you enable the Gmail API in   GCP Project, create an OAuth 2.0 client, and configure Gmail Push Notifications against  our receiving inbox for Dialogflow CX integration. This guide leverages some steps outlined in Implementing Server-Side Authorization and Push Notifications.\n",
    "\n",
    "To get started using Gmail API,  you need to enable the API and create credentials for the application.\n",
    "\n",
    "1. Select   GCP Project in the first window.  \n",
    "\n",
    "2. Navigate to APIs & Services > Credentials and select \"Create Credentials\" and \"Help Me Choose\" to create OAuth 2.0 credentials. Specify the Gmail API and your  Data to create an OAuth 2.0 Client. Then, click Next.  \n",
    "<img src='./images/dfcx_9_2.png' width=800 height=400>  \n",
    "\n",
    "3. In the next screen, give the App a name and provide a support email address and developer contact address.  \n",
    "<img src='./images/dfcx_9_3.png' width=800 height=400>  \n",
    "\n",
    "4. In the next screen, continue without defining Scopes. Scopes are defined in the application code when authorization is requested.  \n",
    "<img src='./images/dfcx_9_4.png' width=800 height=400>  \n",
    "\n",
    "Next, select the Desktop app as the Application Type and give it a name like “Desktop Authorization App.”\n",
    "\n",
    "**NOTE**:  In this example, you perform 2 one-time authorizations locally to create the access & refresh tokens, which are included in our deployments to GCP. One authorization is for a Python Scheduled Task to renew the Gmail Push Notification & Pub/Sub integration daily. The second authorization is for the NodeJs app that are parsing & sending emails on behalf of   Dialogflow CX agent.\n",
    "\n",
    "Only the one-time authorizations are needed to establish the connection between   Gmail or Google Workspace inbox for the application unless access scopes are changed or the OAuth Client Id changes.\n",
    "\n",
    "Next, click Download to save the OAuth client secret for use in a later step. Then click Done.  \n",
    "<img src='./images/dfcx_9_5.png' width=800 height=400>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed1237c-565b-48fa-9b8f-aebbc189eebb",
   "metadata": {},
   "source": [
    "## Step 10: Enable Gmail Push Notifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4141e80-fd39-44d9-9ed0-f5c5d0567161",
   "metadata": {},
   "source": [
    "In this section,  you execute a Python script locally to generate an access token for the Gmail API and enable Gmail Push Notifications to the Pub/Sub Topic created earlier.\n",
    "\n",
    "1. Locate the Oauth client secrets file downloaded in the prior step and upload the file to Cloud Shell.  \n",
    "<img src='./images/dfcx_10_1.png' width=800 height=400>  \n",
    "\n",
    "2. The file uploaded to the home directory in Cloud Shell. Let’s rename the file and move it to the working directory for this section with the following command.  \n",
    "```bash\n",
    "cp ~/client_secret_*.apps.googleusercontent.com.json ~/dialogflow-email-integration/enable_push_notifications/client_credentials.json\n",
    "```\n",
    "\n",
    "3. Next, open the Cloud Shell Editor and the file dialogflow-email-integration/enable_push_notifications/config.yaml. Update GMAIL_ID with the email address for the Gmail or Google Workspace inbox  you use for Dialogflow integration. Save the change and return to the Cloud Shell Terminal.  \n",
    "<img src='./images/dfcx_10_3.png' width=800 height=400>  \n",
    "\n",
    "4. Run the following commands to authorize the Gmail API and enable Push Notifications to Pub/Sub. Follow the prompts to allow access to the email account specified.\n",
    "```bash\n",
    "cd ~/dialogflow-email-agent-demo/gmail_push_notifications\n",
    "pip3 install -r requirements.txt\n",
    "export GCP_PROJECT= -project-id\n",
    "export PUBSUB_TOPIC=gmail-inbox-watch\n",
    "export GMAIL_ID= -dialogflow-inbox@gmail.com\n",
    "python3 -c 'import main; main.main()'\n",
    "```\n",
    "\n",
    "<img src='./images/dfcx_10_4i.png' width=800 height=400>  \n",
    "\n",
    "A response like the following shows that you are successful. Notice that a token.json was stored in the working directory. This contains the refresh token for our server-side application.\n",
    "\n",
    "<img src='./images/dfcx_10_4ii.png' width=800 height=400>  \n",
    "\n",
    "```bash\n",
    "cd ~/dialogflow-email-agent-demo/gmail_push_notifications\n",
    "gcloud functions deploy renew-gmail-watch --entry-point main --runtime python39 --trigger-topic renew-gmail-watch --env-vars-file config.yaml --project  -project-id\n",
    "```\n",
    "\n",
    "Navigate to Cloud Scheduler and create a new job. Choose a location based on   needs, a daily frequency (the screenshot below shows daily at 1am CDT)\n",
    "\n",
    "Configure the following attributes as shown in the screenshot below.  \n",
    "<img src='./images/dfcx_10_4iii.png' width=800 height=400>  \n",
    "\n",
    "Configure advanced settings and deploy.  \n",
    "<img src='./images/dfcx_10_4iv.png' width=800 height=400>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f103c332-05c2-424c-9a49-77c3d01f3de0",
   "metadata": {},
   "source": [
    "## Step 11 : Deploy a NodeJS Email Integration Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3627b4-2990-45ae-a16e-a02891f356f8",
   "metadata": {},
   "source": [
    "Finally you need to deploy a NodeJS application using Cloud Functions that handles processing of emails between Dialogflow and Gmail. The source code for the NodeJS integration service can be found in ~/dialogflow-email-agent-demo/df_integration_service.\n",
    "\n",
    "1. First, run the following command to copy the OAuth client credentials into this directory, which is used to generate a token for the Gmail API.\n",
    "```bash\n",
    "cp ~/client_secret_*.apps.googleusercontent.com.json ~/dialogflow-email-agent-demo/df_integration_service/credentials.json\n",
    "```\n",
    "\n",
    "2. Update ~/dialogflow-email-agent-demo/df_integration_service/config.yaml     with the appropriate values using the Cloud Shell Editor. Here is an explanation of the variables that need to be populated.\n",
    "\n",
    "* **GMAIL_ID** : The email address of the agent / support gmail inbox.\n",
    "* **GCP_PROJECT** :   GCP project id.\n",
    "* **LOCATION** : The location of the agent which can be found in the Dialogflow CX console. ex. us-central1\n",
    "* **AGENT_ID** : The id of an agent which can be found in the Dialogflow CX console.\n",
    "* **SUBJECT_KEY** : The subject you needs to use in order for a response to be sent from the application. This prevents unwanted emails from being sent by our application.\n",
    "* **ENTITY_EXTRACT_MODEL_ID** : The id of the entity extraction model which can be found in the AutoML Natural Language console for the deployed model.\n",
    "* **TOPIC_CLASSIFY_MODEL_ID** : The id of the topic classification model which can be found in the AutoML Natural Language console for the deployed model.\n",
    "\n",
    "3. Next, execute the following commands to generate the access token. If the token.json file has been generated then  you are ready to deploy.\n",
    "\n",
    "```bash\n",
    "cd ~/dialogflow-email-agent-demo/df_integration_service/  \n",
    "export GMAIL_ID= -agent-email@gmail.com  \n",
    "export GCP_PROJECT= -project-id  \n",
    "export LOCATION=us-central1  \n",
    "npm install node \n",
    "const SCOPES = ['https://www.googleapis.com/auth/gmail.readonly', 'https://www.googleapis.com/auth/gmail.compose']; \n",
    "var gmailHelper = require('./gmail_auth_helper.js'); \n",
    "const gmail = gmailHelper.newClient('credentials.json', SCOPES); \n",
    "```\n",
    "\n",
    "4. Finally, execute the following to deploy the Cloud Function.\n",
    "\n",
    "```bash\n",
    "gcloud functions deploy main --runtime=nodejs14 --trigger-topic=gmail-inbox-watch --env-vars-file=config.yaml --project=$GCP_PROJECT\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8217c9-5fc9-4c8b-a4cd-fdefa4b62d3e",
   "metadata": {},
   "source": [
    "## Step 12 : Upload Knowledgebase of Reference Links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06752a5c-f3cc-47dc-be1a-f96f36546f6b",
   "metadata": {},
   "source": [
    "When our AutoML model discovers relevant GCP products within the incoming email, you can lookup reference materials for these products using Cloud Firestore in Database Mode. An exported Datastore entity has been included with the repository for upload to Cloud Datastore to simplify setup of this demo.\n",
    "\n",
    "1. First, upload the demo knowledge base to Cloud Storage.\n",
    "```bash\n",
    "export BUCKET_NAME=new-storage-bucket-name\n",
    "cd edialogflow-email-agent-demo\n",
    "unzip datastore-knowledgebase.zip\n",
    "gsutil mb -l us-central1 gs://$BUCKET_NAME\n",
    "gsutil -m cp -r datastore-knowledgebase gs://$BUCKET_NAME/datastore-knowledgebase\n",
    "```\n",
    "\n",
    "2. Next, navigate to Datastore from the console navigation menu. Use the Import/Export pane to import the knowledgebase data that  you uploaded to Cloud Storage in the prior step. Specify knowledgeBase as the Datastore Kind when performing this import. See the screenshot below.\n",
    "<img src='./images/dfcx_12.png' width=800 height=400>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57c8a31-3fcf-4342-8c46-3b7757fe9c67",
   "metadata": {},
   "source": [
    "## Step 13 : Test the Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ca58ad-4fea-4c57-89ad-778c2c0ab3ee",
   "metadata": {},
   "source": [
    "1. Send an email to the agent email address with the subject that  you specified as the \"SUBJECT_KEY\" in the section above. Try the following message:\n",
    "Hi,\n",
    "\n",
    "I'm having problems with an App Engine service and Cloud Storage bucket. My application isn't authenticating users correctly. I'd also like to cancel a prior request that I had created.\n",
    "\n",
    "Thanks, Greg\n",
    "\n",
    "2. The agent should respond with something like…\n",
    "<img src='./images/dfcx_13_2.png' width=800 height=400>  \n",
    "\n",
    "3. Provide a reference number for the request to cancel.\n",
    "I'd like to cancel request #123456\n",
    "Best, Greg\n",
    "\n",
    "4.The final response from the agent should look something like this.\n",
    "<img src='./images/dfcx_13_4.png' width=800 height=400>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2315dd3e-1d8b-47e2-b34b-3323e4965310",
   "metadata": {},
   "source": [
    "# Email Ingestion Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e578ea-90d9-4945-987d-01a8b6fc41ce",
   "metadata": {},
   "source": [
    "<img src='./images/eit.png' width=800 height=400>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c94d289-d03e-44d4-af29-d744d330c420",
   "metadata": {},
   "source": [
    "### Prequisites :\n",
    "* Gmail API\n",
    "* Cloud Scheduler\n",
    "* Cloud Functions (Python)\n",
    "\n",
    "### Enable Google Cloud APIs\n",
    "Enable the following APIs in the project.\n",
    "* Cloud Functions API\n",
    "* Cloud Scheduler \n",
    "* Gmail API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afe6991-a0dd-4bde-8e72-aa217a1e65ed",
   "metadata": {},
   "source": [
    "## Step 1 : Create Service Account\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db3406c-de9b-445c-b257-72702f353c79",
   "metadata": {},
   "source": [
    "1. Navigate to the IAM & Admin > Service Account. Click on create service account.  \n",
    "<img src='./images/eit_1_1.png' width=800 height=400>  \n",
    "\n",
    "2. Provide a name for the service account and click on done.  \n",
    "<img src='./images/eit_1_2.png' width=800 height=400>   \n",
    "\n",
    "For additional information about creating service account [Click here](https://cloud.google.com/iam/docs/creating-managing-service-accounts)\n",
    "\n",
    "**NOTE**: Provide the Cloud Function ( cloudfunctions.invoker ) and Cloud Storage (Storage Object Creator -  roles/storage.objectCreator to create files and \n",
    "Storage Object Viewer -  roles/storage.objectViewer to read files)\n",
    " permission to this service account.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089a6207-e94f-40ab-a2e0-f0e04359f696",
   "metadata": {},
   "source": [
    "## Step 2 : Enable Gmail API & Create an OAuth 2.0 Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7e46ed-c9ae-4b02-be9d-c84d45922360",
   "metadata": {},
   "source": [
    "In this section,  you enable the Gmail API in   GCP Project, create an OAuth 2.0 client, and configure Gmail Push Notifications against receiving inboxes for Dialogflow CX integration. This guide leverages some steps outlined in Implementing Server-Side Authorization and Push Notifications.\n",
    "\n",
    "To get started using Gmail API,  you need to enable the API and create credentials for the application.\n",
    "\n",
    "1. Search APIs & Services in the search box and click on Enable APIS and Service Button shown in the screenshot.  \n",
    "<img src='./images/eit_2_1.png' width=800 height=400>\n",
    "\n",
    "2. Search for Gmail API and select it.  \n",
    "<img src='./images/eit_2_2.png' width=800 height=400>\n",
    "\n",
    "3. Enable the API if  you see the enable button instead of the manage button.  \n",
    "<img src='./images/eit_2_3.png' width=800 height=400>\n",
    "\n",
    "4. Navigate to APIs & Services > Credentials  \n",
    "<img src='./images/eit_2_4.png' width=800 height=400>\n",
    "\n",
    "5. select \"Create Credentials\" and \"Help Me Choose\" to create OAuth 2.0 credentials.  \n",
    "<img src='./images/eit_2_5.png' width=800 height=400>\n",
    "\n",
    "6. Specify the Gmail API and your  Data to create an OAuth 2.0 Client. Then, click Next.  \n",
    "<img src='./images/eit_2_6.png' width=800 height=400>\n",
    "\n",
    "7. Click on Save and continue.  \n",
    "<img src='./images/eit_2_7.png' width=800 height=400>\n",
    "\n",
    "8. Select the Desktop App from dropdown and enter the name and click on create.  \n",
    "<img src='./images/eit_2_8.png' width=800 height=400>\n",
    "\n",
    "9. Download the json file by clicking the download button and click on done  \n",
    "<img src='./images/eit_2_9.png' width=800 height=400>\n",
    "\n",
    "Rename this file to `client_credentials.json`. This is needed in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cddd90-229b-43e4-909a-5c409042268f",
   "metadata": {},
   "source": [
    "## Step 3 : Create Token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f8f10e-9c66-48e3-8c07-72d91cbd60a2",
   "metadata": {},
   "source": [
    "In this section you create a token for the first time for our script.\n",
    "\n",
    "1. Here is the file structure for creating the token. Copy the code with the file name mentioned above the respective code block in cloud shell.  \n",
    "<img src='./images/eit_3_1i.png' width=800 height=400>  \n",
    "\n",
    "### File Structure for creating token \n",
    "#### client_credentials.json  \n",
    "`Paste the file which  you downloaded in above step`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee55ca2-93f3-482d-806c-d5a45ea04bc7",
   "metadata": {},
   "source": [
    "#### main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62cb87d1-0e49-43a9-97ff-e5f6aaf959f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "import json\n",
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "TOKEN_FILE = os.environ.get(\"TOKEN_FILE\")\n",
    "OAUTH_CLIENT_CREDS = os.environ.get(\"OAUTH_CLIENT_CREDS\")\n",
    "BUCKET_NAME = os.environ.get(\"GCP_BUCKET\")\n",
    "# If modifying these scopes, delete the file token.json.\n",
    "SCOPES = [\"https://www.googleapis.com/auth/gmail.readonly\"]\n",
    "\n",
    "\n",
    "def read_token():\n",
    "    try:\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(BUCKET_NAME)\n",
    "        blob = bucket.blob(TOKEN_FILE)\n",
    "\n",
    "        # Download the contents of the blob as a string and then parse it using json.loads() method\n",
    "\n",
    "        token = json.loads(json.loads((blob.download_as_text(client=None))))\n",
    "    except:\n",
    "        token = None\n",
    "    return token\n",
    "\n",
    "\n",
    "def main(*args, **kwargs):\n",
    "    creds = None\n",
    "    # getting token from bucket\n",
    "    token_as_json = read_token()\n",
    "    if token_as_json:\n",
    "        creds = Credentials(\n",
    "            token=token_as_json.get(\"token\"),\n",
    "            refresh_token=token_as_json.get(\"refresh_token\"),\n",
    "            token_uri=token_as_json.get(\"token_uri\"),\n",
    "            client_id=token_as_json.get(\"client_id\"),\n",
    "            client_secret=token_as_json.get(\"client_secret\"),\n",
    "        )\n",
    "\n",
    "    # if os.path.exists(TOKEN_FILE):\n",
    "    #     creds = Credentials.from_authorized_user_file(TOKEN_FILE, SCOPES)\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(OAUTH_CLIENT_CREDS, SCOPES)\n",
    "            creds = flow.run_console()\n",
    "        # Save the credentials for the next run\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(BUCKET_NAME)\n",
    "        blob = bucket.blob(TOKEN_FILE)\n",
    "        blob.upload_from_string(\n",
    "            data=json.dumps(creds.to_json()), content_type=\"application/json\"\n",
    "        )\n",
    "    print(\"Token Saved.\")\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688ef78c-2f06-4c0c-850a-6e9b2333c8e0",
   "metadata": {},
   "source": [
    "#### requirements.txt \n",
    "```python\n",
    "google-api-python-client\n",
    "google-auth\n",
    "google-auth-oauthlib\n",
    "google-cloud-storage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b96f95-8b0a-4c27-91e1-bfaf54306dde",
   "metadata": {},
   "source": [
    "File structure should look like this.  \n",
    "<img src='./images/eit_3_1ii.png' width=800 height=400>  \n",
    "\n",
    "2. Run this command in cloud shell\n",
    "```bash\n",
    "pip3 install -r requirements.txt\n",
    "export TOKEN_FILE=token-file-location/token.json\n",
    "export OAUTH_CLIENT_CREDS=client_credentials.json\n",
    "export GCP_BUCKET=bucket-name\n",
    "python3 -c 'import main; main.main()'\n",
    "```\n",
    "\n",
    "3. This creates an authentication link shown in image,click on the link.  \n",
    "<img src='./images/eit_3_3.png' width=800 height=400>  \n",
    "\n",
    "4. Select your account   \n",
    "<img src='./images/eit_3_4.png' width=800 height=400>  \n",
    "\n",
    "5. Click on the allow button.  \n",
    "<img src='./images/eit_3_5.png' width=800 height=400>  \n",
    "\n",
    "6. Copy the authentication code.  \n",
    "<img src='./images/eit_3_6.png' width=800 height=400>  \n",
    "\n",
    "7. Paste the code in the terminal and hit enter.  \n",
    "<img src='./images/eit_3_7.png' width=800 height=400>  \n",
    "\n",
    "This creates the token.json file in the storage bucket location which  you mentioned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a610ac52-392b-43c6-9cf6-f3f420adb58c",
   "metadata": {},
   "source": [
    "## Step 4 : Deploy on cloud function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b574dc6-816e-4224-a12f-6f0ae35f3aae",
   "metadata": {},
   "source": [
    "### File Structure for deployment \n",
    "1. Copy the code with the file name mentioned above the respective code block.\n",
    "\n",
    "#### client_credentials.json \n",
    "`'replace this file with  client credentials file downloaded earlier'`\n",
    "\n",
    "#### config.yaml\n",
    "```yaml\n",
    "OAUTH_CLIENT_CREDS: \"client_credentials.json\"\n",
    "TOKEN_FILE: \"path-of-json-file\"\n",
    "GCP_BUCKET: \" -bucket-name\"\n",
    "OUTPUT_PATH: \"mail_dataset_v2\"\n",
    "```\n",
    "\n",
    "**NOTE**: provide the `TOKEN_FILE` bucket location where token.json is downloaded in step 2.\n",
    "\n",
    "#### requirements.txt\n",
    "```python\n",
    "google-api-python-client\n",
    "google-auth\n",
    "google-auth-oauthlib\n",
    "google-cloud-storage\n",
    "Flask\n",
    "functions-framework\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7355076d-6547-45e5-84bc-68cbc52f777c",
   "metadata": {},
   "source": [
    "#### main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40e2813d-e5a4-4361-9545-2990fd47b5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required libraries\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "import pickle\n",
    "import os.path\n",
    "import base64\n",
    "import email\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google.cloud import storage\n",
    "from datetime import date, timedelta\n",
    "import mimetypes\n",
    "import json\n",
    "import datetime\n",
    "from flask import escape\n",
    "import functions_framework\n",
    "\n",
    "\n",
    "# Scope only to read mails\n",
    "SCOPES = [\"https://www.googleapis.com/auth/gmail.readonly\"]\n",
    "# USER_ID = \"mohammadzaida@google.com\"\n",
    "OAUTH_CLIENT_CREDS = os.environ.get(\"OAUTH_CLIENT_CREDS\")\n",
    "BUCKET_NAME = os.environ.get(\"GCP_BUCKET\")\n",
    "TOKEN_FILE = os.environ.get(\"TOKEN_FILE\")\n",
    "OUTPUT_PATH = os.environ.get(\"OUTPUT_PATH\")\n",
    "\n",
    "\n",
    "def upload_blob(source_file_name, data, data_type):\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(BUCKET_NAME)\n",
    "    blob = bucket.blob(source_file_name)\n",
    "    blob.upload_from_string(data, content_type=data_type)\n",
    "\n",
    "    return \"uploaded\"\n",
    "\n",
    "\n",
    "def read_token():\n",
    "    try:\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(BUCKET_NAME)\n",
    "        blob = bucket.blob(TOKEN_FILE)\n",
    "\n",
    "        # Download the contents of the blob as a string and then parse it using json.loads() method\n",
    "\n",
    "        token = json.loads(json.loads((blob.download_as_text(client=None))))\n",
    "\n",
    "    except:\n",
    "        token = {}\n",
    "    return token\n",
    "\n",
    "\n",
    "def refresh_token():\n",
    "    creds = None\n",
    "    # getting token from bucket\n",
    "    token_as_json = read_token()\n",
    "    if token_as_json:\n",
    "        creds = Credentials(\n",
    "            token=token_as_json.get(\"token\"),\n",
    "            refresh_token=token_as_json.get(\"refresh_token\"),\n",
    "            token_uri=token_as_json.get(\"token_uri\"),\n",
    "            client_id=token_as_json.get(\"client_id\"),\n",
    "            client_secret=token_as_json.get(\"client_secret\"),\n",
    "        )\n",
    "\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(OAUTH_CLIENT_CREDS, SCOPES)\n",
    "            creds = flow.run_console()\n",
    "        # Save the credentials for the next run\n",
    "        upload_blob(TOKEN_FILE, json.dumps(creds.to_json()), \"application/json\")\n",
    "    print(\"Token Refreshed.\")\n",
    "    return creds\n",
    "\n",
    "    print(\"Token Refreshed.\")\n",
    "    return creds\n",
    "\n",
    "\n",
    "def get_attachments(service, msg_id):\n",
    "    try:\n",
    "        message = service.users().messages().get(userId=\"me\", id=msg_id).execute()\n",
    "\n",
    "        for part in message[\"payload\"][\"parts\"]:\n",
    "            if part[\"filename\"]:\n",
    "                if \"data\" in part[\"body\"]:\n",
    "                    data = part[\"body\"][\"data\"]\n",
    "                else:\n",
    "                    att_id = part[\"body\"][\"attachmentId\"]\n",
    "                    att = (\n",
    "                        service.users()\n",
    "                        .messages()\n",
    "                        .attachments()\n",
    "                        .get(userId=\"me\", messageId=msg_id, id=att_id)\n",
    "                        .execute()\n",
    "                    )\n",
    "                    data = att[\"data\"]\n",
    "                file_data = base64.urlsafe_b64decode(data.encode(\"UTF-8\"))\n",
    "                path = part[\"filename\"]\n",
    "\n",
    "                try:\n",
    "                    file_type = mimetypes.guess_type(path)[0]\n",
    "                    if file_type == None:\n",
    "                        file_type = \"text/plain\"\n",
    "                except:\n",
    "                    file_type = \"text/plain\"\n",
    "                print(\"\\t\", file_type)\n",
    "                upload_blob(f\"{OUTPUT_PATH}/{msg_id}/{path}\", file_data, file_type)\n",
    "\n",
    "    except Exception as error:\n",
    "        print(\"Attachment error\", error)\n",
    "\n",
    "\n",
    "def get_message(service, msg_id):\n",
    "    data = \"\"\n",
    "    try:\n",
    "        message = service.users().messages().get(userId=\"me\", id=msg_id).execute()\n",
    "        headers = message[\"payload\"][\"headers\"]\n",
    "        subject = [i[\"value\"] for i in headers if i[\"name\"] == \"Subject\"]\n",
    "        subject = subject[0] if subject else \"\"\n",
    "        if message[\"payload\"][\"mimeType\"] == \"multipart/mixed\":\n",
    "            for part in message[\"payload\"][\"parts\"]:\n",
    "                for sub_part in part[\"parts\"]:\n",
    "                    if sub_part[\"mimeType\"] == \"text/plain\":\n",
    "                        data = sub_part[\"body\"][\"data\"]\n",
    "                        break\n",
    "                if data:\n",
    "                    break\n",
    "        else:\n",
    "            for part in message[\"payload\"][\"parts\"]:\n",
    "                if part[\"mimeType\"] == \"text/plain\":\n",
    "                    data = part[\"body\"][\"data\"]\n",
    "                    break\n",
    "\n",
    "        # content = base64.b64decode(data).decode('utf-8')\n",
    "        content = data\n",
    "        # content = base64.b64decode(data).decode('utf-8',errors='ignore')\n",
    "        content = base64.urlsafe_b64decode(content.encode(\"UTF-8\"))\n",
    "        upload_blob(f\"{OUTPUT_PATH}/{msg_id}/body.txt\", content, \"text/plain\")\n",
    "        # subject = base64.b64decode(subject).decode('utf-8',errors='ignore')\n",
    "        upload_blob(f\"{OUTPUT_PATH}/{msg_id}/subject.txt\", subject, \"text/plain\")\n",
    "        return content\n",
    "    except Exception as error:\n",
    "        print(\"mail body Error\", error)\n",
    "\n",
    "\n",
    "@functions_framework.http\n",
    "def email_ingestion(request, *args, **kwargs):\n",
    "    creds = refresh_token()\n",
    "    # Connect to the Gmail API\n",
    "    service = build(\"gmail\", \"v1\", credentials=creds)\n",
    "    today = datetime.datetime.today()\n",
    "    yesterday = today - timedelta(1)\n",
    "\n",
    "    # query = \"before: {0} after: {1}\".format(today.strftime('%Y/%m/%d'),\n",
    "    #                                     yesterday.strftime('%Y/%m/%d'))\n",
    "    query = \"before:{0} after:{1}\".format(\n",
    "        int(today.timestamp()), int(yesterday.timestamp())\n",
    "    )\n",
    "\n",
    "    # query = \"after:{0}\".format(int(today.timestamp()))\n",
    "    # query = \"after:{0}\".format(today.strftime('%Y/%m/%d'))\n",
    "    print(query)\n",
    "    # query = \"\"\n",
    "    # request a list of all the messages\n",
    "    # default message size is 100\n",
    "    # you can also pass maxResults to get any number of emails. Like this:\n",
    "    # result = service.users().messages().list(maxResults=5, userId='me').execute()\n",
    "    result = (\n",
    "        service.users().messages().list(maxResults=500, userId=\"me\", q=query).execute()\n",
    "    )\n",
    "\n",
    "    messages = result.get(\"messages\")\n",
    "    if not messages:\n",
    "        print(\"No email found OR Something went wrong\")\n",
    "        return \"\"\n",
    "    # messages is a list of dictionaries where each dictionary contains a message id.\n",
    "\n",
    "    # iterate through all the messages\n",
    "    print(\"No of emails : \", len(messages))\n",
    "    for msg in messages:\n",
    "        print(msg[\"id\"])\n",
    "        get_message(service, msg[\"id\"])\n",
    "        get_attachments(service, msg[\"id\"])\n",
    "\n",
    "    print(\"Finish\")\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d534da40-75cf-41bc-9ea5-0bf5f72e7e5b",
   "metadata": {},
   "source": [
    "File structure should look like this.  \n",
    "<img src='./images/eit_4_1.png' width=800 height=400>  \n",
    "\n",
    "2. Deploy the cloud function by this command and provide the service account which you created in step 1.  \n",
    "<img src='./images/eit_4_2.png' width=800 height=400>  \n",
    "\n",
    "```bash\n",
    "gcloud functions deploy email_ingestion --runtime=python39  --env-vars-file=config.yaml --project= -project-name --service-account= your-service-account --trigger-http\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707c3546-269c-44c0-977a-94b018e0dccd",
   "metadata": {},
   "source": [
    "## Step 5 : Schedule By Cloud Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ec79f3-078e-4145-9cb1-4e2c6ded7145",
   "metadata": {},
   "source": [
    "1. Search for Cloud Scheduler in the search box and click on the create job.  \n",
    "<img src='./images/eit_5_1.png' width=800 height=400>  \n",
    "\n",
    "2. Enter the name for the scheduler, enter the frequency and select the timezone as shown in image and click continue.  \n",
    "<img src='./images/eit_5_2i.png' width=800 height=400>  \n",
    "\n",
    "   Select http from the drop down.  \n",
    "<img src='./images/eit_5_2ii.png' width=800 height=400>  \n",
    "\n",
    "3. Copy the Cloud function url by following steps\n",
    "    a. Navigate to the Cloud Function and click on the  cloud function which you created  \n",
    "    <img src='./images/eit_5_3i.png' width=800 height=400>  \n",
    "    \n",
    "    b. Go to the trigger section and copy the URL.  \n",
    "    <img src='./images/eit_5_3ii.png' width=800 height=400>  \n",
    "\n",
    "4. Paste the url in   Cloud scheduler and select the GET as Http method.  \n",
    "<img src='./images/eit_5_4.png' width=800 height=400>  \n",
    "\n",
    "5. Select the Add OIDC token option from dropdown.  \n",
    "<img src='./images/eit_5_5.png' width=800 height=400>  \n",
    "\n",
    "6. Select the service account which  you created in step 1 and click on continue.  \n",
    "<img src='./images/eit_5_6.png' width=800 height=400>  \n",
    "\n",
    "7. Enter Max retry attempts (2) and enter the Max retry duration (5s). Click on create.  \n",
    "<img src='./images/eit_5_7.png' width=800 height=400>  \n",
    "\n",
    "8. Cloud Scheduler  has scheduled for the cloud function which runs every day at 12 AM.  \n",
    "<img src='./images/eit_5_8.png' width=800 height=400>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a38e24-0741-4d08-8cde-78c6f1bb63cd",
   "metadata": {},
   "source": [
    "## Output "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba98a652-376f-47ff-8f29-19c2f66bc0d9",
   "metadata": {},
   "source": [
    "\n",
    "Output from this script is stored in a bucket with folder name mentioned in config.yaml file as OUTPUT_PATH with respective mails IDs as folders inside it.  \n",
    "<img src='./images/output_sample_1.png' width=800 height=400> \n",
    "\n",
    "Inside each folder there is  subject.txt containing the subject of that email, body.txt containing body of the mail and the attachments with their name in the email.  \n",
    "<img src='./images/output_sample_2.png' width=800 height=400>  \n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
