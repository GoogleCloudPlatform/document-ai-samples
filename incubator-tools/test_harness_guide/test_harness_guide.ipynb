{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59293fe4-9e05-4da4-a8a7-c2651f86b99b",
   "metadata": {},
   "source": [
    "# Test Harness Guide"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6951c4f4-a155-4f5d-afd0-7fa23fb032b5",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c303fbc-982a-4dc6-8f2d-1be6775c98b7",
   "metadata": {},
   "source": [
    "# Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the **DocAI Incubator Team**. No guarantees of performance are implied. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b58d610-f67f-4f62-bd04-eb73a39e2b3a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Objective\n",
    "\n",
    "In software testing, a test harness or automated test framework is a collection of software and test data configured to test a program unit by running it under varying conditions and monitoring its behavior and outputs.\n",
    "\n",
    "The Test Harness tool is a useful tool to check: \n",
    "* If the DocAI API is working properly or not and \n",
    "* Check the differences between output of a parser in different time intervals. \n",
    "\n",
    "This tool is designed to take a few documents from Google Cloud Storage (GCS) and parse them via a DocAI processor and compare the results to previously accepted results and then provide the output as a Google Sheet containing a report of entities matching percentage as well as fuzzy ratio.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba34498c-a907-4ee3-8fe8-25690aed4997",
   "metadata": {},
   "source": [
    "## Test harness tool working chart\n",
    "\n",
    "<img src=\"./images/flow_chart.png\" width=600 height=300> </img>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e604aff3-5664-4ce8-9189-5efbb5bdd7c3",
   "metadata": {},
   "source": [
    "# Modes of Test harness tool"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a2693a0-bb80-4030-9eed-d0b598b74d0b",
   "metadata": {},
   "source": [
    "1. Mode- `I`  ( Initial Setup Mode)\n",
    "    * This Mode help us to create Initial json files for the given documents by running the documents in a parser at time frame t.\n",
    "2. Mode- `C` (Comparison Mode)\n",
    "    * To run this comparison mode, Executing of Mode-`I` is mandatory as the Initial json files created in the Mode- `I` are used to compare.\n",
    "    * This Mode  help us to create json files for the given documents by running the documents in a parser at time frame t+k.\n",
    "    * Both the Initial Jsons(output from Mode-`I`) and Jsons created in the Mode-`C` are compared and the differences are provided in the G-sheet thru mail. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "596bc29c-ba8d-4e7d-a75d-a380443bf828",
   "metadata": {},
   "source": [
    "# Modes of operation\n",
    "\n",
    "There are two types of processes possible in the test harness tool.  \n",
    "i. Synchronous Mode  \n",
    "ii. Asynchronous Mode  \n",
    "### Synchronous Mode\n",
    "* In **Synchronous mode**, call the DocAI Processor and wait until it returns - all other code execution and user interaction is stopped until the call returns.\n",
    "\n",
    "* Here only a single document is processed at a time and receives the output for it. It cannot process multiple documents at the same time.\n",
    "\n",
    "### Asynchronous Mode\n",
    "* In **Asynchronous mode**, no need to stop all other operations while waiting for the web service call to return. Other code executes and/or the user can continue to interact with the page (or program).\n",
    "\n",
    "* Here multiple documents can be processed simultaneously. \n",
    "\n",
    "To know more about synchronous and asynchronous please refer to this [article](https://cloud.google.com/blog/topics/developers-practitioners/differences-between-synchronous-web-apis-and-asynchronous-stateful-apis)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55fe519c-1754-4e42-89e4-86f8bd31b976",
   "metadata": {},
   "source": [
    "# Installation Guide\n",
    "This test harness tool is in a Python notebook script format which can be used in **Vertex AI JupyterLab Environment or Google Colab**. First, put the test harness tool script in JupyterLab and then, put all the reference documents in a specific folder . Also, create or use an existing folder as an output folder. \n",
    "\n",
    "For further details please see the steps to use the test harness tool in Synchronous mode or Asynchronous mode below as per requirement.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f8ce35b-b550-4d67-a205-b29a2c22f633",
   "metadata": {},
   "source": [
    "# Service Account and API key Creation\n",
    "Google sheets is a great tool to see and analyze the data hence a service account is needed which is going to create and own the google sheet, also it’ll share the sheet to the user.\n",
    "\n",
    "Hence, before hitting the run button a service account in our google cloud platform project is a must.\n",
    "\n",
    "Please use the steps below to create a service account or [this link](https://cloud.google.com/iam/docs/creating-managing-service-accounts) to be referred to see in detail."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1080d69d-f042-4e7e-a971-65020fc54310",
   "metadata": {},
   "source": [
    "# Creating a service account\n",
    "**Prerequisites** :\n",
    "* Enable the IAM, Drive & Sheets API’s\n",
    "* Understand IAM service accounts\n",
    "* Install the Google Cloud CLI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "027b6ab8-10dd-4531-9647-840ec6f8a830",
   "metadata": {},
   "source": [
    "**Required roles** : \n",
    "To get the permissions that are required to manage service accounts, ask the administrator to grant the following IAM roles on the project:\n",
    "* To view service accounts and service account metadata: View Service Accounts (roles/iam.serviceAccountViewer)\n",
    "* To view and create service accounts: Create Service Accounts (roles/iam.serviceAccountCreator)\n",
    "* To view and delete service accounts: Delete Service Accounts (roles/iam.serviceAccountDeleter)\n",
    "* To fully manage (view, create, update, disable, enable, delete, undelete, and manage access to) service accounts: Service Account Admin (roles/iam.serviceAccountAdmin)  \n",
    "\n",
    "For more information about granting roles, see [Manage access](https://cloud.google.com/iam/docs/granting-changing-revoking-access).  \n",
    "To learn more about these roles, see [Service Accounts roles](https://cloud.google.com/iam/docs/understanding-roles#service-accounts-roles).  \n",
    "IAM basic roles also contain permissions to manage service accounts. One should not grant basic roles in a production environment, but can grant them in a development or test environment.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c266b107-5a75-44b9-bb47-7c830f74e77f",
   "metadata": {},
   "source": [
    "# Creating a service account : \n",
    "\n",
    "When a service account is needed, one must provide an alphanumeric ID (SA_NAME in the samples below), such as my-service-account. The ID must be between 6 and 30 characters, and can contain lowercase alphanumeric characters and dashes. After a service account is created , one cannot change its name.  \n",
    "\n",
    "The service account's name appears in the email address that is provisioned during creation, in the format SA_NAME@PROJECT_ID.iam.gserviceaccount.com.  \n",
    "\n",
    "Each service account also has a permanent, unique numeric ID, which is generated automatically. \n",
    "\n",
    "Also provide the following information when a service account is created:\n",
    "* **SA_DESCRIPTION** is an optional description for the service account.\n",
    "* **SA_DISPLAY_NAME** is a friendly name for the service account.\n",
    "* **PROJECT_ID** is the ID of Google Cloud project. \n",
    "\n",
    "After creation of a service account, one might need to wait for 60 seconds or more before use the service account. This behavior occurs because read operations are eventually consistent; it can take time for the new service account to become visible. If one tries to read or use a service account immediately after creating, and receives an error, can [retry the request with exponential backoff](https://cloud.google.com/iam/docs/retry-strategy). \n",
    "\n",
    "After a service account is created, [grant one or more roles to the service account](https://cloud.google.com/iam/docs/granting-changing-revoking-access) so that it can act on one’s behalf. \n",
    "\n",
    "Also, if the service account needs to access resources in other projects, usually one must [enable the APIs](https://cloud.google.com/apis/docs/getting-started#enabling_apis) for those resources in the project where the service account is created.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "effb6a52-e12c-43fa-b988-71b647f337f6",
   "metadata": {},
   "source": [
    "# Service Account Key Download \n",
    "\n",
    "In the test harness tool, API keys are used to authenticate and service account is used so that google sheet is generated and used to get the comparison reports. \n",
    "\n",
    "### Downloading Service Account Key : \n",
    "To download the Service Account Key please follow these steps:\n",
    "1. Go to Google Cloud Console and search for “Service accounts” and select “Service Accounts” of “IAM and Admin”.  \n",
    "<img src=\"./images/sk_1.png\"></img>\n",
    "2. Select the Service Account made for the Test Harness Tool.  \n",
    "<img src=\"./images/sk_2.png\"></img>\n",
    "3. Select “Keys”.  \n",
    "<img src=\"./images/sk_3.png\"></img>\n",
    "4. Select \"Add Key\" and then Choose \"Create New Key\".  \n",
    "<img src=\"./images/sk_4.png\"></img>\n",
    "5. Choose “Json” and then Click “Create”.  \n",
    "<img src=\"./images/sk_5.png\"></img>\n",
    "6. Wait for the Success message.  \n",
    "<img src=\"./images/sk_6.png\"></img>  \n",
    " Also the newly added key can be seen in the console.  \n",
    "<img src=\"./images/sk_final.png\"></img>\n",
    "7. Check in Local Machine for the “Json Key”."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f8b42da-c75a-443a-a935-608134599bb4",
   "metadata": {},
   "source": [
    "# Tool Modes and prerequisites\n",
    "## Tool Mode -’i’(Initial setup mode):\n",
    "\n",
    "This mode has to be used to initially get the output json files when you create a processor( at time t).\n",
    "\n",
    "**Prerequisites**:\n",
    "1. Fill all input variables for Test harness tool script.\n",
    "2. Service account and api key to be generated as per above instructions given in corresponding sections.\n",
    "\n",
    "## Tool Mode -’C’(Comparison mode):\n",
    "This mode has to be used to get the output json files when you want to test a processor( at time t+k).\n",
    "\n",
    "\n",
    "**Prerequisites**:\n",
    "1. Run the test harness tool in Tool Mode ‘I’ to create initial json outputs to compare.\n",
    "2. General Parameters specified in the configuration file and Test harness tool script.\n",
    "3. Service account and api key to be generated as per instructions given.\n",
    "4. i. If there are more than 1 documents to test, use process type as ‘A’ and fill the details required under async details in configuration file  \n",
    "    ii. If you want to test with 1 document , use the process type as ‘S’ and fill the details required under sync details in the configuration file.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a8a096e-1446-4e3c-b233-1be9f98c0cc2",
   "metadata": {},
   "source": [
    "#### Run the below cell to install required-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "066f5be5-27ee-47d5-95de-52b5696a586a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# uncomment and run this cell to install required packages\n",
    "# !pip install colorama\n",
    "# !pip install gspread\n",
    "# !pip install gspread_formatting\n",
    "# !pip install google-cloud-documentai\n",
    "# !pip install google-cloud-storage\n",
    "# !pip install google-api-core\n",
    "# !pip install df2gspread\n",
    "# !pip install configparser\n",
    "# !pip install google-cloud\n",
    "# !pip install oauth2client"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4c57bd0-be9a-47ca-a0ea-fcd8bce1fcff",
   "metadata": {},
   "source": [
    "#### Import Required Packages/Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbfc944-ff20-4a43-afd9-560a14502a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to download utilities module\n",
    "# !wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea963427-c583-4a52-875d-12c0cc7e67dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple\n",
    "\n",
    "import gspread\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from colorama import Back, Fore, Style\n",
    "from google.api_core import operation\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from google.cloud import storage\n",
    "from gspread import spreadsheet\n",
    "from gspread_formatting import (\n",
    "    BooleanCondition,\n",
    "    BooleanRule,\n",
    "    CellFormat,\n",
    "    Color,\n",
    "    ConditionalFormatRule,\n",
    "    GridRange,\n",
    "    get_conditional_format_rules,\n",
    "    textFormat,\n",
    ")\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "from utilities import (\n",
    "    copy_blob,\n",
    "    documentai_json_proto_downloader,\n",
    "    file_names,\n",
    "    find_match,\n",
    "    get_match_ratio,\n",
    "    process_document_sample,\n",
    "    remove_row,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ec13b9b-ef7f-44fb-9c0f-951ef1aa56fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Input Variables Description\n",
    "* **EMAIL_ADDRESS** : Enter the email address to which you wanted the comparison G-sheet to be sent.\n",
    "* **API_KEY** :  Enter the path of Apikey json file like _example: /content/apikey.json_\n",
    "* **PROCESS_TYPE** : Enter `a` or `s`, if you want to test the tool with a single document enter `s` else `a`.\n",
    "* **PROJECT_NUMBER** :  Enter the google cloud project number.\n",
    "* **PROJECT_NAME**: Enter the google cloud project name(this is required only when you run the code in google colab)\n",
    "* **PROCESSOR_ID** : Enter the processor id from which you wanted to get the files to get parsed.\n",
    "* **LOCATION** : Enter the Location(`us` or `eu`) of your processor chosen while creating.\n",
    "* **INITIAL_JSON_BUCKET** : Enter the path to store the initial output json files when you run the tool with Mode I\n",
    "* **MODE** : Enter the mode of test harness tool\n",
    "    * **I** - Initial setup mode , this mode is to get the output jsons initially from a processor.\n",
    "    * **C** - Comparison mode is for comparing output jsons which we have taken initially from mode `I` and output while testing the processor.\n",
    "* **ASYNC_INPUT_BUCKET_PDFS** : Enter the path of documents to parse and compare\n",
    "* **ASYNC_OUTPUT_BUCKET** : Enter the path where you wanted to store the output json files while using Mode `C` and Asynchronous mode.(required only in async mode and tool Mode `C`).\n",
    "* **SYNC_INPUT_BUCKET_PDF** :  Enter the path of files for Tool mode `C` and Process mode `S` for a single file.(required only in sync mode and tool Mode `C`)\n",
    "* **SYNC_INPUT_BUCKET_JSON** :  Enter the path of initial output json file to compare\n",
    "* **PROCESSOR_VERSION_ID** : Enter the processor version ID for which you wanted to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b653204-0a24-4062-93b4-aa35fe0db18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL_ADDRESS = \"<mail-id@domain.com>\"\n",
    "API_KEY = \"<path/to/apikey.json>\"\n",
    "PROCESS_TYPE = \"a\"  # \"a\" or \"s\"\n",
    "PROJECT_NUMBER = \"<xx-xx-xx>\"\n",
    "PROCESSOR_ID = \"<xx-xx>\"\n",
    "LOCATION = \"us\"  # 'us' or 'eu'\n",
    "INITIAL_JSON_BUCKET = \"gs://xx-xx/test_harness_guide/output/mode_I\"\n",
    "MODE = \"c\"  # \"i\" or \"c\"\n",
    "ASYNC_INPUT_BUCKET_PDFS = \"gs://xx-xx/test_harness_guide/input/async\"\n",
    "ASYNC_PROJECT_ID = PROJECT_NUMBER\n",
    "ASYNC_PROCESSOR_ID = PROCESSOR_ID\n",
    "ASYNC_LOCATION = LOCATION\n",
    "ASYNC_OUTPUT_BUCKET = \"gs://xx-xx/test_harness_guide/output/async\"\n",
    "SYNC_PROJECT_ID = PROJECT_NUMBER\n",
    "SYNC_PROCESSOR_ID = PROCESSOR_ID\n",
    "SYNC_LOCATION = LOCATION\n",
    "SYNC_INPUT_BUCKET_PDF = \"gs://xx-xx/test_harness_guide/input/sync/fake_invoice_1.pdf\"\n",
    "SYNC_INPUT_BUCKET_JSON = \"gs://xx-xx/test_harness_guide/output/sync/fake_invoice_1.json\"\n",
    "PROCESSOR_VERSION_ID = \"<processor-version>\"  # \"pretrained-invoice-v1.3-2022-07-15\"\n",
    "\n",
    "GS_CREDENTIALS_PATH = API_KEY\n",
    "SCOPE = [\n",
    "    \"https://spreadsheets.google.com/feeds\",\n",
    "    \"https://www.googleapis.com/auth/drive\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5a1cc2-b7c0-4d1b-ad3b-cac07f5bce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_folder(bucket_name: str, folder_name: str) -> None:\n",
    "    \"\"\"This Method will delete the folder in GCP bucket\n",
    "\n",
    "    Args:\n",
    "        bucket_name (str): GCS bucket name\n",
    "        folder_name (str): GCS path prefix(usually startsafter gcs bucket-name in gs://bucket-name/xx-xx)\n",
    "    \"\"\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blobs = list(bucket.list_blobs(prefix=folder_name))\n",
    "    bucket.delete_blobs(blobs)\n",
    "    print(f\"Folder {folder_name} deleted.\")\n",
    "\n",
    "\n",
    "def batch_process_documents_sample(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    processor_id: str,\n",
    "    gcs_input_uri: str,\n",
    "    gcs_output_uri: str,\n",
    "    processor_version_id: str,\n",
    "    timeout: int = 500,\n",
    ") -> operation.Operation:\n",
    "    \"\"\"It will perform Batch Process on raw input documents\n",
    "\n",
    "    Args:\n",
    "        project_id (str): GCP project ID\n",
    "        location (str): Processor location `us` or `eu`\n",
    "        processor_id (str): GCP DocumentAI ProcessorID\n",
    "        gcs_input_uri (str): GCS path which contains all input files\n",
    "        gcs_output_uri (str): GCS path to store processed JSON results\n",
    "        processor_version_id (str): VesrionID of GCP DocumentAI Processor\n",
    "        timeout (int, optional): Maximum waiting time for operation to complete. Defaults to 500.\n",
    "\n",
    "    Returns:\n",
    "        operation.Operation: LRO operation ID for current batch-job\n",
    "    \"\"\"\n",
    "\n",
    "    opts = {\"api_endpoint\": f\"{location}-documentai.googleapis.com\"}\n",
    "    client = documentai.DocumentProcessorServiceClient(client_options=opts)\n",
    "    input_config = documentai.BatchDocumentsInputConfig(\n",
    "        gcs_prefix=documentai.GcsPrefix(gcs_uri_prefix=gcs_input_uri)\n",
    "    )\n",
    "    output_config = documentai.DocumentOutputConfig(\n",
    "        gcs_output_config={\"gcs_uri\": gcs_output_uri}\n",
    "    )\n",
    "    print(\"Documents are processing(batch-documents)...\")\n",
    "    name = client.processor_version_path(\n",
    "        project_id, location, processor_id, processor_version_id\n",
    "    )\n",
    "    request = documentai.types.document_processor_service.BatchProcessRequest(\n",
    "        name=name,\n",
    "        input_documents=input_config,\n",
    "        document_output_config=output_config,\n",
    "    )\n",
    "    operation = client.batch_process_documents(request)\n",
    "    print(\"Waiting for operation to complete...\")\n",
    "    operation.result(timeout=timeout)\n",
    "    return operation\n",
    "\n",
    "\n",
    "def batchprocess_intial_mode1(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    processor_id: str,\n",
    "    gcs_input_uri: str,\n",
    "    initial_json_path: str,\n",
    "    processor_version_id: str,\n",
    "    timeout: int = 500,\n",
    ") -> Tuple[operation.Operation, str]:\n",
    "    \"\"\"It will call Batch Process Job and moving parsed JSON files to specified GCS path\n",
    "\n",
    "    Args:\n",
    "        project_id (str): GCP project ID\n",
    "        location (str): Processor location `us` or `eu`\n",
    "        processor_id (str): GCP DocumentAI ProcessorID\n",
    "        gcs_input_uri (str): GCS path which contains all input files\n",
    "        initial_json_path (str): GCS path to store processed JSON results for mode `I`\n",
    "        processor_version_id (str): VesrionID of GCP DocumentAI Processor\n",
    "        timeout (int, optional): Maximum waiting time for operation to complete. Defaults to 500.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[operation.Operation, str]: It reurns LRO operation ID for current batch-job\n",
    "                                            & GCS path for Mode-I\n",
    "    \"\"\"\n",
    "\n",
    "    now = datetime.now()\n",
    "    async_output_dir_prefix = now.strftime(\"%H%M%S%d%m%Y\")\n",
    "    temp_destination_uri = (\n",
    "        \"/\".join(f\"{gcs_input_uri}\".split(\"/\")[:-1])\n",
    "        + \"/\"\n",
    "        + \"initial_jsons_\"\n",
    "        + f\"{async_output_dir_prefix}\"\n",
    "        + \"/\"\n",
    "    )\n",
    "    operation_base = batch_process_documents_sample(\n",
    "        project_id,\n",
    "        location,\n",
    "        processor_id,\n",
    "        gcs_input_uri,\n",
    "        temp_destination_uri,\n",
    "        processor_version_id,\n",
    "        timeout,\n",
    "    )\n",
    "    temp_bucket = temp_destination_uri.split(\"/\")[2]\n",
    "    temp_folder = (\"/\").join(temp_destination_uri.split(\"/\")[3:])\n",
    "    storage_client = storage.Client()\n",
    "    source_bucket = storage_client.get_bucket(temp_bucket)\n",
    "    destination_bucket = initial_json_path.split(\"/\")[2]\n",
    "    filename = [\n",
    "        filename.name\n",
    "        for filename in list(\n",
    "            source_bucket.list_blobs(\n",
    "                prefix=((\"/\").join(temp_destination_uri.split(\"/\")[3:]))\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "    for file in filename:\n",
    "        prefix = (\n",
    "            (\"/\").join(initial_json_path.split(\"/\")[3:])\n",
    "            + \"/\"\n",
    "            + file.split(\"/\")[-1].split(\"-0\")[0]\n",
    "            + \".json\"\n",
    "        )\n",
    "        copy_blob(temp_bucket, file, destination_bucket, prefix)\n",
    "        print(\n",
    "            f\"Blob copied from gs://{temp_bucket}/{file}\"\n",
    "            f\" to\\n\\t\\tgs://{destination_bucket}/{prefix}\"\n",
    "        )\n",
    "    delete_folder(temp_bucket, temp_folder)\n",
    "    return operation_base, initial_json_path\n",
    "\n",
    "\n",
    "def colouring_function(file_name: str, sheet_name: str) -> None:\n",
    "    \"\"\"Used to color the cells\n",
    "\n",
    "    Args:\n",
    "        file_name (str): It is a Title for gsheet\n",
    "        sheet_name (str): It is as Tab name for gsheet\n",
    "    \"\"\"\n",
    "\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name(\n",
    "        GS_CREDENTIALS_PATH, SCOPE\n",
    "    )\n",
    "    gc = gspread.authorize(credentials)\n",
    "    sheet_1 = gc.open(file_name).worksheet(sheet_name)\n",
    "\n",
    "    rule = ConditionalFormatRule(\n",
    "        ranges=[GridRange.from_a1_range(\"E\", sheet_1)],\n",
    "        booleanRule=BooleanRule(\n",
    "            condition=BooleanCondition(\"NUMBER_LESS_THAN_EQ\", [\"0.5\"]),\n",
    "            format=CellFormat(\n",
    "                textFormat=textFormat(bold=True),\n",
    "                backgroundColor=Color(\n",
    "                    0.9803921568627451, 0.5019607843137255, 0.4470588235294118\n",
    "                ),\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    rules = get_conditional_format_rules(sheet_1)\n",
    "    rules.append(rule)\n",
    "    rules.save()\n",
    "\n",
    "    rule = ConditionalFormatRule(\n",
    "        ranges=[GridRange.from_a1_range(\"E\", sheet_1)],\n",
    "        booleanRule=BooleanRule(\n",
    "            condition=BooleanCondition(\"NUMBER_BETWEEN\", [\"0.51\", \"0.75\"]),\n",
    "            format=CellFormat(\n",
    "                textFormat=textFormat(bold=True), backgroundColor=Color(1.0, 1.0, 0.6)\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    rules = get_conditional_format_rules(sheet_1)\n",
    "    rules.append(rule)\n",
    "    rules.save()\n",
    "\n",
    "    rule = ConditionalFormatRule(\n",
    "        ranges=[GridRange.from_a1_range(\"E\", sheet_1)],\n",
    "        booleanRule=BooleanRule(\n",
    "            condition=BooleanCondition(\"NUMBER_GREATER\", [\"0.75\"]),\n",
    "            format=CellFormat(\n",
    "                textFormat=textFormat(bold=True),\n",
    "                backgroundColor=Color(\n",
    "                    0.5647058823529412, 0.9333333333333333, 0.5647058823529412\n",
    "                ),\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    rules = get_conditional_format_rules(sheet_1)\n",
    "    rules.append(rule)\n",
    "    rules.save()\n",
    "\n",
    "\n",
    "def get_x_y_list(\n",
    "    bounding_poly: documentai.BoundingPoly,\n",
    ") -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"It takes BoundingPoly object and separates it x & y normalized coordinates as lists\n",
    "\n",
    "    Args:\n",
    "        bounding_poly (documentai.BoundingPoly): A token of Document Page object\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[float], List[float]]: It returns x & y normalized coordinates as separate lists\n",
    "    \"\"\"\n",
    "\n",
    "    x, y = [], []\n",
    "    normalized_vertices = bounding_poly.normalized_vertices\n",
    "    for nv in normalized_vertices:\n",
    "        x.append(nv.x)\n",
    "        y.append(nv.y)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def add_entity_to_dataframe(\n",
    "    entity: documentai.Document.Entity, df: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"It will append entity data to given DataFrame\n",
    "\n",
    "    Args:\n",
    "        entity (documentai.Document.Entity): An entity from Document Object\n",
    "        df (pd.DataFrame): Target Dataframe to add an entity as new row\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: It is a Dataframe with newly appended entity as row\n",
    "    \"\"\"\n",
    "\n",
    "    if entity.mention_text:\n",
    "        coord1, _, coord3, _ = entity.page_anchor.page_refs[\n",
    "            0\n",
    "        ].bounding_poly.normalized_vertices\n",
    "        bbox = [coord1.x, coord1.y, coord3.x, coord3.y]\n",
    "        df.loc[len(df.index)] = [entity.type_, entity.mention_text, bbox]\n",
    "    else:\n",
    "        df.loc[len(df.index)] = [entity.type_, \"Entity not found.\", []]\n",
    "    return df\n",
    "\n",
    "\n",
    "def doc_proto_to_dataframe(data: documentai.Document) -> pd.DataFrame:\n",
    "    \"\"\"It will convert Document Proto object to DataFrame. Returns entities in dataframe format\n",
    "\n",
    "    Args:\n",
    "        data (documentai.Document): It is Document Proto Object\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: It is a DataFrame which having all entities data as rows\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame(columns=[\"type_\", \"mention_text\", \"bbox\"])\n",
    "    if not data.entities:\n",
    "        print(\"No entities Found\")\n",
    "        return df\n",
    "    for entity in data.entities:\n",
    "        if entity.properties:\n",
    "            for sub_entity in entity.properties:\n",
    "                df = add_entity_to_dataframe(sub_entity, df)\n",
    "            continue\n",
    "        df = add_entity_to_dataframe(entity, df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def compare_doc_proto_convert_dataframe(\n",
    "    file1: documentai.Document, file2: documentai.Document\n",
    ") -> Tuple[pd.DataFrame, np.float64]:\n",
    "    \"\"\"Compares the entities between two files and returns the results in a dataframe\n",
    "\n",
    "    Args:\n",
    "        file1 (documentai.Document): It is Document Proto Object\n",
    "        file2 (documentai.Document): It is also Document Proto Object to compare with previous\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, np.float64]: It returns Dataframe and matched score\n",
    "                                            between two Document Protos\n",
    "    \"\"\"\n",
    "\n",
    "    df_file1 = doc_proto_to_dataframe(file1)\n",
    "    df_file2 = doc_proto_to_dataframe(file2)\n",
    "    file1_entities = [entity[0] for entity in df_file1.values]\n",
    "    file2_entities = [entity[0] for entity in df_file2.values]\n",
    "\n",
    "    # find entities which are present only once in both files\n",
    "    # these entities will be matched directly\n",
    "    common_entities = set(file1_entities).intersection(set(file2_entities))\n",
    "    exclude_entities = []\n",
    "    for entity in common_entities:\n",
    "        if file1_entities.count(entity) > 1 or file2_entities.count(entity) > 1:\n",
    "            exclude_entities.append(entity)\n",
    "    for entity in exclude_entities:\n",
    "        common_entities.remove(entity)\n",
    "    df_compare = pd.DataFrame(\n",
    "        columns=[\"entity_name\", \"initial_prediction\", \"current_prediction\"]\n",
    "    )\n",
    "    for entity in common_entities:\n",
    "        value1 = df_file1[df_file1[\"type_\"] == entity].iloc[0][\"mention_text\"]\n",
    "        value2 = df_file2[df_file2[\"type_\"] == entity].iloc[0][\"mention_text\"]\n",
    "        df_compare.loc[len(df_compare.index)] = [entity, value1, value2]\n",
    "        # common entities are removed from df_file1 and df_file2\n",
    "        df_file1 = remove_row(df_file1, entity)\n",
    "        df_file2 = remove_row(df_file2, entity)\n",
    "\n",
    "    # remaining entities are matched comparing the area of IOU across them\n",
    "    mention_text2 = pd.Series(dtype=str)\n",
    "    for index, row in enumerate(df_file1.values):\n",
    "        matched_index = find_match(row, df_file2)\n",
    "        if matched_index is not None:\n",
    "            mention_text2.loc[index] = df_file2.loc[matched_index][1]\n",
    "            df_file2 = df_file2.drop(matched_index)\n",
    "        else:\n",
    "            mention_text2.loc[index] = \"Entity not found.\"\n",
    "\n",
    "    df_file1[\"mention_text2\"] = mention_text2.values\n",
    "    df_file1 = df_file1.drop([\"bbox\"], axis=1)\n",
    "    df_file1.rename(\n",
    "        columns={\n",
    "            \"type_\": \"entity_name\",\n",
    "            \"mention_text\": \"initial_prediction\",\n",
    "            \"mention_text2\": \"current_prediction\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "    df_compare = pd.concat([df_compare, df_file1], ignore_index=True)\n",
    "\n",
    "    # adding entities which are present in file2 but not in file1\n",
    "    for row in df_file2.values:\n",
    "        df_compare.loc[len(df_compare.index)] = [row[0], \"Entity not found.\", row[1]]\n",
    "\n",
    "    df_compare[\"match\"] = (\n",
    "        df_compare[\"initial_prediction\"] == df_compare[\"current_prediction\"]\n",
    "    )\n",
    "    df_compare[\"fuzzy ratio\"] = df_compare.apply(get_match_ratio, axis=1)\n",
    "    if list(df_compare.index):\n",
    "        score = df_compare[\"fuzzy ratio\"].sum() / len(df_compare.index)\n",
    "    else:\n",
    "        score = 0\n",
    "    return df_compare, score\n",
    "\n",
    "\n",
    "def get_entity_level_statistics(\n",
    "    df_all_files: pd.DataFrame, df: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Returns dataframes with entity level stats - missed and wrongly captured entities.\n",
    "\n",
    "    Args:\n",
    "        df_all_files (pd.DataFrame): It is a DataFrame with following columns\n",
    "                                     ['Entity', 'Total entities',\n",
    "                                     'Entities not captured', 'Entities mismatch']\n",
    "        df (pd.DataFrame): It is also DataFrame but with different columns\n",
    "                           ['entity_name', 'initial_prediction', 'current_prediction']\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: It is DataFrame whic has all required Statistics about entities\n",
    "    \"\"\"\n",
    "\n",
    "    df_entities_count = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"Entity\",\n",
    "            \"Total entities\",\n",
    "            \"Entities not captured\",\n",
    "            \"Entities mismatch\",\n",
    "        ]\n",
    "    )\n",
    "    entity_count = df[\"entity_name\"].value_counts()\n",
    "    for entity in entity_count.index:\n",
    "        entities_not_captured = 0\n",
    "        entities_mismatch = 0\n",
    "        for value in df[df[\"entity_name\"] == entity].values:\n",
    "            if value[1] == \"Entity not found.\" or value[2] == \"Entity not found.\":\n",
    "                entities_not_captured += 1\n",
    "            elif value[4] > 0.0 and value[4] < 1.0:\n",
    "                entities_mismatch += 1\n",
    "        df_entities_count.loc[len(df_entities_count.index)] = [\n",
    "            entity,\n",
    "            entity_count[entity],\n",
    "            entities_not_captured,\n",
    "            entities_mismatch,\n",
    "        ]\n",
    "\n",
    "    for row in df_entities_count.values:\n",
    "        match = df_all_files[df_all_files[\"Entity\"] == row[0]]\n",
    "        if match.shape[0] == 0:\n",
    "            df_all_files.loc[len(df_all_files.index)] = row\n",
    "        else:\n",
    "            df_all_files.at[match.index[0], \"Total entities\"] = (\n",
    "                df_all_files.loc[match.index[0]][\"Total entities\"] + row[1]\n",
    "            )\n",
    "            df_all_files.at[match.index[0], \"Entities not captured\"] = (\n",
    "                df_all_files.loc[match.index[0]][\"Entities not captured\"] + row[2]\n",
    "            )\n",
    "            df_all_files.at[match.index[0], \"Entities mismatch\"] = (\n",
    "                df_all_files.loc[match.index[0]][\"Entities mismatch\"] + row[3]\n",
    "            )\n",
    "\n",
    "    return df_all_files\n",
    "\n",
    "\n",
    "def get_error_entities(df: pd.DataFrame, file_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Returns dataframes with mismtach and error entities\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): It is DataFrame with following columns\n",
    "                           ['entity_name', 'initial_prediction', 'current_prediction']\n",
    "        file_name (str): Name Of the JSON file with errors\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: It returns DataFrame with following fields\n",
    "                      ['File name', 'Entity', 'initial_prediction_value',\n",
    "                      'current_prediction_value', 'Match score']\n",
    "    \"\"\"\n",
    "\n",
    "    df_entities_error = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"File name\",\n",
    "            \"Entity\",\n",
    "            \"initial_prediction_value\",\n",
    "            \"current_prediction_value\",\n",
    "            \"Match score\",\n",
    "        ]\n",
    "    )\n",
    "    entity_count = df[\"entity_name\"].value_counts()\n",
    "    for entity in entity_count.index:\n",
    "        for value in df[df[\"entity_name\"] == entity].values:\n",
    "            if value[1] == \"Entity not found.\" or value[2] == \"Entity not found.\":\n",
    "                df_entities_error.loc[len(df_entities_error.index)] = [\n",
    "                    file_name,\n",
    "                    entity,\n",
    "                    value[1],\n",
    "                    value[2],\n",
    "                    value[4],\n",
    "                ]\n",
    "            elif value[4] > 0.0 and value[4] < 1.0:\n",
    "                df_entities_error.loc[len(df_entities_error.index)] = [\n",
    "                    file_name,\n",
    "                    entity,\n",
    "                    value[1],\n",
    "                    value[2],\n",
    "                    value[4],\n",
    "                ]\n",
    "    return df_entities_error\n",
    "\n",
    "\n",
    "def create_gsheet(file_name: str, user_email: str) -> spreadsheet.Spreadsheet:\n",
    "    \"\"\"Creates a google sheet\n",
    "\n",
    "    Args:\n",
    "        file_name (str): It is title for gsheet\n",
    "        user_email (str): User mail-id to get report of tool output data as gsheet\n",
    "\n",
    "    Returns:\n",
    "        spreadsheet.Spreadsheet: It returns newly created gsheet object\n",
    "    \"\"\"\n",
    "\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name(\n",
    "        GS_CREDENTIALS_PATH, SCOPE\n",
    "    )\n",
    "    client = gspread.authorize(credentials)\n",
    "    sheet = client.create(file_name)\n",
    "    sheet.share(user_email, perm_type=\"user\", role=\"writer\")\n",
    "    return sheet\n",
    "\n",
    "\n",
    "def save_data_to_sheet(gsheet_name: str, df: pd.DataFrame, name: str) -> None:\n",
    "    \"\"\"It writes dataframe to google sheet\n",
    "\n",
    "    Args:\n",
    "        gsheet_name (str): Title of gsheet\n",
    "        df (pd.DataFrame): It is DataFrame which need to write to gsheet\n",
    "        name (str): It is Name of the Tab in gsheet\n",
    "    \"\"\"\n",
    "\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name(\n",
    "        GS_CREDENTIALS_PATH, SCOPE\n",
    "    )\n",
    "    gc = gspread.authorize(credentials)\n",
    "    sheet_1 = gc.open(gsheet_name)\n",
    "    sheet_1.add_worksheet(title=name, rows=\"1000\", cols=\"20\")\n",
    "    sheet_2 = gc.open(gsheet_name).worksheet(name)\n",
    "    values = [df.columns.values.tolist()]\n",
    "    values.extend(df.values.tolist())\n",
    "    try:\n",
    "        sheet_2.update(range_name=\"A1:Z1000\", values=values)\n",
    "    except (TypeError, Exception) as e:\n",
    "        print(\n",
    "            \"sheet.update() -> method takes positional args only a1-notation & list[list[...]]\"\n",
    "        )\n",
    "        sheet_2.update(\"A1:Z1000\", values)\n",
    "    colouring_function(gsheet_name, name)\n",
    "\n",
    "\n",
    "def batch_process_documents(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    processor_id: str,\n",
    "    gcs_input_uri: str,\n",
    "    gcs_output_uri: str,\n",
    "    gcs_output_uri_prefix: str,\n",
    "    processor_version_id: str,\n",
    "    timeout: int = 500,\n",
    ") -> operation.Operation:\n",
    "    \"\"\"It will call `batch_process_documents_sample` to perform batch-job\n",
    "\n",
    "    Args:\n",
    "        project_id (str): GCP project ID\n",
    "        location (str): Processor location `us` or `eu`\n",
    "        processor_id (str): GCP DocumentAI ProcessorID\n",
    "        gcs_input_uri (str): GCS path which contains all input files\n",
    "        gcs_output_uri (str): GCS path without prefix to store processed JSON results\n",
    "        gcs_output_uri_prefix (str): GCS path prefix which need to append to gcs_output_uri\n",
    "        processor_version_id (str): VesrionID of GCP DocumentAI Processor\n",
    "        timeout (int, optional): Maximum waiting time for operation to complete. Defaults to 500.\n",
    "\n",
    "    Returns:\n",
    "        operation.Operation: LRO operation ID for current batch-job\n",
    "    \"\"\"\n",
    "\n",
    "    destination_uri = f\"{gcs_output_uri}/{gcs_output_uri_prefix}/\"\n",
    "    operation = batch_process_documents_sample(\n",
    "        project_id,\n",
    "        location,\n",
    "        processor_id,\n",
    "        gcs_input_uri,\n",
    "        destination_uri,\n",
    "        processor_version_id,\n",
    "        timeout,\n",
    "    )\n",
    "    return operation\n",
    "\n",
    "\n",
    "def asynchronous(\n",
    "    async_input_bucket_pdfs: str,\n",
    "    async_input_bucket_jsons: str,\n",
    "    async_output_bucket: str,\n",
    "    async_project_id: str,\n",
    "    async_processor_id: str,\n",
    "    processor_version_id: str,\n",
    "    async_location: str,\n",
    "    email_address: str,\n",
    ") -> None:\n",
    "    \"\"\"Here we are going to generate name of output folder as\n",
    "        Time+date and then process the documents\n",
    "\n",
    "    Args:\n",
    "        async_input_bucket_pdfs (str): GCS Path of source PDF files\n",
    "        async_input_bucket_jsons (str): GCS Path of JSON file\n",
    "        async_output_bucket (str): GCS Path to store currently-processed JSON file\n",
    "        async_project_id (str): GCP Project ID\n",
    "        async_processor_id (str): DocumentAI Processor ID\n",
    "        processor_version_id (str): VesrionID of GCP DocumentAI Processor\n",
    "        async_location (str): Processor location `us` or `eu`\n",
    "        email_address (str): User mail-id to get asynchronous-report of tool output data as gsheet\n",
    "    \"\"\"\n",
    "\n",
    "    now = datetime.now()\n",
    "    async_output_dir_prefix = now.strftime(\"%H%M%S%d%m%Y\")\n",
    "    output_bucket_name = re.split(\"/\", async_output_bucket)[2]\n",
    "    input_bucket_name = re.split(\"/\", async_input_bucket_jsons)[2]\n",
    "    start_time_async = time.time()\n",
    "    async_batch_process_result = batch_process_documents(\n",
    "        project_id=async_project_id,\n",
    "        location=async_location,\n",
    "        processor_id=async_processor_id,\n",
    "        gcs_input_uri=async_input_bucket_pdfs,\n",
    "        gcs_output_uri=async_output_bucket,\n",
    "        gcs_output_uri_prefix=async_output_dir_prefix,\n",
    "        processor_version_id=processor_version_id,\n",
    "        timeout=500,\n",
    "    )\n",
    "    end_time_async = time.time()\n",
    "    time_taken_async = end_time_async - start_time_async\n",
    "    # Now we have to create a table to map reference documents and previously accepted results\n",
    "    async_batch_process_result_metadata = async_batch_process_result.metadata\n",
    "    print(Back.CYAN + str(async_batch_process_result_metadata))\n",
    "    print(\n",
    "        Back.CYAN\n",
    "        + str(async_batch_process_result_metadata)\n",
    "        .replace(\"{\", \"}\")\n",
    "        .split(\"}\")[0]\n",
    "        .split(\"\\n\")[0]\n",
    "        .split(\":\")[1]\n",
    "    )\n",
    "    print(\"\\n\")\n",
    "    print(Back.CYAN + \"time_taken \\n\")\n",
    "    print(Back.YELLOW + \"    seconds: \" + str(time_taken_async) + \"\\n\")\n",
    "    temp_individual_process_status_list = re.split(\n",
    "        \"individual_process_statuses\", str(async_batch_process_result_metadata)\n",
    "    )\n",
    "    empty_data_array = []\n",
    "    for element in temp_individual_process_status_list:\n",
    "        if element[:23] == \" {\\n  input_gcs_source: \":\n",
    "            temp_dict_d = {}\n",
    "            temp_var_x = re.split('\"', element)\n",
    "            temp_dict_d[\"input_file\"] = temp_var_x[1]\n",
    "            temp_dict_d[\"output_folder\"] = temp_var_x[3]\n",
    "            empty_data_array.append(temp_dict_d)\n",
    "    dataframe_without_human_review_status = pd.DataFrame.from_dict(empty_data_array)\n",
    "    dataframe_without_human_review_status_dict = (\n",
    "        dataframe_without_human_review_status.to_dict()\n",
    "    )\n",
    "    pdf_to_output_folder_map = {}\n",
    "    for keys_input, values_input in dataframe_without_human_review_status_dict[\n",
    "        \"input_file\"\n",
    "    ].items():\n",
    "        temp_var_a = re.split(\"/\", values_input)[-1]\n",
    "        temp_var_b = \"/\".join(\n",
    "            dataframe_without_human_review_status_dict[\"output_folder\"][\n",
    "                keys_input\n",
    "            ].split(\"/\")[3:]\n",
    "        )\n",
    "        pdf_to_output_folder_map[temp_var_a] = temp_var_b\n",
    "    #  Now input files is mapped to output folders and\n",
    "    #  now we have to get input file to output file map\n",
    "    client_obj = storage.Client()\n",
    "    bucket_obj = client_obj.get_bucket(output_bucket_name)\n",
    "    blobs_list = bucket_obj.list_blobs()\n",
    "    temp_blobs_list = []\n",
    "    for i in blobs_list:\n",
    "        temp_blobs_list.append(str(i.name))\n",
    "    pdf_to_output_file_map = {}\n",
    "    for keys, values in pdf_to_output_folder_map.items():\n",
    "        for i in temp_blobs_list[:]:\n",
    "            if re.split(\"/\", values) == re.split(\"/\", i)[:-1]:\n",
    "                pdf_to_output_file_map[keys] = i\n",
    "    #  Now we have to map input pdf files to input jsons\n",
    "\n",
    "    input_json_blob_name_list = re.split(\"/\", async_input_bucket_jsons)[3:]\n",
    "    pdf_to_input_file_map = {}\n",
    "    for keys, values in pdf_to_output_folder_map.items():\n",
    "        for i in temp_blobs_list[:]:\n",
    "            if input_json_blob_name_list == re.split(\"/\", i)[:-1]:\n",
    "                if keys.split(\".\")[0] == re.split(\"/\", i)[-1][:-5]:\n",
    "                    pdf_to_input_file_map[keys] = i\n",
    "    #  Create a output Gsheet\n",
    "    print(\"Creating Comparision Gsheet,Wait for the Success message...\")\n",
    "    create_gsheet(\"Asynchronous-\" + async_output_dir_prefix, email_address)\n",
    "    # Now both input and output jsons are mapped and one by one\n",
    "    # we'll load in memory and compare it and load it in gSheet.\n",
    "    df_entities_all_files = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"Entity\",\n",
    "            \"Total entities\",\n",
    "            \"Entities not captured\",\n",
    "            \"Entities mismatch\",\n",
    "        ]\n",
    "    )\n",
    "    df_entities_error_all_files = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"File name\",\n",
    "            \"Entity\",\n",
    "            \"initial_prediction_value\",\n",
    "            \"current_prediction_value\",\n",
    "            \"Match score\",\n",
    "        ]\n",
    "    )\n",
    "    for key, value in pdf_to_output_file_map.items():\n",
    "        print(\"for file:\", key)\n",
    "        output_json_file = documentai_json_proto_downloader(output_bucket_name, value)\n",
    "        input_json_file = documentai_json_proto_downloader(\n",
    "            input_bucket_name, pdf_to_input_file_map[key]\n",
    "        )\n",
    "        async_output_dataframe, _ = compare_doc_proto_convert_dataframe(\n",
    "            input_json_file, output_json_file\n",
    "        )\n",
    "        time.sleep(5)\n",
    "        save_data_to_sheet(\n",
    "            \"Asynchronous-\" + async_output_dir_prefix, async_output_dataframe, str(key)\n",
    "        )\n",
    "        df_entities_all_files = get_entity_level_statistics(\n",
    "            df_entities_all_files, async_output_dataframe\n",
    "        )\n",
    "        df_error_entities = get_error_entities(async_output_dataframe, str(key))\n",
    "        df_entities_error_all_files = pd.concat(\n",
    "            [df_entities_error_all_files, df_error_entities], ignore_index=True\n",
    "        )\n",
    "    time.sleep(5)\n",
    "    save_data_to_sheet(\n",
    "        \"Asynchronous-\" + async_output_dir_prefix, df_entities_all_files, \"All Entities\"\n",
    "    )\n",
    "    time.sleep(5)\n",
    "    save_data_to_sheet(\n",
    "        \"Asynchronous-\" + async_output_dir_prefix,\n",
    "        df_entities_error_all_files,\n",
    "        \"Error entities\",\n",
    "    )\n",
    "\n",
    "\n",
    "def synchronous(\n",
    "    sync_input_bucket_pdf: str,\n",
    "    sync_input_bucket_json: str,\n",
    "    sync_project_id: str,\n",
    "    sync_processor_id: str,\n",
    "    processor_version_id: str,\n",
    "    sync_location: str,\n",
    "    email_address: str,\n",
    "):\n",
    "    \"\"\"Here we have to download the pdf file into memory first and then use it.\n",
    "\n",
    "    Args:\n",
    "        sync_input_bucket_pdf (str): GCS URI of source PDF files\n",
    "        sync_input_bucket_json (str): GCS URI of JSON file\n",
    "        sync_project_id (str): GCP Project ID\n",
    "        sync_processor_id (str): DocumentAI Processor ID\n",
    "        processor_version_id (str): VesrionID of GCP DocumentAI Processor\n",
    "        sync_location (str): Processor location `us` or `eu`\n",
    "        email_address (str): User mail-id to get aynchronous-report of tool output data as gsheet\n",
    "    \"\"\"\n",
    "\n",
    "    pdf_file_name = sync_input_bucket_pdf.split(\"/\")[-1]\n",
    "    storage_client_obj = storage.Client()\n",
    "    bucket_name = re.split(\"/\", sync_input_bucket_pdf)[2]\n",
    "    blob_name_list = re.split(\"/\", sync_input_bucket_pdf)[3:]\n",
    "    temp_blob_name = \"\"\n",
    "    for blob_name in blob_name_list:\n",
    "        temp_blob_name = temp_blob_name + \"/\" + blob_name\n",
    "    blob_name = temp_blob_name[1:]\n",
    "    bucket_obj = storage_client_obj.bucket(bucket_name)\n",
    "    blob_obj = bucket_obj.blob(blob_name)\n",
    "    document_as_string = blob_obj.download_as_string()\n",
    "    # Now we have document in memory as string and we have to process it.\n",
    "    time_begin_sync = time.time()\n",
    "    print(\"Document is processing...\")\n",
    "    processed_document = process_document_sample(\n",
    "        sync_project_id,\n",
    "        sync_location,\n",
    "        sync_processor_id,\n",
    "        document_as_string,\n",
    "        processor_version_id,\n",
    "    )\n",
    "    time_end_sync = time.time()\n",
    "    time_taken = time_end_sync - time_begin_sync\n",
    "    print(Back.CYAN + '\"Processed 1 document successfully\"\\n')\n",
    "    print(Back.CYAN + \"time_taken \\n\")\n",
    "    print(Back.YELLOW + \"    seconds: \" + str(time_taken) + \"\\n\")\n",
    "    # Document has been processed now we have\n",
    "    # to download previously accepted result from bucket\n",
    "    bucket_name_json = re.split(\"/\", sync_input_bucket_json)[2]\n",
    "    blob_name_list_json = re.split(\"/\", sync_input_bucket_json)[3:]\n",
    "    temp_blob_name_json = \"\"\n",
    "    for blob_name in blob_name_list_json:\n",
    "        temp_blob_name_json = temp_blob_name_json + \"/\" + blob_name\n",
    "    blob_name_json = temp_blob_name_json[1:]\n",
    "    previously_accepted_result = documentai_json_proto_downloader(\n",
    "        bucket_name_json, blob_name_json\n",
    "    )\n",
    "    print(\n",
    "        \"Creating comparision Gsheet, Wait for success message and check your mail...\"\n",
    "    )\n",
    "    # Load processed_document and previously_accepted_result as\n",
    "    # json files and compare using helper functions\n",
    "    sync_output_dataframe, _ = compare_doc_proto_convert_dataframe(\n",
    "        previously_accepted_result, processed_document.document\n",
    "    )\n",
    "    df_entities = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"Entity\",\n",
    "            \"Total entities\",\n",
    "            \"Entities not captured\",\n",
    "            \"Entities mismatch\",\n",
    "        ]\n",
    "    )\n",
    "    df_entities = get_entity_level_statistics(df_entities, sync_output_dataframe)\n",
    "    df_entities_error = get_error_entities(sync_output_dataframe, pdf_file_name)\n",
    "    create_gsheet(\"Synchronous\", email_address)\n",
    "    save_data_to_sheet(\"Synchronous\", sync_output_dataframe, pdf_file_name)\n",
    "    save_data_to_sheet(\"Synchronous\", df_entities, \"All Entities\")\n",
    "    save_data_to_sheet(\"Synchronous\", df_entities_error, \"Error entities\")\n",
    "\n",
    "\n",
    "if MODE.upper() == \"I\":\n",
    "    start_time_async = time.time()\n",
    "    now = datetime.now()\n",
    "    async_output_dir_prefix = now.strftime(\"%H%M%S%d%m%Y\")\n",
    "    pdfs_names_list, _ = file_names(ASYNC_INPUT_BUCKET_PDFS)\n",
    "    jsons_names_list, _ = file_names(INITIAL_JSON_BUCKET)\n",
    "    file_name_dict = {a.split(\".\")[0]: a for a in pdfs_names_list}\n",
    "    json_name_dict = {a.split(\".\")[0]: a for a in jsons_names_list}\n",
    "    files_list = list(file_name_dict.keys())\n",
    "    temp_bucket = ASYNC_INPUT_BUCKET_PDFS.split(\"/\")[2]\n",
    "    storage_client = storage.Client()\n",
    "    source_bucket = storage_client.get_bucket(temp_bucket)\n",
    "    list_new = []\n",
    "    for file in files_list:\n",
    "        if file in json_name_dict.keys():\n",
    "            print(f\"Initial Json file already exists for:{file_name_dict[file]}\")\n",
    "            continue\n",
    "        list_new.append(file)\n",
    "        source_blob = source_bucket.blob(file_name_dict[file])\n",
    "        temp = f\"{file_name_dict[file]}\"\n",
    "        file_name_temp = (\"/\").join(ASYNC_INPUT_BUCKET_PDFS.split(\"/\")[3:]) + \"/\" + temp\n",
    "        prefix = (\n",
    "            ASYNC_INPUT_BUCKET_PDFS.rsplit(\"/\", maxsplit=1)[-1]\n",
    "            + \"/\"\n",
    "            + \"initial_pdfs_\"\n",
    "            + f\"{async_output_dir_prefix}\"\n",
    "            + \"/\"\n",
    "            + temp\n",
    "        )\n",
    "        copy_blob(temp_bucket, file_name_temp, temp_bucket, prefix)\n",
    "        print(\n",
    "            f\"Blob copied from gs://{temp_bucket}/{file_name_temp}\"\n",
    "            f\" to\\n\\t\\tgs://{temp_bucket}/{prefix}\"\n",
    "        )\n",
    "    temp_initial_path = (\n",
    "        \"gs://\"\n",
    "        + temp_bucket\n",
    "        + \"/\"\n",
    "        + ASYNC_INPUT_BUCKET_PDFS.rsplit(\"/\", maxsplit=1)[-1]\n",
    "        + \"/\"\n",
    "        + \"initial_pdfs_\"\n",
    "        + f\"{async_output_dir_prefix}\"\n",
    "    )\n",
    "    if len(list_new) == 0:\n",
    "        print(\"There are NO new files to Parse\")\n",
    "    else:\n",
    "        async_batch_initial, initial_json_path = batchprocess_intial_mode1(\n",
    "            project_id=ASYNC_PROJECT_ID,\n",
    "            location=ASYNC_LOCATION,\n",
    "            processor_id=ASYNC_PROCESSOR_ID,\n",
    "            processor_version_id=PROCESSOR_VERSION_ID,\n",
    "            gcs_input_uri=temp_initial_path,\n",
    "            initial_json_path=INITIAL_JSON_BUCKET,\n",
    "            timeout=500,\n",
    "        )\n",
    "        temp_folder = (\n",
    "            (\"/\").join(ASYNC_INPUT_BUCKET_PDFS.split(\"/\")[-1:])\n",
    "            + \"/\"\n",
    "            + \"initial_pdfs_\"\n",
    "            + f\"{async_output_dir_prefix}\"\n",
    "            + \"/\"\n",
    "        )\n",
    "        delete_folder(temp_bucket, temp_folder)\n",
    "        end_time_async = time.time()\n",
    "        time_taken_async = end_time_async - start_time_async\n",
    "        print(\n",
    "            \"Time taken to process the files and create initial json files:\",\n",
    "            f\"{time_taken_async}\",\n",
    "        )\n",
    "        print(\"The Intitial Json files are saved in: \", f\"{initial_json_path}\")\n",
    "elif MODE.upper() == \"C\":\n",
    "    if PROCESS_TYPE.upper() == \"A\":\n",
    "        asynchronous(\n",
    "            ASYNC_INPUT_BUCKET_PDFS,\n",
    "            INITIAL_JSON_BUCKET,\n",
    "            ASYNC_OUTPUT_BUCKET,\n",
    "            ASYNC_PROJECT_ID,\n",
    "            ASYNC_PROCESSOR_ID,\n",
    "            PROCESSOR_VERSION_ID,\n",
    "            ASYNC_LOCATION,\n",
    "            EMAIL_ADDRESS,\n",
    "        )\n",
    "    elif PROCESS_TYPE.upper() == \"S\":\n",
    "        synchronous(\n",
    "            SYNC_INPUT_BUCKET_PDF,\n",
    "            SYNC_INPUT_BUCKET_JSON,\n",
    "            SYNC_PROJECT_ID,\n",
    "            SYNC_PROCESSOR_ID,\n",
    "            PROCESSOR_VERSION_ID,\n",
    "            SYNC_LOCATION,\n",
    "            EMAIL_ADDRESS,\n",
    "        )\n",
    "\n",
    "print(Fore.GREEN + Back.CYAN + Style.BRIGHT + \"*\" * 47 + \"SUCCESS\" + \"*\" * 47)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cfdbcca5-c332-4972-8c1a-9997a1e64734",
   "metadata": {},
   "source": [
    "# Step by Step procedure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c24298b7-5fbd-4b63-aa40-1aaa78169cb8",
   "metadata": {},
   "source": [
    "### Step 1 : Fill the details for all input variables\n",
    "Type of process have to be S for Synchronous process or A for Asynchronous process.\n",
    "\n",
    "### Step 2 : Run the Python Code Cells and wait for the success message\n",
    "If everything is smooth then after some time a message appear on screen as shown below.  \n",
    "<img src=\"./images/time_taken_by_api.png\" width=500 height=100> </img>  \n",
    "<!-- ![](./images/time_taken_by_api.png) -->\n",
    "Here yellow coloured highlighted text is the time taken by DocAI API to process the document. It means till now everything is smooth and now it compare the processed result with previously accepted result file and generate a google sheet as output.\n",
    "\n",
    "Now wait for the process to finish. Once it is done, a “SUCCESS” message would be displayed.\n",
    "Which means that a Google sheet has been shared to the given email address having reports of test harness tool results.  \n",
    "<img src=\"./images/success_message.png\" width=400 height=100> </img>  \n",
    "<!-- ![](./images/success_message.png) -->\n",
    "\n",
    "### Step 3 : Check the mailbox\n",
    "Now check in the mailbox of “Email” that has been provided and find a Google sheet shared by the service account having all details of side by side comparison.  \n",
    "<img src=\"./images/check_mail.png\" width=400 height=200> </img>  \n",
    "<!-- ![](./images/check_mail.png) -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75322d77-4453-47dd-9b86-68199431db3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using Google Colab instead of vertex AI Notebook\n",
    "Google colab can be used instead of vertex AI notebook which can be a cost effective way of using the Test Harness Tool.\n",
    "For this there are few preparation steps.\n",
    "### Step 1 : Installing modules\n",
    "Open a new colab notebook and run commands for required packages \n",
    "```python\n",
    "!pip install colorama  \n",
    "!pip install gspread_formatting  \n",
    "!pip install google-cloud-documentai  \n",
    "!pip install df2gspread  \n",
    "!pip install configparser  \n",
    "!pip install google-cloud \n",
    "!pip install spread\n",
    "```\n",
    "It should  install the modules.  \n",
    "\n",
    "### Step 2 : Restart the runtime\n",
    "Running Step 1 give a button to restart the runtime. Click on “ Restart Runtime” and “Yes” to restart the runtime.  \n",
    "<img src=\"./images/restart_runtime.png\" width=400 height=100> </img>  \n",
    "<!-- ![](./images/restart_runtime.png) -->\n",
    "\n",
    "### Step 3 : Set the colab environment\n",
    "Now in a new cell put these commands : \n",
    "```python\n",
    "os.environ['GCLOUD_PROJECT']= PROJECT_NUMBER  \n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS']=\"PATH/to/apikey.json\"  \n",
    "!export GOOGLE_APPLICATION_CREDENTIALS=/content/config/apiKey.json  \n",
    "!gcloud auth login   \n",
    "!gcloud config set project PROJECT_NAME \n",
    "```\n",
    "<img src=\"./images/set_colab_environment_2.png\" width=200 height=100> </img>  \n",
    "<!-- ![](./images/set_colab_environment_2.png) -->\n",
    "\n",
    "Now after running the cell it should give a link to authenticate google project. Click on the link.   \n",
    "<img src=\"./images/click_here.png\" width=800 height=100> </img>  \n",
    "<!-- ![](./images/click_here.png)   -->\n",
    "It will open a new popup window in which “choose your account” and”Allow” and copy the code.  \n",
    "<img src=\"./images/gcloud_cli.png\" width=400 height=400> </img>  \n",
    "<!-- ![](./images/gcloud_cli.png)   -->\n",
    "Now come back to the colab tab and paste the copied code and “Enter”.  \n",
    "<img src=\"./images/paste_code_here.png\" width=800 height=100> </img>  \n",
    "<!-- ![](./images/paste_code_here.png) -->\n",
    "\n",
    "### Step 4 : Copy and paste the test harness tool script (code) and run\n",
    "Now just copy and paste the test harness tool script as it is in the next cell and run it.\n",
    "And Follow the steps as per the Modes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb98c893-528d-4ca8-9465-f3c946526f0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sample outputs and Explanation\n",
    "\n",
    "### Synchronous Output\n",
    "Sample Output file : **Synchronous**  \n",
    "Explanation : In the Synchronous mode output should be be three worksheets ”fileName.pdf”, “All Entities” and “Error Entities”.  \n",
    "<img src=\"./images/tabs_sync_out.png\" width=800 height=100> </img>  \n",
    "<!-- ![](./images/tabs_sync_out.png)   -->\n",
    "In the “fileName.pdf” worksheet there are five columns named as “A:entity_name”, “B: initial_prediction”, “C:current_prediction”, “D:match” and  “E:fuzzy ratio”.\n",
    "\n",
    "Here “A:entity_name” represents the name of the entities present in both input and output json files. “B: initial_prediction” represents the data associated with the entity in file 1 (i.e json file generated using docAI API) similarly in “C:current_prediction” represents the data associated with corresponding entity in file 2 (i.e previously accepted result json file), “D:match”  shows boolean values (i.e True of False) where True means the the value is exactly same as it was in the previously accepted result and false means there is some mismatch between the data in entity. Last “E:fuzzy ratio” gives a number between 0 to 1 which tells that if an entity was present in both of the files but still there is some difference in data then how much that difference is. Column “E” also shows three possible colors.\n",
    "\n",
    "The color coding is defined to show the clear difference between two schemas:  \n",
    "* Green→<img src=\"./images/green.png\" width=20 height=7> </img>→ Entity matches in both files more than 75%.\n",
    "* Yellow→<img src=\"./images/yellow.png\" width=20 height=7> </img>→Entity matches in both files from 51% to 75%.\n",
    "* Pink→<img src=\"./images/pink.png\" width=20 height=7> </img>→Entity matching percentage is below 51%.\n",
    "\n",
    "Next worksheet “All Entities” is the report of total entities of various types out of which how many were not captured and how many were captured but with a slight change.\n",
    "\n",
    "The last worksheet is “Error Entities”  which contains a list of all entities which have some issues and what is wrong with it. It has five columns representing “ input file name”, “ name of entity”, “ value in file 1”, “ value in file 2” and “matching score”. In the last column  color coding is also there as we’ve used above.\n",
    "\n",
    "### Asynchronous Output\n",
    "Sample Output file : **Asynchronous-14224306092022**\n",
    "Explanation : If there are N numbers of input file should be N+2 worksheets in the output file.\n",
    "<img src=\"./images/tabs_async_out.png\" width=800 height=50> </img>  \n",
    "<!-- ![](./images/tabs_async_out.png)   -->\n",
    "Similar to synchronous mode, in asynchronous mode output should be N + 2 worksheets, N worksheets labeled as  ”fileName.pdf”, “All Entities” and “Error Entities”.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca2b6896-02e0-40d4-9867-f46ae9aa125d",
   "metadata": {},
   "source": [
    "### Output Color Coding Sample\n",
    "<img src=\"./images/color_code_sample_async.png\" width=800 height=400></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c048ab-c8f5-482e-b3f4-2a781cf544ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c75ad0b-856b-4a83-83ad-15803e6dba2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m104",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m104"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
