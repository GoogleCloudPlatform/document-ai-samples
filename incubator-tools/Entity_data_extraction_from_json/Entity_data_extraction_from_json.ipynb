{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df4f9f28-6a40-46f7-9c2b-86751719d5a5",
   "metadata": {},
   "source": [
    "# Entity data extraction from DocAI Parsed JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23abed54-f52b-43cc-9174-175b94ea9ad0",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dee95c-b9c8-45b6-9866-f90d45e3771c",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the DocAI Incubator Team. No guarantees of performance are implied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac998a81-92fe-4547-9fe1-f17a3d292db4",
   "metadata": {},
   "source": [
    "## Purpose and Description\n",
    "This tool allows you to extract entities and their confidence scores from the Input DocAI parsed JSON files, saving the results in either JSON or CSV format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ff0ed1-5ec5-4847-a4d9-19fe49e44a80",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Vertex AI Notebook or Google Colab\n",
    "2. GCS bucket for processing of  the input json and output json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a671611-da12-4f94-9b33-9d7c5b0f9ac4",
   "metadata": {},
   "source": [
    "## Step by Step procedure "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f8f9d2-53f6-49e5-9643-7a2ffe3e04c4",
   "metadata": {},
   "source": [
    "### 1. Install the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dff621-4682-4fc2-ae8b-7e2f0e7c812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google-cloud-storage\n",
    "%pip install google-cloud-documentai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c52ad40-f1f9-4a21-b833-270cb8e7e6e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fca3486-789c-4966-b683-ccdad4ec2d52",
   "metadata": {},
   "source": [
    "### 2. Import the required libraries/Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323ae747-9edf-4146-8cb6-206d9989eb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from google.cloud import storage\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "from utilities import (\n",
    "    file_names,\n",
    "    documentai_json_proto_downloader,\n",
    "    store_document_as_json,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad11a754-0f71-4c64-afd0-8e65eba489cf",
   "metadata": {},
   "source": [
    "### 3. Input Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e10603a-fe45-4bc4-b3f9-d9e8164880b9",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li><b>gcs_input_path :</b> GCS Storage name. It should contain DocAI processed output json files. This bucket is used for processing input files.</li>\n",
    "    <li><b>gcs_output_path:</b> This is the path where the extracted entities with their associated confidence scores are put together in a Json file and stored in the output bucket.</li>\n",
    "    <li><b>local_output_csv_path:</b> This is the path where the csv file will be stored </li>\n",
    "    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2608704f-b209-41b3-8e8e-5347c1375824",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_input_path = \"gs://Bucket_name/path_to_docs/\"\n",
    "gcs_output_path = \"gs://Bucket_name/path_to_docs/\"\n",
    "local_output_csv_path = \"output.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8bef4b-58d4-442a-9d92-e0698bbd36f2",
   "metadata": {},
   "source": [
    "### 4.Execute the code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9f1874-de45-41b7-9336-ae2b46e9149f",
   "metadata": {},
   "source": [
    "#### Storing the entities data as Json File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ef22cd-ca57-4364-8667-6bbad3a38648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hierarchical_data(json_dict):\n",
    "    def parse_entity(entity):\n",
    "        parsed_entity = {\n",
    "            \"Entity Type\": entity.type,\n",
    "            \"Entity Mentiontext\": entity.mention_text,\n",
    "            \"Confidence Score\": entity.confidence,\n",
    "        }\n",
    "        # print(entity)\n",
    "        if entity.properties:\n",
    "            parsed_entity[\"Children\"] = [\n",
    "                parse_entity(sub_entity) for sub_entity in entity.properties\n",
    "            ]\n",
    "        return parsed_entity\n",
    "\n",
    "    entities_data = [parse_entity(entity) for entity in json_dict.entities]\n",
    "    return entities_data\n",
    "\n",
    "\n",
    "# Function to process each file and generate the structured output\n",
    "def process_files(file_dict):\n",
    "    data = []\n",
    "    for filename, filepath in tqdm(file_dict.items(), desc=\"Progress\"):\n",
    "        print(\"Processing >>>>>>>>>\", filename)\n",
    "        input_bucket_name = gcs_input_path.split(\"/\")[2]\n",
    "        document_proto = documentai_json_proto_downloader(input_bucket_name, filepath)\n",
    "        # Get hierarchical data for the current file\n",
    "        entities_data = get_hierarchical_data(document_proto)\n",
    "        # Append the structured data to the list\n",
    "        data.append({\"File Name\": filename, \"Entities\": entities_data})\n",
    "    return data\n",
    "\n",
    "\n",
    "def main(gcs_input_path, gcs_output_path):\n",
    "    # Load the file names and paths (you need to define this function or replace it with your implementation)\n",
    "    file_names_list, file_dict = file_names(gcs_input_path)\n",
    "    # Process the files and get the structured output\n",
    "    data = process_files(file_dict)\n",
    "    # Save the structured output to GCS\n",
    "    output_bucket_name = gcs_output_path.split(\"/\")[2]\n",
    "    output_path_within_bucket = \"/\".join(gcs_output_path.split(\"/\")[3:])\n",
    "    for entry in data:\n",
    "        filename = entry[\"File Name\"]\n",
    "        store_document_as_json(\n",
    "            json.dumps(entry),\n",
    "            output_bucket_name,\n",
    "            f\"{output_path_within_bucket}/{filename}\",\n",
    "        )\n",
    "\n",
    "\n",
    "main(gcs_input_path, gcs_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f0a512-b3e1-4055-a349-b88ef3287717",
   "metadata": {},
   "source": [
    "#### Storing the entities data in a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217f8fba-9813-4d6d-b6dd-61e936e96b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_hierarchical_data(entity, parent_type=\"\", parent_text=\"\"):\n",
    "    flat_data = []\n",
    "    entity_data = {\n",
    "        \"Parent Entity Type\": parent_type,\n",
    "        \"Parent Entity Mentiontext\": parent_text,\n",
    "        \"Entity Type\": entity.type,\n",
    "        \"Entity Mentiontext\": entity.mention_text,\n",
    "        \"Confidence Score\": entity.confidence,\n",
    "    }\n",
    "    flat_data.append(entity_data)\n",
    "    if entity.properties:\n",
    "        for sub_entity in entity.properties:\n",
    "            flat_data.extend(\n",
    "                flatten_hierarchical_data(sub_entity, entity.type, entity.mention_text)\n",
    "            )\n",
    "    return flat_data\n",
    "\n",
    "\n",
    "def get_flat_data(json_dict):\n",
    "    flat_data = []\n",
    "    for entity in json_dict.entities:\n",
    "        flat_data.extend(flatten_hierarchical_data(entity))\n",
    "    return flat_data\n",
    "\n",
    "\n",
    "# Function to process each file and generate the structured output\n",
    "def process_files(file_dict):\n",
    "    all_data = []\n",
    "    for filename, filepath in tqdm(file_dict.items(), desc=\"Progress\"):\n",
    "        print(\"Processing >>>>>>>>>\", filename)\n",
    "        input_bucket_name = gcs_input_path.split(\"/\")[2]\n",
    "        document_proto = documentai_json_proto_downloader(input_bucket_name, filepath)\n",
    "        # Get flat data for the current file\n",
    "        flat_data = get_flat_data(document_proto)\n",
    "        for entry in flat_data:\n",
    "            entry[\"File Name\"] = filename\n",
    "            all_data.append(entry)\n",
    "    return all_data\n",
    "\n",
    "\n",
    "def main(Gcs_input_path, local_output_csv_path):\n",
    "    # Load the file names and paths (you need to define this function or replace it with your implementation)\n",
    "    file_names_list, file_dict = file_names(gcs_input_path)\n",
    "    # Process the files and get the structured output\n",
    "    data = process_files(file_dict)\n",
    "    # Convert the structured output to a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    # Save the DataFrame to a CSV file\n",
    "    output_csv_path = local_output_csv_path  # Replace with your desired output path\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Data has been successfully saved to {output_csv_path}\")\n",
    "\n",
    "\n",
    "main(gcs_input_path, local_output_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c02c60-a015-4318-a8d5-a119302a7244",
   "metadata": {},
   "source": [
    "### 5.Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35324007-522c-41d0-85e9-220d2bab240f",
   "metadata": {},
   "source": [
    "The post-processed JSON file be found in the storage path provided by the user during the script execution, which is output_bucket_path. The CSV file is also located in the specified location. <br><hr>\n",
    "<b>Sample json And CSV file </b><br><br>\n",
    "<i><h4>Post processing results<h4><i><br>\n",
    "    \n",
    "<b>json  file </b>\n",
    "    \n",
    "<img src= \"./images/image1.png\" width=800 height=400>\n",
    "<br><br>   \n",
    "<b> CSV  file </b>\n",
    "<br>\n",
    "<img src= \"./images/image2.png\" width=800 height=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4ac41d-4323-4682-a29e-7a277f0cfaaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
