{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66a897de-24dc-428a-afb9-21eede2c8545",
   "metadata": {},
   "source": [
    "# Entity Label Restructuring Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1d66f7-145a-4865-bfd2-644d8bb87c4f",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048272f7-2c36-49ce-96ef-af2ac250a7e3",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the **DocAI Incubator Team**. No guarantees of performance are implied.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e292fb-27fd-4cb6-959b-aa1c1f8add5e",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "This tool will first restructure the labeling, eliminating nested entities and treating them as standard entities. Subsequently, post-processing will reinstate the visual grouping originally established by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56373ed-9505-4389-b66a-70d7252d4f34",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* Vertex AI Notebook\n",
    "* DocumentAI Parser output\n",
    "* GCS bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87224b66-9c71-4246-bfad-e0c6dd580256",
   "metadata": {},
   "source": [
    "## Step by Step procedure "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8909a50-01f7-452e-bc28-1b718cfa32a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.Importing Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd376493-75d9-4ad7-9d9d-527807d65d16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7787d2-00c8-4439-9b53-0f9cdf7b6d4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install google-cloud-documentai google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fa38e8-fa31-4eb7-a61a-3dd586f3c62b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from google.api_core.client_options import ClientOptions\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from google.cloud import storage\n",
    "from utilities import store_document_as_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d317056e-bf75-4285-b26f-77da6472dc87",
   "metadata": {},
   "source": [
    "### 2.Setup the inputs\n",
    "* `project_id` : Provide GCP project Number\n",
    "* `location` : The region where the resources or services are hosted\n",
    "* `processor_id` : The unique identifier of the Google Cloud Processor.\n",
    "* `schema_file_path`: The Google Cloud Storage (gs) path for the schema file to store\n",
    "* `export_dataset_path`: The Google Cloud Storage (gs) path for exported labeled_jsons where you wish to store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64300ddd-8b3b-4b6d-908f-8ee66306c58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = \"project_id\"\n",
    "location = \"us\" or \"eu\"\n",
    "processor_id = \"xxxx-xxxx-xxxx\"\n",
    "schema_file_path = \"gs://bucket_name/path_to_schema_file/\"\n",
    "export_dataset_path = \"gs://bucket_name/path_to_output_folder/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df84544-02b0-4467-80fe-53aecd1cfcb8",
   "metadata": {},
   "source": [
    "### 3.Run the required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a765651-6b65-4dc4-a46c-7177aca64fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_documents(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    processor: str,\n",
    "    page_size: int = 100,\n",
    "    page_token: str = \"\",\n",
    ") -> documentai.types.ListDocumentsResponse:\n",
    "    \"\"\"\n",
    "    Lists documents in a dataset for a specified Document AI processor.\n",
    "\n",
    "    Args:\n",
    "        project_id (str): The ID of the Google Cloud project.\n",
    "        location (str): The location of the Document AI processor.\n",
    "        processor (str): The ID of the Document AI processor.\n",
    "        page_size (int, optional): The maximum number of documents to return per page. Default is 100.\n",
    "        page_token (str, optional): A token for pagination to retrieve the next set of results.\n",
    "\n",
    "    Returns:\n",
    "        documentai.types.ListDocumentsResponse: A response object containing the list of documents.\n",
    "    \"\"\"\n",
    "    client = documentai.DocumentServiceClient()\n",
    "    dataset = (\n",
    "        f\"projects/{project_id}/locations/{location}/processors/{processor}/dataset\"\n",
    "    )\n",
    "    request = documentai.types.ListDocumentsRequest(\n",
    "        dataset=dataset,\n",
    "        page_token=page_token,\n",
    "        page_size=page_size,\n",
    "        return_total_size=True,\n",
    "    )\n",
    "    operation = client.list_documents(request)\n",
    "    return operation\n",
    "\n",
    "\n",
    "def get_document(\n",
    "    project_id: str, location: str, processor: str, doc_id: str\n",
    ") -> documentai.types.Document:\n",
    "    \"\"\"\n",
    "    Retrieves a specific document from a dataset.\n",
    "\n",
    "    Args:\n",
    "        project_id (str): The ID of the Google Cloud project.\n",
    "        location (str): The location of the Document AI processor.\n",
    "        processor (str): The ID of the Document AI processor.\n",
    "        doc_id (str): The ID of the document to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        documentai.types.Document: The retrieved document object.\n",
    "    \"\"\"\n",
    "    client = documentai.DocumentServiceClient()\n",
    "    dataset = (\n",
    "        f\"projects/{project_id}/locations/{location}/processors/{processor}/dataset\"\n",
    "    )\n",
    "    request = documentai.types.GetDocumentRequest(dataset=dataset, document_id=doc_id)\n",
    "    operation = client.get_document(request)\n",
    "    return operation.document\n",
    "\n",
    "\n",
    "def get_dataset_schema(\n",
    "    project_id: str, processor_id: str, location: str\n",
    ") -> documentai.types.DatasetSchema:\n",
    "    \"\"\"\n",
    "    Retrieves the dataset schema for a specified Document AI processor.\n",
    "\n",
    "    Args:\n",
    "        project_id (str): The ID of the Google Cloud project.\n",
    "        processor_id (str): The ID of the Document AI processor.\n",
    "        location (str): The location of the Document AI processor.\n",
    "\n",
    "    Returns:\n",
    "        documentai.types.DatasetSchema: The dataset schema object.\n",
    "    \"\"\"\n",
    "    # Create a client\n",
    "    processor_name = (\n",
    "        f\"projects/{project_id}/locations/{location}/processors/{processor_id}\"\n",
    "    )\n",
    "    client = documentai.DocumentServiceClient()\n",
    "    request = documentai.GetDatasetSchemaRequest(\n",
    "        name=processor_name + \"/dataset/datasetSchema\"\n",
    "    )\n",
    "    # Make the request\n",
    "    response = client.get_dataset_schema(request=request)\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def remove_child_entities(json_dict: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Removes child entities from the given JSON dictionary and returns a new dictionary.\n",
    "\n",
    "    Args:\n",
    "        json_dict (dict): The JSON dictionary containing entities.\n",
    "\n",
    "    Returns:\n",
    "        dict: The modified JSON dictionary with child entities removed.\n",
    "    \"\"\"\n",
    "    new_entities = []\n",
    "    for entity in json_dict.entities:\n",
    "        if entity.properties:\n",
    "            new_entities.extend(entity.properties)\n",
    "            # for prop in entity.properties:\n",
    "            #     new_entities.append(prop)\n",
    "        else:\n",
    "            new_entities.append(entity)\n",
    "    json_dict.entities = new_entities\n",
    "\n",
    "    return json_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb5eaf4-672d-41e2-a903-63a3b256aa60",
   "metadata": {},
   "source": [
    "### 4.Run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52242db-5122-4a88-96c0-44f33dc4ddef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    exported_schema = get_dataset_schema(project_id, processor_id, location)\n",
    "    dict_schema = documentai.types.dataset.DatasetSchema.to_dict(exported_schema)\n",
    "    store_document_as_json(\n",
    "        json.dumps(dict_schema),\n",
    "        schema_file_path.split(\"/\")[2],\n",
    "        (\"/\").join(schema_file_path.split(\"/\")[3:]) + \"/schema_file.json\",\n",
    "    )\n",
    "    results = list_documents(project_id, location, processor_id)\n",
    "    document_list = results.document_metadata\n",
    "    while len(document_list) != results.total_size:\n",
    "        page_token = results.next_page_token\n",
    "        results = list_documents(\n",
    "            project_id, location, processor_id, page_token=page_token\n",
    "        )\n",
    "        document_list.extend(results.document_metadata)\n",
    "    print(\"Exporting Dataset...\")\n",
    "    for doc in tqdm(document_list):\n",
    "        doc_id = doc.document_id\n",
    "        split_type = doc.dataset_type\n",
    "        if split_type == 3:\n",
    "            split = \"unassigned\"\n",
    "        elif split_type == 2:\n",
    "            split = \"test\"\n",
    "        elif split_type == 1:\n",
    "            split = \"train\"\n",
    "        else:\n",
    "            split = \"unknown\"\n",
    "        file_name = doc.display_name\n",
    "        res = get_document(project_id, location, processor_id, doc_id)\n",
    "        exported_path = (\"/\").join(export_dataset_path.split(\"/\")[3:])\n",
    "        output_file_name = f\"{exported_path}/{split}/{file_name}.json\"\n",
    "        json_data = documentai.Document.to_json(remove_child_entities(res))\n",
    "        store_document_as_json(\n",
    "            json_data, export_dataset_path.split(\"/\")[2], output_file_name\n",
    "        )\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f35804f-db9a-473f-8709-744f312fc186",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.Output\n",
    "\n",
    "The updated JSON files with labels will be saved in the specified output folder. Also it will save the schema form the processor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbe5b1e-c8e2-40fa-b46b-e14b9045408a",
   "metadata": {},
   "source": [
    "#### Updated json files and schema from the processor stored in the Storage Bucket\n",
    "\n",
    "<img src=\"./Images/Storage_path.png\" width=800 height=400 ></img>\n",
    "\n",
    "### Flatten schema of the updated json files in the DocAI Processor UI\n",
    "\n",
    "<img src=\"./Images/Flatten_entities.png\" width=800 height=400 ></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9813ac99-58ef-4e18-975e-21c4672ce55b",
   "metadata": {},
   "source": [
    "## Grouping nested entities according to schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e320d2-8b66-439c-869a-be6e8e21dd67",
   "metadata": {},
   "source": [
    "### 1.Importing Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49da68d-9f89-4f72-ba18-fca662b002cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from google.api_core.client_options import ClientOptions\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from google.cloud import storage\n",
    "from utilities import (\n",
    "    file_names,\n",
    "    documentai_json_proto_downloader,\n",
    "    store_document_as_json,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e08d66-5f61-4a99-94d1-426f72357d9d",
   "metadata": {},
   "source": [
    "### 2.Setup the inputs\n",
    "* `documents_path`: The Google Cloud Storage (gs) path for documents processed\n",
    "* `schema_file`: The Google Cloud Storage (gs)path for the schema file\n",
    "* `final_output_path`: The Google Cloud Storage (gs) path for updated labeled jsons to store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c0a4ed-4834-4e01-a26a-5f7434d1f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_path = \"gs://bucket_name/path_to_parsed_jsons/\"\n",
    "schema_file = \"gs://bucket_name/path_to_schema file/file_name.json\"\n",
    "final_output_path = \"gs://bucket_name/path_to_output_folder/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd9e96c-6538-4b67-9427-c37e1d06e06a",
   "metadata": {},
   "source": [
    "### 3.Run the required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3d78bb-f5af-4c5b-aef5-2ea56a698ed2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_schema(schema: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Finds and organizes the schema properties from a document schema.\n",
    "\n",
    "    Args:\n",
    "        schema (dict): The document schema containing entity types and their properties.\n",
    "\n",
    "    Returns:\n",
    "        dict: A nested schema mapping entity types to their properties.\n",
    "    \"\"\"\n",
    "    nested_schema = {}\n",
    "    for schema_metadata in schema[\"document_schema\"][\"entity_types\"]:\n",
    "        if schema_metadata[\"name\"] == \"custom_extraction_document_type\":\n",
    "            if len(schema_metadata[\"properties\"]) > 0:\n",
    "                for schema_property in schema_metadata[\"properties\"]:\n",
    "                    if schema_property[\"name\"] == schema_property[\"value_type\"]:\n",
    "                        nested_schema[schema_property[\"name\"]] = []\n",
    "        else:\n",
    "            if schema_metadata[\"name\"] in nested_schema:\n",
    "                for schema_property in schema_metadata[\"properties\"]:\n",
    "                    nested_schema[schema_metadata[\"name\"]].append(\n",
    "                        schema_property[\"name\"]\n",
    "                    )\n",
    "            else:\n",
    "                nested_schema[chema_metadata[\"name\"]] = []\n",
    "                for schema_property in schema_metadata[\"properties\"]:\n",
    "                    nested_schema[schema_metadata[\"name\"]].append(\n",
    "                        schema_property[\"name\"]\n",
    "                    )\n",
    "    return nested_schema\n",
    "\n",
    "\n",
    "def get_page_bbox(entity: documentai.Document.Entity) -> list:\n",
    "    \"\"\"\n",
    "    Retrieves the bounding box coordinates of a document entity.\n",
    "\n",
    "    Args:\n",
    "        entity (documentai.Document.Entity): The document entity for which to get the bounding box.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing the bounding box coordinates [min_x, min_y, max_x, max_y].\n",
    "    \"\"\"\n",
    "\n",
    "    bound_poly = entity.page_anchor.page_refs\n",
    "    norm_ver = bound_poly[0].bounding_poly.normalized_vertices\n",
    "    x_values = [vertex.x for vertex in norm_ver]\n",
    "    y_values = [vertex.y for vertex in norm_ver]\n",
    "    bbox = [min(x_values), min(y_values), max(x_values), max(y_values)]\n",
    "\n",
    "    return bbox\n",
    "\n",
    "\n",
    "def tag_line_items(\n",
    "    nested_schema: dict, json_dict: documentai.Document\n",
    ") -> documentai.Document:\n",
    "    \"\"\"\n",
    "    Tags line items in a JSON dictionary based on a nested schema.\n",
    "\n",
    "    Args:\n",
    "        nested_schema (dict): A mapping of entity types to their properties.\n",
    "        json_dict (documentai.Document): The document object containing entities to be tagged.\n",
    "\n",
    "    Returns:\n",
    "        documentai.Document: The updated document object with tagged line items.\n",
    "    \"\"\"\n",
    "    child_items = {}\n",
    "    for i in range(len(json_dict.entities) - 1, -1, -1):\n",
    "        entity = json_dict.entities[i]\n",
    "        for parent in nested_schema:\n",
    "            if entity.type in nested_schema[parent]:\n",
    "                if parent in child_items:\n",
    "                    child_items[parent].append(entity)\n",
    "                    del json_dict.entities[i]\n",
    "                    break\n",
    "                else:\n",
    "                    child_items[parent] = [entity]\n",
    "                    del json_dict.entities[i]\n",
    "                    break\n",
    "    grouped_line_items = []\n",
    "    for parent in child_items.keys():\n",
    "        line_item_temp = {\n",
    "            \"mention_text\": \"\",\n",
    "            \"page_anchor\": {\n",
    "                \"page_refs\": [{\"bounding_poly\": {\"normalized_vertices\": []}}]\n",
    "            },\n",
    "            \"properties\": [],\n",
    "            \"text_anchor\": {\"text_segments\": []},\n",
    "            \"type\": parent,\n",
    "        }\n",
    "        text_anc_temp = []\n",
    "        page_anc_temp = {\"x\": [], \"y\": []}\n",
    "        mt_temp = \"\"\n",
    "        for child_1 in child_items[parent]:\n",
    "            bbox_temp = get_page_bbox(child_1)\n",
    "            line_item_temp[\"properties\"].append(child_1)\n",
    "            page_anc_temp[\"x\"].extend([bbox_temp[0], bbox_temp[2]])\n",
    "            page_anc_temp[\"y\"].extend([bbox_temp[1], bbox_temp[3]])\n",
    "            seg_temp = child_1.text_anchor.text_segments\n",
    "            page = child_1.page_anchor.page_refs[0].page\n",
    "            for seg in seg_temp:\n",
    "                text_anc_temp.append(\n",
    "                    {\n",
    "                        \"start_index\": str(seg.start_index),\n",
    "                        \"end_index\": str(seg.end_index),\n",
    "                    }\n",
    "                )\n",
    "        if not text_anc_temp:\n",
    "            continue\n",
    "        else:\n",
    "            sorted_data = sorted(text_anc_temp, key=lambda x: int(x[\"end_index\"]))\n",
    "            for sort_text in sorted_data:\n",
    "                mt_temp = (\n",
    "                    mt_temp\n",
    "                    + \" \"\n",
    "                    + json_dict.text[\n",
    "                        int(sort_text[\"start_index\"]) : int(sort_text[\"end_index\"])\n",
    "                    ]\n",
    "                )\n",
    "            line_item_temp[\"page_anchor\"][\"page_refs\"][0][\"page\"] = page\n",
    "            line_item_temp[\"text_anchor\"][\"text_segments\"] = sorted_data\n",
    "            line_item_temp[\"mention_text\"] = mt_temp\n",
    "            line_item_temp[\"page_anchor\"][\"page_refs\"][0][\"bounding_poly\"][\n",
    "                \"normalized_vertices\"\n",
    "            ] = [\n",
    "                {\"x\": min(page_anc_temp[\"x\"]), \"y\": min(page_anc_temp[\"y\"])},\n",
    "                {\"x\": max(page_anc_temp[\"x\"]), \"y\": min(page_anc_temp[\"y\"])},\n",
    "                {\"x\": max(page_anc_temp[\"x\"]), \"y\": max(page_anc_temp[\"y\"])},\n",
    "                {\"x\": min(page_anc_temp[\"x\"]), \"y\": max(page_anc_temp[\"y\"])},\n",
    "            ]\n",
    "            grouped_line_items.append(line_item_temp)\n",
    "    json_dict.entities.extend(grouped_line_items)\n",
    "    return json_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b871b920-a4ab-42b3-ac70-27277e24fcae",
   "metadata": {},
   "source": [
    "### 4.Run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6240b31-abe3-486b-ac4f-3da96d8bafcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    storage_client = storage.Client()\n",
    "    input_bucket_obj = storage_client.get_bucket(schema_file.split(\"/\")[2])\n",
    "    schema = json.loads(\n",
    "        input_bucket_obj.blob(\n",
    "            (\"/\").join(schema_file.split(\"/\")[3:])\n",
    "        ).download_as_bytes()\n",
    "    )\n",
    "    nested_schema = find_schema(schema)\n",
    "    file_name_list, file_path_dict = file_names(documents_path)\n",
    "    for i in range(len(file_name_list)):\n",
    "        file_path = (\n",
    "            \"gs://\"\n",
    "            + documents_path.split(\"/\")[2]\n",
    "            + \"/\"\n",
    "            + file_path_dict[file_name_list[i]]\n",
    "        )\n",
    "        print(\"Running on this file path : \", file_path)\n",
    "        json_data = documentai_json_proto_downloader(\n",
    "            file_path.split(\"/\")[2], (\"/\").join(file_path.split(\"/\")[3:])\n",
    "        )\n",
    "        updated_json = tag_line_items(nested_schema, json_data)\n",
    "        store_document_as_json(\n",
    "            documentai.Document.to_json(updated_json),\n",
    "            final_output_path.split(\"/\")[2],\n",
    "            (\"/\").join(final_output_path.split(\"/\")[3:]) + \"/\" + file_name_list[i],\n",
    "        )\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f92afd-9b58-4fbb-bad2-cb197479e21d",
   "metadata": {},
   "source": [
    "### 5.Output\n",
    "\n",
    "The code will modify the JSON files to incorporate nested entities as defined by the schema file. These updated files, complete with the necessary labels, will then be stored within the designated output folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f19cc9c-e858-4d85-abca-50558c983252",
   "metadata": {},
   "source": [
    "#### Exported Schema in the DocAI Processor UI\n",
    "\n",
    "<img src=\"./Images/UI_Nested_Entities.png\" width=800 height=400 ></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff40f1c4-17fc-4080-99fc-579c2e87a58a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
