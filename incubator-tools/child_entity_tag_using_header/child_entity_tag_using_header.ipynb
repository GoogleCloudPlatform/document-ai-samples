{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72fd064f-24f5-4d61-b0ad-2b2f3fe9427d",
   "metadata": {
    "id": "72fd064f-24f5-4d61-b0ad-2b2f3fe9427d"
   },
   "source": [
    "# Child Entity Tag Using Header Keyword"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5756f1a-631f-4c8a-bba0-98c6821d31a9",
   "metadata": {
    "id": "f5756f1a-631f-4c8a-bba0-98c6821d31a9"
   },
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1d12ef-55dd-4fbd-8389-db14ed038eb1",
   "metadata": {
    "id": "9b1d12ef-55dd-4fbd-8389-db14ed038eb1"
   },
   "source": [
    "# Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the **DocAI Incubator Team**. No guarantees of performance are implied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94527514-1ae2-470b-96e2-0f48e4aa5e81",
   "metadata": {
    "id": "94527514-1ae2-470b-96e2-0f48e4aa5e81"
   },
   "source": [
    "# Purpose and Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf4462a-fc0d-477f-9c39-4fec383ba4ca",
   "metadata": {
    "id": "cdf4462a-fc0d-477f-9c39-4fec383ba4ca"
   },
   "source": [
    "This tool uses labeled json files in GCS bucket and header words as input and creates a new child entity tagging the values under the header keyword matching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c8ce78-bfdd-4f27-ba71-87a8847d25ae",
   "metadata": {
    "id": "70c8ce78-bfdd-4f27-ba71-87a8847d25ae"
   },
   "source": [
    "**Example**\n",
    "![](./images/objective.png)\n",
    "\n",
    "All those `5-tokens` needs to convert as entities of type `line_item` & its child-entities of type `total_amount`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8783f52-627b-4efa-b5d9-664ae2ca2564",
   "metadata": {
    "id": "a8783f52-627b-4efa-b5d9-664ae2ca2564"
   },
   "source": [
    "# Prerequisites\n",
    "\n",
    "1. Vertex AI Notebook\n",
    "2. Labeled json files in GCS Folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cc5540-deb1-4449-8278-716488c54e5c",
   "metadata": {
    "tags": [],
    "id": "55cc5540-deb1-4449-8278-716488c54e5c"
   },
   "source": [
    "# Step by Step procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdc2a30-2ba2-41cc-ba4f-f1bd65ab5cc2",
   "metadata": {
    "id": "9bdc2a30-2ba2-41cc-ba4f-f1bd65ab5cc2"
   },
   "source": [
    "## 1. Import Modules/Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e11f715-319d-4f37-81e0-cd777004a1a6",
   "metadata": {
    "id": "6e11f715-319d-4f37-81e0-cd777004a1a6"
   },
   "outputs": [],
   "source": [
    "# # Download incubator-tools utilities module to present-working-directory\n",
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b1a8a4-07ce-4f66-810c-d8c88d7bfaeb",
   "metadata": {
    "id": "60b1a8a4-07ce-4f66-810c-d8c88d7bfaeb"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Union, DefaultDict\n",
    "\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from google.cloud import storage\n",
    "\n",
    "import utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49f5b2d-f7fd-4403-a175-b95cc804f6ba",
   "metadata": {
    "tags": [],
    "id": "d49f5b2d-f7fd-4403-a175-b95cc804f6ba"
   },
   "source": [
    "## 2. Input Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397acecf-72a3-4913-b5b3-68256ac06f0f",
   "metadata": {
    "tags": [],
    "id": "397acecf-72a3-4913-b5b3-68256ac06f0f"
   },
   "source": [
    " * **GCS_INPUT_PATH:** provide the folder name of the input jsons which needs to be processed.\n",
    " * **GCS_OUTPUT_PATH:** provide for the folder name where jsons will be saved after processing.\n",
    " * **LIST_TOTAL_AMOUNT:** This is the list of header words have to be used and the values under those headers will be tagged with child type total_amount_type.\n",
    "  * **TOTAL_AMOUNT_TYPE:** enitity name under which values will be tagged.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceefcfc1-2a9d-424a-821e-d438a342571c",
   "metadata": {
    "id": "ceefcfc1-2a9d-424a-821e-d438a342571c"
   },
   "outputs": [],
   "source": [
    "GCS_INPUT_PATH = \"gs://xx_bucket_xx/path_to/input\"\n",
    "GCS_OUTPUT_PATH = \"gs://xx_bucket_xx/path_to/output\"\n",
    "# List of header words to match with tokens\n",
    "LIST_TOTAL_AMOUNT = [\n",
    "    \"Total value\",\n",
    "    \"Amount\",\n",
    "    \"Nettowert\",\n",
    "    \"Nettowert in EUR\",\n",
    "    \"Wert\",\n",
    "    \"Importo\",\n",
    "    \"Nettobetrag\",\n",
    "    \"Extension\",\n",
    "    \"Net value\",\n",
    "    \"Ext. price\",\n",
    "    \"Extended Amt\",\n",
    "    \"Costo riga\",\n",
    "    \"Imp. Netto\",\n",
    "    \"Summe\",\n",
    "    \"Gesamtpreis\",\n",
    "    \"Gesamt\",\n",
    "    \"Gesamtgewicht\",\n",
    "    \"Betrag\",\n",
    "    \"Bedrag\",\n",
    "    \"Wartość\",\n",
    "    \"Wartość netto\",\n",
    "    \"Value\",\n",
    "    \"TOTAL\",\n",
    "    \"Line Total\",\n",
    "    \"Net\",\n",
    "    \"Net Amount\",\n",
    "    \"cost\",\n",
    "    \"Subtotal\",\n",
    "]\n",
    "TOTAL_AMOUNT_TYPE = \"line_item/total_amount\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fa4328-3d99-4de4-a5eb-0f2033d78b79",
   "metadata": {
    "id": "a6fa4328-3d99-4de4-a5eb-0f2033d78b79"
   },
   "source": [
    "## 3. Run the Below Code-Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dd6197-28cf-49a0-bfea-16ab2c6ba759",
   "metadata": {
    "id": "84dd6197-28cf-49a0-bfea-16ab2c6ba759"
   },
   "outputs": [],
   "source": [
    "def split_gcs_folder(path: str, need_trailing_slash: bool = False) -> Tuple[str, str]:\n",
    "    \"\"\"It will split GCS uri-path into bucket name and path-prefix\n",
    "\n",
    "    Args:\n",
    "        path (str): GCS uri-path(like-gs://bucket_name/path_to/folder)\n",
    "        need_trailing_slash (bool, optional): If this parameter True then it will append \"/\" at end of gcs path-prefix. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, str]: It contains bucket-name & uri-prefix of folder/file\n",
    "    \"\"\"\n",
    "\n",
    "    pattern = re.compile(\"gs://(?P<bucket>.*?)/(?P<files_dir>.*)\")\n",
    "    path = path.rstrip(\"/\")\n",
    "    uri = re.match(pattern, path)\n",
    "    bucket, files_dir = uri.group(\"bucket\"), uri.group(\"files_dir\")\n",
    "    if need_trailing_slash:\n",
    "        files_dir += \"/\"\n",
    "    return bucket, files_dir\n",
    "\n",
    "\n",
    "def get_total_amount_type(doc: documentai.Document, total_amount_type: str) -> str:\n",
    "    \"\"\"It give's final total_amount_type(Entity.type_) to consider for entity and its properties\n",
    "\n",
    "    Args:\n",
    "        doc (documentai.Document): Documnet Proto Object\n",
    "        total_amount_type (str): Initial total amount type given by user\n",
    "\n",
    "    Returns:\n",
    "        str: Final entity object type to consider and use for its properties\n",
    "    \"\"\"\n",
    "\n",
    "    consider_ent_type = \"\"\n",
    "    for ent in doc.entities:\n",
    "        if not (ent.properties and ent.type_ == \"line_item\"):\n",
    "            continue\n",
    "        for sub_ent in ent.properties:\n",
    "            consider_ent_type = (\n",
    "                \"line_item/total_amount\"\n",
    "                if (\"line_item\" in sub_ent.type_)\n",
    "                else \"total_amount\"\n",
    "            )\n",
    "    if \"/\" in consider_ent_type:\n",
    "        if \"/\" not in total_amount_type:\n",
    "            return \"line_item\" + \"/\" + total_amount_type\n",
    "    if \"/\" in total_amount_type:\n",
    "        return total_amount_type.split(\"/\")[-1]\n",
    "    return total_amount_type\n",
    "\n",
    "\n",
    "def get_page_wise_entities(\n",
    "    doc: documentai.Document,\n",
    ") -> DefaultDict[int, List[documentai.Document.Entity]]:\n",
    "    \"\"\"It gives page-wise entites for all pages in Document object\n",
    "\n",
    "    Args:\n",
    "        doc (documentai.Document): Documnet Proto Object\n",
    "\n",
    "    Returns:\n",
    "        Dict[int, List[documentai.Document.Entity]]: Dictionary which contains page number as key and list of all entities in corresponding page\n",
    "    \"\"\"\n",
    "\n",
    "    entities_page = defaultdict(list)\n",
    "    for ent in doc.entities:\n",
    "        page_no = ent.page_anchor.page_refs[0].page\n",
    "        entities_page[page_no].append(ent)\n",
    "    return entities_page\n",
    "\n",
    "\n",
    "def get_all_line_items(\n",
    "    entities: List[documentai.Document.Entity], entity_type: str = \"line_item\"\n",
    ") -> List[documentai.Document.Entity]:\n",
    "    \"\"\"It will check for entity_type in given list of entities\n",
    "\n",
    "    Args:\n",
    "        entities (List[documentai.Document.Entity]): A list of all entities to check for entity_type\n",
    "        entity_type (str, optional): To filter the entities based on its type. Defaults to \"line_item\".\n",
    "\n",
    "    Returns:\n",
    "        List[documentai.Document.Entity]: It returns list entities after filtreing based on entity_type\n",
    "    \"\"\"\n",
    "\n",
    "    line_items_all = []\n",
    "    for entity in entities:\n",
    "        if entity.properties and entity.type_ == entity_type:\n",
    "            line_items_all.append(entity)\n",
    "    return line_items_all\n",
    "\n",
    "\n",
    "def get_x_y_list(\n",
    "    bounding_poly: documentai.BoundingPoly,\n",
    ") -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"It takes BoundingPoly object and separates it x & y normalized coordinates as lists\n",
    "\n",
    "    Args:\n",
    "        bounding_poly (documentai.BoundingPoly): A token of Document Page object\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[float], List[float]]: It returns x & y normalized coordinates as separate lists\n",
    "    \"\"\"\n",
    "\n",
    "    x, y = [], []\n",
    "    normalized_vertices = bounding_poly.normalized_vertices\n",
    "    for nv in normalized_vertices:\n",
    "        x.append(nv.x)\n",
    "        y.append(nv.y)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def get_token(\n",
    "    doc: documentai.Document,\n",
    "    page: int,\n",
    "    text_anchors_check: documentai.Document.TextAnchor.TextSegment,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"It will be useful to get minimum and maximum values for xy-coordinates\n",
    "\n",
    "    Args:\n",
    "        doc (documentai.Document): Documnet Proto Object\n",
    "        page (int): It is target page-number to look for all entities in it\n",
    "        text_anchors_check (documentai.Document.TextAnchor.TextSegment): It is TextSegment object which contians start&end indices\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: It contains min and max values of xy-coordinates of a token\n",
    "    \"\"\"\n",
    "\n",
    "    x, y = [], []\n",
    "    start_check = text_anchors_check.start_index - 2\n",
    "    end_check = text_anchors_check.end_index + 2\n",
    "    for token in doc.pages[page].tokens:\n",
    "        text_anc = token.layout.text_anchor.text_segments[0]\n",
    "        start_temp = text_anc.start_index\n",
    "        end_temp = text_anc.end_index\n",
    "        if not (\n",
    "            (start_temp >= start_check)\n",
    "            and (end_temp <= end_check)\n",
    "            and (end_temp - start_temp > 3)\n",
    "        ):\n",
    "            continue\n",
    "        x_acc, y_acc = get_x_y_list(token.layout.bounding_poly)\n",
    "        x += x_acc\n",
    "        y += y_acc\n",
    "    return {\"min_x\": min(x), \"min_y\": min(y), \"max_x\": max(x), \"max_y\": max(y)}\n",
    "\n",
    "\n",
    "def remove_selected_items_from_groups_same_y(\n",
    "    selected_elements: List[\n",
    "        Dict[str, Union[str, float, documentai.Document.TextAnchor.TextSegment]]\n",
    "    ],\n",
    "    groups_same_y: List[\n",
    "        List[Dict[str, Union[str, float, documentai.Document.TextAnchor.TextSegment]]]\n",
    "    ],\n",
    ") -> List[\n",
    "    List[Dict[str, Union[str, float, documentai.Document.TextAnchor.TextSegment]]]\n",
    "]:\n",
    "    \"\"\"It will remove duplicated items from groups_same_y data by using text-anchors of selected_elements\n",
    "\n",
    "    Args:\n",
    "        selected_elements (List[Dict[str, Union[str, float, documentai.Document.TextAnchor.TextSegment]]]): TextSegement object for an token/entity\n",
    "        groups_same_y (List[List[Dict[str, Union[str, float, documentai.Document.TextAnchor.TextSegment]]]]): It contains group of same line-items as list-of-lists\n",
    "\n",
    "    Returns:\n",
    "        List[List[Dict[str, Union[str, float, documentai.Document.TextAnchor.TextSegment]]]]: It contains filtered version fo group_same_y parameter data\n",
    "    \"\"\"\n",
    "\n",
    "    text_anchors = [element[\"text_anc\"] for element in selected_elements]\n",
    "    # removing selected items from groups_same_y\n",
    "    for groups in groups_same_y:\n",
    "        for item in groups:\n",
    "            if item[\"text_anc\"] in text_anchors:\n",
    "                groups.remove(item)\n",
    "    return groups_same_y\n",
    "\n",
    "\n",
    "def remove_unwanted_line_items(\n",
    "    line_items_temp: List[documentai.Document.Entity],\n",
    "    groups_same_y: List[\n",
    "        List[Dict[str, Union[str, float, documentai.Document.TextAnchor.TextSegment]]]\n",
    "    ],\n",
    ") -> List[documentai.Document.Entity]:\n",
    "    \"\"\"It will removes unwanted line-items by using text-anchor filter on groups_same_y parameter data\n",
    "\n",
    "    Args:\n",
    "        line_items_temp (List[documentai.Document.Entity]): It contains a list of entities fo Document object\n",
    "        groups_same_y (List[List[Dict[str, Union[str, float, documentai.Document.TextAnchor.TextSegment]]]]): It contains the text-anchor of entity which needs to be removed from list-of-items\n",
    "\n",
    "    Returns:\n",
    "        List[documentai.Document.Entity]: It is filtered version of list of entities\n",
    "    \"\"\"\n",
    "\n",
    "    text_anchors = [item[\"text_anc\"] for groups in groups_same_y for item in groups]\n",
    "    # removing unwanted entity from line_items_temp\n",
    "    for line_item in line_items_temp:\n",
    "        if line_item.text_anchor.text_segments in text_anchors:\n",
    "            line_items_temp.remove(line_item)\n",
    "    return line_items_temp\n",
    "\n",
    "\n",
    "def get_same_y_entities(\n",
    "    line_items_temp: List[documentai.Document.Entity],\n",
    ") -> List[Dict[str, Union[str, float, documentai.Document.TextAnchor.TextSegment]]]:\n",
    "    \"\"\"It groups the entities which falls under same-y-coordinate of a line-item\n",
    "\n",
    "    Args:\n",
    "        line_items_temp (List[documentai.Document.Entity]): It contains the list of entities of Document object\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Union[str, float, documentai.Document.TextAnchor.TextSegment]]]: It contains group of same line-items as list-of-lists\n",
    "    \"\"\"\n",
    "\n",
    "    same_y_ent = []\n",
    "    for dup in line_items_temp:\n",
    "        temp_same_y = {\"min_y\": \"\", \"max_y\": \"\", \"min_x\": \"\", \"text_anc\": []}\n",
    "        x, y = get_x_y_list(dup.page_anchor.page_refs[0].bounding_poly)\n",
    "        temp_same_y[\"min_y\"] = min(y)\n",
    "        temp_same_y[\"max_y\"] = max(y)\n",
    "        temp_same_y[\"min_x\"] = min(x)\n",
    "        temp_same_y[\"text_anc\"] = dup.text_anchor.text_segments\n",
    "        same_y_ent.append(temp_same_y)\n",
    "    return same_y_ent\n",
    "\n",
    "\n",
    "def get_group_same_y(\n",
    "    sorted_same_y_ent: List[\n",
    "        Dict[str, Union[str, float, documentai.Document.TextAnchor.TextSegment]]\n",
    "    ]\n",
    ") -> List[\n",
    "    List[Dict[str, Union[str, float, documentai.Document.TextAnchor.TextSegment]]]\n",
    "]:\n",
    "    \"\"\"It takes list of tokens/entities and groups them based on gap-between y-coordinate of token/entity\n",
    "\n",
    "    Args:\n",
    "        sorted_same_y_ent (List[Dict[str, Union[str, float, documentai.Document.TextAnchor.TextSegment]]]): It is a sorted-list of of entities which falls under same line-item-group\n",
    "\n",
    "    Returns:\n",
    "        List[List[Dict[str, Union[str, float, documentai.Document.TextAnchor.TextSegment]]]]: It contains list of lists whih falls undre same line-item based on difference between y-coordinates\n",
    "    \"\"\"\n",
    "\n",
    "    groups_same_y = []\n",
    "    current_group = []\n",
    "    for data in sorted_same_y_ent:\n",
    "        if (not current_group) or (data[\"min_y\"] - current_group[-1][\"min_y\"] < 0.005):\n",
    "            current_group.append(data)\n",
    "        else:\n",
    "            groups_same_y.append(current_group)\n",
    "            current_group = [data]\n",
    "    if current_group:\n",
    "        groups_same_y.append(current_group)\n",
    "    return groups_same_y\n",
    "\n",
    "\n",
    "def tag_ref_child_item(\n",
    "    doc: documentai.Document,\n",
    "    page: int,\n",
    "    ent_min_dict: Dict[str, Dict[str, float]],\n",
    "    consider_ent: str,\n",
    "    total_amount_type: str,\n",
    "    min_y_start: float,\n",
    "    max_stop_y: float,\n",
    ") -> List[documentai.Document.Entity]:\n",
    "    \"\"\"This function used to tag child items to a line-item of an entity\n",
    "\n",
    "    Args:\n",
    "        doc (documentai.Document): It is Document proto object\n",
    "        page (int): It is target page-number to look for all entities in it\n",
    "        ent_min_dict (Dict[str, Dict[str, float]]): It contains min and max values of xy-coordinates of a specific-token\n",
    "        consider_ent (str): Entity type which is considered\n",
    "        total_amount_type (str): Its value is set as type for an entity, here for all properties in an entity\n",
    "        min_y_start (float): Minimum y-coordinate to start checking\n",
    "        max_stop_y (float): Maximum y-coordinate to stop checking\n",
    "\n",
    "    Returns:\n",
    "        List[documentai.Document.Entity]: It contains a list of all entities which falls with-in target-header(with-in this x-coor range)\n",
    "    \"\"\"\n",
    "\n",
    "    consider_type = total_amount_type\n",
    "    line_items_temp = []\n",
    "    for token in doc.pages[page].tokens:\n",
    "        pr = documentai.Document.PageAnchor.PageRef(page=page)\n",
    "        pa = documentai.Document.PageAnchor(page_refs=[pr])\n",
    "        line_item_ent = documentai.Document.Entity(\n",
    "            confidence=1.0, type_=\"line_item\", page_anchor=pa\n",
    "        )\n",
    "        sub_ent = documentai.Document.Entity(confidence=1.0, type_=\"\", page_anchor=pa)\n",
    "        x, y = get_x_y_list(token.layout.bounding_poly)\n",
    "        min_x, max_x = min(x), max(x)\n",
    "        min_y, max_y = min(y), max(y)\n",
    "        within_header_coords = (\n",
    "            (min_y > min_y_start)\n",
    "            and (min_x >= ent_min_dict[consider_ent][\"min_x\"] - 0.05)\n",
    "            and (max_x <= ent_min_dict[consider_ent][\"max_x\"] + 0.1)\n",
    "            and (max_y <= max_stop_y)\n",
    "            and (max_x > ent_min_dict[consider_ent][\"min_x\"])\n",
    "        )\n",
    "        if not within_header_coords:\n",
    "            continue\n",
    "        end_index = token.layout.text_anchor.text_segments[0].end_index\n",
    "        start_index = token.layout.text_anchor.text_segments[0].start_index\n",
    "        search_pattern = re.compile(r\"[0-9\\s\\\\\\/]+\")\n",
    "        search_string = (\n",
    "            doc.text[start_index:end_index].replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "        )\n",
    "        check_pattern = search_pattern.search(search_string)\n",
    "        if not check_pattern:\n",
    "            continue\n",
    "        line_item_ent.mention_text = doc.text[start_index:end_index]\n",
    "        line_item_ent.page_anchor.page_refs[\n",
    "            0\n",
    "        ].bounding_poly.normalized_vertices = (\n",
    "            token.layout.bounding_poly.normalized_vertices\n",
    "        )\n",
    "        line_item_ent.text_anchor.text_segments = token.layout.text_anchor.text_segments\n",
    "        sub_ent.mention_text = doc.text[start_index:end_index]\n",
    "        sub_ent.page_anchor.page_refs[\n",
    "            0\n",
    "        ].bounding_poly.normalized_vertices = (\n",
    "            token.layout.bounding_poly.normalized_vertices\n",
    "        )\n",
    "        sub_ent.text_anchor.text_segments = token.layout.text_anchor.text_segments\n",
    "        sub_ent.type_ = consider_type\n",
    "        line_item_ent.properties = [sub_ent]\n",
    "        line_items_temp.append(line_item_ent)\n",
    "    same_y_ent = get_same_y_entities(line_items_temp)\n",
    "    sorted_same_y_ent = sorted(same_y_ent, key=lambda x: x[\"min_y\"])\n",
    "    groups_same_y = []\n",
    "    if sorted_same_y_ent:\n",
    "        groups_same_y = get_group_same_y(sorted_same_y_ent)\n",
    "    selected_elements = [\n",
    "        min(\n",
    "            lst,\n",
    "            key=lambda elem: abs(elem[\"min_x\"] - ent_min_dict[consider_ent][\"min_x\"]),\n",
    "        )\n",
    "        for lst in groups_same_y\n",
    "    ]\n",
    "    if not groups_same_y:\n",
    "        return line_items_temp\n",
    "    groups_same_y = remove_selected_items_from_groups_same_y(\n",
    "        selected_elements, groups_same_y\n",
    "    )\n",
    "    line_items_temp = remove_unwanted_line_items(line_items_temp, groups_same_y)\n",
    "    return line_items_temp\n",
    "\n",
    "\n",
    "def get_y_min_max(\n",
    "    line_items_all: List[documentai.Document.Entity],\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"It is used to get minimum and maximum values of y-coordinate's for all line-items in specific page\n",
    "\n",
    "    Args:\n",
    "        line_items_all (List[documentai.Document.Entity]): It contains a list of entities of Document object\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: It return ma& min y-coord values for all line-items in same page\n",
    "    \"\"\"\n",
    "\n",
    "    min_y_line = 1\n",
    "    max_y_line = 0\n",
    "    min_y_child = 1\n",
    "    for line_item in line_items_all:\n",
    "        _, y = get_x_y_list(line_item.page_anchor.page_refs[0].bounding_poly)\n",
    "        min_y_temp = min(y)\n",
    "        max_y_temp = max(y)\n",
    "        if max_y_line < max_y_temp:\n",
    "            max_y_line = max_y_temp\n",
    "        if not (min_y_line > min_y_temp):\n",
    "            continue\n",
    "        min_y_line = min_y_temp\n",
    "        for child_ent in line_item.properties:\n",
    "            norm_ver_child = child_ent.page_anchor.page_refs[\n",
    "                0\n",
    "            ].bounding_poly.normalized_vertices\n",
    "            min_y_child_temp = min(vertex.y for vertex in norm_ver_child)\n",
    "            if min_y_child > min_y_child_temp:\n",
    "                min_y_child = min_y_child_temp\n",
    "    return min_y_line, max_y_line\n",
    "\n",
    "\n",
    "def get_entity_text_anchor(\n",
    "    doc: documentai.Document, page: int, min_y_line: float, list_total_amount: List[str]\n",
    ") -> Dict[str, documentai.Document.TextAnchor.TextSegment]:\n",
    "    \"\"\"It will give Token text and its TextSegmnet Object\n",
    "\n",
    "    Args:\n",
    "        doc (documentai.Document): It is Document proto object\n",
    "        page (int): It is target page-number to look for all entities in it\n",
    "        min_y_line (float): It is minimum y-coordinate of a line-item in given page\n",
    "        list_total_amount (List[str]): It is a list of header words which will be used to identity and the values under those headers will be tagged with child type `total_amount_type`\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, documentai.Document.TextAnchor.TextSegment]: It contains text & TextSegement of a Token\n",
    "    \"\"\"\n",
    "\n",
    "    check_text = \"\"\n",
    "    start_temp = 100000000\n",
    "    end_temp = 0\n",
    "    total_amount_textanc = {}\n",
    "    for token in doc.pages[page].tokens:\n",
    "        _, y = get_x_y_list(token.layout.bounding_poly)\n",
    "        max_y_temp_token = max(y)\n",
    "        min_y_temp_token = min(y)\n",
    "        if (\n",
    "            min_y_line >= (max_y_temp_token - 0.02)\n",
    "            and abs(min_y_line - min_y_temp_token) <= 0.15\n",
    "        ):\n",
    "            end_index = token.layout.text_anchor.text_segments[0].end_index\n",
    "            start_index = token.layout.text_anchor.text_segments[0].start_index\n",
    "            check_text = check_text + doc.text[start_index:end_index]\n",
    "            if start_temp > start_index:\n",
    "                start_temp = start_index\n",
    "            if end_temp < end_index:\n",
    "                end_temp = end_index\n",
    "\n",
    "    for check in list_total_amount:\n",
    "        if check.lower() not in check_text.lower():\n",
    "            continue\n",
    "        matches = re.finditer(check.lower(), doc.text[start_temp:end_temp].lower())\n",
    "        starting_indices = [match.start() for match in matches]\n",
    "        start_index_temp1 = max(starting_indices)\n",
    "        start_index_1 = start_index_temp1 + start_temp\n",
    "        end_index_1 = start_index_1 + len(check)\n",
    "        total_amount_textanc[check] = documentai.Document.TextAnchor.TextSegment(\n",
    "            start_index=start_index_1, end_index=end_index_1\n",
    "        )\n",
    "    return total_amount_textanc\n",
    "\n",
    "\n",
    "def total_amount_entities(\n",
    "    doc: documentai.Document, total_amount_type: str, list_total_amount: List[str]\n",
    ") -> documentai.Document:\n",
    "    \"\"\"It will append new entities to Document Proto, whose token segments falls with in range of Header token\n",
    "\n",
    "    Args:\n",
    "        doc (documentai.Document): It is Document proto object\n",
    "        total_amount_type (str): Its value is set as type for an entity, here for all properties in an entity\n",
    "        list_total_amount (List[str]): It is a list of header words which will be used to identity and the values under those headers will be tagged with child type `total_amount_type`\n",
    "\n",
    "    Returns:\n",
    "        documentai.Document: It is Document proto object, which contains newly added entities as well\n",
    "    \"\"\"\n",
    "\n",
    "    total_amount_type = get_total_amount_type(doc, total_amount_type)\n",
    "    page_wise_ent = get_page_wise_entities(doc)\n",
    "    previous_page_headers = \"\"\n",
    "    total_amount_entities = []\n",
    "\n",
    "    for page, ent2 in page_wise_ent.items():\n",
    "        line_items_all = get_all_line_items(ent2)\n",
    "        if not line_items_all:\n",
    "            continue\n",
    "        if not (len(line_items_all) > 1 or len(line_items_all[0].properties) > 2):\n",
    "            continue\n",
    "        min_y_line, max_y_line = get_y_min_max(line_items_all)\n",
    "        total_amount_textanc = get_entity_text_anchor(\n",
    "            doc, page, min_y_line, list_total_amount\n",
    "        )\n",
    "        final_key = \"\"\n",
    "        for key in total_amount_textanc.keys():\n",
    "            if len(final_key) < len(key):\n",
    "                final_key = key\n",
    "        if final_key:\n",
    "            total_amount_dict = {\n",
    "                \"total_amount\": get_token(doc, page, total_amount_textanc[final_key])\n",
    "            }\n",
    "            previous_page_headers = total_amount_dict\n",
    "        else:\n",
    "            total_amount_dict = previous_page_headers\n",
    "        if not total_amount_dict:\n",
    "            continue\n",
    "        total_amount_line_items = tag_ref_child_item(\n",
    "            doc,\n",
    "            page,\n",
    "            total_amount_dict,\n",
    "            \"total_amount\",\n",
    "            total_amount_type,\n",
    "            min_y_line,\n",
    "            max_y_line,\n",
    "        )\n",
    "        for item in total_amount_line_items:\n",
    "            total_amount_entities.append(item)\n",
    "    for total_en in total_amount_entities:\n",
    "        doc.entities.append(total_en)\n",
    "    return doc\n",
    "\n",
    "\n",
    "def child_entity_tag_using_header(\n",
    "    gcs_input_path: str,\n",
    "    gcs_output_path: str,\n",
    "    list_total_amount: List[str],\n",
    "    total_amount_type: str,\n",
    ") -> None:\n",
    "    \"\"\"It takes labeled json files in GCS bucket and header words as input and creates a new child entity tagging the values under the header keyword matching\n",
    "\n",
    "    Args:\n",
    "        gcs_input_path (str): GCS input uri-path(like-gs://bucket_name/path_to/folder)\n",
    "        gcs_output_path (str): GCS output uri-path(like-gs://bucket_name/path_to/folder)\n",
    "        list_total_amount (List[str]): It is a list of header words which will be used to identity and the values under those headers will be tagged with child type `total_amount_type`\n",
    "        total_amount_type (str): Its value is set as type for an entity, here for all properties in an entity\n",
    "    \"\"\"\n",
    "\n",
    "    count = 0\n",
    "    issue_files = {}\n",
    "    input_bucket, input_dir = split_gcs_folder(\n",
    "        gcs_input_path, need_trailing_slash=False\n",
    "    )\n",
    "    output_bucket, output_dir = split_gcs_folder(\n",
    "        gcs_output_path, need_trailing_slash=False\n",
    "    )\n",
    "    _, file_dict = utilities.file_names(GCS_INPUT_PATH)\n",
    "    file_dict = {fn: fp for fn, fp in file_dict.items() if fn.endswith(\".json\")}\n",
    "    for filename, filepath in file_dict.items():\n",
    "        print(f\"Process started for - {filename} ...\")\n",
    "        input_filepath = filepath\n",
    "        doc = utilities.documentai_json_proto_downloader(input_bucket, input_filepath)\n",
    "        output_filepath = f\"{output_dir}/{filename}\"\n",
    "        if not doc.pages[0].tokens:\n",
    "            print(f\"\\tNo Tokens, Unable to parse {input_filepath}\")\n",
    "            issue_files[input_filepath] = \"No Tokens\"\n",
    "            count += 1\n",
    "            document = documentai.Document.to_json(doc)\n",
    "            utilities.store_document_as_json(document, output_bucket, filepath)\n",
    "            continue\n",
    "        doc = total_amount_entities(doc, total_amount_type, list_total_amount)\n",
    "        document = documentai.Document.to_json(doc)\n",
    "        utilities.store_document_as_json(document, output_bucket, filepath)\n",
    "        print(\"\\tCompleted.\")\n",
    "    print(\"Processing Completed for all files\\n\")\n",
    "    if issue_files:\n",
    "        print(f\"Below {count}-file(s) have some issues to process...\")\n",
    "        print(issue_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7feec5-06c7-4706-a992-50157f67ec78",
   "metadata": {
    "id": "ba7feec5-06c7-4706-a992-50157f67ec78"
   },
   "source": [
    "After Executed all above code cells, run below Entry-Function cell to start processing input files, Whose child entities need to tagged using header Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2238a2-b797-41ae-b7c4-e5665883d932",
   "metadata": {
    "tags": [],
    "id": "6d2238a2-b797-41ae-b7c4-e5665883d932"
   },
   "outputs": [],
   "source": [
    "child_entity_tag_using_header(\n",
    "    gcs_input_path=GCS_INPUT_PATH,\n",
    "    gcs_output_path=GCS_OUTPUT_PATH,\n",
    "    list_total_amount=LIST_TOTAL_AMOUNT,\n",
    "    total_amount_type=TOTAL_AMOUNT_TYPE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02a0f30-9f93-46bf-b871-f34b4df08894",
   "metadata": {
    "id": "b02a0f30-9f93-46bf-b871-f34b4df08894"
   },
   "source": [
    "## 4. Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4232146-0beb-4713-92ff-bf23606d45bf",
   "metadata": {
    "id": "b4232146-0beb-4713-92ff-bf23606d45bf"
   },
   "source": [
    "The items which are below the matched keyword will be tagged as entity name given"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b486827-ddd2-488e-b9fb-5ac249d23e98",
   "metadata": {
    "id": "5b486827-ddd2-488e-b9fb-5ac249d23e98"
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td> Sample JSON-Input file, Entities Count(24)</td>\n",
    "        <td> <img src=\"./images/before_child_entity_tag.png\" width=800 height=400> </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> Sample JSON-Output file, Entities Count(25+4)</td>\n",
    "        <td> <img src=\"./images/after_child_entity_tag.png\" width=800 height=400>  </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a8c587-8d67-4953-b787-7e4c63a3a5ed",
   "metadata": {
    "id": "d9a8c587-8d67-4953-b787-7e4c63a3a5ed"
   },
   "source": [
    "JSON Sample output after tagging child-entities, **5-new entities** added to Document Proto\n",
    "<img src=\"./images/after_child_entity_tag_json_results.png\" width=450 height=150></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9900525",
   "metadata": {
    "id": "a9900525"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "l9AV-8GfCHs6"
   },
   "id": "l9AV-8GfCHs6",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m104",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m104"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}