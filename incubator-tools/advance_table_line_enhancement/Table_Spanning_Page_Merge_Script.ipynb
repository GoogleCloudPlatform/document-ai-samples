{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc34f8f6-9b81-48d5-b317-7da269c078d6",
   "metadata": {
    "id": "cc34f8f6-9b81-48d5-b317-7da269c078d6"
   },
   "source": [
    "# DocumentAI Merge Specific-Use-Case Table(Table Across Two Pages) Script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85f7bae-ae78-4b22-bb23-98badf204211",
   "metadata": {
    "id": "b85f7bae-ae78-4b22-bb23-98badf204211"
   },
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a7ab42-e144-40ce-a258-d65455092b8d",
   "metadata": {
    "id": "29a7ab42-e144-40ce-a258-d65455092b8d"
   },
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the **DocAI Incubator Team**. No guarantees of performance are implied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04591d9d-9ad3-483f-ab45-3d4e0b7570e9",
   "metadata": {
    "id": "04591d9d-9ad3-483f-ab45-3d4e0b7570e9",
    "tags": []
   },
   "source": [
    "## Objective\n",
    "\n",
    "DocumentAI Page Merger is a tool built using Python programming language. Its purpose is to provide technique for merging table(**Specific use case tables**) which spans across two pages. This document highlights the working of the tool(script) and its requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290ab039-258e-4761-b277-6d81db637ae0",
   "metadata": {
    "id": "290ab039-258e-4761-b277-6d81db637ae0"
   },
   "source": [
    "**NOTE**:\n",
    "* Input pdf files contains only use-case table which spans across two pages.\n",
    "* You need to train CDE processor for *specific use-case table* by annotating `row_header` and `column_header` entities. These headers are needed to run this page merger script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e185f3a-333b-4dfa-bc50-5febbd27444b",
   "metadata": {
    "id": "3e185f3a-333b-4dfa-bc50-5febbd27444b"
   },
   "source": [
    "This tool requires the following services:\n",
    "\n",
    " * Vertex AI Notebook instance\n",
    " * Access to Document AI CDE Processor\n",
    " * Folder containing input PDFs\n",
    "\n",
    "Google Jupyter Notebook is used for running the python notebook file. Input folder should have the input files to this script. CDE processor to train a model which detects row headers and column headers for your specific usecase table by annotating row headers and column headers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yEPx3vNjUn18",
   "metadata": {
    "id": "yEPx3vNjUn18"
   },
   "source": [
    "## Approach  \n",
    "* Using the CDE processor output identifies pairs of consecutive pages where a table starts on the first page (with row headers) and continues on the next page (without row headers but with column headers), indicating a split table across those pages.\n",
    "* Using Pillow, identified excess regions with no white pixels on the PDFs, and cropped the white space on the right of the first page and the left side of the second page for each identified pair.\n",
    "* Horizontally merged the cropped pages to create seamless and complete tables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a7f1ce-3cf2-472a-a42a-aae0c1d3c387",
   "metadata": {
    "id": "56a7f1ce-3cf2-472a-a42a-aae0c1d3c387"
   },
   "source": [
    "CDE for Headers, Create a Custom Document Extractor(CDE) Processor & Configure HITL to review poor performing documents. Train your CDE as per your use-case table by annotating **row headers** & **column headers** for specific use-case-table\n",
    "* Input for this step is GCS bucket containing re-builted PDF files (which are output from step-2a advance_table_parser.ipynb), now run `batch_process_documents`\n",
    "* Output JSON files will be store GCS bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478d7676-5d9d-4096-9d11-6d4b64fc47cd",
   "metadata": {
    "id": "478d7676-5d9d-4096-9d11-6d4b64fc47cd"
   },
   "source": [
    "Sample image after training CDE processor for row columns & header columns\n",
    "<table>\n",
    "  <tr>\n",
    "      <td><b>CDE Sample</b></td>\n",
    "    <td><img src=\"./Images/cde_train_sample.png\" width=500 height=200></td>\n",
    "  </tr>\n",
    "</table>\n",
    "Here are sample row headers and column headers which we followed while training CDE for our specific use-case table  \n",
    "\n",
    "**column headers** are as follow a[\"SCC\", \"DNSH\", \"DNSH_P\", \"code\", \"business_measure\", \"DNSH_BE\", \"DNSH_CCA\", \"DNSH_CCM\", \"DNSH_CE\", \"DNSH_WMR\", \"min_safeguards\", \"proportion_of_bm\", \"SCC_BE\", \"SCC_CCA\", \"SCC_CCM\", \"SCC_CE\", \"SCC_P\", \"SCC_WMR\"] and **row headers** are as follow [\"taxonomy_disclosure\", \"activity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdfb175-3c18-48ed-afbb-3a81b889d936",
   "metadata": {
    "id": "7fdfb175-3c18-48ed-afbb-3a81b889d936"
   },
   "source": [
    "# Script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d2888a-633d-48dc-be50-156df01f7eba",
   "metadata": {
    "id": "b9d2888a-633d-48dc-be50-156df01f7eba"
   },
   "source": [
    "## 1. Import Modules/Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5993b837-8055-485d-8c73-86084d1b3480",
   "metadata": {},
   "source": [
    "**Note** : Please download the **tool_helper_functions.py** Python file before proceeding to further steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2827a0a-8028-4fa3-b532-793fd97babc8",
   "metadata": {
    "id": "a2827a0a-8028-4fa3-b532-793fd97babc8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "import img2pdf\n",
    "import pandas as pd\n",
    "from google.cloud import documentai_v1 as documentai\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image, PpmImagePlugin\n",
    "from PyPDF2 import PdfMerger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d339050c-eed6-497e-9eb4-4b9bdddcea9d",
   "metadata": {
    "id": "d339050c-eed6-497e-9eb4-4b9bdddcea9d"
   },
   "source": [
    "## 2. Input Details : Configure below Input variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8b8e24-398d-4e9b-8a36-33cde0118a6e",
   "metadata": {
    "id": "ea8b8e24-398d-4e9b-8a36-33cde0118a6e"
   },
   "source": [
    "* **PROJECT_ID** : Provide your GCP Project ID\n",
    "* **LOCATION** : Provide the location of processor like `us` or `eu`\n",
    "* **PROCESSOR_ID** : Provide ID of CDE processor\n",
    "* **FOLDER_PATH** : Folder which hold input pdf files(pdf pages should be having only use-case table pages)\n",
    "* **OUTPUT_FOLDER** : Set your output folder path where the merged pdfs should be stored in your local system\n",
    "* **MIME_TYPE** : Provide mime type of input documents\n",
    "* **COL_HEADERS** : Provide list of all entities(entity type) which are annotated in CDE processor to identify *column headers*\n",
    "* **ROW_HEADERS** : Provide list of all entities(entity type) which are annotated in CDE processor to identify *row headers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1164e220-9b94-4a22-bb91-d92349684bc8",
   "metadata": {
    "id": "1164e220-9b94-4a22-bb91-d92349684bc8"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"<your-project-id>\"\n",
    "LOCATION = \"<location>\"\n",
    "PROCESSOR_ID = \"<processor-id>\"\n",
    "FOLDER_PATH = \"dir_path/to/input_folder/\"\n",
    "OUTPUT_FOLDER = \"output_dir/path/\"\n",
    "MIME_TYPE = \"application/pdf\"\n",
    "# replace COL_HEADERS & ROW_HEADERS with your list of annotation_types\n",
    "COL_HEADERS = [\n",
    "    \"SCC\",\n",
    "    \"DNSH\",\n",
    "    \"DNSH_P\",\n",
    "    \"code\",\n",
    "    \"business_measure\",\n",
    "    \"DNSH_BE\",\n",
    "    \"DNSH_CCA\",\n",
    "    \"DNSH_CCM\",\n",
    "    \"DNSH_CE\",\n",
    "    \"DNSH_WMR\",\n",
    "    \"min_safeguards\",\n",
    "    \"proportion_of_bm\",\n",
    "    \"SCC_BE\",\n",
    "    \"SCC_CCA\",\n",
    "    \"SCC_CCM\",\n",
    "    \"SCC_CE\",\n",
    "    \"SCC_P\",\n",
    "    \"SCC_WMR\",\n",
    "]\n",
    "ROW_HEADERS = [\"taxonomy_disclosure\", \"activity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d557a339-3bce-43a4-888f-deb8281a84f0",
   "metadata": {
    "id": "d557a339-3bce-43a4-888f-deb8281a84f0"
   },
   "source": [
    "Below image shows, after annotating row headers & column headers for CDE\n",
    "![](./Images/cde_train_sample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d239dcb8-f71f-4bbe-af9a-efa748dd6811",
   "metadata": {
    "id": "d239dcb8-f71f-4bbe-af9a-efa748dd6811"
   },
   "source": [
    "## 3. Run the below code.\n",
    "\n",
    "Use the below code and Run all the cells (Update the Path parameter if it is not available in the current working directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18f7309-60ee-4ecc-9fc7-7ebb9cf9f47b",
   "metadata": {
    "id": "e18f7309-60ee-4ecc-9fc7-7ebb9cf9f47b"
   },
   "outputs": [],
   "source": [
    "def online_process(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    processor_id: str,\n",
    "    file_path: str,\n",
    "    mime_type: str,\n",
    ") -> documentai.Document:\n",
    "    \"\"\"\n",
    "    Processes a document using the Document AI Online Processing API.\n",
    "    \"\"\"\n",
    "\n",
    "    opts = {\"api_endpoint\": f\"{location}-documentai.googleapis.com\"}\n",
    "\n",
    "    # Instantiates a client\n",
    "    documentai_client = documentai.DocumentProcessorServiceClient(client_options=opts)\n",
    "\n",
    "    # The full resource name of the processor, e.g.:\n",
    "    # projects/project-id/locations/location/processor/processor-id\n",
    "    # You must create new processors in the Cloud Console first\n",
    "    resource_name = documentai_client.processor_path(project_id, location, processor_id)\n",
    "\n",
    "    # Read the file into memory\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        file_content = file.read()\n",
    "\n",
    "    # Load Binary Data into Document AI RawDocument Object\n",
    "    raw_document = documentai.RawDocument(content=file_content, mime_type=mime_type)\n",
    "\n",
    "    # Configure the process request\n",
    "    request = documentai.ProcessRequest(name=resource_name, raw_document=raw_document)\n",
    "    print(f\"\\tOnline Document Process started..\")\n",
    "    # Use the Document AI client to process the sample form\n",
    "    result = documentai_client.process_document(request=request)\n",
    "    print(\"\\tSuccessfully document process completed\")\n",
    "    return result.document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706ca789-de08-4424-a51b-056cae57bb1d",
   "metadata": {
    "id": "706ca789-de08-4424-a51b-056cae57bb1d"
   },
   "outputs": [],
   "source": [
    "def crop_right(\n",
    "    img: Union[Image.Image, PpmImagePlugin.PpmImageFile]\n",
    ") -> Union[Image.Image, PpmImagePlugin.PpmImageFile]:\n",
    "    \"\"\"\n",
    "    Function to crop right side of the image\n",
    "    \"\"\"\n",
    "\n",
    "    img_data = img.getdata()\n",
    "    non_empty_columns = [\n",
    "        i\n",
    "        for i in range(img.width)\n",
    "        if not all(\n",
    "            img_data[i + j * img.width][:3] == (255, 255, 255)\n",
    "            for j in range(img.height)\n",
    "        )\n",
    "    ]\n",
    "    left = 0\n",
    "    right = max(non_empty_columns)\n",
    "    img_cropped = img.crop((left, 0, right, img.height))\n",
    "    return img_cropped\n",
    "\n",
    "\n",
    "def crop_left(\n",
    "    img: Union[Image.Image, PpmImagePlugin.PpmImageFile]\n",
    ") -> Union[Image.Image, PpmImagePlugin.PpmImageFile]:\n",
    "    \"\"\"\n",
    "    Function to crop left side of the image\n",
    "    \"\"\"\n",
    "\n",
    "    img_data = img.getdata()\n",
    "    non_empty_columns = [\n",
    "        i\n",
    "        for i in range(img.width)\n",
    "        if not all(\n",
    "            img_data[i + j * img.width][:3] == (255, 255, 255)\n",
    "            for j in range(img.height)\n",
    "        )\n",
    "    ]\n",
    "    left = min(non_empty_columns)\n",
    "    right = img.width\n",
    "    img_cropped = img.crop((left, 0, right, img.height))\n",
    "    return img_cropped\n",
    "\n",
    "\n",
    "def process_pdf(\n",
    "    pdf_path: str, page_pairs: List[Tuple[int, int]], OUTPUT_FOLDER: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    This function processes pages of complex-table which spans across two pages\n",
    "    \"\"\"\n",
    "\n",
    "    pdf_merger = PdfMerger()\n",
    "    for pair in page_pairs:\n",
    "        images = convert_from_path(\n",
    "            pdf_path, first_page=pair[0] + 1, last_page=pair[1] + 1\n",
    "        )  # incrementing page numbers\n",
    "        if len(images) != 2:\n",
    "            print(\"More than 2 pages, skipping..\")\n",
    "            continue\n",
    "        img1, img2 = images\n",
    "        img1_cropped = crop_right(img1)\n",
    "        img2_cropped = crop_left(img2)\n",
    "\n",
    "        # Merge horizontally\n",
    "        total_width = img1_cropped.width + img2_cropped.width\n",
    "        max_height = max(img1_cropped.height, img2_cropped.height)\n",
    "\n",
    "        new_img = Image.new(\"RGB\", (total_width, max_height), (255, 255, 255))\n",
    "        new_img.paste(img1_cropped, (0, 0))\n",
    "        new_img.paste(img2_cropped, (img1_cropped.width, 0))\n",
    "\n",
    "        # Save as temporary image file\n",
    "        temp_img_path = \"temp_merged.png\"\n",
    "        new_img.save(temp_img_path, \"PNG\")\n",
    "\n",
    "        # Convert to PDF\n",
    "        with open(temp_img_path, \"rb\") as f:\n",
    "            pdf_bytes = img2pdf.convert(f.read())\n",
    "\n",
    "        temp_pdf_path = \"temp_merged.pdf\"\n",
    "        with open(temp_pdf_path, \"wb\") as f:\n",
    "            f.write(pdf_bytes)\n",
    "\n",
    "        pdf_merger.append(temp_pdf_path)\n",
    "\n",
    "        # Remove temporary files\n",
    "        os.remove(temp_img_path)\n",
    "        os.remove(temp_pdf_path)\n",
    "\n",
    "    # Naming the output based on the input file\n",
    "    output_name = os.path.join(\n",
    "        OUTPUT_FOLDER, os.path.basename(pdf_path).replace(\".pdf\", \"_merged.pdf\")\n",
    "    )\n",
    "    pdf_merger.write(output_name)\n",
    "    pdf_merger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf1c71a-89a3-47e2-906f-2d409c08318a",
   "metadata": {
    "id": "8bf1c71a-89a3-47e2-906f-2d409c08318a"
   },
   "outputs": [],
   "source": [
    "def get_entities(document: documentai.Document) -> Tuple[List[str], List[int]]:\n",
    "    \"\"\"\n",
    "    It will be used to return all entities data and its corresponding page-number of Document object\n",
    "    \"\"\"\n",
    "\n",
    "    types = []\n",
    "    page_no = []\n",
    "    for entity in document.entities:\n",
    "        types.append(entity.type_)\n",
    "        page_no.append(entity.page_anchor.page_refs[0].page)\n",
    "        for prop in entity.properties:\n",
    "            types.append(prop.type_)\n",
    "            page_no.append(entity.page_anchor.page_refs[0].page)\n",
    "    return types, page_no\n",
    "\n",
    "\n",
    "def page_merger() -> None:\n",
    "    \"\"\"\n",
    "    Entry function to start page merging process\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Page Merger Pileline started\")\n",
    "    pdfs_and_pages = {}\n",
    "    for filename in os.listdir(FOLDER_PATH):\n",
    "        if not filename.endswith(\".pdf\"):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(FOLDER_PATH, filename)\n",
    "        print(\n",
    "            \"Processing \",\n",
    "            filename,\n",
    "        )\n",
    "        # processing for each PDF file\n",
    "        document = online_process(\n",
    "            PROJECT_ID, LOCATION, PROCESSOR_ID, file_path, MIME_TYPE\n",
    "        )\n",
    "        types, page_no = get_entities(document)\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                \"Type\": types,\n",
    "                \"Page No.\": page_no,\n",
    "            }\n",
    "        )\n",
    "        df_sorted = df.sort_values(by=\"Page No.\")\n",
    "        unique_pages = df_sorted[\"Page No.\"].unique()\n",
    "        col_headers = COL_HEADERS\n",
    "        row_headers = ROW_HEADERS\n",
    "        page_to_headers = {}\n",
    "        for page in unique_pages:\n",
    "            page_to_headers[page] = set(\n",
    "                df_sorted[df_sorted[\"Page No.\"] == page][\"Type\"].tolist()\n",
    "            )\n",
    "\n",
    "        split_pages = []\n",
    "        for i in range(len(unique_pages) - 1):\n",
    "            current_page, next_page = unique_pages[i], unique_pages[i + 1]\n",
    "            current_headers, next_headers = (\n",
    "                page_to_headers[current_page],\n",
    "                page_to_headers[next_page],\n",
    "            )\n",
    "            if all(row in current_headers for row in row_headers) and not any(\n",
    "                row in next_headers for row in row_headers\n",
    "            ):\n",
    "                if any(header in next_headers for header in col_headers):\n",
    "                    split_pages.append((current_page, next_page))\n",
    "        if len(split_pages) > 0:\n",
    "            pdfs_and_pages[filename] = split_pages\n",
    "            print(f\"\\t\\tDetected split pages in {filename},  pages are -{split_pages}\")\n",
    "\n",
    "    for pdf_file, pages in pdfs_and_pages.items():\n",
    "        print(\"Processed and saved: \", pdf_file)\n",
    "        full_pdf_path = os.path.join(FOLDER_PATH, pdf_file)\n",
    "        pathlib.Path(OUTPUT_FOLDER).mkdir(exist_ok=True)\n",
    "        if pages:\n",
    "            print(f\"\\tPage splits are - {pages}\")\n",
    "            process_pdf(full_pdf_path, pages, OUTPUT_FOLDER)\n",
    "\n",
    "    print(\"Page Merger Pipeline successfully completed for all files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f2935a-8f83-4129-b899-a508a8de6173",
   "metadata": {
    "id": "f7f2935a-8f83-4129-b899-a508a8de6173"
   },
   "source": [
    "To start Page Merger pipeline execute `page_merger()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee2d784-2af3-4189-87ee-0796f50d6396",
   "metadata": {
    "id": "8ee2d784-2af3-4189-87ee-0796f50d6396",
    "outputId": "5b554b8c-d4e3-48dc-a45a-953863db55ff"
   },
   "outputs": [],
   "source": [
    "page_merger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344adb11-1299-458b-bd3d-019feff26bfb",
   "metadata": {
    "id": "344adb11-1299-458b-bd3d-019feff26bfb"
   },
   "source": [
    "# 4. Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea28429-61c6-4f87-9088-2fb0620c982b",
   "metadata": {
    "id": "5ea28429-61c6-4f87-9088-2fb0620c982b"
   },
   "source": [
    "If table span across two pages then it will be processed and appends them as one page side-by-side, if not spans across two pages then that file is skipped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27f5f2f-15d2-43ff-a7d8-e3dbc7c8f94d",
   "metadata": {
    "id": "b27f5f2f-15d2-43ff-a7d8-e3dbc7c8f94d"
   },
   "source": [
    "You can find processed files  in the given OUTPUT_FOLDER."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b119f5cd-7db1-4d1f-89cc-2904174ebfb2",
   "metadata": {
    "id": "b119f5cd-7db1-4d1f-89cc-2904174ebfb2"
   },
   "source": [
    "### Input file have table across two pages\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"./Images/page_merger_input_1.png\" width=300 height=150></td>\n",
    "    <td><img src=\"./Images/page_merger_input_2.png\" width=300 height=150></td>\n",
    "  </tr>\n",
    " </table>\n",
    "\n",
    "### After running page_merger script you can find table in single page\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"./Images/page_merger_output.png\" width=600 height=300></td>\n",
    "      <td> </td>\n",
    "  </tr>\n",
    "    </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IzD1CrQvUvoe",
   "metadata": {
    "id": "IzD1CrQvUvoe"
   },
   "source": [
    "## Limitations\n",
    "* *CDE Prediction Impact*: The accuracy of split table detection relies on the precise CDE predictions for row and column headers. Inaccuracies in predictions could lead to false positives or missed splits, affecting the merging process.\n",
    "* *Row Headers Across Both Pages*: If both pages contain headers, the CDE might not correctly differentiate between them.\n",
    "* *Table Spanning Multiple Pages*: If a single table spans more than two pages, then the CDE might not detect.\n",
    "* *Inconsistent Split Position*: If the split between the first and second page varies in terms of rows or columns alignment.\n",
    "* *Single Page Split*: The approach might miss cases where a table is split within a single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7525df2d-a88f-4764-b06d-d3008bb7eb85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
