{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79c0a10c-1269-4a48-b4c5-ef4bbbdd2644",
   "metadata": {},
   "source": [
    "# Form Parser Table Results to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f01fe57-ffc0-402b-81ac-5f77ebd77ece",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa07f2d-5087-4d8d-9ceb-5d46a21e8926",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the **DocAI Incubator Team**. No guarantees of performance are implied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c6bbc7-49af-4a2d-98b8-09b9b8739e57",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "This tool built using Python programming language. It converts tables present in pdf to csv files and stores them in GCS bucket by using Form Parser results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107ff6ab-a7a9-4e31-b00f-9e7706fca09f",
   "metadata": {},
   "source": [
    "# Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b0a705-8144-40d4-a92a-15488bd4d1bf",
   "metadata": {},
   "source": [
    "This tool requires the following services:\n",
    "\n",
    " * Vertex AI Notebook instance\n",
    " * Access to Document AI Form Parser & CDE Processor\n",
    " * GCS Bucket containing input PDFs & to store output results\n",
    " \n",
    "Google Jupyter Notebook is used for running the python notebook file. Cloud Storage Buckets is needed to store and pass input files to this script & to store results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c53efdb-4975-4c19-85c6-cd0262f18cf8",
   "metadata": {},
   "source": [
    "CDE for Headers, Create a Custom Document Extractor(CDE) Processor & Configure HITL to review poor performing documents. Train your CDE as per your use-case table by annotating **row headers** & **column headers** for specific use-case-table\n",
    "* Input for this step is GCS bucket containing re-builted PDF files(which have only tables), now run `batch_process_documents`\n",
    "* Output JSON files will be store GCS bucket "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21aa662-6e89-4839-a902-adcd55d63580",
   "metadata": {},
   "source": [
    "Sample image after training CDE processor for row columns & header columns\n",
    "<table>\n",
    "  <tr>\n",
    "      <td><b>CDE Sample</b></td>\n",
    "    <td><img src=\"./Images/cde_train_sample.png\" width=500 height=200></td>\n",
    "  </tr>\n",
    "</table> \n",
    "Here are sample row headers and column headers which we followed while training CDE for our specific use-case table  \n",
    "\n",
    "**column headers** are as follow a[\"SCC\", \"DNSH\", \"DNSH_P\", \"code\", \"business_measure\", \"DNSH_BE\", \"DNSH_CCA\", \"DNSH_CCM\", \"DNSH_CE\", \"DNSH_WMR\", \"min_safeguards\", \"proportion_of_bm\", \"SCC_BE\", \"SCC_CCA\", \"SCC_CCM\", \"SCC_CE\", \"SCC_P\", \"SCC_WMR\"] and **row headers** are as follow [\"taxonomy_disclosure\", \"activity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53acbcf2-499b-481d-a64d-f2f334554577",
   "metadata": {},
   "source": [
    "# Script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad3f0db-8f32-4fec-b480-a17cd2b39967",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Import Modules/Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f5f52a-b0d5-4483-afa6-f8c78317c4cd",
   "metadata": {},
   "source": [
    "**Note** : Please download the **tool_helper_functions.py** Python file before proceeding to further steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a789a344-ca2e-49a6-b8ce-e7c9b1a0d673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from tool_helper_functions import (\n",
    "    batch_process_documents,\n",
    "    poll_hitl_operations,\n",
    "    get_processor_metadata,\n",
    "    parse_document_tables,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60bfb4d-f9e2-48de-894a-f0c736ba45e6",
   "metadata": {},
   "source": [
    "# 2. Input Details : Configure below Input variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b880fe-b8f5-466d-b7aa-fe387b7008ad",
   "metadata": {},
   "source": [
    "* **project_id**: GCP project ID\n",
    "* **location**: Processor location `us` or `eu`\n",
    "* **fp_processor_id**: FP Processor ID to call batch process\n",
    "* **gcs_input_uri**: GCS folder which contains input pdf files(files with only specific-use-case tables)\n",
    "* **input_mime_type**: Mime type of input files which is `application/pdf` here\n",
    "* **gcs_output_bucket_uri**: GCS output bucket uri without trailing slash\n",
    "* **gcs_output_uri_prefix**: GCS output folder path to store results\n",
    "* **field_mask**:  To store specific keys of document proto (entities,pages.pageNumber)\n",
    "* **timeout**: to wait for batch process LRO operation to complete\n",
    "* **fp_processor_v**: FP version(V1 or V2) ID to call batch process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d6c085-a232-480e-ba7d-7ac00d47e185",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = \"<your-project-id>\"\n",
    "location = \"<processor-location>\"  # us or eu\n",
    "fp_processor_id = \"<fp-processor-id>\"\n",
    "gcs_input_uri = f\"gs://bucket_name/prefix/to_input/{datetime_suffix}\"\n",
    "input_mime_type = \"<mime-type-of-input-file>\"  # \"application/pdf\"\n",
    "gcs_output_bucket_uri = \"gs://bucket_name\"\n",
    "gcs_output_uri_prefix = f\"tables_to_csv/output_folder/prefix/{datetime_suffix}\"\n",
    "field_mask = None\n",
    "timeout = 5000\n",
    "fp_processor_v = \"<fp-version-id>\"  # FP processor V1 or V2 id\n",
    "\n",
    "gcs_output_bucket = gcs_output_bucket_uri.replace(\"gs://\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc17cd73-dc06-44c9-9e1c-95b91224b077",
   "metadata": {},
   "source": [
    "# 3. Run below code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070b4fdc-86ca-4fb9-acc7-8882d5e55f02",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now call `batch_process_documents` function to process all files in input folder(each file contains specific-use-case table only), it results metadata & operation_id of batch process(Long Running Operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6810abf-b33f-4501-8f8a-d81aaca01d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_metadata, fp_operation = batch_process_documents(\n",
    "    project_id,\n",
    "    location,\n",
    "    fp_processor_id,\n",
    "    gcs_input_uri,\n",
    "    input_mime_type,\n",
    "    gcs_output_bucket_uri,\n",
    "    f\"{gcs_output_uri_prefix}/fp_output\",\n",
    "    field_mask,\n",
    "    timeout,\n",
    "    fp_processor_v,\n",
    ")\n",
    "print(\"FP batch process completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83a9335-8e1c-4a3d-9fb2-8ddd1c568605",
   "metadata": {},
   "source": [
    "If you configured HITL then you can use below `get_processor_metadata` and `poll_hitl_operations` function, if not you can skip running these two function and proceed with running parse_document_tables function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01269740-9680-4c83-84ed-7b1f2a2836b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_input_output_map = get_processor_metadata(fp_metadata, fp=True)\n",
    "poll_hitl_operations(project_num, location, fp_input_output_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9583c9-3e01-441f-a2ac-135d72a83709",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_document_tables(\n",
    "    gcs_output_bucket,\n",
    "    f\"{gcs_output_uri_prefix}/fp_output\",\n",
    "    f\"{gcs_output_uri_prefix}/tables_csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589ae870-f711-4736-ab1e-0b08fa189e74",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Output Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596f22c7-e68a-4f34-8ce9-e7b4822d31f2",
   "metadata": {},
   "source": [
    "Table Sample from pdf file\n",
    "![](./Images/line_enhancement_basic_table_img.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6746622b-634a-44fe-9e65-c8cf1c678775",
   "metadata": {},
   "source": [
    "Sample output folder structure\n",
    "![](./Images/fp_tables_to_csv_output_folder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac67de3-7306-4e41-b046-7021ae086d79",
   "metadata": {},
   "source": [
    "output sample for one-table which stored as csv files in GCS bucket\n",
    "![](./Images/fp_tables_to_csv_output_csv_sample.png)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
