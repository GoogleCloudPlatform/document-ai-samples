{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71514062-c6e7-48ff-bb2a-04117943f105",
   "metadata": {},
   "source": [
    "# Line Items Improver (Post-Processing) User Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbbca70-d6e4-4e06-9df3-f64cd4818959",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the DocAI Incubator Team. No guarantees of performance are implied.\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954eeed3-7acb-439b-8d9d-3e5d2c780ff3",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "This tool takes in parsed json (prediction result) files and a list of child entities which occurs only once (per line item) in the schema of line_items to better group the child entities into correct line_items(parents)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea3343-4547-4a88-a759-e50864f516bc",
   "metadata": {},
   "source": [
    "## Prerequisite\n",
    "\n",
    "* Vertex AI Notebook\n",
    "* Parsed json files in a GCS Folder\n",
    "* List of child entities which occur once(optional once or required once)\n",
    "* Output folder to upload the updated json files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c913981-a471-4201-be20-c685745160ba",
   "metadata": {},
   "source": [
    "## Line_item improver logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a358df2a-480b-49c5-a0e1-14e04bf9476d",
   "metadata": {},
   "source": [
    "<img src=\"./images/image_1.png\" width=800 height=400></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf4bfd0-2af2-4ac2-a221-ea47dcf99f99",
   "metadata": {},
   "source": [
    "<img src=\"./images/image_2.png\" width=800 height=400></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89d93fb-e03c-471f-b6ef-0b334334e325",
   "metadata": {},
   "source": [
    "## Choosing anchor entity and grouping:\n",
    "\n",
    "1. As majority count is 5 , the entities(line_item/quantity,line_item/amount) which has minimum ‘y’ is chosen as anchor entity \n",
    "2. If there is no majority count then maximum is taken a majority and anchor entity\n",
    "3. The region from minimum ‘y’ of first occurrence of anchor entity to minimum ‘y’ of next occurrence of anchor entity is considered as line_item_region\n",
    "4. The line_item/ entities which ever come under this  region are grouped into a single line item\n",
    "5. This is followed for all the occurrences of anchor entities. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4986c352-f020-4840-8486-31cdcc36ea6c",
   "metadata": {},
   "source": [
    "## Step by Step procedure "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc3fcbf-1135-452c-b341-9d3bf1340946",
   "metadata": {},
   "source": [
    "## 1. Input Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4e341f-26db-457d-8d8b-2e9cf523fb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "gcs_input_path = \"gs://<bucket_name>/<input_folder_name>\"  # Parsed json files path, end '/' is mandatory\n",
    "project_id = \"<your_project_id>\"  # project ID\n",
    "gcs_output_path = \"gs://<bucket_name>/<output_folder_name>\"  # output path where the updated jsons to be saved, end '/' is mandatory\n",
    "\n",
    "unique_entities = [\"product_code\", \"unit_price\", \"quantity\"]\n",
    "desc_merge_update = \"Yes\"  # update to Yes if you want to combine description within the line item, else NO#\n",
    "line_item_across_pages = (\n",
    "    \"Yes\"  # update to Yes if you want to group line items across pages#\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea8bc1c-d79d-41d0-aed5-6db552e8b76a",
   "metadata": {},
   "source": [
    "## 2. Run the Code\n",
    "\n",
    "Run the below source code to get the updated json files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b65d7c-c7a1-4c64-98cf-a3032cc80ede",
   "metadata": {},
   "source": [
    "### Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d174d6-33d5-4b3b-92e1-37ddd7d404c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download incubator-tools utilities module to present-working-directory\n",
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccc77ac-19c8-40e0-8b1a-5854b74a2101",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm google-cloud-storage google-cloud-documentai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b1cfb2-3f0a-47e1-aa4d-67b151f96233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from google.cloud import storage\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import copy\n",
    "from collections import Counter\n",
    "import utilities\n",
    "from typing import List, Dict, Tuple, Any, Optional, Iterable\n",
    "\n",
    "# list of child entities in line_item which is optional_once or required_once in schema like below example#\n",
    "gcs_input_path = \"gs://<bucket_name>/<input_folder_name>\"  # Parsed json files path, end '/' is mandatory\n",
    "project_id = \"<your_project_id>\"  # project ID\n",
    "gcs_output_path = \"gs://<bucket_name>/<output_folder_name>\"  # output path where the updated jsons to be saved, end '/' is mandatory\n",
    "\n",
    "unique_entities = [\"product_code\", \"unit_price\", \"quantity\"]\n",
    "desc_merge_update = \"Yes\"  # update to Yes if you want to combine description within the line item, else NO#\n",
    "line_item_across_pages = (\n",
    "    \"Yes\"  # update to Yes if you want to group line items across pages#\n",
    ")\n",
    "\n",
    "output_bucket_name = gcs_output_path.split(\"/\")[2]  # Extract the output bucket name\n",
    "\n",
    "storage_client = storage.Client()\n",
    "\n",
    "# Get the list of file names and their paths from the input bucket\n",
    "file_names_list, file_dict = utilities.file_names(gcs_input_path)\n",
    "\n",
    "\n",
    "def get_page_wise_entities(document) -> Dict[str, List]:\n",
    "    \"\"\"\n",
    "    Organizes entities in a document by the page they appear on.\n",
    "\n",
    "    Args:\n",
    "        document: The document object containing entities with page information.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary where the keys are page numbers (as strings) and the values\n",
    "        are lists of entities found on those pages.\n",
    "    \"\"\"\n",
    "    entities_page = {}\n",
    "    for entity in document.entities:\n",
    "        page = \"0\"\n",
    "        try:\n",
    "            if \"page\" in entity.page_anchor.page_refs[0]:\n",
    "                page = entity.page_anchor.page_refs[0].page\n",
    "\n",
    "            if page in entities_page:\n",
    "                entities_page[page].append(entity)\n",
    "            else:\n",
    "                entities_page[page] = [entity]\n",
    "        except:\n",
    "            pass\n",
    "    return entities_page\n",
    "\n",
    "\n",
    "def merge_entities(document):\n",
    "    \"\"\"\n",
    "    Merges entities in a document, especially handling line items across multiple pages.\n",
    "\n",
    "    Args:\n",
    "        document: The document object containing entities.\n",
    "\n",
    "    Returns:\n",
    "        The document object with merged entities.\n",
    "    \"\"\"\n",
    "    entities_page_wise = get_page_wise_entities(document)\n",
    "    line_entities_classified_pagewise = []\n",
    "\n",
    "    for page, entities in entities_page_wise.items():\n",
    "        line_item_count = line_item_check(entities, unique_entities)\n",
    "\n",
    "        if line_item_count == 1:\n",
    "            line_entities_temp = single_line_item_merge(entities, page)\n",
    "            line_entities_classified_pagewise.append(line_entities_temp)\n",
    "        elif line_item_count > 1:\n",
    "            line_entities_temp, considered_boundary_ent = multi_page_entites(\n",
    "                entities, page\n",
    "            )\n",
    "            line_entities_classified_pagewise.extend(line_entities_temp)\n",
    "        else:\n",
    "            print(\"no line items\")\n",
    "\n",
    "    final_entities = [\n",
    "        entity for entity in document.entities if entity.type != \"line_item\"\n",
    "    ]\n",
    "    final_entities.extend(line_entities_classified_pagewise)\n",
    "    document.entities = final_entities\n",
    "\n",
    "    return document\n",
    "\n",
    "\n",
    "def line_item_check(entities: List, unique_entities: List[str]) -> int:\n",
    "    \"\"\"\n",
    "    Checks line items within a list of entities based on unique entity types.\n",
    "\n",
    "    Args:\n",
    "        entities: A list of entities to be checked.\n",
    "        unique_entities: A list of unique entity types to be considered for checking.\n",
    "\n",
    "    Returns:\n",
    "        An integer representing the type of line item found.\n",
    "        1 for single line item, 2 for multiple, and 0 for none.\n",
    "    \"\"\"\n",
    "    entity_types = [\n",
    "        subentity.type\n",
    "        for entity in entities\n",
    "        if hasattr(entity, \"properties\")\n",
    "        for subentity in entity.properties\n",
    "    ]\n",
    "\n",
    "    entity_counts = {unique: entity_types.count(unique) for unique in unique_entities}\n",
    "    multiple_entities_count = sum(count > 1 for count in entity_counts.values())\n",
    "\n",
    "    if any(count == 1 for count in entity_counts.values()):\n",
    "        return 1\n",
    "    elif multiple_entities_count and (\n",
    "        len(unique_entities) >= 3 or multiple_entities_count >= 1\n",
    "    ):\n",
    "        return 2\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def single_line_item_merge(entities: List, page: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Merges single line item entities into a unified line item.\n",
    "\n",
    "    Args:\n",
    "        entities: A list of entities to be merged.\n",
    "        page: The page number as a string where these entities are found.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary representing the merged line item.\n",
    "    \"\"\"\n",
    "    line_item_sub_entities = [\n",
    "        subentity\n",
    "        for entity in entities\n",
    "        if entity.type == \"line_item\"\n",
    "        for subentity in entity.properties\n",
    "    ]\n",
    "\n",
    "    text_anchors = [\n",
    "        item.text_anchor.text_segments[0] for item in line_item_sub_entities\n",
    "    ]\n",
    "    normalized_vertices = [\n",
    "        vertex\n",
    "        for item in line_item_sub_entities\n",
    "        for vertex in item.page_anchor.page_refs[0].bounding_poly.normalized_vertices\n",
    "    ]\n",
    "\n",
    "    min_x = min(normalized_vertices, key=lambda d: d.x).x\n",
    "    max_x = max(normalized_vertices, key=lambda d: d.x).x\n",
    "    min_y = min(normalized_vertices, key=lambda d: d.y).y\n",
    "    max_y = max(normalized_vertices, key=lambda d: d.y).y\n",
    "    vertices_final = [\n",
    "        {\"x\": min_x, \"y\": min_y},\n",
    "        {\"x\": min_x, \"y\": max_y},\n",
    "        {\"x\": max_x, \"y\": min_y},\n",
    "        {\"x\": max_x, \"y\": max_y},\n",
    "    ]\n",
    "\n",
    "    line_item = {\n",
    "        \"mention_text\": \" \".join(item.mention_text for item in line_item_sub_entities),\n",
    "        \"page_anchor\": {\n",
    "            \"page_refs\": [\n",
    "                {\"bounding_poly\": {\"normalized_vertices\": vertices_final}, \"page\": page}\n",
    "            ]\n",
    "        },\n",
    "        \"properties\": line_item_sub_entities,\n",
    "        \"text_anchor\": {\n",
    "            \"text_segments\": sorted(text_anchors, key=lambda x: int(x.end_index))\n",
    "        },\n",
    "        \"type\": \"line_item\",\n",
    "    }\n",
    "\n",
    "    return line_item\n",
    "\n",
    "\n",
    "def group_line_items(document, schema: Optional[Any] = None) -> Any:\n",
    "    \"\"\"\n",
    "    Groups line items in a document, potentially across multiple pages, based on a schema.\n",
    "\n",
    "    Args:\n",
    "        document: The document object containing entities to be grouped.\n",
    "        schema: (Optional) A schema to guide the grouping. If None, a temporary schema is generated.\n",
    "\n",
    "    Returns:\n",
    "        The modified document with grouped line items.\n",
    "    \"\"\"\n",
    "    if schema is None:\n",
    "        schema = generate_temp_schema(document)\n",
    "\n",
    "    line_items_by_page = sort_line_items_by_page(document)\n",
    "    groups_across = find_line_item_groups_across_pages(line_items_by_page)\n",
    "    schema_across = get_schema_across_groups(groups_across)\n",
    "    entity_spread = calculate_entity_spread(schema_across, schema)\n",
    "\n",
    "    if entity_spread:\n",
    "        document = move_entities_across_pages(entity_spread, groups_across, document)\n",
    "\n",
    "    return document\n",
    "\n",
    "\n",
    "def generate_temp_schema(document) -> dict:\n",
    "    \"\"\"\n",
    "    Generates a temporary schema for line items in a document based on the most common types.\n",
    "\n",
    "    Args:\n",
    "        document: The document object containing entities.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary representing the majority schema of line item types.\n",
    "    \"\"\"\n",
    "    line_items = [\n",
    "        entity for entity in document.entities if hasattr(entity, \"properties\")\n",
    "    ]\n",
    "    type_counts = [\n",
    "        Counter([child.type for child in item.properties]) for item in line_items\n",
    "    ]\n",
    "    merged_counts = Counter()\n",
    "    for type_count in type_counts:\n",
    "        for key, value in type_count.items():\n",
    "            merged_counts[key] += value\n",
    "    majority_schema = {\n",
    "        key: value.most_common(1)[0][0]\n",
    "        for key, value in merged_counts.items()\n",
    "        if isinstance(value, Counter)\n",
    "    }\n",
    "    return majority_schema\n",
    "\n",
    "\n",
    "def sort_line_items_by_page(document):\n",
    "    \"\"\"\n",
    "    Sorts line items in a document by their page and y-coordinate.\n",
    "\n",
    "    Args:\n",
    "        document: The document object containing entities.\n",
    "\n",
    "    Returns:\n",
    "        A mapping each page to its first and last line items based on y-coordinates.\n",
    "    \"\"\"\n",
    "    line_items_by_page = defaultdict(lambda: {\"first\": None, \"last\": None})\n",
    "    for entity in document.entities:\n",
    "        if hasattr(entity, \"properties\"):\n",
    "            page = str(\n",
    "                entity.page_anchor.page_refs[0].page\n",
    "                if entity.page_anchor.page_refs\n",
    "                else 0\n",
    "            )\n",
    "            y_coords = [\n",
    "                vertex.y\n",
    "                for vertex in entity.page_anchor.page_refs[\n",
    "                    0\n",
    "                ].bounding_poly.normalized_vertices\n",
    "            ]\n",
    "            update_page_sort(line_items_by_page[page], \"last\", max(y_coords), entity)\n",
    "            update_page_sort(line_items_by_page[page], \"first\", min(y_coords), entity)\n",
    "    return line_items_by_page\n",
    "\n",
    "\n",
    "def update_page_sort(\n",
    "    page_dict: dict, position: str, y_value: float, entity: Any\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Updates a page dictionary with a new line item based on y-coordinate and position.\n",
    "\n",
    "    Args:\n",
    "        page_dict: The dictionary representing a page's line items.\n",
    "        position: The position to update ('first' or 'last').\n",
    "        y_value: The y-coordinate of the line item.\n",
    "        entity: The line item entity.\n",
    "    \"\"\"\n",
    "    if page_dict[position] is None or compare_y(page_dict[position], y_value, position):\n",
    "        page_dict[position] = {\"y_value\": y_value, \"entity\": entity}\n",
    "\n",
    "\n",
    "def compare_y(current_dict: dict, new_y: float, position: str) -> bool:\n",
    "    \"\"\"\n",
    "    Compares y-coordinates to determine if a new line item should update the current one.\n",
    "\n",
    "    Args:\n",
    "        current_dict: The current line item information.\n",
    "        new_y: The new y-coordinate.\n",
    "        position: The position being compared ('first' or 'last').\n",
    "\n",
    "    Returns:\n",
    "        A boolean indicating if the new line item should replace the current one.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        (new_y >= current_dict[\"y_value\"])\n",
    "        if position == \"last\"\n",
    "        else (new_y <= current_dict[\"y_value\"])\n",
    "    )\n",
    "\n",
    "\n",
    "def find_line_item_groups_across_pages(sorted_items: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Identifies groups of line items across pages in a sorted items structure.\n",
    "\n",
    "    Args:\n",
    "        sorted_items: A dictionary of sorted line items by page.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary mapping pages to their associated group of line items across pages.\n",
    "    \"\"\"\n",
    "    groups_across = {}\n",
    "    for page, items in sorted_items.items():\n",
    "        next_page = str(int(page) + 1)\n",
    "        if next_page in sorted_items:\n",
    "            groups_across[next_page] = [\n",
    "                items[\"last\"][\"entity\"],\n",
    "                sorted_items[next_page][\"first\"][\"entity\"],\n",
    "            ]\n",
    "    return groups_across\n",
    "\n",
    "\n",
    "def get_schema_across_groups(\n",
    "    groups_across: Dict[str, Any]\n",
    ") -> Dict[str, Dict[int, Counter]]:\n",
    "    \"\"\"\n",
    "    Generates a schema across groups of line items, counting property types for each group.\n",
    "\n",
    "    Args:\n",
    "        groups_across: A dictionary mapping group identifiers to lists of line item entities.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary representing the schema across groups, with counts of property types.\n",
    "    \"\"\"\n",
    "    schema_across = {}\n",
    "    for group, matches in groups_across.items():\n",
    "        for i, match in enumerate(matches):\n",
    "            for property in match.properties:\n",
    "                property_type = property.type\n",
    "                schema_across.setdefault(group, {}).setdefault(i, Counter())[\n",
    "                    property_type\n",
    "                ] += 1\n",
    "    return schema_across\n",
    "\n",
    "\n",
    "def calculate_entity_spread(\n",
    "    schema_across: Dict[str, Dict[int, Counter]], schema: Dict[str, int]\n",
    ") -> Dict[str, Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Calculates the spread of entities across the schema, identifying missing and excess entities.\n",
    "\n",
    "    Args:\n",
    "        schema_across: A dictionary representing the schema across groups.\n",
    "        schema: A dictionary representing the majority schema of line item types.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary indicating the spread of entities in terms of missing and excess counts.\n",
    "    \"\"\"\n",
    "    entity_spread = {}\n",
    "    for group, schemas in schema_across.items():\n",
    "        missing_entities = {key: schema[key] - schemas[0].get(key, 0) for key in schema}\n",
    "        for entity_type, count in missing_entities.items():\n",
    "            if count > 0 and entity_type in schemas[1]:\n",
    "                excess_count = schemas[1][entity_type] - schema[entity_type]\n",
    "                if excess_count > 0 or len(schemas[1]) < (len(schema) / 2):\n",
    "                    entity_spread.setdefault(group, {})[entity_type] = excess_count\n",
    "    return entity_spread\n",
    "\n",
    "\n",
    "def move_entities_across_pages(\n",
    "    entity_spread: Dict[str, Dict[str, int]],\n",
    "    groups_across: Dict[str, Any],\n",
    "    document: Any,\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Moves entities across pages in a document based on the entity spread.\n",
    "\n",
    "    Args:\n",
    "        entity_spread: A dictionary indicating the spread of entities.\n",
    "        groups_across: A dictionary of line item groups across pages.\n",
    "        document: The document object containing entities to be moved.\n",
    "\n",
    "    Returns:\n",
    "        The modified document with entities moved across pages.\n",
    "    \"\"\"\n",
    "    for group, _ in entity_spread.items():\n",
    "        page_1 = get_page_number(groups_across[group][0])\n",
    "        page_2 = get_page_number(groups_across[group][1])\n",
    "        if page_1 < page_2:\n",
    "            entities_to_move = get_entities_to_shuffle(group, 1, entity_spread)\n",
    "            temp_group = move_entities(group, 1, 0, entities_to_move, groups_across)\n",
    "        else:\n",
    "            entities_to_move = get_entities_to_shuffle(group, 0, entity_spread)\n",
    "            temp_group = move_entities(group, 0, 1, entities_to_move, groups_across)\n",
    "        document.entities.extend(\n",
    "            update_json_entities(document.entities, groups_across[group], temp_group)\n",
    "        )\n",
    "    return document\n",
    "\n",
    "\n",
    "def get_page_number(entity: Any) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves the page number from a given entity.\n",
    "\n",
    "    Args:\n",
    "        entity: The entity from which the page number is to be retrieved.\n",
    "\n",
    "    Returns:\n",
    "        The page number as a string. Returns '0' if no page reference is found.\n",
    "    \"\"\"\n",
    "    return entity.page_anchor.page_refs[0].page if entity.page_anchor.page_refs else \"0\"\n",
    "\n",
    "\n",
    "def get_entities_to_shuffle(\n",
    "    group: str, index: int, entity_spread: Dict[str, Dict[str, int]]\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Determines which entities need to be shuffled within a group.\n",
    "\n",
    "    Args:\n",
    "        group: The identifier of the group where entities are to be shuffled.\n",
    "        index: The index within the group to consider for shuffling.\n",
    "        entity_spread: A dictionary indicating the spread of entities.\n",
    "\n",
    "    Returns:\n",
    "        A list of entities that need to be shuffled.\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    for entity_type, count in entity_spread[group].items():\n",
    "        entities.extend(get_sorted_entities_by_y(group, index, entity_type, count))\n",
    "    return entities\n",
    "\n",
    "\n",
    "def get_sorted_entities_by_y(\n",
    "    group: str, index: int, entity_type: str, count: int\n",
    ") -> List[Any]:\n",
    "    \"\"\"\n",
    "    Retrieves and sorts entities of a certain type within a group based on their y-coordinate.\n",
    "\n",
    "    Args:\n",
    "        group: The identifier of the group to search within.\n",
    "        index: The index within the group to consider.\n",
    "        entity_type: The type of entity to filter and sort.\n",
    "        count: The number of entities to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        A list of entities sorted by their y-coordinate.\n",
    "    \"\"\"\n",
    "    entities_by_y = {}\n",
    "    for property in groups_across[group][index].properties:\n",
    "        if property.type == entity_type:\n",
    "            y_value = min(\n",
    "                vertex.y\n",
    "                for vertex in property.page_anchor.page_refs[\n",
    "                    0\n",
    "                ].bounding_poly.normalized_vertices\n",
    "            )\n",
    "            entities_by_y.setdefault(y_value, []).append(property)\n",
    "    return [\n",
    "        entity for y in sorted(entities_by_y)[:count] for entity in entities_by_y[y]\n",
    "    ]\n",
    "\n",
    "\n",
    "def move_entities(\n",
    "    group: str,\n",
    "    from_index: int,\n",
    "    to_index: int,\n",
    "    entities_to_move: List[Any],\n",
    "    groups_across: Dict[str, Any],\n",
    ") -> Iterable:\n",
    "    \"\"\"\n",
    "    Moves entities from one index to another within a group.\n",
    "\n",
    "    Args:\n",
    "        group: The identifier of the group where entities are to be moved.\n",
    "        from_index: The index from which entities are to be moved.\n",
    "        to_index: The index to which entities are to be moved.\n",
    "        entities_to_move: A list of entities to move.\n",
    "        groups_across: A dictionary of groups with their associated entities.\n",
    "\n",
    "    Returns:\n",
    "        An iterable containing the updated group entities.\n",
    "    \"\"\"\n",
    "    temp_group = copy.deepcopy(groups_across[group])\n",
    "    remove_entities_from_group(entities_to_move, temp_group[from_index].properties)\n",
    "    temp_group[to_index].properties.extend(entities_to_move)\n",
    "    update_page_text(temp_group[to_index])\n",
    "    return temp_group.values()\n",
    "\n",
    "\n",
    "def remove_entities_from_group(\n",
    "    entities_to_remove: List[Any], properties: List[Any]\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Removes specified entities from a list of properties.\n",
    "\n",
    "    Args:\n",
    "        entities_to_remove: A list of entities to be removed.\n",
    "        properties: A list of properties/entities from which to remove the specified entities.\n",
    "    \"\"\"\n",
    "    for entity in entities_to_remove:\n",
    "        properties.remove(entity)\n",
    "\n",
    "\n",
    "def update_page_text(entity: Any) -> None:\n",
    "    \"\"\"\n",
    "    Updates the text of an entity based on its properties.\n",
    "\n",
    "    Args:\n",
    "        entity: The entity whose text is to be updated.\n",
    "    \"\"\"\n",
    "    segments = [\n",
    "        segment\n",
    "        for property in entity.properties\n",
    "        for segment in property.text_anchor.text_segments\n",
    "    ]\n",
    "    entity.text_anchor.text_segments = sorted(segments, key=lambda x: x.start_index)\n",
    "    entity.mention_text = \" \".join(\n",
    "        segment.content for segment in entity.text_anchor.text_segments\n",
    "    )\n",
    "\n",
    "\n",
    "def update_json_entities(\n",
    "    current_entities: List[Any], original_group: Iterable, updated_group: Iterable\n",
    ") -> List[Any]:\n",
    "    \"\"\"\n",
    "    Updates a list of entities by removing the original group and adding the updated group.\n",
    "\n",
    "    Args:\n",
    "        current_entities: The current list of entities.\n",
    "        original_group: The original group of entities to be removed.\n",
    "        updated_group: The updated group of entities to be added.\n",
    "\n",
    "    Returns:\n",
    "        The updated list of entities.\n",
    "    \"\"\"\n",
    "    for entity in original_group:\n",
    "        current_entities.remove(entity)\n",
    "    current_entities.extend(updated_group)\n",
    "    return current_entities\n",
    "\n",
    "\n",
    "def extract_entity_types_and_subentities(\n",
    "    entities_pagewise: List[Any],\n",
    ") -> (List[str], List[Any]):\n",
    "    \"\"\"\n",
    "    Extracts entity types and subentities from a list of entities.\n",
    "\n",
    "    Args:\n",
    "        entities_pagewise: A list of entities, potentially including line items with subentities.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing a list of entity types and a list of line item subentities.\n",
    "    \"\"\"\n",
    "    entity_types = []\n",
    "    line_item_sub_entities = []\n",
    "    for entity in entities_pagewise:\n",
    "        if hasattr(entity, \"properties\"):\n",
    "            if entity.type == \"line_item\":\n",
    "                for subentity in entity.properties:\n",
    "                    entity_types.append(subentity.type)\n",
    "                    line_item_sub_entities.append(subentity)\n",
    "        else:\n",
    "            entity_types.append(entity.type)\n",
    "    return entity_types, line_item_sub_entities\n",
    "\n",
    "\n",
    "def count_unique_entities(entity_types: List[str]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Counts the occurrences of each unique entity type in a given list and returns a dictionary\n",
    "    containing only those entities which appear more than once.\n",
    "\n",
    "    Args:\n",
    "    entity_types (List[str]): A list of entity types.\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, int]: A dictionary where keys are entity types that occur more than once, and\n",
    "                    values are the counts of these occurrences.\n",
    "    \"\"\"\n",
    "    entity_type_counts = Counter(entity_types)\n",
    "    return {entity: count for entity, count in entity_type_counts.items() if count > 1}\n",
    "\n",
    "\n",
    "def get_entity_types_with_max_count(line_items_multi_dict: Dict[str, int]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Identifies the entity types with the maximum occurrence count in a dictionary.\n",
    "\n",
    "    Args:\n",
    "    line_items_multi_dict (Dict[str, int]): A dictionary where keys are entity types and\n",
    "                                            values are their occurrence counts.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of entity types that have the maximum count in the given dictionary.\n",
    "    \"\"\"\n",
    "    value_counts = Counter(line_items_multi_dict.values())\n",
    "    max_count = max(value_counts.values())\n",
    "    return [\n",
    "        entity_type\n",
    "        for entity_type, count in line_items_multi_dict.items()\n",
    "        if value_counts[count] == max_count\n",
    "    ]\n",
    "\n",
    "\n",
    "def find_optimal_region(\n",
    "    dict_unique_ent: Dict[str, List[object]]\n",
    ") -> Tuple[Dict[str, List[float]], Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Calculates the optimal region for different entity types based on their bounding box coordinates.\n",
    "\n",
    "    Args:\n",
    "    dict_unique_ent (Dict[str, List[object]]): A dictionary where keys are entity types and\n",
    "                                               values are lists of sub-entity objects.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[Dict[str, List[float]], Dict[str, float]]: A tuple containing two dictionaries.\n",
    "                                                     The first dictionary maps entity types to sorted y-coordinates.\n",
    "                                                     The second dictionary maps entity types to the maximum difference\n",
    "                                                     in y-coordinates.\n",
    "    \"\"\"\n",
    "    region_line_items_x = {}\n",
    "    region_line_items_y = {}\n",
    "    opt_region = {}\n",
    "    for ent_type, sub_entities in dict_unique_ent.items():\n",
    "        min_x_1 = [\n",
    "            min(\n",
    "                vertex.x\n",
    "                for vertex in item.page_anchor.page_refs[\n",
    "                    0\n",
    "                ].bounding_poly.normalized_vertices\n",
    "            )\n",
    "            for item in sub_entities\n",
    "        ]\n",
    "        min_y_1 = [\n",
    "            min(\n",
    "                vertex.y\n",
    "                for vertex in item.page_anchor.page_refs[\n",
    "                    0\n",
    "                ].bounding_poly.normalized_vertices\n",
    "            )\n",
    "            for item in sub_entities\n",
    "        ]\n",
    "        region_line_items_x[ent_type] = min_x_1\n",
    "        region_line_items_y[ent_type] = min_y_1\n",
    "        opt_region[ent_type] = max(min_y_1) - min(min_y_1)\n",
    "    return {k: sorted(v) for k, v in region_line_items_y.items()}, opt_region\n",
    "\n",
    "\n",
    "def classify_line_items(\n",
    "    sub_entities: List[object], regions_line_y: Dict[str, List[float]]\n",
    ") -> Tuple[Dict[str, List[object]], List[object]]:\n",
    "    \"\"\"\n",
    "    Classifies sub-entities into different line items based on their y-coordinate regions.\n",
    "\n",
    "    Args:\n",
    "    sub_entities (List[object]): A list of sub-entity objects.\n",
    "    regions_line_y (Dict[str, List[float]]): A dictionary mapping line numbers to y-coordinate regions.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[Dict[str, List[object]], List[object]]: A tuple containing a dictionary of classified line items and\n",
    "                                                  a list of categorized sub-entities.\n",
    "    \"\"\"\n",
    "    line_item_dict_final = {}\n",
    "    sub_entities_categorized = []\n",
    "\n",
    "    for subentity in sub_entities:\n",
    "        y_ent = [\n",
    "            vertex.y\n",
    "            for vertex in subentity.page_anchor.page_refs[\n",
    "                0\n",
    "            ].bounding_poly.normalized_vertices\n",
    "        ]\n",
    "        for line_no, region in regions_line_y.items():\n",
    "            # Check if region is a tuple (meaning it has a start and end)\n",
    "            if isinstance(region, tuple):\n",
    "                # If y_ent is within the region bounds\n",
    "                if region[0] <= min(y_ent) < region[1]:\n",
    "                    line_item_dict_final.setdefault(line_no, []).append(subentity)\n",
    "                    sub_entities_categorized.append(subentity)\n",
    "            else:\n",
    "                # If y_ent is greater than or equal to the single value region\n",
    "                if min(y_ent) >= region[0]:  # Assuming region is a tuple with one value\n",
    "                    line_item_dict_final.setdefault(line_no, []).append(subentity)\n",
    "                    sub_entities_categorized.append(subentity)\n",
    "    return line_item_dict_final, sub_entities_categorized\n",
    "\n",
    "\n",
    "def create_lineitem(sub_entities: List[object], page: int) -> Dict[str, object]:\n",
    "    \"\"\"\n",
    "    Creates a line item from a list of sub-entities.\n",
    "\n",
    "    Args:\n",
    "    sub_entities (List[object]): A list of sub-entity objects.\n",
    "    page (int): The page number where the line item is located.\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, object]: A dictionary representing a line item with various properties.\n",
    "    \"\"\"\n",
    "    mention_text = \" \".join(e.mention_text for e in sub_entities)\n",
    "    bounding_vertices = [\n",
    "        vertex\n",
    "        for e in sub_entities\n",
    "        for vertex in e.page_anchor.page_refs[0].bounding_poly.normalized_vertices\n",
    "    ]\n",
    "    text_segments = [\n",
    "        segment for e in sub_entities for segment in e.text_anchor.text_segments\n",
    "    ]\n",
    "\n",
    "    x_coords = [vertex.x for vertex in bounding_vertices]\n",
    "    y_coords = [vertex.y for vertex in bounding_vertices]\n",
    "    bounding_poly = [\n",
    "        {\"x\": min(x_coords), \"y\": min(y_coords)},\n",
    "        {\"x\": max(x_coords), \"y\": max(y_coords)},\n",
    "    ]\n",
    "\n",
    "    line_item = {\n",
    "        \"mention_text\": mention_text,\n",
    "        \"page_anchor\": {\n",
    "            \"page_refs\": [\n",
    "                {\"bounding_poly\": {\"normalized_vertices\": bounding_poly}, \"page\": page}\n",
    "            ]\n",
    "        },\n",
    "        \"properties\": sub_entities,\n",
    "        \"text_anchor\": {\"text_segments\": text_segments},\n",
    "        \"type\": \"line_item\",\n",
    "    }\n",
    "    return line_item\n",
    "\n",
    "\n",
    "def multi_page_entites(\n",
    "    entities_pagewise: List[object], page: int\n",
    ") -> Tuple[List[Dict], str]:\n",
    "    \"\"\"\n",
    "    Processes entities page-wise to classify and create line items.\n",
    "\n",
    "    Args:\n",
    "    entities_pagewise (List[object]): A list of entity objects per page.\n",
    "    page (int): The page number of the entities.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[List[Dict], str]: A tuple containing a list of classified line items and the entity type\n",
    "                             with the maximum boundary.\n",
    "    \"\"\"\n",
    "    entity_types, line_item_sub_entities = extract_entity_types_and_subentities(\n",
    "        entities_pagewise\n",
    "    )\n",
    "\n",
    "    line_items_multi_dict = count_unique_entities(entity_types)\n",
    "    entity_types_keys = get_entity_types_with_max_count(line_items_multi_dict)\n",
    "\n",
    "    dict_unique_ent = {key: [] for key in entity_types_keys}\n",
    "    for entity in line_item_sub_entities:\n",
    "        entity_type = entity.type\n",
    "        if entity_type in dict_unique_ent:\n",
    "            dict_unique_ent[entity_type].append(entity)\n",
    "\n",
    "    sorted_region_line_y, opt_region = find_optimal_region(dict_unique_ent)\n",
    "\n",
    "    considered_boundry_ent = max(opt_region, key=opt_region.get)\n",
    "    regions_line_y_final = {\n",
    "        ent: {\n",
    "            i: (start, end)\n",
    "            for i, (start, end) in enumerate(zip(regions, regions[1:]), 1)\n",
    "        }\n",
    "        for ent, regions in sorted_region_line_y.items()\n",
    "        if ent == considered_boundry_ent\n",
    "    }\n",
    "\n",
    "    line_item_dict_final, sub_entities_categorized = classify_line_items(\n",
    "        line_item_sub_entities, regions_line_y_final[considered_boundry_ent]\n",
    "    )\n",
    "\n",
    "    line_items_classified = [\n",
    "        create_lineitem(sub_entities, page)\n",
    "        for sub_entities in line_item_dict_final.values()\n",
    "    ]\n",
    "\n",
    "    return line_items_classified, considered_boundry_ent\n",
    "\n",
    "\n",
    "def desc_merge_update(document: object) -> object:\n",
    "    \"\"\"\n",
    "    Merges and updates the description of entities within a document.\n",
    "\n",
    "    Args:\n",
    "    document (object): The document containing entities to be processed.\n",
    "\n",
    "    Returns:\n",
    "    object: The updated document with merged entity descriptions.\n",
    "    \"\"\"\n",
    "\n",
    "    def desc_merge_1(ent_desc):\n",
    "        desc_merge = documentai.types.documentai_pb2.Document.Entity()\n",
    "        desc_merge.type_ = \"line_item/description\"\n",
    "        desc_merge.mention_text = \"\"\n",
    "\n",
    "        text_anchors, pagerefs = extract_text_anchors_and_pagerefs(ent_desc)\n",
    "        desc_merge.text_anchor.text_segments.extend(text_anchors)\n",
    "        desc_merge.page_anchor.page_refs.extend(pagerefs)\n",
    "\n",
    "        sorted_list_text = sorted(text_anchors, key=lambda x: int(x.end_index))\n",
    "\n",
    "        desc_mention_text, desc_normalizedvertices = merge_descriptions(\n",
    "            sorted_list_text, ent_desc\n",
    "        )\n",
    "        desc_merge.mention_text = desc_mention_text\n",
    "        desc_merge.page_anchor.page_refs[0].bounding_poly.normalized_vertices.extend(\n",
    "            desc_normalizedvertices\n",
    "        )\n",
    "\n",
    "        return desc_merge\n",
    "\n",
    "    def extract_text_anchors_and_pagerefs(ent_desc):\n",
    "        text_anchors = []\n",
    "        pagerefs = []\n",
    "        for item in ent_desc:\n",
    "            text_anchors.append(item.text_anchor.text_segments[0])\n",
    "            pagerefs.extend(item.page_anchor.page_refs)\n",
    "        return text_anchors, pagerefs\n",
    "\n",
    "    def merge_descriptions(sorted_list_text, ent_desc):\n",
    "        desc_mention_text = \"\"\n",
    "        desc_normalizedvertices = []\n",
    "        for index in sorted_list_text:\n",
    "            for item in ent_desc:\n",
    "                if index in item.text_anchor.text_segments:\n",
    "                    desc_mention_text += \" \" + item.mention_text\n",
    "                    desc_normalizedvertices.extend(\n",
    "                        item.page_anchor.page_refs[0].bounding_poly.normalized_vertices\n",
    "                    )\n",
    "        return desc_mention_text, desc_normalizedvertices\n",
    "\n",
    "    def calculate_bounding_box(vertices):\n",
    "        min_x = min(vertices, key=lambda d: d.x).x\n",
    "        max_x = max(vertices, key=lambda d: d.x).x\n",
    "        min_y = min(vertices, key=lambda d: d.y).y\n",
    "        max_y = max(vertices, key=lambda d: d.y).y\n",
    "        return [\n",
    "            documentai.types.documentai_pb2.Document.Page.Layout.NormalizedVertex(\n",
    "                x=min_x, y=min_y\n",
    "            ),\n",
    "            documentai.types.documentai_pb2.Document.Page.Layout.NormalizedVertex(\n",
    "                x=min_x, y=max_y\n",
    "            ),\n",
    "            documentai.types.documentai_pb2.Document.Page.Layout.NormalizedVertex(\n",
    "                x=max_x, y=min_y\n",
    "            ),\n",
    "            documentai.types.documentai_pb2.Document.Page.Layout.NormalizedVertex(\n",
    "                x=max_x, y=max_y\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "    for entity in document.entities:\n",
    "        if entity.type == \"line_item\":\n",
    "            ent_desc = [\n",
    "                prop\n",
    "                for prop in entity.properties\n",
    "                if prop.type == \"line_item/description\"\n",
    "            ]\n",
    "            if len(ent_desc) > 1:\n",
    "                desc_merge = desc_merge_1(ent_desc)\n",
    "                entity.properties = [\n",
    "                    prop for prop in entity.properties if prop not in ent_desc\n",
    "                ]\n",
    "                entity.properties.append(desc_merge)\n",
    "\n",
    "    return document\n",
    "\n",
    "\n",
    "for filename, filepath in tqdm(file_dict.items(), desc=\"Progress\"):\n",
    "    if \".json\" in filepath:\n",
    "        # Construct the full file path\n",
    "        full_file_path = f\"gs://{gcs_input_path.split('/')[2]}/{filepath}\"\n",
    "        print(filename)\n",
    "        # Download the JSON file content\n",
    "        bucket = storage_client.get_bucket(gcs_input_path.split(\"/\")[2])\n",
    "        blob = bucket.blob(filepath)\n",
    "        json_dict = json.loads(blob.download_as_string().decode(\"utf-8\"))\n",
    "\n",
    "        print(json_dict.keys())\n",
    "        # Perform the necessary processing with json_dict\n",
    "        document = documentai.Document.from_json(json.dumps(json_dict))\n",
    "        # print(document)\n",
    "        json_dict_updated = merge_entities(document)\n",
    "\n",
    "        if line_item_across_pages == \"Yes\":\n",
    "            json_dict_updated = group_line_items(json_dict_updated)\n",
    "\n",
    "        if desc_merge_update == \"Yes\":\n",
    "            json_dict_updated = desc_merge_update(json_dict_updated)\n",
    "\n",
    "        # Define the full path within the output bucket where the file should be saved\n",
    "        output_path_within_bucket = \"/\".join(gcs_output_path.split(\"/\")[3:]) + filename\n",
    "\n",
    "        print(type(json_dict_updated))\n",
    "        temp_dict = json.loads(documentai.Document.to_json(json_dict_updated))\n",
    "\n",
    "        # Save the updated JSON back to the output bucket using the provided function\n",
    "        updated_document_json = json.dumps(temp_dict, ensure_ascii=False)\n",
    "        utilities.store_document_as_json(\n",
    "            updated_document_json, output_bucket_name, output_path_within_bucket\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d290293d-7a57-4b9f-b059-7484dc82d07a",
   "metadata": {},
   "source": [
    "## 3. Output\n",
    "\n",
    "### Groups the child items of line_item into right parent (line_item)\n",
    "### Before post processing the parsed json has multiple line items without proper grouping\n",
    "\n",
    "<img src=\"./images/line_item_improver_pre_sample.png\" width=800 height=400></img>\n",
    "\n",
    "### After post processing, properly grouped line items\n",
    "\n",
    "<img src=\"./images/line_item_improver_pre_sample.png\" width=800 height=400></img>"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m104",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m104"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
