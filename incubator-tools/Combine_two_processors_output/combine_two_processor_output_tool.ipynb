{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1af5ebbb-4a9c-4c9a-8ae8-d9574c754397",
   "metadata": {},
   "source": [
    "# Combine Two processor Output Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dde429f-5406-4a57-a6f9-d6adf8ae3c5b",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aa9549-e3a4-47b1-9648-c95e2a02eea6",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the **DocAI Incubator Team**. No guarantees of performance are implied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6dc7e8-a4a9-4279-8271-000cabe72bf7",
   "metadata": {},
   "source": [
    "## Objective\n",
    "The objective of the tooling is to efficiently integrate the output of one AI processor (proto) with another. This integration results in a comprehensive final output that reflects the combined capabilities of both parsers. Technically, this process involves sending the document proto object from one parser to the next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd252261-d116-4f31-9b46-c19b14bba143",
   "metadata": {},
   "source": [
    "## Prerequisites \n",
    "* Python : Jupyter notebook (Vertex AI) or Google Colab \n",
    "* Permission to the Google project and Document AI \n",
    "* Input PDF Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8382ad-e669-4f07-87ae-91a09895eacb",
   "metadata": {},
   "source": [
    "## Sync Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20289168-5b69-4a39-9bb7-1bd8f0bb7047",
   "metadata": {},
   "source": [
    "### Importing Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c7ea6e-3c37-499d-84b7-989924e233ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.api_core.client_options import ClientOptions\n",
    "import google.auth.transport.requests\n",
    "from google import auth\n",
    "from google.cloud import documentai\n",
    "from google.cloud import storage\n",
    "import requests\n",
    "import json\n",
    "import mimetypes\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856cd580-126e-40b2-85c7-c5144908361f",
   "metadata": {},
   "source": [
    "### Setup the required inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b258c2-d255-4755-b528-ba0f1697795d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First and Second Processor Configuration Details\n",
    "# Replace with your Google Cloud Project ID\n",
    "PROJECT_ID = \"<YOUR_FIRST_PROCESSOR_PROJECT_ID>\"  # e.g., \"project-123\"\n",
    "# Specify the location for the first processor\n",
    "LOCATION = \"<LOCATION_FOR_FIRST_PROCESSOR>\"  # e.g., \"us\"\n",
    "# Replace with the ID of your first processor\n",
    "PROCESSOR_ID = \"<YOUR_FIRST_PROCESSOR_ID>\"  # e.g., \"1234abcd\"\n",
    "\n",
    "# The MIME type for the files to be processed\n",
    "MIME_TYPE = \"application/pdf\"  # Keep as-is if processing PDF files\n",
    "\n",
    "# Configuration for the second processor\n",
    "# Replace with your Google Cloud Project ID for the second processor\n",
    "PROJECT_ID_2 = \"<YOUR_SECOND_PROCESSOR_PROJECT_ID>\"  # e.g., \"project-456\"\n",
    "# Specify the location for the second processor\n",
    "LOCATION_2 = \"<LOCATION_FOR_SECOND_PROCESSOR>\"  # e.g., \"us\"\n",
    "# Replace with the ID of your second processor\n",
    "PROCESSOR_ID_2 = \"<YOUR_SECOND_PROCESSOR_ID>\"  # e.g., \"5678efgh\"\n",
    "\n",
    "# Google Cloud Storage Bucket Paths\n",
    "# Specify the path to the input PDF files\n",
    "input_path = \"<PATH_TO_INPUT_PDF_FILES>\"  # e.g., \"bucket/input_pdf/\" gs:// is not required at the beginning.\n",
    "# Specify the path for output from the first parser\n",
    "output_path1 = \"<PATH_FOR_FIRST_PARSER_OUTPUT>\"  # e.g., \"bucket/first_parser_output\" gs:// is not required at the beginning.\n",
    "# Specify the path for output from the second parser\n",
    "output_path2 = \"<PATH_FOR_SECOND_PARSER_OUTPUT>\"  # e.g., \"bucket/second_parser_output\"  gs:// is not required at the beginning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc6af03-68d9-474d-8124-381110db8902",
   "metadata": {},
   "source": [
    "* `PROJECT_ID :`  Google Cloud Project ID for the first processor.\n",
    "* `LOCATION :`  Google Cloud project location for the first processor.\n",
    "* `PROCESSOR_ID :`  Processor ID from the first processor.\n",
    "* `MIME_TYPE :`   The MIME type for the files to be processed.\n",
    "* `PROJECT_ID_2 :` Google Cloud Project ID for the second processor.\n",
    "* `LOCATION_2 :`  Google Cloud project location for the second processor.\n",
    "* `PROCESSOR_ID_2 :`  Processor ID from the second processor.\n",
    "* `input_path :`  The path to the input PDF files.\n",
    "* `output_path1 :`  The path for output from the first parser.\n",
    "* `output_path2 :`  The path for output from the second parser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc8f035-59b9-46a6-b2db-73e44ab7e372",
   "metadata": {},
   "source": [
    "### Run the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5186722b-524d-4ef0-8efc-952309aa6878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get credentials of current user / service account\n",
    "def get_access_token() -> str:\n",
    "    \"\"\"\n",
    "    Retrieves the access token for authentication.\n",
    "\n",
    "    Returns:\n",
    "        str: The access token.\n",
    "    \"\"\"\n",
    "\n",
    "    credentials, _ = auth.default()\n",
    "    credentials.refresh(google.auth.transport.requests.Request())\n",
    "    return credentials.token\n",
    "\n",
    "\n",
    "def list_files(bucket_name: str, prefix: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Lists all files in a Google Cloud Storage (GCS) bucket with the given prefix.\n",
    "\n",
    "    Args:\n",
    "        bucket_name (str): The name of the GCS bucket.\n",
    "        prefix (str): The prefix to filter files in the bucket.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of file names in the bucket with the specified prefix.\n",
    "    \"\"\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    blobs = storage_client.list_blobs(bucket_name, prefix=prefix)\n",
    "    return [blob.name for blob in blobs]\n",
    "\n",
    "\n",
    "def download_blob(\n",
    "    bucket_name: str, source_blob_name: str, destination_file_name: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Downloads a blob from a GCS bucket.\n",
    "\n",
    "    Args:\n",
    "        bucket_name (str): The name of the GCS bucket.\n",
    "        source_blob_name (str): The name of the source blob.\n",
    "        destination_file_name (str): The name of the destination file.\n",
    "\n",
    "    Returns:\n",
    "        str: The path to the downloaded file.\n",
    "    \"\"\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "    return destination_file_name\n",
    "\n",
    "\n",
    "def upload_blob(\n",
    "    bucket_name: str, source_file_name: str, destination_blob_name: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Uploads a file to a GCS bucket.\n",
    "\n",
    "    Args:\n",
    "        bucket_name (str): The name of the GCS bucket.\n",
    "        source_file_name (str): The name of the source file.\n",
    "        destination_blob_name (str): The name of the destination blob.\n",
    "    \"\"\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "\n",
    "def get_bucket_and_prefix(full_path: str) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Extracts the bucket name and prefix from a full path.\n",
    "\n",
    "    Args:\n",
    "        full_path (str): The full path containing the bucket name and prefix.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, str]: A tuple containing the bucket name and prefix.\n",
    "    \"\"\"\n",
    "\n",
    "    parts = full_path.split(\"/\")\n",
    "    bucket_name = parts[0]\n",
    "    prefix = \"/\".join(parts[1:])\n",
    "    return bucket_name, prefix\n",
    "\n",
    "\n",
    "def second_processer_calling(\n",
    "    document: object, PROJECT_ID_2: str, LOCATION_2: str, PROCESSOR_ID_2: str\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Calls the second Document AI processor to process the document.\n",
    "\n",
    "    Args:\n",
    "        document (object): The document to be processed.\n",
    "        PROJECT_ID_2 (str): The Google Cloud project ID.\n",
    "        LOCATION_2 (str): The location of the Document AI processor.\n",
    "        PROCESSOR_ID_2 (str): The ID of the Document AI processor.\n",
    "\n",
    "    Returns:\n",
    "        dict: The JSON response from the second processor.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Processing through Second Parser\")\n",
    "    url = f\"https://us-documentai.googleapis.com/v1/projects/{PROJECT_ID_2}/locations/{LOCATION_2}/processors/{PROCESSOR_ID_2}:process\"\n",
    "    headers = {\"Authorization\": f\"Bearer {get_access_token()}\"}\n",
    "    json_data = documentai.Document.to_json(document)\n",
    "    json_data_dict = json.loads(json_data)  # Parse the JSON string to a dictionary\n",
    "\n",
    "    create_process_request = {\"inlineDocument\": json_data_dict}\n",
    "    create_processor_response = requests.post(\n",
    "        url, headers=headers, json=create_process_request\n",
    "    )\n",
    "    create_processor_response.raise_for_status()\n",
    "    json_object = create_processor_response.json()\n",
    "    return json_object[\"document\"]\n",
    "\n",
    "\n",
    "def online_process(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    processor_id: str,\n",
    "    file_content: bytes,\n",
    "    mime_type: str,\n",
    ") -> object:\n",
    "    \"\"\"\n",
    "    Process a document with the given MIME type and content using the Document AI processor.\n",
    "\n",
    "    Args:\n",
    "        project_id (str): The Google Cloud project ID.\n",
    "        location (str): The location of the Document AI processor.\n",
    "        processor_id (str): The ID of the Document AI processor.\n",
    "        file_content (bytes): The content of the document.\n",
    "        mime_type (str): The MIME type of the document content.\n",
    "\n",
    "    Returns:\n",
    "        object: The processed Document AI document.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Processing through First Parser\")\n",
    "    opts = {\"api_endpoint\": f\"{location}-documentai.googleapis.com\"}\n",
    "    documentai_client = documentai.DocumentProcessorServiceClient(client_options=opts)\n",
    "    resource_name = documentai_client.processor_path(project_id, location, processor_id)\n",
    "\n",
    "    # Load Binary Data into Document AI RawDocument Object\n",
    "    raw_document = documentai.RawDocument(content=file_content, mime_type=mime_type)\n",
    "\n",
    "    # Configure the process request\n",
    "    request = documentai.ProcessRequest(name=resource_name, raw_document=raw_document)\n",
    "\n",
    "    # Use the Document AI client to process the document\n",
    "    result = documentai_client.process_document(request=request)\n",
    "\n",
    "    return result.document\n",
    "\n",
    "\n",
    "def process_files_in_bucket(input_path: str, output_path1: str, output_path2: str):\n",
    "    \"\"\"\n",
    "    Processes files in a bucket and saves output to other buckets.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): The path to the input bucket and prefix.\n",
    "        output_path1 (str): The path to the output bucket and prefix for the first parser output.\n",
    "        output_path2 (str): The path to the output bucket and prefix for the second parser output.\n",
    "    \"\"\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    input_bucket_name, input_prefix = get_bucket_and_prefix(input_path)\n",
    "    output_bucket1_name, output_prefix1 = get_bucket_and_prefix(output_path1)\n",
    "    output_bucket2_name, output_prefix2 = get_bucket_and_prefix(output_path2)\n",
    "    input_bucket = storage_client.bucket(input_bucket_name)\n",
    "\n",
    "    for blob in input_bucket.list_blobs(prefix=input_prefix):\n",
    "        if (\n",
    "            not blob.name.endswith(\"/\") and blob.name != input_prefix\n",
    "        ):  # Skip directories and the prefix itself\n",
    "            file_name = blob.name\n",
    "            content = blob.download_as_bytes()\n",
    "\n",
    "            # Removing the original extension and appending .json\n",
    "            base_file_name = os.path.splitext(os.path.basename(file_name))[0] + \".json\"\n",
    "\n",
    "            print(\"Processing file:\", file_name)  # Debug print\n",
    "            mime_type = mimetypes.guess_type(file_name)[0] or \"application/octet-stream\"\n",
    "            print(\"Detected MIME type:\", mime_type)  # Debug print\n",
    "\n",
    "            # Process with first parser\n",
    "            first_parser_output = online_process(\n",
    "                PROJECT_ID, LOCATION, PROCESSOR_ID, content, mime_type\n",
    "            )\n",
    "            first_parser_output_json = json.loads(\n",
    "                documentai.Document.to_json(first_parser_output)\n",
    "            )\n",
    "\n",
    "            # Save first parser output to output_bucket1\n",
    "            output_blob1 = storage.Blob(\n",
    "                output_prefix1 + \"/\" + base_file_name,\n",
    "                storage_client.bucket(output_bucket1_name),\n",
    "            )\n",
    "            output_blob1.upload_from_string(\n",
    "                json.dumps(first_parser_output_json, indent=2),\n",
    "                content_type=\"application/json\",\n",
    "            )\n",
    "\n",
    "            # Process with second parser\n",
    "            second_parser_output = second_processer_calling(\n",
    "                first_parser_output, PROJECT_ID_2, LOCATION_2, PROCESSOR_ID_2\n",
    "            )\n",
    "\n",
    "            # Save second parser output to output_bucket2\n",
    "            output_blob2 = storage.Blob(\n",
    "                output_prefix2 + \"/\" + base_file_name,\n",
    "                storage_client.bucket(output_bucket2_name),\n",
    "            )\n",
    "            output_blob2.upload_from_string(\n",
    "                json.dumps(second_parser_output, indent=2),\n",
    "                content_type=\"application/json\",\n",
    "            )\n",
    "\n",
    "\n",
    "# Call the function\n",
    "process_files_in_bucket(input_path, output_path1, output_path2)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20aa173-4dbc-415d-bf1c-887f5f57b4ab",
   "metadata": {},
   "source": [
    "### Async Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd56f6f-d520-46a1-80da-9adb38d74f7d",
   "metadata": {},
   "source": [
    "### Importing Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fed567-6c79-4fc9-b9a6-8c1d416e5f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.auth.transport.requests\n",
    "from google import auth\n",
    "from google.cloud import documentai\n",
    "from google.cloud import storage\n",
    "import requests\n",
    "import json\n",
    "import mimetypes\n",
    "import asyncio\n",
    "import aiohttp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3c0fe2-3063-42ba-9415-571e8a503950",
   "metadata": {},
   "source": [
    "### Run the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b554003-baad-4848-a36a-1075d27e0600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get credentials of current user / service account\n",
    "def get_access_token() -> str:\n",
    "    \"\"\"\n",
    "    Retrieves the access token for authentication.\n",
    "\n",
    "    Returns:\n",
    "        str: The access token.\n",
    "    \"\"\"\n",
    "\n",
    "    credentials, _ = auth.default()\n",
    "    credentials.refresh(google.auth.transport.requests.Request())\n",
    "    return credentials.token\n",
    "\n",
    "\n",
    "def batch_process_documents(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    processor_id: str,\n",
    "    gcs_input_uri: str,\n",
    "    gcs_output_uri: str,\n",
    "    timeout: int = 6000,\n",
    ") -> object:\n",
    "    \"\"\"\n",
    "    Batch process documents using the Document AI processor.\n",
    "\n",
    "    Args:\n",
    "        project_id (str): The Google Cloud project ID.\n",
    "        location (str): The location of the Document AI processor.\n",
    "        processor_id (str): The ID of the Document AI processor.\n",
    "        gcs_input_uri (str): The URI of the input documents in Google Cloud Storage.\n",
    "        gcs_output_uri (str): The URI of the output documents in Google Cloud Storage.\n",
    "        timeout (int): Timeout for the operation in seconds. Defaults to 6000.\n",
    "\n",
    "    Returns:\n",
    "        documentai.BatchDocumentsResponse: The response object containing the batch processing operation result.\n",
    "    \"\"\"\n",
    "\n",
    "    from google.cloud import documentai_v1beta3 as documentai\n",
    "\n",
    "    # You must set the api_endpoint if you use a location other than 'us', e.g.:\n",
    "    opts = {}\n",
    "    if location == \"eu\":\n",
    "        opts = {\"api_endpoint\": \"eu-documentai.googleapis.com\"}\n",
    "    elif location == \"us\":\n",
    "        opts = {\"api_endpoint\": \"us-documentai.googleapis.com\"}\n",
    "        # opts = {\"api_endpoint\": \"us-autopush-documentai.sandbox.googleapis.com\"}\n",
    "    client = documentai.DocumentProcessorServiceClient(client_options=opts)\n",
    "\n",
    "    destination_uri = f\"{gcs_output_uri}/\"\n",
    "\n",
    "    input_config = documentai.BatchDocumentsInputConfig(\n",
    "        gcs_prefix=documentai.GcsPrefix(gcs_uri_prefix=gcs_input_uri)\n",
    "    )\n",
    "\n",
    "    # Where to write results\n",
    "    output_config = documentai.DocumentOutputConfig(\n",
    "        gcs_output_config={\"gcs_uri\": destination_uri}\n",
    "    )\n",
    "\n",
    "    # Location can be 'us' or 'eu'\n",
    "    name = f\"projects/{project_id}/locations/{location}/processors/{processor_id}\"\n",
    "    request = documentai.types.document_processor_service.BatchProcessRequest(\n",
    "        name=name,\n",
    "        input_documents=input_config,\n",
    "        document_output_config=output_config,\n",
    "    )\n",
    "\n",
    "    operation = client.batch_process_documents(request)\n",
    "\n",
    "    # Wait for the operation to finish\n",
    "    operation.result(timeout=timeout)\n",
    "    return operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd889c2-fcb9-4072-a30e-0e599841e0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = batch_process_documents(\n",
    "    project_id=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    "    processor_id=PROCESSOR_ID,\n",
    "    gcs_input_uri=f\"gs://{input_path}\",\n",
    "    gcs_output_uri=f\"gs://{output_path1}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f160c3e6-fd5e-474e-ab37-4074b580e96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def second_processer_calling(\n",
    "    document: object,\n",
    "    PROJECT_ID_2: str,\n",
    "    LOCATION_2: str,\n",
    "    PROCESSOR_ID_2: str,\n",
    "    session: aiohttp.ClientSession,\n",
    ") -> object:\n",
    "    \"\"\"\n",
    "    Asynchronously calls the second Document AI processor.\n",
    "\n",
    "    Args:\n",
    "        document (object): The document to process.\n",
    "        PROJECT_ID_2 (str): The Google Cloud project ID for the second processor.\n",
    "        LOCATION_2 (str): The location of the second processor.\n",
    "        PROCESSOR_ID_2 (str): The ID of the second processor.\n",
    "        session (aiohttp.ClientSession): The aiohttp session for making HTTP requests.\n",
    "\n",
    "    Returns:\n",
    "        object: The processed document.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Processing through Second Parser\")\n",
    "    url = f\"https://us-documentai.googleapis.com/v1/projects/{PROJECT_ID_2}/locations/{LOCATION_2}/processors/{PROCESSOR_ID_2}:process\"\n",
    "    headers = {\"Authorization\": f\"Bearer {get_access_token()}\"}\n",
    "    # json_data = documentai.Document.to_json(document)\n",
    "    # json_data_dict = json.loads(json_data)\n",
    "\n",
    "    create_process_request = {\"inlineDocument\": document}\n",
    "    async with session.post(\n",
    "        url, headers=headers, json=create_process_request\n",
    "    ) as response:\n",
    "        response.raise_for_status()\n",
    "        json_object = await response.json()\n",
    "        return json_object[\"document\"]\n",
    "\n",
    "\n",
    "async def save_to_gcs(bucket: str, blob_name: str, data: str) -> None:\n",
    "    \"\"\"\n",
    "    Asynchronously saves data to Google Cloud Storage.\n",
    "\n",
    "    Args:\n",
    "        bucket (str): The Google Cloud Storage bucket.\n",
    "        blob_name (str): The name of the blob.\n",
    "        data (str): The data to save.\n",
    "    \"\"\"\n",
    "\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.upload_from_string(data)\n",
    "\n",
    "\n",
    "async def main():\n",
    "    # Splitting the bucket name and prefix from output_path1\n",
    "    bucket_name, prefix = output_path1.split(\"/\", 1)\n",
    "\n",
    "    # Fetch JSON from output_path1 and its subfolders\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blobs = bucket.list_blobs(prefix=prefix)\n",
    "\n",
    "    # Initialize aiohttp session\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        original_filenames = []\n",
    "\n",
    "        for blob in blobs:\n",
    "            if blob.name.endswith(\".json\"):\n",
    "                json_string = blob.download_as_string()\n",
    "                document = json.loads(json_string)\n",
    "                original_filenames.append(os.path.basename(blob.name))\n",
    "\n",
    "                # Schedule asynchronous processing\n",
    "                task = asyncio.ensure_future(\n",
    "                    second_processer_calling(\n",
    "                        document, PROJECT_ID_2, LOCATION_2, PROCESSOR_ID_2, session\n",
    "                    )\n",
    "                )\n",
    "                tasks.append(task)\n",
    "\n",
    "        # Wait for all tasks to complete\n",
    "        results = await asyncio.gather(*tasks)\n",
    "\n",
    "        # Save results to output_path2 with the same original file names\n",
    "        _, output_prefix = output_path2.split(\"/\", 1)\n",
    "        for filename, result in zip(original_filenames, results):\n",
    "            result_json = json.dumps(result)\n",
    "            blob_name = f\"{output_prefix}/{filename}\"\n",
    "            await save_to_gcs(bucket, blob_name, result_json)\n",
    "\n",
    "\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
