{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cbcbc6b-0c0b-42d5-8619-ebd1f4bf8657",
   "metadata": {},
   "source": [
    "# Convert AutoML Response to DocumentAI Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a054d9da-3034-4fb0-b828-cd536198f68c",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108c2600-51ad-4085-9555-7f3332ccc1b4",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the **DocAI Incubator Team**. No guarantees of performance are implied.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8d9223-2537-4ae0-98a5-32e372d1419d",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "This tool helps to call Image object detection endpoint via API call and converts its response to DocumentAI format and stores post-processed results in GCS as JSON files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07918d4f-aae3-428d-8a71-4fbe2d6c8c90",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* GCP Project ID\n",
    "* DocumentAI OCR Processor ID\n",
    "* Endpoint ID from VertexAI Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92d9b0c-4c03-41b6-aa2e-fcf838791610",
   "metadata": {},
   "source": [
    "## Step by Step procedure "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d50f73-088d-4b58-9cfc-f276f2201f9c",
   "metadata": {},
   "source": [
    "### 1.Importing Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad01d013-4198-463f-9c6b-2792ec407efc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad415a6d-6b54-4241-a781-678a527f0ed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "from google.api_core.client_options import ClientOptions\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from google.cloud import storage\n",
    "from PIL import Image\n",
    "\n",
    "from utilities import file_names, store_document_as_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110046b1-ebea-4a2b-990d-5acda4a3b66c",
   "metadata": {},
   "source": [
    "### 2.Setup the inputs\n",
    "* `project_id` : Provide GCP project Number \n",
    "* `processor_location`: Provide DocumentAI processor location (“us” or “eu”)\n",
    "* `processor_id` : Provide GCP DocumentAI processor id\n",
    "* `processor_version` : Provide GCP DocumentAI processor version id\n",
    "* `endpoint_id` : Provide prediction endpoint id\n",
    "* `endpoint_location` : Provide prediction endpoint location\n",
    "* `score_threshold` : Provide threshold value for prediction object confidences. Ranges from 0 - 1\n",
    "* `gcs_input_folder` :  Provide GCS path of images folder\n",
    "* `gcs_output_folder` :  Provide GCS path to store postprocessed results(DocumentAI JSON format results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393c194a-f607-488e-afd5-1747bd5e45b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project_id = \"<<project_id>>\"\n",
    "processor_location = \"<<location>>\"\n",
    "processor_id = \"<<OCR_processor_id>>\"  # OCR\n",
    "processor_version = \"pretrained-ocr-v1.0-2020-09-23\"\n",
    "endpoint_id = \"<<endpoint>>\"\n",
    "endpoint_location = \"<<endpoint_location>>\"\n",
    "score_threshold = 0.1\n",
    "gcs_input_folder = \"gs://<<Bucket_name>>/<<input_files_sub_path>>/\"\n",
    "gcs_output_folder = \"gs://<<Bucket_name>>/<<output_files_sub_path>>/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846011bb-eb92-454d-9ea9-6c9763490bce",
   "metadata": {},
   "source": [
    "### 3.Run the required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6127e1d9-6a45-4827-a10c-481343697abc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_document_sample(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    processor_id: str,\n",
    "    pdf_bytes: bytes,\n",
    "    processor_version: str,\n",
    "    mime_type: str,\n",
    ") -> \"documentai.ProcessResponse\":\n",
    "    \"\"\"\n",
    "    Processes a document using Google Document AI and returns the processed result.\n",
    "\n",
    "    Args:\n",
    "        project_id (str): The ID of the Google Cloud project.\n",
    "        location (str): The location of the Document AI processor (e.g., 'us', 'eu').\n",
    "        processor_id (str): The unique ID of the processor used to process the document.\n",
    "        pdf_bytes (bytes): The byte content of the PDF file to be processed.\n",
    "        processor_version (str): The version of the processor to use (e.g., 'v1').\n",
    "        mime_type (str): The MIME type of the file (e.g., 'application/pdf').\n",
    "\n",
    "    Returns:\n",
    "        documentai.ProcessResponse: The response object containing the results of the document processing.\n",
    "\n",
    "    Raises:\n",
    "        google.api_core.exceptions.GoogleAPIError: If an error occurs during processing.\n",
    "    \"\"\"\n",
    "    # You must set the `api_endpoint` if you use a location other than \"us\".\n",
    "    opts = ClientOptions(api_endpoint=f\"{location}-documentai.googleapis.com\")\n",
    "    client = documentai.DocumentProcessorServiceClient(client_options=opts)\n",
    "\n",
    "    name = client.processor_version_path(\n",
    "        project_id, location, processor_id, processor_version\n",
    "    )\n",
    "\n",
    "    raw_document = documentai.RawDocument(content=pdf_bytes, mime_type=mime_type)\n",
    "\n",
    "    # Configure the process request\n",
    "    request = documentai.ProcessRequest(\n",
    "        name=name, raw_document=raw_document, skip_human_review=False\n",
    "    )\n",
    "\n",
    "    # Recognizes text entities in the PDF document\n",
    "    result = client.process_document(request=request)\n",
    "    return result\n",
    "\n",
    "\n",
    "def predict_image_object_detection_sample(\n",
    "    project: str,\n",
    "    endpoint_id: str,\n",
    "    payload: Dict[str, str],\n",
    "    location: str = \"us-central1\",\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Sends an image object detection request to the AI Platform and returns the prediction result.\n",
    "\n",
    "    Args:\n",
    "        project (str): The ID of the Google Cloud project.\n",
    "        endpoint_id (str): The endpoint ID of the deployed model for object detection.\n",
    "        payload (Dict[str, str]): The input data for prediction, typically containing the image information (e.g., base64 encoded image).\n",
    "        location (str, optional): The region where the endpoint is hosted. Default is 'us-central1'.\n",
    "\n",
    "    Returns:\n",
    "        Dict: The prediction result containing detected objects and their corresponding details (e.g., bounding boxes, labels, confidence scores).\n",
    "\n",
    "    Raises:\n",
    "        google.api_core.exceptions.GoogleAPIError: If an error occurs during the prediction request.\n",
    "    \"\"\"\n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": f\"{location}-aiplatform.googleapis.com\"}\n",
    "    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "    endpoint = client.endpoint_path(\n",
    "        project=project, location=location, endpoint=endpoint_id\n",
    "    )\n",
    "    response = client.predict(endpoint=endpoint, instances=[payload])\n",
    "    return dict(response.predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb2b3e0-8d10-4a57-83ab-330b041fa187",
   "metadata": {},
   "source": [
    "### 4.Run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b255b0-cb06-4f4e-8f0e-75c2d56a52bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    splits = gcs_input_folder.strip(\"/\").split(\"/\")\n",
    "    input_bucket, input_folder = splits[2], \"/\".join(splits[3:])\n",
    "    splits = gcs_output_folder.strip(\"/\").split(\"/\")\n",
    "    output_bucket, output_folder = splits[2], \"/\".join(splits[3:])\n",
    "    _, files_dict = file_names(gcs_input_folder)\n",
    "\n",
    "    input_storage_client = storage.Client()\n",
    "    input_bucket_obj = input_storage_client.get_bucket(input_bucket)\n",
    "    mime_type = \"application/pdf\"\n",
    "    for fn, fp in files_dict.items():\n",
    "        print(f\"Processing {fn}\")\n",
    "        print(\"\\tCalling DocAI API...\")\n",
    "        pdf_bytes = input_bucket_obj.blob(fp).download_as_bytes()\n",
    "        if fn.endswith(\".png\"):\n",
    "            mime_type = \"image/png\"\n",
    "        elif fn.endswith(\".jpeg\"):\n",
    "            mime_type = \"image/jpeg\"\n",
    "        elif fn.endswith(\".jpg\"):\n",
    "            mime_type = \"image/jpeg\"\n",
    "        doc = process_document_sample(\n",
    "            project_id,\n",
    "            processor_location,\n",
    "            processor_id,\n",
    "            pdf_bytes,\n",
    "            processor_version,\n",
    "            mime_type,\n",
    "        ).document\n",
    "        try:\n",
    "            encoded_content = base64.b64encode(pdf_bytes).decode(\"utf-8\")\n",
    "            payload = {\"content\": encoded_content}\n",
    "            res = predict_image_object_detection_sample(\n",
    "                project_id, endpoint_id, payload\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"Unable to process\", fn, str(e))\n",
    "            continue\n",
    "        for page in doc.pages:\n",
    "            img = Image.open(BytesIO(page.image.content))\n",
    "            width, height = img.size\n",
    "            elements = []\n",
    "            for i in range(len(res[\"displayNames\"])):\n",
    "                if res[\"confidences\"][i] < score_threshold:\n",
    "                    continue\n",
    "                xmin, xmax, ymin, ymax = res[\"bboxes\"][i]\n",
    "                visual_element_bbox_v = [\n",
    "                    int(xmin * width),\n",
    "                    int(ymin * height),\n",
    "                    int(xmax * width),\n",
    "                    int(ymax * height),\n",
    "                ]\n",
    "                visual_element_bbox_nv = [xmin, ymin, xmax, ymax]\n",
    "                ele = {}\n",
    "                ele[\"type_\"] = res[\"displayNames\"][i]\n",
    "                ele[\"confidence\"] = float(res[\"confidences\"][i])\n",
    "                ele[\"vertices\"] = visual_element_bbox_v\n",
    "                ele[\"normalized_vertices\"] = visual_element_bbox_nv\n",
    "                elements.append(ele)\n",
    "            entities = []\n",
    "            for ele in elements:\n",
    "                _type = ele[\"type_\"]\n",
    "                _confidence = ele[\"confidence\"]\n",
    "                _mention_text = \"\"\n",
    "\n",
    "                _vertices = []\n",
    "                x, y = ele[\"vertices\"][0::2], ele[\"vertices\"][1::2]\n",
    "                xy = [\n",
    "                    [min(x), min(y)],\n",
    "                    [max(x), min(y)],\n",
    "                    [max(x), max(y)],\n",
    "                    [min(x), max(y)],\n",
    "                ]\n",
    "                for _x, _y in xy:\n",
    "                    vertex = documentai.Vertex(x=_x, y=_y)\n",
    "                    _vertices.append(vertex)\n",
    "\n",
    "                _normalized_vertices = []\n",
    "                x, y = (\n",
    "                    ele[\"normalized_vertices\"][0::2],\n",
    "                    ele[\"normalized_vertices\"][1::2],\n",
    "                )\n",
    "                xy = [\n",
    "                    [min(x), min(y)],\n",
    "                    [max(x), min(y)],\n",
    "                    [max(x), max(y)],\n",
    "                    [min(x), max(y)],\n",
    "                ]\n",
    "                for _x, _y in xy:\n",
    "                    normalized_vertex = documentai.NormalizedVertex(x=_x, y=_y)\n",
    "                    _normalized_vertices.append(normalized_vertex)\n",
    "\n",
    "                _bounding_poly = documentai.BoundingPoly(\n",
    "                    vertices=_vertices, normalized_vertices=_normalized_vertices\n",
    "                )\n",
    "                _page_ref = documentai.Document.PageAnchor.PageRef(\n",
    "                    page=page.page_number - 1,\n",
    "                    bounding_poly=_bounding_poly,\n",
    "                    layout_type=\"LAYOUT_TYPE_UNSPECIFIED\",\n",
    "                )\n",
    "                _page_anchor = documentai.Document.PageAnchor(page_refs=[_page_ref])\n",
    "                _text_anchor = documentai.Document.TextAnchor()\n",
    "                ent = documentai.Document.Entity(\n",
    "                    type_=_type,\n",
    "                    mention_text=_mention_text,\n",
    "                    confidence=_confidence,\n",
    "                    page_anchor=_page_anchor,\n",
    "                    text_anchor=_text_anchor,\n",
    "                )\n",
    "                ent.normalized_value.boolean_value = True\n",
    "                entities.append(ent)\n",
    "\n",
    "                ve = documentai.Document.Page.VisualElement()\n",
    "                ve.type_ = ele[\"type_\"].upper()\n",
    "                ve.layout.bounding_poly = ent.page_anchor.page_refs[0].bounding_poly\n",
    "                page.visual_elements.extend([ve])\n",
    "            doc.entities.extend(entities)\n",
    "\n",
    "        json_str = documentai.Document.to_json(\n",
    "            doc, including_default_value_fields=False\n",
    "        )\n",
    "        fn = fn.split(\".\")[-2] + \".json\"\n",
    "        file_name = f\"{output_folder}/{fn}\"\n",
    "        print(f\"\\t  Output gcs uri - gs://{output_bucket}/{file_name}\")\n",
    "        store_document_as_json(json_str, output_bucket, file_name)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21c9da6-5333-4b95-90dc-7f78425b597e",
   "metadata": {},
   "source": [
    "### 5.Output\n",
    "\n",
    "Sample image of which shows prediction results DocumentAI visual elements & entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6390a5e8-f8c8-49de-b832-05a43d7357d0",
   "metadata": {},
   "source": [
    "#### Visualization from DocumentAI UI\n",
    "<img src=\"./Images/DocAI_UI_Visualization.png\" width=800 height=400 ></img>\n",
    "#### Visualization from JSON File \n",
    "<img src=\"./Images/JSON_Visualization.png\" width=800 height=400 ></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76ccea7-07ef-4705-8108-35f860f19db7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
