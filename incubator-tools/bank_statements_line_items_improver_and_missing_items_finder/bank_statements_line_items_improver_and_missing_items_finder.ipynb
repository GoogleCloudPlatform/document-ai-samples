{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6589fc93-39d1-4d10-be1f-e7eb33fe4087",
   "metadata": {},
   "source": [
    "# Bank Statements Line items improver and Missing items finder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf22bf7-4a47-4f3a-9eef-6f19348a5250",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361f188e-fe11-4a49-b7c8-080e0e69ce7a",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the DocAI Incubator Team. No guarantees of performance are implied. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1036937a-0221-48eb-862e-3fa0b8e646a8",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "The objective of the tool is to find the missing child items and group the correct child items into parent line items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115a4e82-5e83-468a-b0e5-097ca14f15d5",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "* Vertex AI Notebook Or Colab (If using Colab, use authentication)\n",
    "* Storage Bucket for storing input and output json files\n",
    "* Permission For Google Storage and Vertex AI Notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe81de40-5c62-4c0b-adea-937f957b1a6e",
   "metadata": {},
   "source": [
    "## Step by Step procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142123d3-37b1-4aa8-841c-40c3bd52d70c",
   "metadata": {},
   "source": [
    "### 1. Importing Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0643c5f9-29fe-4252-9e6c-e2afc8c2f2b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pandas numpy google-cloud-storage google-cloud-documentai==2.16.0\n",
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7588c13e-0e09-4a76-8c21-85a68ee262c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from tqdm import tqdm\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from pathlib import Path\n",
    "from google.cloud import storage\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Any, Tuple\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7c8c4c-68b8-413c-b4bc-c66f044d3b7a",
   "metadata": {},
   "source": [
    "### 2. Input and Output Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eb2160-f5b4-42da-acdb-1ba4babc2ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the raw parsed JSON files. The path must end with a forward slash ('/').\n",
    "Gcs_input_path = \"gs://xxxxx/xxxxxxxxxxxx/xx/\"\n",
    "# Your Google Cloud project ID.\n",
    "project_id = \"xxx-xxxx-xxxx\"\n",
    "# Path for saving the processed output files. Do not include a trailing forward slash ('/').\n",
    "Gcs_output_path = \"gs://xxxxx/xxxxxxxxxxxx/xx\"\n",
    "parent_type = \"table_item\"\n",
    "Missing_items_flag = \"True\"  # case sensitive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0d4909-e00c-4704-a43b-6534f7403872",
   "metadata": {},
   "source": [
    "* ``Gcs_input_path ``: GCS Input Path. It should contain DocAI processed output json files. \n",
    "* ``Gcs_output_path ``: GCS Output Path. The updated jsons will be saved in output path. \n",
    "* ``project_id`` : It should contains the project id of your current project.\n",
    "* ``parent_type`` : Specify the parent entity type like table_item, line_item \n",
    "* ``Missing_items_flag``:  \"True\" if we need to find the missing child items , missing items step will be skipped if this value is other than True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737d1c70-fef5-49e3-a266-695bf8076a54",
   "metadata": {},
   "source": [
    "### 3. Run the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afc3e35-e12a-40c6-9f81-10460a9a9421",
   "metadata": {},
   "source": [
    "### Note\n",
    "* While using the missing items code , if the line items are closer then `modify the get_token_data function by increasing or decreasing the x and y allowances`.\n",
    "* Human review is recomended after this tool usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c22bfdd-abdc-4d1c-8f7c-86164e7c4103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_page_bbox(entity: documentai.Document.Entity):\n",
    "    \"\"\"\n",
    "    Get the bounding box (bbox) coordinates of a page entity.\n",
    "\n",
    "    Args:\n",
    "    - entity : Document AI entity object.\n",
    "\n",
    "    Returns:\n",
    "    - List[float]: A list containing four float values representing the coordinates of the bounding box.\n",
    "      The format is [min_x, min_y, max_x, max_y].\n",
    "    \"\"\"\n",
    "    bound_poly = entity.page_anchor.page_refs\n",
    "    norm_ver = bound_poly[0].bounding_poly.normalized_vertices\n",
    "    x_values = [vertex.x for vertex in norm_ver]\n",
    "    y_values = [vertex.y for vertex in norm_ver]\n",
    "    bbox = [min(x_values), min(y_values), max(x_values), max(y_values)]\n",
    "\n",
    "    return bbox\n",
    "\n",
    "\n",
    "def get_page_wise_entities(json_dict: documentai.Document):\n",
    "    \"\"\"\n",
    "    Extracts entities from a loaded JSON file and organizes them based on the page they belong to.\n",
    "\n",
    "    Args:\n",
    "    - json_dict : documentai object.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[int, List[Dict[str, Any]]]: A dictionary where keys represent page numbers and values are lists\n",
    "      of entities belonging to that page.\n",
    "    \"\"\"\n",
    "\n",
    "    entities_page = {}\n",
    "    for entity in json_dict.entities:\n",
    "        page = entity.page_anchor.page_refs[0].page\n",
    "        if page in entities_page.keys():\n",
    "            entities_page[page].append(entity)\n",
    "        else:\n",
    "            entities_page[page] = [entity]\n",
    "\n",
    "    return entities_page\n",
    "\n",
    "\n",
    "def get_line_items_schema(line_items: Any):\n",
    "    \"\"\"\n",
    "    Generate a schema for line items along with their corresponding positions on the page.\n",
    "\n",
    "    Args:\n",
    "    - line_items (List[Dict[str, Any]]): A list of line items extracted from the JSON entities.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[Dict[str, int], Dict[str, List[List[float]]], Dict[str, List[List[float]]]]: A tuple containing:\n",
    "      1. A dictionary representing the schema for line items with the count of each type.\n",
    "      2. A dictionary representing the x positions of line items for each type.\n",
    "      3. A dictionary representing the y positions of line items for each type.\n",
    "    \"\"\"\n",
    "    # line_items = [entity for entity in json_dict.entities if entity.properties]\n",
    "    line_item_schema = []\n",
    "    schema_xy = []\n",
    "    for line_item in line_items:\n",
    "        temp_schema = {}\n",
    "        temp_xy = {}\n",
    "        for item in line_item.properties:\n",
    "            temp_schema[item.type] = temp_schema.get(item.type, 0) + 1\n",
    "            bbox = get_page_bbox(item)\n",
    "            if item.type in temp_xy:\n",
    "                temp_xy[item.type].append(bbox)\n",
    "            else:\n",
    "                temp_xy[item.type] = [bbox]\n",
    "\n",
    "            line_item_schema.append(temp_schema)\n",
    "            schema_xy.append(temp_xy)\n",
    "\n",
    "    flat_list = [\n",
    "        (key, value) for item in line_item_schema for key, value in item.items()\n",
    "    ]\n",
    "\n",
    "    counter = Counter(dict(flat_list))\n",
    "    temp_schema_dict = dict(counter)\n",
    "    consolidated_positions_ent = {}\n",
    "    x = []\n",
    "    for k3, v3 in temp_schema_dict.items():\n",
    "        for l3 in schema_xy:\n",
    "            for k4, v4 in l3.items():\n",
    "                if k3 == k4:\n",
    "                    for x12 in v4:\n",
    "                        if k3 in consolidated_positions_ent.keys():\n",
    "                            consolidated_positions_ent[k3].append(x12)\n",
    "                        else:\n",
    "                            consolidated_positions_ent[k3] = [x12]\n",
    "    final_ent_x12 = {}\n",
    "    final_ent_y12 = {}\n",
    "    for ent_typ, va1 in consolidated_positions_ent.items():\n",
    "        sorted_data = sorted(va1, key=lambda x: x[0])\n",
    "        groups = []\n",
    "        current_group = [sorted_data[0]]\n",
    "        difference_threshold = 0.02\n",
    "        for i in range(1, len(sorted_data)):\n",
    "            if abs(sorted_data[i][0] - current_group[-1][0]) <= difference_threshold:\n",
    "                current_group.append(sorted_data[i])\n",
    "            else:\n",
    "                groups.append(current_group)\n",
    "                current_group = [sorted_data[i]]\n",
    "        groups.append(current_group)\n",
    "        for va3 in groups:\n",
    "            if len(va3) >= 1:\n",
    "                if ent_typ in final_ent_x12.keys():\n",
    "                    final_ent_x12[ent_typ].append(\n",
    "                        [min(item[0] for item in va3), max(item[2] for item in va3)]\n",
    "                    )\n",
    "                    final_ent_y12[ent_typ].append(\n",
    "                        [min(item[1] for item in va3), max(item[3] for item in va3)]\n",
    "                    )\n",
    "                else:\n",
    "                    final_ent_x12[ent_typ] = [\n",
    "                        [min(item[0] for item in va3), max(item[2] for item in va3)]\n",
    "                    ]\n",
    "                    final_ent_y12[ent_typ] = [\n",
    "                        [min(item[1] for item in va3), max(item[3] for item in va3)]\n",
    "                    ]\n",
    "\n",
    "    return temp_schema_dict, final_ent_x12, final_ent_y12\n",
    "\n",
    "\n",
    "def get_token_xy(token: Any) -> Tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Extracts the normalized bounding box coordinates (min_x, min_y, max_x, max_y) of a token.\n",
    "\n",
    "    Args:\n",
    "    - token (Any): A token object with layout information.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[float, float, float, float]: The normalized bounding box coordinates.\n",
    "\n",
    "    \"\"\"\n",
    "    vertices = token.layout.bounding_poly.normalized_vertices\n",
    "    minx_token, miny_token = min(point.x for point in vertices), min(\n",
    "        point.y for point in vertices\n",
    "    )\n",
    "    maxx_token, maxy_token = max(point.x for point in vertices), max(\n",
    "        point.y for point in vertices\n",
    "    )\n",
    "\n",
    "    return minx_token, miny_token, maxx_token, maxy_token\n",
    "\n",
    "\n",
    "def get_token_data(\n",
    "    json_dict: documentai.Document,\n",
    "    min_x: float,\n",
    "    max_x: float,\n",
    "    min_y: float,\n",
    "    max_y: float,\n",
    "    page_num: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Extracts token data from the JSON dictionary based on provided bounding box coordinates and page number.\n",
    "\n",
    "    Args:\n",
    "    - json_dict (Dict[str, Any]): The JSON dictionary containing token data.\n",
    "    - min_x (float): Minimum x-coordinate of the bounding box.\n",
    "    - max_x (float): Maximum x-coordinate of the bounding box.\n",
    "    - min_y (float): Minimum y-coordinate of the bounding box.\n",
    "    - max_y (float): Maximum y-coordinate of the bounding box.\n",
    "    - page_num (int): Page number.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[str, List[Dict[str, Any]], List[Dict[str, float]]]: A tuple containing:\n",
    "        1. The extracted text from the tokens.\n",
    "        2. A list of dictionaries containing text anchor data for each token.\n",
    "        3. A list of dictionaries containing page anchor data.\n",
    "    \"\"\"\n",
    "    text_anc_temp = []\n",
    "    text_anc = []\n",
    "    page_anc_temp = {\"x\": [], \"y\": []}\n",
    "    y_allowance = (\n",
    "        0.01  # edit this if the line items are closer and your not getitng desir\n",
    "    )\n",
    "    x_allowance = 0.02\n",
    "    for page in json_dict.pages:\n",
    "        if page_num == page.page_number - 1:\n",
    "            for token in page.tokens:\n",
    "                minx_token, miny_token, maxx_token, maxy_token = get_token_xy(token)\n",
    "                if (\n",
    "                    min_y <= miny_token + y_allowance\n",
    "                    and max_y >= maxy_token - y_allowance\n",
    "                    and min_x <= minx_token + x_allowance\n",
    "                    and max_x >= maxx_token - x_allowance\n",
    "                ):\n",
    "                    temp_anc = token.layout.text_anchor.text_segments[0]\n",
    "                    text_anc.append(temp_anc)\n",
    "                    page_anc_temp[\"x\"].extend([minx_token, maxx_token])\n",
    "                    page_anc_temp[\"y\"].extend([miny_token, maxy_token])\n",
    "                    for seg in token.layout.text_anchor.text_segments:\n",
    "                        text_anc_temp.append([seg.start_index, seg.end_index])\n",
    "    if page_anc_temp != {\"x\": [], \"y\": []}:\n",
    "        page_anc = [\n",
    "            {\"x\": min(page_anc_temp[\"x\"]), \"y\": min(page_anc_temp[\"y\"])},\n",
    "            {\"x\": max(page_anc_temp[\"x\"]), \"y\": min(page_anc_temp[\"y\"])},\n",
    "            {\"x\": min(page_anc_temp[\"x\"]), \"y\": max(page_anc_temp[\"y\"])},\n",
    "            {\"x\": max(page_anc_temp[\"x\"]), \"y\": max(page_anc_temp[\"y\"])},\n",
    "        ]\n",
    "    if text_anc_temp != []:\n",
    "        sorted_data = sorted(text_anc_temp, key=lambda x: x[0])\n",
    "        mention_text = \"\"\n",
    "        for start_index, end_index in sorted_data:\n",
    "            mention_text += json_dict.text[start_index:end_index]\n",
    "\n",
    "        return mention_text, text_anc, page_anc\n",
    "\n",
    "\n",
    "def get_missing_fields(\n",
    "    json_dict,\n",
    "    line_items,\n",
    "    temp_schema_dict,\n",
    "    final_ent_x12,\n",
    "    ent_x_region,\n",
    "    line_item_y_region,\n",
    "):\n",
    "    \"\"\"\n",
    "    Identifies missing fields in line items and fills them based on provided criteria.\n",
    "\n",
    "    Args:\n",
    "    - json_dict (Dict[str, Any]): The JSON dictionary containing relevant data.\n",
    "    - line_items (List[Any]): The list of line items to be processed.\n",
    "    - temp_schema_dict (Dict[str, int]): The schema representing the count of each type.\n",
    "    - final_ent_x12 (Dict[str, List[List[float]]]): The x positions of line items for each type.\n",
    "    - ent_x_region (Dict[str, List[float]]): The x positions of entities for each type.\n",
    "    - line_item_y_region (List[float]): The y positions of line items.\n",
    "\n",
    "    Returns:\n",
    "    - List[Any]: The updated list of line items.\n",
    "    \"\"\"\n",
    "    for line_item in line_items:\n",
    "        import copy\n",
    "\n",
    "        page_num = 0\n",
    "        temp_types = []\n",
    "        mis_type = []\n",
    "        text_anc_line = []\n",
    "        text_anc_mt = []\n",
    "        page_anc_line = {\"x\": [], \"y\": []}\n",
    "        deep_copy_temp_schema = copy.deepcopy(temp_schema_dict)\n",
    "\n",
    "        for child in line_item.properties:\n",
    "            temp_types.append(child.type)\n",
    "            for seg in child.text_anchor.text_segments:\n",
    "                text_anc_line.append(seg)\n",
    "                text_anc_mt.append([seg.start_index, seg.end_index])\n",
    "            for anc4 in child.page_anchor.page_refs:\n",
    "                # page_n=anc4.page\n",
    "                for xy2 in anc4.bounding_poly.normalized_vertices:\n",
    "                    page_anc_line[\"x\"].append(xy2.x)\n",
    "                    page_anc_line[\"y\"].append(xy2.y)\n",
    "        # only for bank statement parser output\n",
    "        for k2 in temp_types:\n",
    "            if \"deposit\" in k2:\n",
    "                modified_schema = {\n",
    "                    key: value\n",
    "                    for key, value in deep_copy_temp_schema.items()\n",
    "                    if \"withdrawal\" not in key\n",
    "                }\n",
    "                break\n",
    "            elif \"withdrawal\" in k2:\n",
    "                modified_schema = {\n",
    "                    key: value\n",
    "                    for key, value in deep_copy_temp_schema.items()\n",
    "                    if \"deposit\" not in key\n",
    "                }\n",
    "                break\n",
    "            if \"modified_schema\" not in locals():\n",
    "                modified_schema = deep_copy_temp_schema\n",
    "\n",
    "        for t1, v1 in modified_schema.items():\n",
    "            if t1 in temp_types:\n",
    "                pass\n",
    "            else:\n",
    "                mis_type.append(t1)\n",
    "\n",
    "        if len(mis_type) > 0:\n",
    "            for typ in mis_type:\n",
    "                for ent_pos in line_item.page_anchor.page_refs:\n",
    "                    page_num = ent_pos.page\n",
    "                    try:\n",
    "                        min_x = ent_x_region[typ][0]\n",
    "                    except:\n",
    "                        min_x = min(\n",
    "                            ver.x for ver in ent_pos.bounding_poly.normalized_vertices\n",
    "                        )\n",
    "                    min_y = min(\n",
    "                        ver.y for ver in ent_pos.bounding_poly.normalized_vertices\n",
    "                    )\n",
    "                    try:\n",
    "                        max_x = ent_x_region[typ][1] - 0.02\n",
    "                    except:\n",
    "                        max_x = (\n",
    "                            max(\n",
    "                                ver.x\n",
    "                                for ver in ent_pos.bounding_poly.normalized_vertices\n",
    "                            )\n",
    "                            - 0.02\n",
    "                        )\n",
    "\n",
    "                    if \"description\" in typ:\n",
    "                        try:\n",
    "                            closest_index_y = min(\n",
    "                                range(len(line_item_y_region)),\n",
    "                                key=lambda i: abs(line_item_y_region[i] - min_y),\n",
    "                            )\n",
    "\n",
    "                            max_y = line_item_y_region[closest_index_y + 1]\n",
    "                        except:\n",
    "                            pass\n",
    "                    else:\n",
    "                        max_y = max(\n",
    "                            ver.y for ver in ent_pos.bounding_poly.normalized_vertices\n",
    "                        )\n",
    "\n",
    "                    try:\n",
    "                        mention_text, text_anc, page_anc = get_token_data(\n",
    "                            json_dict, min_x, max_x, min_y, max_y, page_num\n",
    "                        )\n",
    "                        for an3 in text_anc:\n",
    "                            text_anc_line.append(an3)\n",
    "                            text_anc_mt.append([an3.start_index, an3.end_index])\n",
    "                        for xy3 in page_anc:\n",
    "                            page_anc_line[\"x\"].append(xy3[\"x\"])\n",
    "                            page_anc_line[\"y\"].append(xy3[\"y\"])\n",
    "                        entity_new = {\n",
    "                            \"mention_text\": mention_text,\n",
    "                            \"page_anchor\": {\n",
    "                                \"page_refs\": [\n",
    "                                    {\n",
    "                                        \"bounding_poly\": {\n",
    "                                            \"normalized_vertices\": page_anc\n",
    "                                        },\n",
    "                                        \"page\": str(page_num),\n",
    "                                    }\n",
    "                                ]\n",
    "                            },\n",
    "                            \"text_anchor\": {\n",
    "                                \"content\": mention_text,\n",
    "                                \"text_segments\": text_anc,\n",
    "                            },\n",
    "                            \"type\": typ,\n",
    "                        }\n",
    "                        line_item.properties.append(entity_new)\n",
    "                        # print(typ)\n",
    "                        # print(mention_text)\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "        page_anc_final = [\n",
    "            {\"x\": min(page_anc_line[\"x\"]), \"y\": min(page_anc_line[\"y\"])},\n",
    "            {\"x\": max(page_anc_line[\"x\"]), \"y\": min(page_anc_line[\"y\"])},\n",
    "            {\"x\": min(page_anc_line[\"x\"]), \"y\": max(page_anc_line[\"y\"])},\n",
    "            {\"x\": max(page_anc_line[\"x\"]), \"y\": max(page_anc_line[\"y\"])},\n",
    "        ]\n",
    "        sorted_data_1 = sorted(text_anc_mt, key=lambda x: x[0])\n",
    "        mention_text_final = \"\"\n",
    "        for start_index_1, end_index_1 in sorted_data_1:\n",
    "            mention_text_final = (\n",
    "                mention_text_final + \" \" + json_dict.text[start_index_1:end_index_1]\n",
    "            )\n",
    "\n",
    "        line_item.mention_text = mention_text_final\n",
    "        for anc6 in line_item.page_anchor.page_refs:\n",
    "            anc6.bounding_poly.normalized_vertices = page_anc_final\n",
    "        line_item.text_anchor.text_segments = text_anc_line\n",
    "\n",
    "    new_ent = []\n",
    "\n",
    "    for l1 in line_items:\n",
    "        new_ent.append(l1)\n",
    "\n",
    "    return new_ent\n",
    "\n",
    "\n",
    "def get_schema_with_bbox(line_items: List[Dict[str, Any]]):\n",
    "    \"\"\"\n",
    "    Generates a schema for line items along with their bounding box coordinates.\n",
    "\n",
    "    Args:\n",
    "    - line_items (List[Dict[str, Any]]): A list of line items.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[List[Dict[str, int]], List[Dict[str, List[List[float]]]]]: A tuple containing:\n",
    "        1. A list of dictionaries representing the schema for line items with the count of each type.\n",
    "        2. A list of dictionaries representing the bounding box coordinates of line items for each type.\n",
    "    \"\"\"\n",
    "    line_item_schema = []\n",
    "    schema_xy = []\n",
    "    for line_item in line_items:\n",
    "        temp_schema = {}\n",
    "        temp_xy = {}\n",
    "        for item in line_item.properties:\n",
    "            temp_schema[item.type] = temp_schema.get(item.type, 0) + 1\n",
    "            bbox = get_page_bbox(item)\n",
    "            if item.type in temp_xy:\n",
    "                temp_xy[item.type].append(bbox)\n",
    "            else:\n",
    "                temp_xy[item.type] = [bbox]\n",
    "\n",
    "        line_item_schema.append(temp_schema)\n",
    "        schema_xy.append(temp_xy)\n",
    "\n",
    "    return line_item_schema, schema_xy\n",
    "\n",
    "\n",
    "def get_anchor_entity(\n",
    "    schema_xy: List[Dict[str, List[List[float]]]],\n",
    "    line_item_schema: List[Dict[str, int]],\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Identifies the anchor entity among the entities based on certain criteria.\n",
    "\n",
    "    Args:\n",
    "    - schema_xy (List[Dict[str, List[List[float]]]]): A list of dictionaries representing the bounding box coordinates\n",
    "      of line items for each type.\n",
    "    - line_item_schema (List[Dict[str, int]]): A list of dictionaries representing the schema for line items with\n",
    "      the count of each type.\n",
    "\n",
    "    Returns:\n",
    "    - str: The anchor entity.\n",
    "    \"\"\"\n",
    "    ent_y2 = {}\n",
    "    for sc1 in schema_xy:\n",
    "        for e2, bbox in sc1.items():\n",
    "            if len(bbox) == 1:\n",
    "                for b2 in bbox:\n",
    "                    ent_y2.setdefault(e2, []).extend([b2[1], b2[3]])\n",
    "    # get the min and max y of entities\n",
    "    entity_min_max_y = {}\n",
    "    for en3, val3 in ent_y2.items():\n",
    "        min_y_3 = min(val3)\n",
    "        max_y_3 = max(val3)\n",
    "        entity_min_max_y[en3] = [min_y_3, max_y_3]\n",
    "\n",
    "    # counting times the entity appeared uniquely in all the line items\n",
    "    entity_count = {}\n",
    "    for entry in line_item_schema:\n",
    "        for entity, value in entry.items():\n",
    "            if value == 1:\n",
    "                if entity in entity_count:\n",
    "                    entity_count[entity] += 1\n",
    "                else:\n",
    "                    entity_count[entity] = 1\n",
    "\n",
    "    value_counts = {}\n",
    "    for value in entity_count.values():\n",
    "        value_counts[value] = value_counts.get(value, 0) + 1\n",
    "    # Find the maximum value\n",
    "    max_value = max(value_counts.values())\n",
    "\n",
    "    # Find keys with the maximum value\n",
    "    keys_with_max_value = [\n",
    "        key for key, value in value_counts.items() if value == max_value\n",
    "    ]\n",
    "\n",
    "    # Find the key with the maximum value (in case of ties, choose the maximum key)\n",
    "    max_key = max(keys_with_max_value)\n",
    "\n",
    "    repeated_key = [key for key, value in entity_count.items() if value == max_key]\n",
    "\n",
    "    filtered_entities = {\n",
    "        key: entity_min_max_y[key] for key in repeated_key if key in entity_min_max_y\n",
    "    }\n",
    "\n",
    "    if len(filtered_entities) > 1:\n",
    "        anchor_entity = min(filtered_entities, key=lambda k: filtered_entities[k][0])\n",
    "    else:\n",
    "        anchor_entity = list(filtered_entities.keys())[0]\n",
    "\n",
    "    return anchor_entity\n",
    "\n",
    "\n",
    "def entity_region_x(\n",
    "    schema_xy: List[Dict[str, List[List[float]]]]\n",
    ") -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Calculates the x-regions for different types of entities based on their bounding boxes.\n",
    "\n",
    "    Args:\n",
    "    - schema_xy (List[Dict[str, List[List[float]]]]): A list of dictionaries representing the bounding box coordinates\n",
    "      of line items for each type.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, List[float]]: A dictionary containing the x-regions for different types of entities.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_margin(min_y_bin: List[float], min_values: str = \"YES\") -> float:\n",
    "        \"\"\"\n",
    "        Computes the margin based on the minimum y-bin values.\n",
    "\n",
    "        Args:\n",
    "        - min_y_bin (List[float]): List of minimum y-bin values.\n",
    "        - min_values (str): A flag indicating whether to compute the minimum value.\n",
    "\n",
    "        Returns:\n",
    "        - float: The computed margin.\n",
    "        \"\"\"\n",
    "        # Sort the list in ascending order\n",
    "        min_y_bin.sort()\n",
    "\n",
    "        bins = []\n",
    "        current_bin = [min_y_bin[0]]\n",
    "        # Iterate through the values to create bins\n",
    "        for i in range(1, len(min_y_bin)):\n",
    "            if min_y_bin[i] - current_bin[-1] < 0.05:\n",
    "                current_bin.append(min_y_bin[i])\n",
    "            else:\n",
    "                bins.append(current_bin.copy())\n",
    "                current_bin = [min_y_bin[i]]\n",
    "\n",
    "        # Add the last bin\n",
    "        bins.append(current_bin)\n",
    "        final_bins = []\n",
    "        for bin_1 in bins:\n",
    "            if len(bin_1) >= 2:\n",
    "                final_bins.append(bin_1)\n",
    "        if final_bins == []:\n",
    "            for bin_1 in bins:\n",
    "                if len(bin_1) >= 1:\n",
    "                    final_bins.append(bin_1)\n",
    "        if min_values == \"YES\":\n",
    "            return min(min(inner_list) for inner_list in final_bins)\n",
    "        else:\n",
    "            return max(max(inner_list) for inner_list in final_bins)\n",
    "\n",
    "    ent_full_boundries = {}\n",
    "    for line_1 in schema_xy:\n",
    "        for typ_1, bbox_1 in line_1.items():\n",
    "            if len(bbox_1) == 1:\n",
    "                if typ_1 in ent_full_boundries.keys():\n",
    "                    ent_full_boundries[typ_1].append(bbox_1[0])\n",
    "                else:\n",
    "                    ent_full_boundries[typ_1] = bbox_1\n",
    "    ent_margins = {}\n",
    "    for ent_typ_1, values_1 in ent_full_boundries.items():\n",
    "        min_x_bin = []\n",
    "        min_y_bin = []\n",
    "        max_x_bin = []\n",
    "        max_y_bin = []\n",
    "        min_check = len(values_1)\n",
    "        for bbox in values_1:\n",
    "            min_x_bin.append(bbox[0])\n",
    "            min_y_bin.append(bbox[1])\n",
    "            max_x_bin.append(bbox[2])\n",
    "            max_y_bin.append(bbox[3])\n",
    "        min_x = get_margin(min_x_bin, min_values=\"YES\")\n",
    "        min_y = get_margin(min_y_bin, min_values=\"YES\")\n",
    "        max_x = get_margin(max_x_bin, min_values=\"NO\")\n",
    "        max_y = get_margin(max_y_bin, min_values=\"NO\")\n",
    "\n",
    "        ent_margins[ent_typ_1] = [min_x, min_y, max_x, max_y]\n",
    "\n",
    "    ent_margin_withdrawal = {}\n",
    "    ent_margin_deposit = {}\n",
    "    for ent_3, bbox_3 in ent_margins.items():\n",
    "        if \"withdrawal\" in ent_3:\n",
    "            ent_margin_withdrawal[ent_3] = bbox_3\n",
    "        elif \"deposit\" in ent_3:\n",
    "            ent_margin_deposit[ent_3] = bbox_3\n",
    "        else:\n",
    "            ent_margin_withdrawal[ent_3] = bbox_3\n",
    "            ent_margin_deposit[ent_3] = bbox_3\n",
    "\n",
    "    def get_x_region(\n",
    "        ent_margin_withdrawal: Dict[str, List[float]]\n",
    "    ) -> Dict[str, List[float]]:\n",
    "        \"\"\"\n",
    "        Calculates the x-regions for withdrawal entities.\n",
    "\n",
    "        Args:\n",
    "        - ent_margin_withdrawal (Dict[str, List[float]]): A dictionary containing the margins for withdrawal entities.\n",
    "\n",
    "        Returns:\n",
    "        - Dict[str, List[float]]: A dictionary containing the x-regions for withdrawal entities.\n",
    "        \"\"\"\n",
    "        sorted_ent_margin_withdrawal = sorted_data = dict(\n",
    "            sorted(ent_margin_withdrawal.items(), key=lambda x: x[1][0])\n",
    "        )\n",
    "        ent_x_regions = {}\n",
    "        keys_sorted = list(sorted_ent_margin_withdrawal.keys())\n",
    "        for n_1 in range(len(keys_sorted)):\n",
    "            if n_1 < len(keys_sorted) - 1:\n",
    "                if (\n",
    "                    sorted_ent_margin_withdrawal[keys_sorted[n_1]][2]\n",
    "                    > sorted_ent_margin_withdrawal[keys_sorted[n_1 + 1]][0]\n",
    "                ):\n",
    "                    ent_x_regions[keys_sorted[n_1]] = [\n",
    "                        sorted_ent_margin_withdrawal[keys_sorted[n_1]][0],\n",
    "                        sorted_ent_margin_withdrawal[keys_sorted[n_1]][2],\n",
    "                    ]\n",
    "                else:\n",
    "                    ent_x_regions[keys_sorted[n_1]] = [\n",
    "                        sorted_ent_margin_withdrawal[keys_sorted[n_1]][0],\n",
    "                        sorted_ent_margin_withdrawal[keys_sorted[n_1 + 1]][0],\n",
    "                    ]\n",
    "            else:\n",
    "                ent_x_regions[keys_sorted[n_1]] = [\n",
    "                    sorted_ent_margin_withdrawal[keys_sorted[n_1]][0],\n",
    "                    sorted_ent_margin_withdrawal[keys_sorted[n_1]][2],\n",
    "                ]\n",
    "\n",
    "        return ent_x_regions\n",
    "\n",
    "    withdrawal_x_region = get_x_region(ent_margin_withdrawal)\n",
    "    deposit_x_region = get_x_region(ent_margin_deposit)\n",
    "    ent_x_region = {**deposit_x_region, **withdrawal_x_region}\n",
    "\n",
    "    return ent_x_region\n",
    "\n",
    "\n",
    "def get_line_item_y_region(line_items: List[Dict[str, Any]]) -> List[float]:\n",
    "    \"\"\"\n",
    "    Computes the y-regions for line items based on their bounding box coordinates.\n",
    "\n",
    "    Args:\n",
    "    - line_items (List[Dict[str, Any]]): A list of line items.\n",
    "\n",
    "    Returns:\n",
    "    - List[float]: A list containing the y-regions for line items.\n",
    "    \"\"\"\n",
    "    line_item_y_region = []\n",
    "    max_y_line_item = []\n",
    "    for tab_item in line_items:\n",
    "        y_1 = []\n",
    "        for line_details in tab_item.page_anchor.page_refs:\n",
    "            page = line_details.page\n",
    "            for xy_1 in line_details.bounding_poly.normalized_vertices:\n",
    "                y_1.append(xy_1.y)\n",
    "        line_item_y_region.append(min(y_1))\n",
    "        max_y_line_item.append(max(y_1))\n",
    "\n",
    "    line_item_y_region.append(max(max_y_line_item))\n",
    "    sorted_line_item_y_region = sorted(line_item_y_region)\n",
    "\n",
    "    return sorted_line_item_y_region\n",
    "\n",
    "\n",
    "def get_line_item_y_region_by_anchor(\n",
    "    line_items: List[Dict[str, Any]], anchor_entity: str\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Computes the y-regions for line items based on their bounding box coordinates related to the anchor entity.\n",
    "\n",
    "    Args:\n",
    "    - line_items (List[Dict[str, Any]]): A list of line items.\n",
    "    - anchor_entity (str): The anchor entity.\n",
    "\n",
    "    Returns:\n",
    "    - List[float]: A list containing the y-regions for line items related to the anchor entity.\n",
    "    \"\"\"\n",
    "    y_max_anchor = []\n",
    "    y_min_anchor = []\n",
    "\n",
    "    for tab1_item in line_items:\n",
    "        for child in tab1_item.properties:\n",
    "            if child.type_ == anchor_entity:\n",
    "                y_2 = []\n",
    "                for child_details in child.page_anchor.page_refs:\n",
    "                    for xy_2 in child_details.bounding_poly.normalized_vertices:\n",
    "                        y_2.append(xy_2.y)\n",
    "                y_max_anchor.append(max(y_2))\n",
    "                y_min_anchor.append(min(y_2))\n",
    "    sorted_y_max_anchor = sorted(y_max_anchor)\n",
    "    sorted_y_min_anchor = sorted(y_min_anchor)\n",
    "    sorted_y_max_anchor.append(sorted_y_min_anchor[0])\n",
    "    sorted_y_anchor = sorted(sorted_y_max_anchor)\n",
    "\n",
    "    return sorted_y_anchor\n",
    "\n",
    "\n",
    "def get_line_item_region(\n",
    "    schema_xy: List[Dict[str, List[List[float]]]],\n",
    "    anchor_entity: str,\n",
    "    line_items: List[Dict[str, Any]],\n",
    ") -> Tuple[List[List[float]], List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Computes the regions for line items based on the schema and anchor entity.\n",
    "\n",
    "    Args:\n",
    "    - schema_xy (List[Dict[str, List[List[float]]]]): A list of dictionaries representing the bounding box coordinates\n",
    "      of line items for each type.\n",
    "    - anchor_entity (str): The anchor entity.\n",
    "    - line_items (List[Dict[str, Any]]): A list of line items.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[List[List[float]], List[Dict[str, Any]]]: A tuple containing the regions for line items and a list of child items.\n",
    "    \"\"\"\n",
    "    region_y = []\n",
    "    for reg in schema_xy:\n",
    "        for e4, v4 in reg.items():\n",
    "            if \"date\" in e4:  # if e4==anchor_entity:\n",
    "                region_y.append(v4[0][1])\n",
    "    # Get line item total region and getting all child items into single list\n",
    "    bbox_line_y = []\n",
    "    bbox_line_x = []\n",
    "    child_items = []\n",
    "    for line_item in line_items:\n",
    "        bbox_line = get_page_bbox(line_item)\n",
    "        bbox_line_y.extend([bbox_line[1], bbox_line[3]])\n",
    "        bbox_line_x.extend([bbox_line[0], bbox_line[2]])\n",
    "        for child in line_item.properties:\n",
    "            child_items.append(child)\n",
    "    line_item_start_y = min(bbox_line_y)\n",
    "    line_item_end_y = max(bbox_line_y)\n",
    "\n",
    "    # getting Boundry for each line item\n",
    "    line_item_region = []\n",
    "    region_y = sorted(region_y)\n",
    "    for r1 in range(len(region_y)):\n",
    "        if r1 == 0:\n",
    "            line_item_region.append([line_item_start_y, region_y[r1 + 1]])\n",
    "        elif r1 == len(region_y) - 1:\n",
    "            line_item_region.append([region_y[r1], line_item_end_y])\n",
    "        else:\n",
    "            line_item_region.append([region_y[r1], region_y[r1 + 1]])\n",
    "\n",
    "    return line_item_region, child_items\n",
    "\n",
    "\n",
    "def group_line_items(parent_type, child_items, page, line_item_region, json_dict):\n",
    "    \"\"\"\n",
    "    Groups child items into line items based on the provided parent type and line item regions.\n",
    "\n",
    "    Args:\n",
    "    - parent_type (str): The type of the parent line item.\n",
    "    - child_items (List[Dict[str, Any]]): A list of child items.\n",
    "    - page (str): The page number.\n",
    "    - line_item_region (List[List[float]]): A list of line item regions.\n",
    "    - json_dict :documentai.Document\n",
    "\n",
    "    Returns:\n",
    "    - List[Dict[str, Any]]: A list containing the grouped line items.\n",
    "    \"\"\"\n",
    "    grouped_line_items = []\n",
    "\n",
    "    for boundry in line_item_region:\n",
    "        line_item_temp = {\n",
    "            \"mention_text\": \"\",\n",
    "            \"page_anchor\": {\n",
    "                \"page_refs\": [\n",
    "                    {\"bounding_poly\": {\"normalized_vertices\": []}, \"page\": page}\n",
    "                ]\n",
    "            },\n",
    "            \"properties\": [],\n",
    "            \"text_anchor\": {\"text_segments\": []},\n",
    "            \"type\": parent_type,\n",
    "        }\n",
    "        text_anc_temp = []\n",
    "        page_anc_temp = {\"x\": [], \"y\": []}\n",
    "        mt_temp = \"\"\n",
    "        for child_1 in child_items:\n",
    "            bbox_temp = get_page_bbox(child_1)\n",
    "            if (\n",
    "                bbox_temp[1] >= boundry[0] - 0.005\n",
    "                and bbox_temp[3] <= boundry[1] + 0.005\n",
    "            ):\n",
    "                line_item_temp[\"properties\"].append(child_1)\n",
    "                page_anc_temp[\"x\"].extend([bbox_temp[0], bbox_temp[2]])\n",
    "                page_anc_temp[\"y\"].extend([bbox_temp[1], bbox_temp[3]])\n",
    "                seg_temp = child_1.text_anchor.text_segments\n",
    "                for seg in seg_temp:\n",
    "                    text_anc_temp.append(\n",
    "                        {\n",
    "                            \"start_index\": str(seg.start_index),\n",
    "                            \"end_index\": str(seg.end_index),\n",
    "                        }\n",
    "                    )\n",
    "        sorted_data = sorted(text_anc_temp, key=lambda x: int(x[\"end_index\"]))\n",
    "        for sort_text in sorted_data:\n",
    "            mt_temp = (\n",
    "                mt_temp\n",
    "                + \" \"\n",
    "                + json_dict.text[\n",
    "                    int(sort_text[\"start_index\"]) : int(sort_text[\"end_index\"])\n",
    "                ]\n",
    "            )\n",
    "        line_item_temp[\"text_anchor\"][\"text_segments\"] = sorted_data\n",
    "        line_item_temp[\"mention_text\"] = mt_temp\n",
    "        line_item_temp[\"page_anchor\"][\"page_refs\"][0][\"bounding_poly\"][\n",
    "            \"normalized_vertices\"\n",
    "        ] = [\n",
    "            {\"x\": min(page_anc_temp[\"x\"]), \"y\": min(page_anc_temp[\"y\"])},\n",
    "            {\"x\": max(page_anc_temp[\"x\"]), \"y\": min(page_anc_temp[\"y\"])},\n",
    "            {\"x\": max(page_anc_temp[\"x\"]), \"y\": max(page_anc_temp[\"y\"])},\n",
    "            {\"x\": min(page_anc_temp[\"x\"]), \"y\": max(page_anc_temp[\"y\"])},\n",
    "        ]\n",
    "        grouped_line_items.append(line_item_temp)\n",
    "\n",
    "    return grouped_line_items\n",
    "\n",
    "\n",
    "def get_updated_grouped_line_items(json_dict, parent_type):\n",
    "    \"\"\"\n",
    "    Groups child items into line items based on the specified parent type and line item regions.\n",
    "\n",
    "    Args:\n",
    "    - parent_type (str): The type of the parent line item.\n",
    "    - child_items (List[Dict[str, Any]]): A list of child items.\n",
    "    - page (str): The page number.\n",
    "    - line_item_region (List[List[float]]): A list of line item regions.\n",
    "    - json_dict :documentai.Document.\n",
    "\n",
    "    Returns:\n",
    "    documentai.Document.\n",
    "    \"\"\"\n",
    "    final_line_items = []\n",
    "    page_wise_ent = get_page_wise_entities(json_dict)\n",
    "    entities_ungrouped = []\n",
    "    other_entities = []\n",
    "    for page_num, ent in page_wise_ent.items():\n",
    "        try:\n",
    "            line_items = [\n",
    "                entity\n",
    "                for entity in ent\n",
    "                if entity.properties and entity.type == parent_type\n",
    "            ]\n",
    "            line_items_other = [\n",
    "                entity\n",
    "                for entity in ent\n",
    "                if entity.properties and entity.type != parent_type\n",
    "            ]\n",
    "            for other_ent in line_items_other:\n",
    "                other_entities.append(other_ent)\n",
    "            try:\n",
    "                line_item_schema, schema_xy = get_schema_with_bbox(line_items)\n",
    "                anchor_entity = get_anchor_entity(schema_xy, line_item_schema)\n",
    "                line_item_region, child_items = get_line_item_region(\n",
    "                    schema_xy, anchor_entity, line_items\n",
    "                )\n",
    "                grouped_line_items = group_line_items(\n",
    "                    parent_type, child_items, page_num, line_item_region, json_dict\n",
    "                )\n",
    "                for item in grouped_line_items:\n",
    "                    final_line_items.append(item)\n",
    "            except:\n",
    "                entities_ungrouped.append(line_items)\n",
    "                continue\n",
    "        except:\n",
    "            continue\n",
    "    final_entities = []\n",
    "    for en3 in json_dict.entities:\n",
    "        if en3.type != parent_type:\n",
    "            final_entities.append(en3)\n",
    "    for lin_it in final_line_items:\n",
    "        final_entities.append(lin_it)\n",
    "    if len(entities_ungrouped) > 0:\n",
    "        for item_1 in entities_ungrouped:\n",
    "            for item_2 in item_1:\n",
    "                final_entities.append(item_2)\n",
    "\n",
    "    json_dict.entities = final_entities\n",
    "\n",
    "    return json_dict\n",
    "\n",
    "\n",
    "def get_missing_data(\n",
    "    json_dict: documentai.Document, parent_type: str\n",
    ") -> documentai.Document:\n",
    "    \"\"\"\n",
    "    Processes the JSON dictionary to handle missing data by adding new entities.\n",
    "\n",
    "    Args:\n",
    "    - json_dict : documentai.Document.\n",
    "    - parent_type (str): The parent type of line items.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, Any]: The updated JSON dictionary with missing data handled.\n",
    "    \"\"\"\n",
    "    page_wise_ent = get_page_wise_entities(json_dict)\n",
    "    new_added_entities = []\n",
    "    other_entities = []\n",
    "    json_dict = get_updated_grouped_line_items(json_dict, parent_type)\n",
    "    for page_num, ent in page_wise_ent.items():\n",
    "        line_items = [\n",
    "            entity for entity in ent if entity.properties and entity.type == parent_type\n",
    "        ]\n",
    "        line_items_other = [\n",
    "            entity for entity in ent if entity.properties and entity.type != parent_type\n",
    "        ]\n",
    "        for other_ent in line_items_other:\n",
    "            other_entities.append(other_ent)\n",
    "        if len(line_items) > 2:\n",
    "            line_item_schema, schema_xy = get_schema_with_bbox(line_items)\n",
    "            ent_x_region = entity_region_x(schema_xy)\n",
    "            anchor_entity = get_anchor_entity(schema_xy, line_item_schema)\n",
    "            line_item_y_region = get_line_item_y_region(line_items)\n",
    "            temp_schema_dict, final_ent_x12, final_ent_y12 = get_line_items_schema(\n",
    "                line_items\n",
    "            )\n",
    "            new_ent = get_missing_fields(\n",
    "                json_dict,\n",
    "                line_items,\n",
    "                temp_schema_dict,\n",
    "                final_ent_x12,\n",
    "                ent_x_region,\n",
    "                line_item_y_region,\n",
    "            )\n",
    "            for item in new_ent:\n",
    "                new_added_entities.append(item)\n",
    "        else:\n",
    "            for lin_it1 in line_items:\n",
    "                other_entities.append(lin_it1)\n",
    "    final_entities = []\n",
    "    for en3 in json_dict.entities:\n",
    "        if en3.type != parent_type:\n",
    "            final_entities.append(en3)\n",
    "    for lin_it in new_added_entities:\n",
    "        final_entities.append(lin_it)\n",
    "    for lin_it2 in other_entities:\n",
    "        final_entities.append(lin_it2)\n",
    "    json_dict.entities = final_entities\n",
    "\n",
    "    return json_dict\n",
    "\n",
    "\n",
    "def main():\n",
    "    file_name_list, file_path_dict = file_names(Gcs_input_path)\n",
    "    for i in range(len(file_name_list)):\n",
    "        file_path = (\n",
    "            \"gs://\"\n",
    "            + Gcs_input_path.split(\"/\")[2]\n",
    "            + \"/\"\n",
    "            + file_path_dict[file_name_list[i]]\n",
    "        )\n",
    "        print(file_path)\n",
    "        json_data = documentai_json_proto_downloader(\n",
    "            file_path.split(\"/\")[2], (\"/\").join(file_path.split(\"/\")[3:])\n",
    "        )\n",
    "        if Missing_items_flag == \"True\":\n",
    "            json_data = get_missing_data(json_data, parent_type)\n",
    "        json_data = get_updated_grouped_line_items(json_data, parent_type)\n",
    "        store_document_as_json(\n",
    "            documentai.Document.to_json(json_data),\n",
    "            Gcs_output_path.split(\"/\")[2],\n",
    "            (\"/\").join(Gcs_output_path.split(\"/\")[3:]) + \"/\" + file_name_list[i],\n",
    "        )\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae3a0ae-5931-46bb-88d8-1523688935b2",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565fb0ae-b6c8-4336-b0e0-3d56a0916b17",
   "metadata": {},
   "source": [
    "The missing fields will be detected from the existing line items and grouped and updated json is saved in ouput path"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
