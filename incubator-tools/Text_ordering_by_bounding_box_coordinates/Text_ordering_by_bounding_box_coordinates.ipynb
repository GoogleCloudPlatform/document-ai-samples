{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2059a04-244c-46b9-8a9b-b00e95075597",
   "metadata": {},
   "source": [
    "# Text ordering by bounding box coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbc4843-0705-4909-b12d-f709b57e769f",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4d71fd-965e-48e2-a674-ce917a88e7b6",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the **DocAI Incubator Team**. No guarantees of performance are implied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe0d510-ccd8-494e-b463-cf3e302e31ad",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "This document provides a step-by-step guide on how to order text by bounding box coordinates using a Python notebook and a Document AI OCR output JSON file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79448360-806c-4147-94ed-830888272fde",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prerequisites\n",
    "* Access to vertex AI Notebook or Google Colab\n",
    "* Python\n",
    "*  OCR processor output json "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f4ac62-6ac8-4dd2-bb8d-bf0301d95ea6",
   "metadata": {},
   "source": [
    "## Step by Step procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba560d93-a2c0-4243-a7e3-b13623bd00b6",
   "metadata": {},
   "source": [
    "### 1.Importing Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265593e0-b6b6-49a0-a512-2049c1b0617b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c84c3cb-a324-4383-9994-c060c082930c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from operator import itemgetter\n",
    "from typing import Any, Dict, List, Optional, Sequence, Tuple, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7dc3f5-034e-466e-9970-35ab5e2f935f",
   "metadata": {},
   "source": [
    "### 2.Setup the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd9eb3b-979d-40df-a3b8-a4c9620dd714",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Provide your Document AI OCR output Json\n",
    "input_filename = \"your-input.json\"\n",
    "\n",
    "# Provide your output file name to store\n",
    "\n",
    "output_filename = \"ocr_output.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae1f9ef-97b7-425b-bc32-72ba43f57cbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.Run the required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d155689-442f-4658-a2a3-eed9bdb46bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans the input text by removing extra spaces and joins the words with a single space.\n",
    "\n",
    "    Args:\n",
    "        text (str): The raw input text.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned text with excess spaces removed.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def get_line_key(y: float) -> int:\n",
    "    \"\"\"\n",
    "    Returns a rounded integer value for the y-coordinate to group tokens by their approximate line position.\n",
    "\n",
    "    Args:\n",
    "        y (float): The y-coordinate of the token's position.\n",
    "\n",
    "    Returns:\n",
    "        int: The rounded y-coordinate value multiplied by 100 to represent the line key.\n",
    "    \"\"\"\n",
    "    return round(y * 100)\n",
    "\n",
    "\n",
    "def arrange_tokens(tokens: list, text: str) -> list[tuple[str, float, float]]:\n",
    "    \"\"\"\n",
    "    Arranges tokens from the document by extracting their text and normalized coordinates.\n",
    "\n",
    "    Args:\n",
    "        tokens (list): A list of tokens from the document, where each token contains layout, textAnchor, and boundingPoly information.\n",
    "        text (str): The full text of the document used to extract substrings based on token indices.\n",
    "\n",
    "    Returns:\n",
    "        list[tuple[str, float, float]]: A list of tuples where each tuple contains:\n",
    "            - The cleaned substring of text corresponding to the token.\n",
    "            - The x-coordinate of the token's position.\n",
    "            - The y-coordinate of the token's position.\n",
    "    \"\"\"\n",
    "    arranged_tokens = []\n",
    "    for token in tokens:\n",
    "        text_anchor = token.get(\"layout\", {}).get(\"textAnchor\", {})\n",
    "        text_segments = text_anchor.get(\"textSegments\", [{}])\n",
    "        if text_segments:\n",
    "            start_index = int(text_segments[0].get(\"startIndex\", 0))\n",
    "            end_index = int(text_segments[0].get(\"endIndex\", 0))\n",
    "            substring = clean_text(text[start_index:end_index])\n",
    "\n",
    "            bounding_poly = token.get(\"layout\", {}).get(\"boundingPoly\", {})\n",
    "            normalized_vertices = bounding_poly.get(\"normalizedVertices\", [])\n",
    "\n",
    "            if normalized_vertices:\n",
    "                x = normalized_vertices[0].get(\"x\", 0)\n",
    "                y = normalized_vertices[0].get(\"y\", 0)\n",
    "                arranged_tokens.append((substring, x, y))\n",
    "\n",
    "    return arranged_tokens\n",
    "\n",
    "\n",
    "def group_into_lines(arranged_tokens: list[tuple[str, float, float]]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Groups tokens into lines based on their y-coordinate and sorts them by their x-coordinate to reconstruct the line of text.\n",
    "\n",
    "    Args:\n",
    "        arranged_tokens (list[tuple[str, float, float]]): A list of tokens where each token is represented as a tuple containing:\n",
    "            - The token text (str).\n",
    "            - The x-coordinate (float) of the token's position.\n",
    "            - The y-coordinate (float) of the token's position.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of strings where each string is a line of text reconstructed from the tokens.\n",
    "    \"\"\"\n",
    "    lines = {}\n",
    "    for token, x, y in arranged_tokens:\n",
    "        line_key = get_line_key(y)\n",
    "        if line_key not in lines:\n",
    "            lines[line_key] = []\n",
    "        lines[line_key].append((token, x))\n",
    "\n",
    "    sorted_lines = []\n",
    "    for line_key in sorted(lines.keys()):\n",
    "        sorted_line = sorted(lines[line_key], key=itemgetter(1))\n",
    "        line_text = \" \".join(token for token, _ in sorted_line)\n",
    "        sorted_lines.append(line_text)\n",
    "\n",
    "    return sorted_lines\n",
    "\n",
    "\n",
    "with open(output_filename, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    for page_num, page in enumerate(json_data[\"pages\"], 1):\n",
    "        tokens = page.get(\"tokens\", [])\n",
    "        text = json_data.get(\"text\", \"\")\n",
    "\n",
    "        arranged_tokens = arrange_tokens(tokens, text)\n",
    "        lines = group_into_lines(arranged_tokens)\n",
    "        output_file.write(f\"Page: {page_num}\\n\\n\")\n",
    "        for line in lines:\n",
    "            output_file.write(f\"{line}\\n\")\n",
    "        output_file.write(\"\\n\" + \"=\" * 50 + \"\\n\\n\")\n",
    "\n",
    "print(f\"Output has been saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565644d4-c28f-46f0-88f1-715b5b29e2f7",
   "metadata": {},
   "source": [
    "### 4.Output\n",
    "\n",
    "The screenshot below shows before and after comparison where the text is getting aligned by the help of their coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dfa91c-2d62-4226-a685-1eab35d4952f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<b>Comparison Between Input and Output File</b><br><br>\n",
    "<i><h4>Post processing results<h4><i><br>\n",
    "Upon running the post processing script against input data. The resultant output json data is obtained. The following image will show the difference<br>\n",
    "    \n",
    "<table>\n",
    "        <tr>\n",
    "           <td><h3><b>Input Json </b></h3></td>\n",
    "            <td><h3><b>Output Json</b></h3></td>\n",
    "        </tr>\n",
    "    <tr>\n",
    "    <td><img src=\"./Images/input.png\"></td>\n",
    "    <td><img src=\"./Images/output.png\"></td>\n",
    "    </tr>\n",
    "    </table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b559a56-dd63-4245-a8bb-49a4dd26b14b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
