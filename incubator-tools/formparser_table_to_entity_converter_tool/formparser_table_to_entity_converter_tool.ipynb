{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a19abb34-c0de-4a80-b0dd-3764acf98e24",
   "metadata": {},
   "source": [
    "# Formparser Table to Entity Converter Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1040c3c0-e238-4bbd-a250-bed84a4bcc28",
   "metadata": {
    "tags": []
   },
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e96a4db-948c-44ad-b262-f00482c4cfef",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the **DocAI Incubator Team**. No guarantees of performance are implied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122324e9-d807-42a9-b847-edaa93f222a3",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "This document provides a step-by-step guide on how to use the Formparser Table to Entity Converter Tool. The tool converts Formparser tables output to entity-annotated JSON files. The user inputs a dictionary of header names and their corresponding entity names, and the tool uses fuzzy matching to map the headers to the entities. The output JSON files can be used to train and visualize entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a04eb8-65db-4610-adf9-24c4bc8219aa",
   "metadata": {},
   "source": [
    "## Prerequisites \n",
    "* Knowledge of Python\n",
    "* Python : Jupyter notebook (Vertex) or Google Colab \n",
    "* Access to Json Files in the Google Bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1237a68b-985e-47c4-b0f2-e074d9cdf1eb",
   "metadata": {},
   "source": [
    "## Step by step procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db087b3-4698-4583-942a-0fa113e6b71e",
   "metadata": {},
   "source": [
    "### Download and install the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b311d461-e932-4ab0-a52e-a54f2ee3acc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fuzzywuzzy pandas google-cloud-storage google-cloud-documentai\n",
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06a87e6-2a3d-446f-99ec-168e1e64ddb8",
   "metadata": {},
   "source": [
    "### Import the required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e53276d-7b32-45ba-b352-132e7403e929",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import process\n",
    "from google.cloud import storage\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "import utilities\n",
    "from typing import Dict, List, Optional, Tuple, TypedDict, Any, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eae302-0066-4822-b809-e174944f2d8a",
   "metadata": {},
   "source": [
    "### Setup the required inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e135c6c-5b8c-4baf-a2f4-270761fb9e9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = {\n",
    "    \"ItemCode\": \"item code\",\n",
    "    \"Quantity\": \"QTY CASE\",\n",
    "    \"TotalPrice\": \"Unit Price\",\n",
    "    \"UnitPrice\": \"Amount\",\n",
    "}\n",
    "# Specify your bucket and prefix (folder)\n",
    "input_bucket_name = \"xxxxxxxx\"\n",
    "input_prefix = \"xxxxxxxx/xxxxxx/xxxxxx/\"\n",
    "\n",
    "output_bucket_name = \"xxxxxxxx\"\n",
    "output_prefix = \"xxxxxx/xxxxxxxx/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152efded-8cdc-44bc-92fe-3432d83c4dd4",
   "metadata": {},
   "source": [
    "When setting up or modifying the **`user_input`** dictionary, ensure to:\n",
    "\n",
    "Use the appropriate entity name (from your schema) as the key.\n",
    "Match it with the correct header name (from the PDF) as its value.\n",
    "\n",
    "For Example, in the **`user_input dictionary`**:\n",
    "**`\"ItemCode\"`** is an entity name used in a schema.\n",
    "**`\"item code\"`** is the header name that you would look for in a PDF.\n",
    "\n",
    "**Note:** If you wish to modify the Parent Entity Name, simply replace **`\"invoiceItem\"`** in the code with the desired name based on your requirements.\n",
    "\n",
    "**`input_bucket_name`** and **`output_bucket_name`** variables indicate the Google Cloud Storage bucket name. \\\n",
    "**`input_prefix`** denotes the directory path within the GCS bucket where input JSON files reside. \\\n",
    "**`output_prefix`** marks the directory path within the GCS bucket where processed or output JSON files will be stored.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af731067-0776-4578-8e6a-05c358f3cccf",
   "metadata": {},
   "source": [
    "### Run the required Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f392b552-7cdf-44bd-8ae1-edf7e0789af1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_anchor_to_text(\n",
    "    document, text_anchor, page_number\n",
    ") -> Dict[str, Optional[Union[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Extracts text and corresponding bounding box information from a document based on text anchors.\n",
    "\n",
    "    Args:\n",
    "    document (Document): A dictionary representing the document, containing the full text.\n",
    "    text_anchor (TextAnchor): A dictionary representing the text anchor, containing text segments.\n",
    "    page_number (int): The page number where the text anchor is located.\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, Optional[Union[str, BoundingBox]]]: A dictionary containing the extracted text and the bounding box.\n",
    "    The bounding box is represented as a dictionary with 'topLeft' and 'bottomRight' keys, each containing\n",
    "    a dictionary with 'x' and 'y' coordinates. If no bounding box is found, the value is None.\n",
    "    \"\"\"\n",
    "    response = \"\"\n",
    "    text_segments = text_anchor.text_segments if text_anchor else []\n",
    "\n",
    "    all_bounding_boxes = []\n",
    "    for segment in text_segments:\n",
    "        start_index = segment.start_index if segment.start_index else 0\n",
    "        end_index = segment.end_index if segment.end_index else len(document.text)\n",
    "\n",
    "        bounding_box, _, _ = get_token(\n",
    "            document,\n",
    "            page_number,\n",
    "            [{\"start_index\": str(start_index), \"end_index\": str(end_index)}],\n",
    "        )\n",
    "        vertices = {\n",
    "            \"topLeft\": {\"x\": bounding_box[\"min_x\"], \"y\": bounding_box[\"min_y\"]},\n",
    "            \"bottomRight\": {\"x\": bounding_box[\"max_x\"], \"y\": bounding_box[\"max_y\"]},\n",
    "        }\n",
    "\n",
    "        if vertices:\n",
    "            response += document.text[start_index:end_index]\n",
    "            all_bounding_boxes.append(vertices)\n",
    "\n",
    "    if all(box is None for box in all_bounding_boxes):\n",
    "        return {\"text\": response.strip().replace(\"\\n\", \" \"), \"bounding_box\": None}\n",
    "\n",
    "    # Get the min and max values, or use defaults if the lists are empty\n",
    "    min_x_list = [\n",
    "        box[\"topLeft\"][\"x\"]\n",
    "        for box in all_bounding_boxes\n",
    "        if box[\"topLeft\"][\"x\"] is not None\n",
    "    ]\n",
    "    min_y_list = [\n",
    "        box[\"topLeft\"][\"y\"]\n",
    "        for box in all_bounding_boxes\n",
    "        if box[\"topLeft\"][\"y\"] is not None\n",
    "    ]\n",
    "    max_x_list = [\n",
    "        box[\"bottomRight\"][\"x\"]\n",
    "        for box in all_bounding_boxes\n",
    "        if box[\"bottomRight\"][\"x\"] is not None\n",
    "    ]\n",
    "    max_y_list = [\n",
    "        box[\"bottomRight\"][\"y\"]\n",
    "        for box in all_bounding_boxes\n",
    "        if box[\"bottomRight\"][\"y\"] is not None\n",
    "    ]\n",
    "\n",
    "    if not (min_x_list and min_y_list and max_x_list and max_y_list):\n",
    "        return {\"text\": response.strip().replace(\"\\n\", \" \"), \"bounding_box\": None}\n",
    "    min_x = min(min_x_list)\n",
    "    min_y = min(min_y_list)\n",
    "    max_x = max(max_x_list)\n",
    "    max_y = max(max_y_list)\n",
    "\n",
    "    page_anchor = {\n",
    "        \"topLeft\": {\"x\": min_x, \"y\": min_y},\n",
    "        \"bottomRight\": {\"x\": max_x, \"y\": max_y},\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"text\": response.strip().replace(\"\\n\", \" \"),\n",
    "        \"page_anchor\": page_anchor,\n",
    "        \"text_anchor\": text_anchor,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_token(document, page_num, text_anchors_check) -> Tuple[Any, List[Dict], float]:\n",
    "    \"\"\"\n",
    "    Extracts the bounding box, text anchor tokens, and confidence level for specified text anchors in a document.\n",
    "\n",
    "    Args:\n",
    "    document (Document): A dictionary representing the document, containing pages with tokens.\n",
    "    page_num (int): The page number to search for tokens.\n",
    "    text_anchors_check (List[TextAnchorCheck]): A list of dictionaries containing 'start_index' and 'end_index'\n",
    "    for text anchors to be checked.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[BoundingBox, List[Dict], float]: A tuple containing the bounding box of the text (if found),\n",
    "    a list of text anchor tokens, and the highest confidence level among the tokens.\n",
    "    The bounding box is a dictionary with 'min_x', 'min_y', 'max_x', 'max_y'. If no bounding box is found, values are None.\n",
    "    \"\"\"\n",
    "    min_x = min_y = max_x = max_y = None\n",
    "    text_anc_token = []\n",
    "    confidence = 0.0\n",
    "\n",
    "    for page in document.pages:\n",
    "        if page.page_number - 1 == page_num:\n",
    "            for token in page.tokens:\n",
    "                vertices = token.layout.bounding_poly.normalized_vertices\n",
    "                min_x_token = min(vertex.x for vertex in vertices)\n",
    "                min_y_token = min(vertex.y for vertex in vertices)\n",
    "                max_x_token = max(vertex.x for vertex in vertices)\n",
    "                max_y_token = max(vertex.y for vertex in vertices)\n",
    "\n",
    "                start_index = token.layout.text_anchor.text_segments[0].start_index\n",
    "                end_index = token.layout.text_anchor.text_segments[0].end_index\n",
    "\n",
    "                # Adjusting the logic to match the text anchors\n",
    "                for text_anchor_check in text_anchors_check:\n",
    "                    start_index_check = int(text_anchor_check[\"start_index\"])\n",
    "                    end_index_check = int(text_anchor_check[\"end_index\"])\n",
    "\n",
    "                    if (\n",
    "                        start_index <= start_index_check\n",
    "                        and end_index >= end_index_check\n",
    "                    ):\n",
    "                        min_x = (\n",
    "                            min_x_token if min_x is None else min(min_x, min_x_token)\n",
    "                        )\n",
    "                        min_y = (\n",
    "                            min_y_token if min_y is None else min(min_y, min_y_token)\n",
    "                        )\n",
    "                        max_x = (\n",
    "                            max_x_token if max_x is None else max(max_x, max_x_token)\n",
    "                        )\n",
    "                        max_y = (\n",
    "                            max_y_token if max_y is None else max(max_y, max_y_token)\n",
    "                        )\n",
    "                        text_anc_token.append(token.layout.text_anchor.text_segments)\n",
    "                        confidence = max(confidence, token.layout.confidence)\n",
    "\n",
    "    return (\n",
    "        {\"min_x\": min_x, \"min_y\": min_y, \"max_x\": max_x, \"max_y\": max_y},\n",
    "        text_anc_token,\n",
    "        confidence,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_table_data(document, rows, page_number) -> List[List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Extracts text data and bounding boxes from table rows in a Document AI object.\n",
    "\n",
    "    Args:\n",
    "    document (Dict[str, Any]): The Document AI object, representing the processed document.\n",
    "    rows (List[Dict[str, Any]]): A list of row objects extracted from a table in the document.\n",
    "    page_number (int): The page number where the table is located.\n",
    "\n",
    "    Returns:\n",
    "    List[List[Dict[str, Any]]]: A nested list where each sublist represents a row in the table.\n",
    "    Each element in the sublist is a dictionary containing the text data and its corresponding bounding box\n",
    "    for a cell in the row.\n",
    "    \"\"\"\n",
    "    all_values = []\n",
    "    for row in rows:\n",
    "        current_row_values = []\n",
    "        for cell in row.cells:\n",
    "            cell_data = text_anchor_to_text(\n",
    "                document, cell.layout.text_anchor, page_number\n",
    "            )\n",
    "            current_row_values.append(cell_data)\n",
    "        all_values.append(current_row_values)\n",
    "    return all_values\n",
    "\n",
    "\n",
    "def read_json_from_gcs(bucket_name: str, blob_name: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Reads a JSON file from Google Cloud Storage (GCS) and returns its contents.\n",
    "\n",
    "    Args:\n",
    "    bucket_name (str): The name of the GCS bucket.\n",
    "    blob_name (str): The name of the blob (file) in the GCS bucket.\n",
    "\n",
    "    Returns:\n",
    "    Dict: The contents of the JSON file as a dictionary.\n",
    "    \"\"\"\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    json_data = json.loads(blob.download_as_text())\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c56978c-33d7-4173-8ebf-c8dcc57ebce7",
   "metadata": {},
   "source": [
    "### Execute the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38e9801-765b-4791-a22b-559c4a29f105",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up the Google Cloud Storage client\n",
    "client = storage.Client()\n",
    "\n",
    "# List all .json files in the input GCS bucket with the given prefix\n",
    "blobs = client.list_blobs(input_bucket_name, prefix=input_prefix)\n",
    "json_files = [blob.name for blob in blobs if blob.name.endswith(\".json\")]\n",
    "\n",
    "\n",
    "for json_file in json_files:\n",
    "    print(f\"Processing: {json_file}\")\n",
    "    json_data = read_json_from_gcs(input_bucket_name, json_file)\n",
    "    if \"entities\" not in json_data:\n",
    "        json_data[\"entities\"] = []\n",
    "    result = []\n",
    "    json_string = json.dumps(json_data)\n",
    "    document = documentai.Document.from_json(json_string)\n",
    "    # print(document.entities)\n",
    "    for page_index, page in enumerate(document.pages):\n",
    "        page_number = page_index\n",
    "        for table in page.tables:\n",
    "            # Convert RepeatedComposite to Python lists and concatenate\n",
    "            all_rows = list(table.header_rows) + list(table.body_rows)\n",
    "\n",
    "            # Extract cell values from rows\n",
    "            table_data = []\n",
    "            for row in all_rows:\n",
    "                row_data = get_table_data(document, [row], page_number)\n",
    "                table_data.append(row_data[0])\n",
    "\n",
    "            df = pd.DataFrame(data=table_data)\n",
    "            df.index = df.index + 1\n",
    "            df = df.sort_index()\n",
    "\n",
    "            # display(df)\n",
    "\n",
    "            if df.shape[1] > 7:\n",
    "                first_row = df.iloc[0]\n",
    "                actual_headers = [elem[\"text\"] for elem in first_row]\n",
    "\n",
    "                # Update the dataframe's columns to the actual headers\n",
    "                df.columns = actual_headers\n",
    "\n",
    "                # Mapping the user input columns to actual headers\n",
    "                matched_headers = {}\n",
    "                for friendly_name, input_header in user_input.items():\n",
    "                    best_match, score = process.extractOne(input_header, actual_headers)\n",
    "                    if score >= 70:  # Adjust the threshold if needed\n",
    "                        matched_headers[friendly_name] = best_match\n",
    "                    else:\n",
    "                        print(f\"No match found for '{input_header}'\")\n",
    "\n",
    "                # Filter the dataframe for matched columns\n",
    "                df = df[\n",
    "                    [\n",
    "                        matched_headers[friendly_name]\n",
    "                        for friendly_name in matched_headers\n",
    "                    ]\n",
    "                ]\n",
    "\n",
    "                for _, row in df.iterrows():\n",
    "                    row_data = {\"properties\": [], \"type\": \"\", \"mention_text\": \"\"}\n",
    "                    combined_mention_text = \"\"\n",
    "                    parent_type = \"invoiceItem\"\n",
    "\n",
    "                    for friendly_name, matched_header in matched_headers.items():\n",
    "                        cell = row[matched_header]\n",
    "                        mention_text = cell.get(\"text\")\n",
    "                        text_anchor = cell.get(\"text_anchor\")\n",
    "                        page_anchor = cell.get(\"page_anchor\")\n",
    "\n",
    "                        if (\n",
    "                            mention_text is None\n",
    "                            or text_anchor is None\n",
    "                            or page_anchor is None\n",
    "                        ):\n",
    "                            continue\n",
    "\n",
    "                        # Handling TextAnchor with multiple text segments\n",
    "                        text_segments = []\n",
    "                        for segment in text_anchor.text_segments:\n",
    "                            text_segments.append(\n",
    "                                {\n",
    "                                    \"start_index\": segment.start_index,\n",
    "                                    \"end_index\": segment.end_index,\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "                        # Nesting text_segments under text_anchor\n",
    "                        text_anchor_dict = {\"text_segments\": text_segments}\n",
    "\n",
    "                        child_entity_type = friendly_name\n",
    "                        child_entity = {\n",
    "                            \"type\": child_entity_type,\n",
    "                            \"mention_text\": mention_text,\n",
    "                            \"text_anchor\": text_anchor_dict,  # Using the nested text_anchor structure\n",
    "                        }\n",
    "\n",
    "                        vertices = [\n",
    "                            {\n",
    "                                \"x\": page_anchor[\"topLeft\"][\"x\"],\n",
    "                                \"y\": page_anchor[\"topLeft\"][\"y\"],\n",
    "                            },\n",
    "                            {\n",
    "                                \"x\": page_anchor[\"bottomRight\"][\"x\"],\n",
    "                                \"y\": page_anchor[\"topLeft\"][\"y\"],\n",
    "                            },\n",
    "                            {\n",
    "                                \"x\": page_anchor[\"bottomRight\"][\"x\"],\n",
    "                                \"y\": page_anchor[\"bottomRight\"][\"y\"],\n",
    "                            },\n",
    "                            {\n",
    "                                \"x\": page_anchor[\"topLeft\"][\"x\"],\n",
    "                                \"y\": page_anchor[\"bottomRight\"][\"y\"],\n",
    "                            },\n",
    "                        ]\n",
    "                        child_entity[\"page_anchor\"] = {\n",
    "                            \"page_refs\": [\n",
    "                                {\n",
    "                                    \"bounding_poly\": {\"normalized_vertices\": vertices},\n",
    "                                    \"page\": str(page_number),\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                        combined_mention_text += mention_text + \" \"\n",
    "                        row_data[\"properties\"].append(child_entity)\n",
    "\n",
    "                    row_data[\"type\"] = parent_type\n",
    "                    row_data[\"mention_text\"] = combined_mention_text\n",
    "                    result.append(row_data)\n",
    "\n",
    "                # display(df)\n",
    "\n",
    "    # print(result)\n",
    "    output_blob_name = output_prefix + json_file.split(\"/\")[-1]\n",
    "\n",
    "    # Convert the JSON string back to a dictionary\n",
    "    json_data = json.loads(documentai.Document.to_json(document))\n",
    "\n",
    "    # Append the serializable result to the 'entities' field\n",
    "    json_data[\"entities\"].extend(result)\n",
    "\n",
    "    # Convert the modified JSON data back to a string\n",
    "    json_string = json.dumps(json_data, indent=4)\n",
    "\n",
    "    # Call the store_document_as_json function\n",
    "    utilities.store_document_as_json(json_string, output_bucket_name, output_blob_name)\n",
    "    # break\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fb1b3b-62f3-46ba-b126-7253303c7066",
   "metadata": {},
   "source": [
    "## **Output** \n",
    "\n",
    "### Input Form Parser Output Json \n",
    "\n",
    "<img src=\"./images/input.png\" width=600 height=600 alt=\"None\">\n",
    "\n",
    "\n",
    "### Output Table to line item entity converted Json\n",
    "\n",
    "The output JSON will contain data extracted from Form parser tables present in the source document, and this data will be structured as line items. The extraction and structuring process will be guided by the specifications provided in the user_input dictionary. The user_input dictionary serves as a blueprint: it maps specific headers (as they appear in the source document) to corresponding entity names (as they should be represented in the output JSON). By following these mappings, the script can convert table structure into line items in the resulting JSON.\n",
    "\n",
    "<img src=\"./images/output.png\" width=600 height=600 alt=\"None\">\n",
    "\n",
    "**Note:**\n",
    "* Code works for tables with over 7 columns and multiple rows or tables resembling the example shown above.\n",
    "* When converting tables to line items, the table header becomes part of the line items and gets included in the processed JSON.\n",
    "* Discrepancies may occur during conversion due to reliance on the form parser table output, resulting in potential merging of columns or rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7834cdc-ae5f-40ef-8782-1de5834a084d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
