{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29399724",
   "metadata": {},
   "source": [
    "# Tag Column Number to OCR Paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4a6def",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6029ebef",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the **DocAI Incubator Team**. No guarantees of performance are implied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bbfedd",
   "metadata": {},
   "source": [
    "# Objective\n",
    "This tool helps to add column number(`col_num`) attribute to all paragraphs present in OCR processor results(JSON result)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a77d9e9",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "* GCP Project ID\n",
    "* DocumentAI Processor ID\n",
    "* Cloud Storage(GCS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad788f5",
   "metadata": {},
   "source": [
    "# Step-by-Step Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e511f7a",
   "metadata": {},
   "source": [
    "## 1. Import Modules/Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "575caa1a-b978-42c7-8174-fdb51d035632",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80ecc19e-d0fb-4435-9284-44318db1937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-api-core\n",
    "# !pip install google-cloud-docuemntai\n",
    "# !pip install google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d4b7289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Optional\n",
    "\n",
    "from google.api_core.operation import Operation\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from google.cloud import storage\n",
    "\n",
    "from utilities import file_names, store_document_as_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa4caea",
   "metadata": {},
   "source": [
    "## 2. Input Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717bb8cc",
   "metadata": {},
   "source": [
    "* **PROJECT_ID** : Provide GCP project ID \n",
    "* **LOCATION** : Provide DocumentAI processor location (“us” or “eu”)\n",
    "* **PROCESSOR_ID** : Provide GCP DocumentAI processor id\n",
    "* **PROCESSOR_VERSION_ID** : Provide GCP DocumentAI processor version id\n",
    "* **GCS_INPUT_URI_RAW_PDF** : Provide GCS path of raw pdf files\n",
    "* **GCS_OUTPUT_URI_JSON** :  Provide GCS path to store batch process results\n",
    "* **GCS_OUTPUT_URI_PROCESSED_JSON** :  Provide GCS path to store post-processed results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9141221e-43ff-4aaf-9a68-07c5d67970ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"xx-xx-project\"\n",
    "LOCATION = \"us\"  # or 'eu'\n",
    "PROCESSOR_ID = \"xx-4cb-xx-cb4-xx\"\n",
    "PROCESSOR_VERSION_ID = \"pretrained-ocr-v2.0-2023-06-02\"\n",
    "GCS_INPUT_URI_RAW_PDF = \"gs://bucket/path_to/input\"\n",
    "GCS_OUTPUT_URI_JSON = \"gs://bucket/path/output/docai_results\"\n",
    "GCS_OUTPUT_URI_PROCESSED_JSON = \"gs://bucket_path_to/output/post_processed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3347e5d1",
   "metadata": {},
   "source": [
    "## 3. Run Below Code-Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cf532b-df23-4e84-a396-4ea33b546cc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_process_documents_sample(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    processor_id: str,\n",
    "    gcs_input_uri: str,\n",
    "    gcs_output_uri: str,\n",
    "    processor_version_id: Optional[str] = None,\n",
    "    timeout: int = 500,\n",
    ") -> Operation:\n",
    "    \"\"\"It will perform Batch Process on raw input documents\n",
    "\n",
    "    Args:\n",
    "        project_id (str): GCP project ID\n",
    "        location (str): Processor location us or eu\n",
    "        processor_id (str): GCP DocumentAI ProcessorID\n",
    "        gcs_input_uri (str): GCS path which contains all input files\n",
    "        gcs_output_uri (str): GCS path to store processed JSON results\n",
    "        processor_version_id (str, optional): VersionID of DocumentAI Processor. Defaults to None.\n",
    "        timeout (int, optional): Maximum waiting time for operation to complete. Defaults to 500.\n",
    "\n",
    "    Returns:\n",
    "        Operation: LRO operation ID for current batch-job\n",
    "    \"\"\"\n",
    "\n",
    "    opts = {\"api_endpoint\": f\"{location}-documentai.googleapis.com\"}\n",
    "    client = documentai.DocumentProcessorServiceClient(client_options=opts)\n",
    "    input_config = documentai.BatchDocumentsInputConfig(\n",
    "        gcs_prefix=documentai.GcsPrefix(gcs_uri_prefix=gcs_input_uri)\n",
    "    )\n",
    "    output_config = documentai.DocumentOutputConfig(\n",
    "        gcs_output_config={\n",
    "            \"gcs_uri\": gcs_output_uri,\n",
    "            \"sharding_config\": {\"pages_per_shard\": 100},\n",
    "        }\n",
    "    )\n",
    "    print(\"Documents are processing(batch-documents)...\")\n",
    "    name = (\n",
    "        client.processor_version_path(\n",
    "            project_id, location, processor_id, processor_version_id\n",
    "        )\n",
    "        if processor_version_id\n",
    "        else client.processor_path(project_id, location, processor_id)\n",
    "    )\n",
    "    request = documentai.types.document_processor_service.BatchProcessRequest(\n",
    "        name=name,\n",
    "        input_documents=input_config,\n",
    "        document_output_config=output_config,\n",
    "    )\n",
    "    operation = client.batch_process_documents(request)\n",
    "    print(\"Waiting for operation to complete...\")\n",
    "    operation.result(timeout=timeout)\n",
    "    print(\"Batch process completed\")\n",
    "    return operation\n",
    "\n",
    "\n",
    "def get_vertices(bounding_poly: documentai.BoundingPoly) -> List[float]:\n",
    "    \"\"\"It returns xy vertices as x&y min , x&y max\n",
    "\n",
    "    Args:\n",
    "        bounding_poly (documentai.BoundingPoly):\n",
    "            Boundingpoly object which holds bbox xy coordinate values\n",
    "\n",
    "    Returns:\n",
    "        List[float]: Bbox as list xmin, ymin, xmax & ymax\n",
    "    \"\"\"\n",
    "\n",
    "    x, y = [], []\n",
    "    for vertex in bounding_poly.vertices:\n",
    "        x.append(vertex.x)\n",
    "        y.append(vertex.y)\n",
    "    return [min(x), min(y), max(x), max(y)]\n",
    "\n",
    "\n",
    "gcs_input_uri_raw_pdf = GCS_INPUT_URI_RAW_PDF.rstrip(\"/\")\n",
    "gcs_output_uri_json = GCS_OUTPUT_URI_JSON.rstrip(\"/\")\n",
    "gcs_output_uri_processed_json = GCS_OUTPUT_URI_PROCESSED_JSON.rstrip(\"/\")\n",
    "#  Calling Batch Process\n",
    "res = batch_process_documents_sample(\n",
    "    PROJECT_ID,\n",
    "    LOCATION,\n",
    "    PROCESSOR_ID,\n",
    "    gcs_input_uri_raw_pdf.strip(\"/\") + \"/\",\n",
    "    gcs_output_uri_json,\n",
    "    PROCESSOR_VERSION_ID,\n",
    ")\n",
    "print(res.metadata)\n",
    "operation_id = res.operation.name.split(\"/\")[-1]\n",
    "gcs_output_uri_json = gcs_output_uri_json + f\"/{operation_id}\"\n",
    "splits = gcs_output_uri_json.split(\"/\")\n",
    "input_bucket, input_prefix = splits[2], \"/\".join(splits[3:])\n",
    "splits = gcs_output_uri_processed_json.split(\"/\")\n",
    "output_bucket, output_prefix = splits[2], \"/\".join(splits[3:])\n",
    "\n",
    "_, files_dict = file_names(gcs_output_uri_json)\n",
    "\n",
    "\n",
    "sc = storage.Client()\n",
    "input_bucket_obj = sc.get_bucket(input_bucket)\n",
    "output_bucket_obj = sc.get_bucket(output_bucket)\n",
    "print(\"Postprocessing started...\")\n",
    "for fn, fp in files_dict.items():\n",
    "    print(\"File: \", fn)\n",
    "    # Downloading json as string\n",
    "    json_string = input_bucket_obj.blob(fp).download_as_string()\n",
    "    doc = documentai.Document.from_json(json_string)\n",
    "    target_all = {}\n",
    "    for page in doc.pages:\n",
    "        target_para = {\"-1\": [], \"1\": [], \"2\": []}\n",
    "        w, h = page.dimension.width, page.dimension.height\n",
    "        x1, x2 = w, 0\n",
    "        for p in page.paragraphs:\n",
    "            point = get_vertices(p.layout.bounding_poly)\n",
    "            x1 = min(x1, point[0])\n",
    "            x2 = max(x2, point[2])\n",
    "        # Midpoint of text span in a page\n",
    "        mid = (x1 + x2) // 2\n",
    "        for p in page.paragraphs:\n",
    "            point = get_vertices(p.layout.bounding_poly)\n",
    "            if point[0] < mid < point[2]:\n",
    "                target_para[\"-1\"].append(p)\n",
    "            elif (point[3] - point[1]) <= 57:  # value(57) is tunable\n",
    "                target_para[\"-1\"].append(p)\n",
    "            elif point[2] <= mid:\n",
    "                target_para[\"1\"].append(p)\n",
    "            elif point[0] >= mid:\n",
    "                target_para[\"2\"].append(p)\n",
    "        target_all[page.page_number] = target_para\n",
    "\n",
    "    json_d = documentai.Document.to_dict(doc)\n",
    "    pages_f = []\n",
    "    for p in json_d[\"pages\"]:\n",
    "        mpn = p[\"page_number\"]\n",
    "        new_p = []\n",
    "        for mp in p[\"paragraphs\"]:\n",
    "            _ts = mp[\"layout\"][\"text_anchor\"][\"text_segments\"]\n",
    "            if _ts:\n",
    "                _si, _ei = _ts[0][\"start_index\"], _ts[0][\"end_index\"]\n",
    "            else:\n",
    "                continue\n",
    "            for pn, values_d1 in target_all.items():\n",
    "                if mpn == pn:\n",
    "                    for col_num, values2 in values_d1.items():\n",
    "                        for ip in values2:\n",
    "                            ts_ = ip.layout.text_anchor.text_segments[0]\n",
    "                            si_, ei_ = ts_.start_index, ts_.end_index\n",
    "                            if int(_si) == si_ and int(_ei) == ei_:\n",
    "                                mp[\"layout\"][\"col_num\"] = col_num\n",
    "                                new_p.append(mp)\n",
    "        p[\"paragraphs\"] = new_p\n",
    "        pages_f.append(p)\n",
    "    json_d[\"pages\"] = pages_f\n",
    "    json_s = json.dumps(json_d)\n",
    "    store_document_as_json(json_s, output_bucket, output_prefix + \"/\" + fn)\n",
    "    print(\"\\t\", f\"File uploaded to gs://{output_bucket}/{output_prefix+'/'+fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eda18a",
   "metadata": {},
   "source": [
    "# 4. Output Details\n",
    "\n",
    "After post processing JSON results, for each paragraph in a page a new attribute/key is added (col_num) which holds value -1 or 1 or 2. 1 and 2 represent paragraphs in the left half of page and right half of page respectively. If the text in paragraph is not satisfying a few conditions then its value is -1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9da4d6-7560-4e1e-96ca-eace4ddcb83b",
   "metadata": {},
   "source": [
    "<b>Sample JSON Keys</b>  \n",
    "<img src=\"./images/pre_processed.png\" height=400 width=300 alt='pre_processed'> </img>   \n",
    "<b> Postprocessed JSON Keys</b>  \n",
    "<img src=\"./images/post_processed.png\" height=400 width=300 alt='post_processed'></img> "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
