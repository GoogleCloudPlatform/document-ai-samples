{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f56344d-0223-44df-ab00-ec2f68424d9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Detecting language of the text within the entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff3b0e6-e27a-4a66-a379-701b6e3367d4",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7831426-4f5e-4bfc-b1f3-64e5edd031ca",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the **DocAI Incubator Team**. No guarantees of performance are implied.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610db5bd-6467-4136-bed8-7777924bfa04",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "This tool facilitates the detection of language codes within text entities by aligning token text anchors with entity text anchors. Subsequently, a new attribute named \"detectedLanguages\" is integrated into the generated JSON file. This allows users to conveniently access the language code associated with each entity directly within the JSON output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0ed300-7443-4133-8830-ec93c2bc1a37",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* Vertex AI Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b9d965-014f-4c1a-9cc2-35da1557bc17",
   "metadata": {},
   "source": [
    "## Step by Step procedure "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5fa6c6-719a-403c-810b-c6fd8bdad7ab",
   "metadata": {},
   "source": [
    "### 1.Importing Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8650653a-c206-4321-bcef-2c36334a2b31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfd261a-8c02-41e4-9238-0fc398b8b657",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from google.cloud import storage\n",
    "from typing import Any, Dict, List, Optional, Sequence, Tuple, Union\n",
    "from pathlib import Path\n",
    "import json\n",
    "from utilities import (\n",
    "    file_names,\n",
    "    documentai_json_proto_downloader,\n",
    "    store_document_as_json,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3ef878-c929-4f94-b0e3-81b43d3b8afe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.Setup the inputs\n",
    "* `gcs_input_path` : It contains the input jsons bucket path. \n",
    "* `gcs_output_path` : It contains the output bucket path where the updated jsons after adding the attribute will be stored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e2d972-e268-4b87-a0a2-b94e00d27f40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gcs_input_path = \"gs://{input_bucket_name}/{subfolder_name}/\"  # '/' should be provided at the end of the path.\n",
    "gcs_output_path = \"gs://{output_bucket_name}/{subfolder_name}/\"  # '/' should be provided at the end of the path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ec39a0-72aa-40fc-8637-45c539f27055",
   "metadata": {},
   "source": [
    "### 3.Run the required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b14d42-42eb-4cb4-bc6b-cae292ef795a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_detected_languages_for_entities_with_multiple_segments(\n",
    "    document_json: dict,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Extracts detected languages for each entity in a document based on overlapping text segments with tokens.\n",
    "\n",
    "    Args:\n",
    "        document_json (dict): The JSON representation of the document, containing entities, pages, and tokens.\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated document JSON with detected languages added to each entity.\n",
    "    \"\"\"\n",
    "    for entity in document_json[\"entities\"]:\n",
    "        detected_languages = set()  # Using a set to avoid duplicate languages\n",
    "\n",
    "        try:\n",
    "            # Iterate over each text segment of the entity\n",
    "            for segment in entity[\"textAnchor\"][\"textSegments\"]:\n",
    "                entity_start_index = segment.get(\"startIndex\", \"0\")\n",
    "                entity_end_index = segment.get(\"endIndex\", \"0\")\n",
    "                # print(entity_start_index,entity_end_index)\n",
    "\n",
    "                # Iterate through each page and each token in the document\n",
    "                for page in document_json[\"pages\"]:\n",
    "                    for token in page[\"tokens\"]:\n",
    "                        # print(token.layout.text_anchor.text_segments)\n",
    "                        token_start_index = token[\"layout\"][\"textAnchor\"][\n",
    "                            \"textSegments\"\n",
    "                        ][0][\"startIndex\"]\n",
    "                        token_end_index = token[\"layout\"][\"textAnchor\"][\"textSegments\"][\n",
    "                            0\n",
    "                        ][\"endIndex\"]\n",
    "\n",
    "                        # Check if the entity's text segment overlaps with the token's text anchor\n",
    "                        if (\n",
    "                            entity_start_index >= token_start_index\n",
    "                            and entity_end_index <= token_end_index\n",
    "                        ) or (\n",
    "                            token_start_index >= entity_start_index\n",
    "                            and token_end_index <= entity_end_index\n",
    "                        ):\n",
    "                            # If there's an overlap, add the detected languages to the set\n",
    "                            # print(\"Hi\")\n",
    "                            for language in token[\"detectedLanguages\"]:\n",
    "                                detected_languages.add(language[\"languageCode\"])\n",
    "\n",
    "            # Print the entity type, text content, and detected languages\n",
    "            entity[\"detectedLanguages\"] = list(detected_languages)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    document_json[\"entities\"] = document_json[\"entities\"]\n",
    "\n",
    "    return document_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3642e2d-01b6-40b3-80d7-1f90606760e9",
   "metadata": {},
   "source": [
    "### 4.Run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea09a923-99a5-4675-9428-aa5f90d8d858",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(gcs_input_path, gcs_output_path):\n",
    "    file_names_list, file_dict = file_names(gcs_input_path)\n",
    "    for filename, filepath in file_dict.items():\n",
    "        print(\">>>>>>>>>>>>>>> Processing File : \", filename)\n",
    "        input_bucket_name = gcs_input_path.split(\"/\")[2]\n",
    "        document_proto = documentai_json_proto_downloader(input_bucket_name, filepath)\n",
    "        document_json = json.loads(documentai.Document.to_json(document_proto))\n",
    "\n",
    "        final_json = get_detected_languages_for_entities_with_multiple_segments(\n",
    "            document_json\n",
    "        )\n",
    "        output_bucket_name = gcs_output_path.split(\"/\")[2]\n",
    "        output_path_within_bucket = \"/\".join(gcs_output_path.split(\"/\")[3:])\n",
    "        store_document_as_json(\n",
    "            json.dumps(final_json),\n",
    "            output_bucket_name,\n",
    "            f\"{output_path_within_bucket}{filename}\",\n",
    "        )\n",
    "\n",
    "\n",
    "main(gcs_input_path, gcs_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7b510a-170b-4572-bc38-ac42981724d3",
   "metadata": {},
   "source": [
    "### 5.Output\n",
    "\n",
    "The new attribute 'detectedLanguages' will be added to each entity in the newly generated json file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b098e9e9-7f3a-4352-a047-6c23df46bb9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./Images/image.png\" width=800 height=400 ></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0b4ac7-20ca-4737-abb7-2ce88efe148a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
