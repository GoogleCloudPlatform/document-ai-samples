{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff453a82-8d82-4065-bd79-c1c5b3dbf7a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DocAI Processor Migration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af59829a-da96-470f-8feb-25fdf0d704a4",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34bebc0-32a8-4d21-bb75-37021bb9451a",
   "metadata": {},
   "source": [
    "## Disclaimer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a674fb-2750-49d6-b363-c592b24f6730",
   "metadata": {},
   "source": [
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the DocAI Incubator Team. No guarantees of performance are implied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7565cd-e4f2-470a-9285-fd1bb7fad909",
   "metadata": {},
   "source": [
    "## Purpose and Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673b1c05-a9c7-4676-b8aa-9530f1b1453d",
   "metadata": {},
   "source": [
    "The python script aims to automate the process of migrating a Document AI processor from one project to another by handling tasks such as importing the data, creating the schema, and automatically training the processor using the dataset from the source project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f121726-2461-4957-9990-02bde9bd8143",
   "metadata": {},
   "source": [
    "## Pre-Requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84120c23-ee5c-4ec4-b70c-447e61a1f3e3",
   "metadata": {},
   "source": [
    "* Python : Jupyter notebook (Vertex AI) \n",
    "* Permissions to give access to the service account in both source and destination projects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeae883-86db-4f53-ac36-2eef555a0c78",
   "metadata": {},
   "source": [
    "## Installation Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cbd317-42e7-487d-90fc-e39d58b0e3b5",
   "metadata": {},
   "source": [
    "The script consists of Python code. It can be loaded and run via: \n",
    "* Upload the IPYNB file or copy the code to the Vertex Notebook and follow the step by step procedure.\n",
    "\n",
    "  Drive Link to IPYNB File : [DocAI_Processor_Migration.ipynb](https://drive.google.com/file/d/10GbjjQl56n79D9kj5GYbED3QPxO_ELB7/view?resourcekey=0-3qE82iFBnbX99AS8r_B3fg) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f7085a-7345-440c-9abc-1b220f6e859b",
   "metadata": {},
   "source": [
    "## Step by Step Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b395c7e6-3552-4b6b-9058-7fe1fe95b079",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Identifying the Service Account associated with VertexAI Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec14918b-8fe3-4e50-918d-15248e6d008c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud config list account # This gives your account and active configuration details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e692a0a-fd16-4600-9b28-2628f6dbd432",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Granting Required Permissions to the Service Account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888935b0-5762-4d9c-8293-268469367f5a",
   "metadata": {},
   "source": [
    "![download.png](https://screenshot.googleplex.com/5WN4jJqV4CKYmfx.png)\n",
    "\n",
    "In the Google Cloud project that is the intended destination for migration, add the service account that was acquired in the previous step and assign the below two roles.\n",
    "* Document AI Administrator \n",
    "* Storage Admin \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1304718-671b-487d-a25b-0d1120f29ea9",
   "metadata": {},
   "source": [
    "For the migration to work, the service account used for running this notebook needs to have roles in both source and destination projects to create the dataset bucket (if it does not exist) and read/write all objects.\n",
    "\n",
    "* Document AI Administrator\n",
    "* Storage Admin \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54209eb-0502-4470-9d90-cadd6e314a56",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Installing the dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b28822d-1b23-4434-a197-6101ebb32281",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet google-cloud-documentai ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e01eab-f45a-497b-a69e-d65f28593c72",
   "metadata": {},
   "source": [
    " This is a command used in Python to install packages.    \n",
    " google-cloud-documentai related to Google Cloud's Document AI service, which is a tool for extracting structured information from documents.     \n",
    " ipywidgets is a library for creating interactive widgets in Jupyter notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a663059b-673f-4df2-8c42-1e46ed394815",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. Import the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3ab951-3d4a-4d9a-8659-dedddc9c4297",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary modules\n",
    "\n",
    "from google.api_core.client_options import ClientOptions\n",
    "import google.auth.transport.requests\n",
    "from google import auth\n",
    "from google.cloud import documentai\n",
    "from google.cloud import storage\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from ipywidgets import Output\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7b5d01-71ff-4b00-8cd2-ed85f33d714e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5. Setup the required inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c791d8c2-752f-4a86-b1d6-7aa431e54838",
   "metadata": {},
   "source": [
    "* **SOURCE_PROCESSOR_NAME** - This involves source project_id and source processor_id.\n",
    "\n",
    "     Ex: projects/**Project_number**/locations/us/processors/**Processor_ID**\n",
    "* **DESTINATION_PROJECT_NUMBER** - This contains the project number to which the processor needs to be moved. \n",
    "* **DESTINATION_PROCESSOR_LOCATION** - This indicates the processor destination location. \n",
    "* **DESTINATION_PROCESSOR_DATASET_GCS_URI** - The GCS bucket path which is used for the destination processor dataset, automatically created if it does not exist.\n",
    "* **SOURCE_EXPORTED_DATASET_GCS_URI** - This is the GCS bucket path where the dataset from the source processor has been exported. Ensure that you export your dataset via the user interface and then input its path here.\n",
    "* **DESTINATION_EXPORTED_DATASET_GCS_URI** - This is the GCS bucket path where the dataset from the source processor will be copied over to the destination project. Simply provide an empty bucket path here.\n",
    "  \n",
    "  Allowed path example: gs://bucket\n",
    "  \n",
    "  Not Allowed path example: gs://bucket/sub_folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b08d75-90d0-4881-83e3-84623ec06247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the Inputs\n",
    "KMS_KEY_NAME = \"\"\n",
    "SOURCE_PROCESSOR_NAME = \"projects/<your-project-number>/locations/us/processors/<your-processor-id>\"\n",
    "DESTINATION_PROJECT_NUMBER = \"<your-project-number>\"\n",
    "DESTINATION_PROCESSOR_LOCATION = \"us\"\n",
    "DESTINATION_PROCESSOR_DATASET_GCS_URI = \"gs://<bucket-name-1>\" \n",
    "SOURCE_EXPORTED_DATASET_GCS_URI = \"gs://<bucket-name-2>\" \n",
    "DESTINATION_EXPORTED_DATASET_GCS_URI = \"gs://<bucket-name-3>\" \n",
    "gcs_documents_train = []\n",
    "gcs_documents_test = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e8bd4c-5df5-4b20-ab02-b1f4a84e41fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 6. Run the Required Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457a88cb-030a-49bb-becb-e12923261d57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_destination_dataset_bucket(project_id : str, destination_exported_dataset_gcs_uri : str) -> None:\n",
    "    \"\"\"\n",
    "    This function will create destination dataset bucket.\n",
    "    \n",
    "    Args:\n",
    "      project_id (str): The number representing the Google Cloud project. \n",
    "      destination_exported_dataset_gcs_uri (str): This is the GCS bucket path where the dataset from the source processor will be copied over to the destination project. \n",
    "                                                  Simply provide an empty bucket path here.\n",
    "    \n",
    "    Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "    \n",
    "    client = storage.Client(project=project_id) \n",
    "    bucket = client.bucket(destination_exported_dataset_gcs_uri.split('//')[1])  \n",
    "    if not bucket.exists():   \n",
    "        tqdm.write(f\"Creating bucket {bucket.name}\")  \n",
    "        client.create_bucket(bucket)\n",
    "\n",
    "def move_exported_dataset(source_exported_dataset_gcs_uri : str, destination_exported_dataset_gcs_uri : str) -> None:\n",
    "    \"\"\"\n",
    "    This function will copy files from source exported dataset bucket into destination exported dataset bucket and splitting train and test documents.\n",
    "    \n",
    "    Args:\n",
    "        source_exported_dataset_gcs_uri (str) : This is the bucket path where the dataset from the source processor has been exported.\n",
    "        destination_exported_dataset_gcs_uri (str): This is the GCS bucket path where the dataset from the source processor will be copied over to the destination project. \n",
    "                                                  Simply provide an empty bucket path here.\n",
    "                                                  \n",
    "   Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    client = storage.Client()\n",
    "    bucket_src = client.get_bucket(source_exported_dataset_gcs_uri.split('//')[1])\n",
    "    blobs_src = client.list_blobs(source_exported_dataset_gcs_uri.split('//')[1])\n",
    "    bucket_dest = storage.Bucket(client, destination_exported_dataset_gcs_uri.split('//')[1])\n",
    "\n",
    "    from datetime import datetime\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    print(\"date and time =\", dt_string)\n",
    "    for blob_src in blobs_src:\n",
    "        blob_new = bucket_src.copy_blob(blob_src, bucket_dest, new_name= dt_string + '/' + blob_src.name)\n",
    "        print(f'Copied [{source_exported_dataset_gcs_uri}/{blob_src.name}] into: [{destination_exported_dataset_gcs_uri}/{dt_string}]')\n",
    "        gcs_document = { 'gcsUri': destination_exported_dataset_gcs_uri + '/' + dt_string + '/' + blob_src.name, 'mimeType': \"application/json\" }\n",
    "        if blob_src.name.split('/')[0] == 'train' :\n",
    "            gcs_documents_train.append(gcs_document)     \n",
    "            gcs_documents_train.append(gcs_document)          \n",
    "        if blob_src.name.split('/')[0] == 'test' :\n",
    "            gcs_documents_test.append(gcs_document)     \n",
    "            gcs_documents_test.append(gcs_document)  \n",
    "\n",
    "    print('gcs_documents_train:')\n",
    "    print(gcs_documents_train)\n",
    "    print('\\n')\n",
    "    print('gcs_documents_test:')\n",
    "    print(gcs_documents_test)    \n",
    "    \n",
    "def import_document_by_type(destination_processor_name : str, gcs_documents : List[str], dataset_type : str)-> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    This function will import document to its destination processor by its document type either test or train.\n",
    "    \n",
    "    Args:\n",
    "        destination_processor_name (str) : Name of the destination processor.\n",
    "        gcs_documents (list) : Takes the list of files from splitted train or splitted test documents from destination exported dataset bucket.\n",
    "        dataset_type (str) : Takes the values 'DATASET_SPLIT_TEST' or 'DATASET_SPLIT_TRAIN'.\n",
    "    \n",
    "    Returns:\n",
    "         Dictionary representing JSON data using their names. \n",
    "    \"\"\"\n",
    "    \n",
    "    tqdm.write(\"Import document\")\n",
    "    url = get_base_url(destination_processor_name) + \"/dataset:importDocuments\"\n",
    "    headers = {'Authorization': f'Bearer {get_access_token()}'}\n",
    "    import_documents_request = {\n",
    "            'batch_documents_import_configs': {\n",
    "                'dataset_split': dataset_type,\n",
    "                'batch_input_config': {\n",
    "                    'gcs_documents': {\n",
    "                        'documents': gcs_documents\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    import_document_response = requests.post(url, headers=headers, json=import_documents_request)\n",
    "    import_document_response.raise_for_status()\n",
    "    import_document_result = get_operation_result(import_document_response.json()['name'])\n",
    "    return import_document_result\n",
    " \n",
    "def get_access_token() -> str:\n",
    "    \"\"\"\n",
    "    This function is used as an authentication mechanism to obtain current user / service account credentials. \n",
    "    \n",
    "    Returns:\n",
    "         A string representing the access token.\n",
    "    \"\"\"\n",
    "    \n",
    "    credentials, _ = auth.default()    \n",
    "    credentials.refresh(google.auth.transport.requests.Request())      \n",
    "    return credentials.token\n",
    "\n",
    "def get_base_url(name : str) -> str:\n",
    "    \"\"\"\n",
    "    The function uses a regular expression to extract a specific part of the input name.\n",
    "    \n",
    "    Args:\n",
    "       name (str) : This is a string containing some kind of identifier or path.\n",
    "    \n",
    "    Returns:\n",
    "         A formatted URL string using the extracted location and name.\n",
    "    \"\"\"\n",
    "    \n",
    "    location = re.search(r\"projects/[^/]+/locations/([^/]+)/.*\", name).group(1)\n",
    "    return f\"https://{location}-documentai.googleapis.com/v1beta3/{name}\"\n",
    "\n",
    "def get_operation_result(operation_name : str, message = \"Waiting for operation to finish.\":str) -> Dict[str,str]:\n",
    "    \"\"\"\n",
    "    This function retrieves the result of a long-running operation. \n",
    "    It interacts with an API using HTTP requests and uses the tqdm library for progress reporting\n",
    "    \n",
    "    Args:\n",
    "        operation_name (str): This is a string representing the name or identifier of a long-running operation.\n",
    "        message (str with default value): This is a string that provides a message to be displayed while waiting for the operation to finish. \n",
    "                                          It has a default value of \"Waiting for operation to finish.\"\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary representing JSON data.\n",
    "    \"\"\"\n",
    "    \n",
    "    tqdm.write(message,end='')\n",
    "    url = get_base_url(operation_name)\n",
    "    headers = {'Authorization': f'Bearer {get_access_token()}'}\n",
    "    get_operation_response = requests.get(url, headers=headers)\n",
    "    get_operation_response.raise_for_status()\n",
    "    if not 'done' in get_operation_response.json() or not get_operation_response.json()['done']:\n",
    "        time.sleep(1)\n",
    "        return get_operation_result(operation_name, message = \".\")\n",
    "    tqdm.write(\"\")\n",
    "    return get_operation_response.json()\n",
    "\n",
    "def get_processor_details(processor_name :str) -> Dict[str,str]:\n",
    "    \"\"\"\n",
    "    This function is used to retrieve the processor details using processor name.\n",
    "    \n",
    "    Args: \n",
    "       processor_name (str) : This is the processor name for which you want to retrieve the details of a processor.\n",
    "    \n",
    "    Returns:\n",
    "       Dictionary representing JSON data of processor.\n",
    "    \"\"\"\n",
    "    \n",
    "    tqdm.write(\"Getting processor details\")\n",
    "    url = get_base_url(processor_name)\n",
    "    headers = {'Authorization': f'Bearer {get_access_token()}'}\n",
    "    get_processor_response = requests.get(url, headers=headers)\n",
    "    get_processor_response.raise_for_status()\n",
    "    return get_processor_response.json()\n",
    "\n",
    "def get_processor_version_details(processor_name : str, version_name : str) -> str:\n",
    "    \"\"\"\n",
    "    This function is used to get the processor version details.\n",
    "    \n",
    "    Args: \n",
    "       processor_name (str) : This is the name of the processor for which you want to retrieve details.\n",
    "       version_name (str) : This is the name of the version for which you want to retrieve details.\n",
    "       \n",
    "    Returns:\n",
    "         A String containing the deployed_version displayName.\n",
    "    \"\"\"\n",
    "    \n",
    "    tqdm.write(\"Getting processor version details\")\n",
    "    url = get_base_url(processor_name) + \"/processorVersions\"\n",
    "    headers = {'Authorization': f'Bearer {get_access_token()}'}\n",
    "    get_processor_version_response = requests.get(url, headers=headers)\n",
    "    get_processor_version_response.raise_for_status()\n",
    "    deployed_version = ''\n",
    "    for data in get_processor_version_response.json()['processorVersions']:\n",
    "        if data['name'] == version_name and data['state'] == 'DEPLOYED':\n",
    "            deployed_version = data['displayName']\n",
    "            print(deployed_version)\n",
    "            break\n",
    "    return deployed_version\n",
    "\n",
    "def get_processor_dataset_schema(processor_name : str) -> Dict[str,str]:\n",
    "    \"\"\"\n",
    "    This function is used to get the processor dataset schema.\n",
    "    \n",
    "    Args: \n",
    "       processor_name (str) : This is the name of the processor for which you want to retrieve dataset schema.\n",
    "       \n",
    "    Returns:\n",
    "        Dictionary representing JSON data of processor schema.\n",
    "    \"\"\"\n",
    "    \n",
    "    tqdm.write(\"Getting processor dataset schema\")\n",
    "    url = get_base_url(processor_name) + \"/dataset/datasetSchema\"\n",
    "    headers = {'Authorization': f'Bearer {get_access_token()}'}\n",
    "    get_schema_response = requests.get(url, headers=headers)\n",
    "    get_schema_response.raise_for_status()\n",
    "    return get_schema_response.json()\n",
    "\n",
    "def create_processor(project_id : str, location : str, processor_details : Dict[str,str] , kms_key_name = \"\" : str) -> str:\n",
    "    \"\"\"\n",
    "    This function is used to create a processor in the destination project.\n",
    "    \n",
    "    Args:\n",
    "       project_id (str): This is a string representing the ID of the project.\n",
    "       location (str): This is a string representing the location of the project.\n",
    "       processor_details (dictionary): This is a dictionary containing details about the processor being created.\n",
    "       kms_key_name (str): This is a string representing the Key Management Service (KMS) key name. \n",
    "                           It has a default value of an empty string.\n",
    "                           \n",
    "    Returns: \n",
    "          A string representing the name of the created processor.\n",
    "    \"\"\"\n",
    "    \n",
    "    tqdm.write(\"Create processor\")\n",
    "    url = f\"https://{location}-documentai.googleapis.com/uiv1beta3/projects/{project_id}/locations/{location}/processors\"\n",
    "    headers = {'Authorization': f'Bearer {get_access_token()}'}    \n",
    "    create_processor_request = {\n",
    "      \"type\": processor_details['type'],\n",
    "      \"displayName\": processor_details['displayName'] + \"_v2\",        \n",
    "    }\n",
    "    # enable CMEK if kms_key_name not empty\n",
    "    if kms_key_name:\n",
    "        create_processor_request['kms_key_name'] = kms_key_name\n",
    "    create_processor_response = requests.post(url, headers=headers, json=create_processor_request)\n",
    "    create_processor_response.raise_for_status()\n",
    "    return create_processor_response.json()['name']\n",
    "\n",
    "def add_processor_dataset(processor_name : str, dataset_gcs_uri : str, project_id : str) -> Dict[str,str]:\n",
    "    \"\"\"\n",
    "    This function is used to add processor dataset into destination project.\n",
    "    \n",
    "    Args:\n",
    "        processor_name (str): This is a string representing the name or identifier of the processor.\n",
    "        dataset_gcs_uri (str): This is a string representing the URI of the dataset in Google Cloud Storage.\n",
    "        project_id (str): This is a string representing the ID of the Document AI project.\n",
    "        \n",
    "    Returns:\n",
    "        Return value would likely be a JSON object containing information about the operation status or result.\n",
    "    \"\"\"\n",
    "    \n",
    "    tqdm.write(\"Add processor dataset\")\n",
    "    # first check if bucket of dataset_gcs_uri exists\n",
    "    create_destination_dataset_bucket(project_id, dataset_gcs_uri)\n",
    "    url = get_base_url(processor_name) + \"/dataset\"\n",
    "    headers = {'Authorization': f'Bearer {get_access_token()}'}\n",
    "    # dataset_string =         {'gcsManagedConfig': {'gcsPrefix': {'gcsUriPrefix': 'gs://bachir_test'}}\n",
    "    update_dataset_request = {'gcsManagedConfig': {'gcsPrefix': {'gcsUriPrefix': dataset_gcs_uri}} ,'spannerIndexingConfig':{}}\n",
    "    add_dataset_response = requests.patch(url, headers=headers, json=update_dataset_request)\n",
    "    add_dataset_response.raise_for_status()\n",
    "    add_dataset_result = get_operation_result(add_dataset_response.json()['name'])\n",
    "    return add_dataset_result\n",
    "\n",
    "def update_processor_dataset_schema(processor_name : str, schema : Dict[str,str]) -> Dict[str,str]:\n",
    "    \"\"\"\n",
    "    This function is responsible for updating the processor dataset schema in Document AI Project.\n",
    "    \n",
    "    Args:\n",
    "       processor_name (str) : This is a string representing the name or identifier of the processor.\n",
    "       schema (dictionary) : This is a dictionary containing the updated schema for the dataset.\n",
    "       \n",
    "    Returns:\n",
    "       Dictionary representing JSON data likely to have information about the status of the schema update.\n",
    "    \"\"\"\n",
    "    \n",
    "    tqdm.write(\"Updating processor dataset schema\")\n",
    "    url = get_base_url(processor_name) + \"/dataset/datasetSchema\"\n",
    "    headers = {'Authorization': f'Bearer {get_access_token()}'}\n",
    "    update_schema_response = requests.patch(url, headers=headers, json=schema)\n",
    "    update_schema_response.raise_for_status()\n",
    "    return update_schema_response.json()\n",
    "\n",
    "def get_dataset_split_stats(processor_name : str) -> Dict[str,str]:\n",
    "    \"\"\"\n",
    "    This function retrieves statistics about dataset splits associated with a processor in a Document AI project.\n",
    "    \n",
    "    Args:\n",
    "       processor_name (str) : This is a string representing the name or identifier of the processor.\n",
    "       \n",
    "    Returns:\n",
    "       Dictionary representing JSON data contains information about the dataset split statistics.\n",
    "    \"\"\"\n",
    "    \n",
    "    tqdm.write(\"Getting dataset split statistics\")\n",
    "    url = get_base_url(processor_name) + \"/dataset:getAllDatasetSplitStats\"\n",
    "    headers = {'Authorization': f'Bearer {get_access_token()}'}\n",
    "    get_dataset_split_stats_response = requests.get(url, headers=headers)\n",
    "    get_dataset_split_stats_response.raise_for_status()\n",
    "    return get_dataset_split_stats_response.json()\n",
    "\n",
    "def list_processor_dataset_documents(processor_name : str, page_size = 50 : int, next_page_token = None : str, dataset_split = None : str) -> Dict[str,str]:\n",
    "    \"\"\"\n",
    "    This function will list the processor dataset documents.\n",
    "    \n",
    "    Args:\n",
    "        processor_name (str) : This is a string representing the name or identifier of the processor.\n",
    "        page_size (int) : This parameter is optional and represents the number of documents to retrieve per page. If not provided, it defaults to 50.\n",
    "        next_page_token (str) : This parameter is optional and is used for pagination. It represents a token that indicates which page of results to retrieve next.\n",
    "        dataset_split (str) : This parameter is optional and represents a specific split of the dataset. If provided, it filters the documents based on this split type.\n",
    "        \n",
    "    Returns:\n",
    "        JSON content of the response which is about the listed documents.  \n",
    "    \"\"\"\n",
    "    \n",
    "    tqdm.write(\"List documents in processor dataset\")\n",
    "    document_metadata = []\n",
    "    url = get_base_url(processor_name) + \"/dataset:listDocuments\"\n",
    "    headers = {'Authorization': f'Bearer {get_access_token()}'}\n",
    "    list_documents_request = {}\n",
    "    if next_page_token:\n",
    "        list_documents_request['page_size'] = page_size\n",
    "        list_documents_request['page_token'] = next_page_token\n",
    "    else:\n",
    "        list_documents_request['page_size'] = page_size\n",
    "    if dataset_split:\n",
    "        list_documents_request['filter'] = f\"SplitType={dataset_split}\"\n",
    "    list_documents_response = requests.post(url, headers=headers, json=list_documents_request)\n",
    "    list_documents_response.raise_for_status()\n",
    "    return list_documents_response.json()\n",
    "\n",
    "def get_document(processor_name : str, document_metadata : Dict[str,str]) -> str:\n",
    "    \"\"\"\n",
    "    This function is used to extract a specific document from the corresponding processor name.\n",
    "    \n",
    "    Args:\n",
    "       processor_name (str): This is a string representing the name or identifier of the processor.\n",
    "       document_metadata (dictionary): This is a dictionary containing metadata about the document to retrieve.\n",
    "    \n",
    "    Returns:\n",
    "       A string representing document ID.\n",
    "    \"\"\"\n",
    "    \n",
    "    tqdm.write(\"Get document\")\n",
    "    url = get_base_url(processor_name) + \"/dataset:getDocument\"\n",
    "    headers = {'Authorization': f'Bearer {get_access_token()}'}\n",
    "    params = {'documentId.gcsManagedDocId.gcsUri': document_metadata['documentId']['gcsManagedDocId']['gcsUri']}\n",
    "    get_document_response = requests.get(url, headers=headers, params=params)\n",
    "    get_document_response.raise_for_status()\n",
    "    return get_document_response.json()['document']\n",
    "\n",
    "def upload_document(destination_dataset_gcs_uri : str, display_name : str, document : Dict[str,str]) -> str:\n",
    "    \"\"\"\n",
    "    This function is used to upload document into a temporary GCS bucket.\n",
    "    \n",
    "    Args:\n",
    "       destination_dataset_gcs_uri (str) : This is the GCS bucket path where the document will be copied over to the destination project dataset.\n",
    "       display_name (str) : This is a string which contains display name of the document.\n",
    "       document (dictionary) : This is a dictionary representing the content of the document.\n",
    "       \n",
    "    Returns:\n",
    "       A string representing the GCS URI.  \n",
    "    \"\"\"\n",
    "    \n",
    "    tqdm.write(f\"Upload document to temporary GCS import location\")\n",
    "    storage_client = storage.Client()\n",
    "    gcs_uri = destination_dataset_gcs_uri.strip('/') + '/import/' + display_name\n",
    "    blob = storage.Blob.from_string(gcs_uri, storage_client)\n",
    "    blob.upload_from_string(json.dumps(document), content_type='application/json')\n",
    "    return gcs_uri\n",
    "    \n",
    "def remove_imported_document(gcs_uri : str) -> None:\n",
    "    \"\"\"\n",
    "    This function is used to remove the imported documents from temporary bucket.\n",
    "    \n",
    "    Args:\n",
    "       gcs_uri (str) : This is the bucket from which documents needs to be removed.  \n",
    "    \"\"\"\n",
    "    \n",
    "    tqdm.write(\"Remove document from temporary GCS import location\")\n",
    "    storage_client = storage.Client()\n",
    "    blob = storage.Blob.from_string(gcs_uri, storage_client)\n",
    "    blob.delete()\n",
    "\n",
    "def migrate_documents(source_processor_name : str, destination_processor_name : str, destination_dataset_gcs_uri : str) -> None:\n",
    "    \"\"\"\n",
    "    This function is used to migrate documents from source processor to destination processor.\n",
    "    \n",
    "    Args:\n",
    "        source_processor_name (str) : This is a String containing the source processor name of Document AI project.\n",
    "        destination_processor_name (str) : This is a String containing the destination processor name of Document AI project.\n",
    "        destination_dataset_gcs_uri (str) : The GCS bucket path which is used for the destination processor dataset, automatically created if it does not exist.\n",
    "    \n",
    "    Raise:\n",
    "        ValueError : \"List document response is missing documentMetadata\"\n",
    "    \"\"\"\n",
    "    \n",
    "    get_dataset_split_stats_response = get_dataset_split_stats(source_processor_name)\n",
    "    total_documents = sum(dataset_split_stat.get('datasetStats',{}).get('documentCount',0) for dataset_split_stat in get_dataset_split_stats_response['splitStats'])\n",
    "    print(total_documents)\n",
    "    progress_bar = tqdm(total=total_documents,desc=\"Migrating documents\",unit=\"document(s)\")\n",
    "    print(f\"Migrating {total_documents} documents\")  \n",
    "    \n",
    "    counter = 0\n",
    "    s = set()\n",
    "    for dataset_split in [\"DATASET_SPLIT_TEST\",\"DATASET_SPLIT_TRAIN\",\"DATASET_SPLIT_UNASSIGNED\"]:\n",
    "        total_documents = sum(dataset_split_stat.get('datasetStats',{}).get('documentCount',0) for dataset_split_stat in get_dataset_split_stats_response['splitStats'] if dataset_split_stat.get('type',\"\") == dataset_split)\n",
    "        print(f\" Migrating {total_documents} documents of dataset split type {dataset_split}\")\n",
    "        next_page_token = None\n",
    "        while True:\n",
    "            out = Output()\n",
    "            display(out)\n",
    "            with out: \n",
    "                list_documents_response = list_processor_dataset_documents(source_processor_name, next_page_token = next_page_token, dataset_split = dataset_split)                         \n",
    "                clear_output()\n",
    "            if not list_documents_response:\n",
    "                break\n",
    "            if 'documentMetadata' in list_documents_response:\n",
    "                document_metadata_list = list_documents_response['documentMetadata']\n",
    "            else:\n",
    "                raise ValueError(\"List document response is missing documentMetadata\")\n",
    "            print(f\"  Migrating batch of {len(document_metadata_list)} documents\")\n",
    "            out = Output()\n",
    "            display(out)\n",
    "            with out: \n",
    "                gcs_documents = []\n",
    "                for document_metadata in document_metadata_list:\n",
    "                    document = get_document(source_processor_name, document_metadata)\n",
    "                    \n",
    "                    if document_metadata[\"displayName\"] not in s:\n",
    "                        gcs_uri = upload_document(destination_dataset_gcs_uri, document_metadata['displayName'], document)\n",
    "                        gcs_document = { 'gcsUri': gcs_uri, 'mimeType': \"application/json\" }\n",
    "                        gcs_documents.append(gcs_document) \n",
    "                        s.add(document_metadata[\"displayName\"])\n",
    "                    else:\n",
    "                        print(\"removed document as it is already present in the new processor\", dataset_split)\n",
    "                    counter += 1\n",
    "                    clear_output()\n",
    "            \n",
    "                import_document(destination_processor_name, gcs_documents, dataset_split)\n",
    "                for gcs_document in gcs_documents:\n",
    "                    try:\n",
    "                        remove_imported_document(gcs_document['gcsUri'])   \n",
    "                        clear_output()\n",
    "                    except:\n",
    "                        print(\"file removal error\")  \n",
    "                progress_bar.update(len(document_metadata_list))\n",
    "                try:\n",
    "                    next_page_token = list_documents_response['nextPageToken']\n",
    "                except KeyError:\n",
    "                    break\n",
    "                except:\n",
    "                    break\n",
    "            print(\"dataset_split \", dataset_split)\n",
    "            print(\"len(set)= \", len(s))\n",
    "        print(\"set = \", s)\n",
    "\n",
    "def train_processor(destination_processor_name : str,version_display_name : str) -> Dict[str,str]:\n",
    "    \"\"\"\n",
    "    This function is used to train the destination processor for the required version.\n",
    "    \n",
    "    Args:\n",
    "       destination_processor_name (str) : This is the name of the destination processor for which you want to train the processor.\n",
    "       version_display_name (str) : This is the name of the version for which you want to train the processsor.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of JSON data.\n",
    "    \"\"\"\n",
    "    \n",
    "    tqdm.write(\"Training Processor\")\n",
    "    url = get_base_url(destination_processor_name) + \"/processorVersions:train\"\n",
    "    headers = {'Authorization': f'Bearer {get_access_token()}'}\n",
    "    train_processor_request =  {'processorVersion': {'displayName': version_display_name}}\n",
    "    train_processor_response = requests.post(url, headers=headers, json=train_processor_request)\n",
    "    train_processor_response.raise_for_status()\n",
    "    return train_processor_response\n",
    "\n",
    "def deploy_processor(trained_processor_version : str) -> str:\n",
    "    \"\"\"\n",
    "    This function is used to deploy the processor after its usage inorder to avoid Quota Issues.\n",
    "    \n",
    "    Args:\n",
    "       trained_processor_version (str) : This is a string having trained processor version.\n",
    "       \n",
    "    Returns: \n",
    "       A String having the name of deployed processor.\n",
    "    \"\"\"\n",
    "    \n",
    "    tqdm.write(\"Deploying Processor\")\n",
    "    url = get_base_url(trained_processor_version) + \":deploy\"\n",
    "    headers = {'Authorization': f'Bearer {get_access_token()}'}\n",
    "    deploy_processor_response = requests.post(url, headers=headers)\n",
    "    deploy_processor_response.raise_for_status()\n",
    "    deploy_processor_result = get_operation_result(deploy_processor_response.json()['name'])\n",
    "    return deploy_processor_result\n",
    "\n",
    "def migrate_processor(source_processor_name : str, destination_project_id : str, destination_processor_location : str, destination_dataset_gcs_uri : str,kms_key_name : str) -> None:\n",
    "    \"\"\"\n",
    "    This is the main function which we need to run for migration of processor from one project to another.\n",
    "    \n",
    "    Args:\n",
    "       source_processor_name (str) : This is a String containing the source processor name of Document AI project.\n",
    "       destination_project_id (str) : This is a String containing the destination project ID of the destination project.\n",
    "       destination_processor_location (str) : This is a String containing the destination project processor location.\n",
    "       destination_dataset_gcs_uri (str) : The GCS bucket path which is used for the destination processor dataset, automatically created if it does not exist.\n",
    "       kms_key_name (str) : This is a string representing the Key Management Service (KMS) key name.\n",
    "    \"\"\"\n",
    "    \n",
    "    processor_details = get_processor_details(source_processor_name)\n",
    "    tqdm.write(f\"Migrating processor {processor_details['displayName']} of type {processor_details['type']}\")\n",
    "    destination_processor_name = create_processor(destination_project_id,destination_processor_location,processor_details)\n",
    "    tqdm.write(f\"Destination processor created with processor name {destination_processor_name}\")\n",
    "    add_processor_dataset(destination_processor_name, destination_dataset_gcs_uri, destination_project_id)\n",
    "    schema = get_processor_dataset_schema(source_processor_name)\n",
    "    update_processor_dataset_schema(destination_processor_name, schema)\n",
    "    create_destination_dataset_bucket(destination_project_id, DESTINATION_EXPORTED_DATASET_GCS_URI)\n",
    "    move_exported_dataset(SOURCE_EXPORTED_DATASET_GCS_URI, DESTINATION_EXPORTED_DATASET_GCS_URI)\n",
    "    import_document_by_type(destination_processor_name, gcs_documents_train, 'DATASET_SPLIT_TRAIN')\n",
    "    import_document_by_type(destination_processor_name, gcs_documents_test, 'DATASET_SPLIT_TEST') \n",
    "    \n",
    "    tqdm.write(f\"Link to UI of migrated processor dataset: https://console.cloud.google.com/ai/document-ai/{'/'.join(destination_processor_name.split('/')[2:])}/dataset?project={destination_project_id}\")\n",
    "    version_display_name = get_processor_version_details(SOURCE_PROCESSOR_NAME, processor_details['defaultProcessorVersion'])\n",
    "    trained_processor_response = train_processor(destination_processor_name,version_display_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f30546-ce3f-46cc-bd48-23c962767737",
   "metadata": {},
   "source": [
    "**NOTE**: To automatically deploy the processor upon completion of training, remove the comment symbol from the above line of code before executing            the final code. Alternatively, manually deploy the processor through the user interface once training is complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4e49dd-d70f-42a3-8ba2-1dc267fddaa6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 7. Execute the Processor Migration code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cad540-063b-4602-b875-469f1d25adf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = migrate_processor(SOURCE_PROCESSOR_NAME, DESTINATION_PROJECT_NUMBER, DESTINATION_PROCESSOR_LOCATION, DESTINATION_PROCESSOR_DATASET_GCS_URI, KMS_KEY_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cb3946-3c38-4518-a284-6668354dc4df",
   "metadata": {},
   "source": [
    "**NOTE**:  If you encounter a rate limiting error, go to the destination processor. You can find the destination processor ID in the output when you execute the above command. Then, in the destination processor, try to manually trigger the training. If there is any minimum criteria issue, fix the issue from the UI and trigger the training from the UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb67ffb4-0a84-4b74-8dfd-ff3422bb9417",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 8.OUTPUT:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ca1939-253b-4422-976d-7a7f510ba1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Getting processor details\n",
    "Migrating processor test_processor of type CUSTOM_EXTRACTION_PROCESSOR\n",
    "Create processor\n",
    "Destination processor created with processor name projects/XXXXXXXXX/locations/us/processors/XXXXXXXXXX\n",
    "Add processor dataset\n",
    "Waiting for operation to finish....\n",
    "Getting processor dataset schema\n",
    "Updating processor dataset schema\n",
    "date and time = 2023-01-10-15-41-17\n",
    ".............................................\n",
    "................................\n",
    ".......................\n",
    "Import document\n",
    "Waiting for operation to finish....\n",
    "Import document\n",
    "Waiting for operation to finish....\n",
    "Link to UI of migrated processor dataset: https://console.cloud.google.com/ai/document-ai/locations/us/processors/XXXXXXXXX/dataset?project=XXXXXXXX\n",
    "Getting processor version details\n",
    "Training Processor\n",
    "Waiting for operation to finish....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d1d389-2559-42f0-975a-9695043b390d",
   "metadata": {},
   "source": [
    "### This is the screenshot of source Project having certain schema.\n",
    "![Screenshot](https://screenshot.googleplex.com/AHfcddgxDLUPeuH.png)\n",
    "\n",
    "### This is the screenshot of destination Project having the same schema of original source project.\n",
    "![Screenshot](https://screenshot.googleplex.com/BvsZ26gH6BPxyaM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ea7c1d-cb74-42fb-9ca4-ea3d31e67a3d",
   "metadata": {},
   "source": [
    "##### The link in the output is where you can find the newly created processor and access it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716f6bb2-abe8-4312-92d5-e0e9b3d7761f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 9. Deploy the processor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed4dcb8-d006-4136-bb0b-d732f204f234",
   "metadata": {},
   "source": [
    "After the new processor has completed training, deploy it by navigating to the \"Manage versions\" tab in the user interface. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200470ae-1531-4720-a8e1-2b10d0061c4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Reference Links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bac3ef-98fd-4a7c-875a-f2ccea000a2f",
   "metadata": {},
   "source": [
    "Drive Link to IPYNB File : [DocAI_Processor_Migration.ipynb](https://drive.google.com/file/d/10GbjjQl56n79D9kj5GYbED3QPxO_ELB7/view?resourcekey=0-3qE82iFBnbX99AS8r_B3fg)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m104",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m104"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
