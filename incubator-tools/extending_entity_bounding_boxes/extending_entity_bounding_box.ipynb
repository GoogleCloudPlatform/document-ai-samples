{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6589fc93-39d1-4d10-be1f-e7eb33fe4087",
   "metadata": {},
   "source": [
    "# Extending Entity Bounding Box\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5bf22bf7-4a47-4f3a-9eef-6f19348a5250",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "361f188e-fe11-4a49-b7c8-080e0e69ce7a",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the DocAI Incubator Team. No guarantees of performance are implied. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1036937a-0221-48eb-862e-3fa0b8e646a8",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "This document guides how to maximize the bounding boxes of the entities using the parsed jsons and this updated documents can be used for training processor which covers maximum area of entities and gives better results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "115a4e82-5e83-468a-b0e5-097ca14f15d5",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "* Vertex AI Notebook Or Colab (If using Colab, use authentication)\n",
    "* Storage Bucket for storing input and output json files\n",
    "* Permission For Google Storage and Vertex AI Notebook.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe81de40-5c62-4c0b-adea-937f957b1a6e",
   "metadata": {},
   "source": [
    "## Step by Step procedure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "142123d3-37b1-4aa8-841c-40c3bd52d70c",
   "metadata": {},
   "source": [
    "### 1. Importing Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0643c5f9-29fe-4252-9e6c-e2afc8c2f2b8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pandas numpy google-cloud-storage google-cloud-documentai==2.16.0 PyPDF2 configparser\n",
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7588c13e-0e09-4a76-8c21-85a68ee262c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from google.cloud import storage\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "import signal\n",
    "import io\n",
    "from PIL import Image\n",
    "import json\n",
    "from typing import Any, Dict, List, Optional, Sequence, Tuple, Union\n",
    "from utilities import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd7c8c4c-68b8-413c-b4bc-c66f044d3b7a",
   "metadata": {},
   "source": [
    "### 2. Input and Output Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d9b3ca3-e486-4614-81c2-da8e1f695666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the GCS storage path of parsed jsons in the below input_path\n",
    "input_path = \"gs://xxxx/xxxx/xxx\"\n",
    "# Replace the GCS storage path where the updated jsons have to be stored\n",
    "output_path = \"gs://xxx/xxxxx/xxxx/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da0d4909-e00c-4704-a43b-6534f7403872",
   "metadata": {},
   "source": [
    "* `input_path` : GCS Input Path. It should contain DocAI processed output json files. \n",
    "* `output_path` : GCS Output Path. The updated jsons will be saved in output path. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "737d1c70-fef5-49e3-a266-695bf8076a54",
   "metadata": {},
   "source": [
    "### 3. Run the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c22bfdd-abdc-4d1c-8f7c-86164e7c4103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# functions\n",
    "def get_expanded_coordinates(\n",
    "    binary: np.ndarray,\n",
    "    bbox_coords: Tuple[Tuple[int, int], Tuple[int, int]],\n",
    "    open_cv_image: np.ndarray,\n",
    ") -> Tuple[Tuple[int, int], Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Expand the bounding box until black pixels are detected in the specified binary image.\n",
    "\n",
    "    Parameters:\n",
    "    - binary (numpy.ndarray): Binary image containing the object of interest.\n",
    "    - bbox_coords (tuple): Tuple containing the coordinates of the bounding box in the format ((x1, y1), (x2, y2)).\n",
    "    - open_cv_image (numpy.ndarray): Original image (in OpenCV format) for dimension reference.\n",
    "\n",
    "    Returns:\n",
    "    Tuple: Updated bounding box coordinates in the format ((new_x1, new_y1), (new_x2, new_y2)).\n",
    "    \"\"\"\n",
    "    x, y = bbox_coords[0][0], bbox_coords[0][1]\n",
    "    w, h = bbox_coords[2][0] - bbox_coords[0][0], bbox_coords[2][1] - bbox_coords[0][1]\n",
    "    # print(x,y,w,h)\n",
    "    # Initialize step size for scanning\n",
    "    step_size = 1  # pixel\n",
    "\n",
    "    # Initialize padding\n",
    "    padding = [3, 3, 3, 3]  # top, bottom, left, right\n",
    "\n",
    "    # Define the threshold for stopping (number of black pixels encountered)\n",
    "    black_pixel_threshold = 5  # This is an arbitrary threshold for when to stop\n",
    "\n",
    "    # Start the timeout counter (2 seconds)\n",
    "    signal.alarm(2)\n",
    "\n",
    "    class TimeoutException(Exception):\n",
    "        pass\n",
    "\n",
    "    # Timeout handler function\n",
    "    def timeout_handler(signum, frame):\n",
    "        raise TimeoutException\n",
    "\n",
    "    # Apply the timeout handler for the signal.SIGALRM\n",
    "    signal.signal(signal.SIGALRM, timeout_handler)\n",
    "\n",
    "    try:\n",
    "        # Expand the bounding box until black pixels are detected\n",
    "        # ... Insert the dynamic enlargement process here ...\n",
    "\n",
    "        # Expand the bounding box until black pixels are detected\n",
    "        for direction in [\"top\", \"bottom\", \"left\", \"right\"]:\n",
    "            expanded = False\n",
    "            while not expanded:\n",
    "                if direction == \"top\":\n",
    "                    scan_line = binary[max(y - padding[0] - step_size, 0), x : x + w]\n",
    "                    padding[0] += step_size\n",
    "                elif direction == \"bottom\":\n",
    "                    scan_line = binary[\n",
    "                        min(y + h + padding[1], open_cv_image.shape[0] - 1), x : x + w\n",
    "                    ]\n",
    "                    padding[1] += step_size\n",
    "                elif direction == \"left\":\n",
    "                    scan_line = binary[y : y + h, max(x - padding[2] - step_size, 0)]\n",
    "                    padding[2] += step_size\n",
    "                else:  # 'right'\n",
    "                    scan_line = binary[\n",
    "                        y : y + h, min(x + w + padding[3], open_cv_image.shape[1] - 1)\n",
    "                    ]\n",
    "                    padding[3] += step_size\n",
    "\n",
    "                # Check if black pixels exceed the threshold\n",
    "                if np.sum(scan_line == 0) > black_pixel_threshold:\n",
    "                    expanded = True\n",
    "\n",
    "        # Calculate the new bounding box with the added padding\n",
    "        x_new = max(x - padding[2], 0)\n",
    "        y_new = max(y - padding[0], 0)\n",
    "        w_new = min(w + padding[2] + padding[3], open_cv_image.shape[1] - x_new)\n",
    "        h_new = min(h + padding[0] + padding[1], open_cv_image.shape[0] - y_new)\n",
    "\n",
    "        # Draw the new bounding box on the image\n",
    "        # cv2.rectangle(image, (x_new, y_new), (x_new + w_new, y_new + h_new), (0, 0, 255), 2)\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(\"Processing entity {entity['pageAnchor']} took too long and was skipped.\")\n",
    "    finally:\n",
    "        # Cancel the alarm\n",
    "        signal.alarm(0)\n",
    "\n",
    "    # Calculate the new bounding box with the added padding\n",
    "    x_new = max(x - padding[2], 0)\n",
    "    y_new = max(y - padding[0], 0)\n",
    "    w_new = min(w + padding[2] + padding[3], open_cv_image.shape[1] - x_new)\n",
    "    h_new = min(h + padding[0] + padding[1], open_cv_image.shape[0] - y_new)\n",
    "\n",
    "    # Draw the new bounding box on the image\n",
    "    # cv2.rectangle(image, (x_new, y_new), (x_new + w_new, y_new + h_new), (0, 0, 255), 2)\n",
    "    return (x_new, y_new), (x_new + w_new, y_new + h_new)\n",
    "\n",
    "\n",
    "def create_image_bbox(\n",
    "    updated_coordinates: Dict[str, List[Tuple[Tuple[int, int], Tuple[int, int]]]],\n",
    "    p1: str,\n",
    "    open_cv_image_1: np.ndarray,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Draw rectangles on the input image based on the updated coordinates.\n",
    "\n",
    "    Parameters:\n",
    "    - updated_coordinates (dict): A dictionary containing 'extended' and 'actual' keys.\n",
    "        - 'extended': List of tuples representing extended bounding box coordinates.\n",
    "        - 'actual': List of tuples representing actual bounding box coordinates.\n",
    "    - p1 (str): Identifier for the image.\n",
    "    - open_cv_image_1 (numpy.ndarray): Input image in OpenCV format.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    for i in updated_coordinates[\"extended\"]:\n",
    "        # print(i)\n",
    "        cv2.rectangle(open_cv_image_1, i[0], i[1], (0, 0, 255), 2)\n",
    "\n",
    "    for actual_ords in updated_coordinates[\"actual\"]:\n",
    "        x_coords, y_coords = zip(*actual_ords)\n",
    "\n",
    "        # Find the minimum and maximum x and y coordinates\n",
    "        min_x = min(x_coords)\n",
    "        max_x = max(x_coords)\n",
    "        min_y = min(y_coords)\n",
    "        max_y = max(y_coords)\n",
    "        cv2.rectangle(open_cv_image_1, (min_x, min_y), (max_x, max_y), (255, 0, 0), 2)\n",
    "\n",
    "    cv2.imwrite(f\"paystub_{p1}.jpg\", open_cv_image_1)\n",
    "\n",
    "\n",
    "def update_extended_bbox(json_data: documentai.Document) -> documentai.Document:\n",
    "    \"\"\"\n",
    "    Update the extended bounding box coordinates in the input DocumentAI JSON data.\n",
    "\n",
    "    Parameters:\n",
    "    - json_data (documentai.Document): DocumentAI JSON data.\n",
    "\n",
    "    Returns:\n",
    "    documentai.Document: Updated DocumentAI JSON data with extended bounding box coordinates.\n",
    "    \"\"\"\n",
    "\n",
    "    x, y = create_pdf_bytes_from_json(documentai.Document.to_dict(json_data))\n",
    "\n",
    "    def extend_coordinates(entity: documentai.Document.Entity) -> None:\n",
    "        \"\"\"\n",
    "        Extend the bounding box coordinates of the given entity.\n",
    "\n",
    "        Parameters:\n",
    "        - entity (documentai.Document.Entity): DocumentAI entity.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        bbox_coords = []\n",
    "        bound_poly = entity.page_anchor.page_refs\n",
    "        coordinates_xy = bound_poly[0].bounding_poly.normalized_vertices\n",
    "        for i in coordinates_xy:\n",
    "            bbox_coords.append(\n",
    "                (int(i.x * open_cv_image.shape[1]), int(i.y * open_cv_image.shape[0]))\n",
    "            )\n",
    "        if len(bbox_coords) == 0:\n",
    "            return\n",
    "        page_ref = entity.page_anchor.page_refs\n",
    "        for p in page_ref:\n",
    "            page = p.page\n",
    "        if int(page) == p1:\n",
    "            sorted_bbox_coords = sorted(\n",
    "                bbox_coords, key=lambda coord: (coord[1], coord[0])\n",
    "            )\n",
    "            sorted_bbox_coords[-2], sorted_bbox_coords[-1] = (\n",
    "                sorted_bbox_coords[-1],\n",
    "                sorted_bbox_coords[-2],\n",
    "            )\n",
    "            extended_bbox = get_expanded_coordinates(\n",
    "                binary, sorted_bbox_coords, open_cv_image\n",
    "            )\n",
    "            xmin = (extended_bbox[0][0]) / open_cv_image.shape[1]\n",
    "            ymin = (extended_bbox[0][1]) / open_cv_image.shape[0]\n",
    "            xmax = (extended_bbox[1][0]) / open_cv_image.shape[1]\n",
    "            ymax = (extended_bbox[1][1]) / open_cv_image.shape[0]\n",
    "            coordinates_extended = [\n",
    "                {\"x\": xmin, \"y\": ymin},\n",
    "                {\"x\": xmax, \"y\": ymin},\n",
    "                {\"x\": xmax, \"y\": ymax},\n",
    "                {\"x\": xmin, \"y\": ymax},\n",
    "            ]\n",
    "            entity.page_anchor.page_refs[\n",
    "                0\n",
    "            ].bounding_poly.normalized_vertices = coordinates_extended\n",
    "\n",
    "    for p1 in range(len(y)):\n",
    "        open_cv_image_1 = np.array(y[p1].convert(\"RGB\"))\n",
    "        open_cv_image = np.array(y[p1].convert(\"RGB\"))\n",
    "        gray = cv2.cvtColor(open_cv_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        edges = cv2.Canny(gray, 50, 150)\n",
    "\n",
    "        # Dilate the edges to thicken them\n",
    "        dilated_edges = cv2.dilate(edges, (3, 3), iterations=1)\n",
    "\n",
    "        # Find the contours in the dilated edges\n",
    "        contours, _ = cv2.findContours(\n",
    "            dilated_edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
    "        )\n",
    "\n",
    "        # Draw the contours on the original image in black\n",
    "        for contour in contours:\n",
    "            cv2.drawContours(open_cv_image, [contour], -1, (0, 0, 0), 2)\n",
    "\n",
    "        _, binary = cv2.threshold(open_cv_image, 128, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        updated_coordinates = {\"actual\": [], \"extended\": []}\n",
    "\n",
    "        for entity in json_data.entities:\n",
    "            if len(entity.properties) > 0:\n",
    "                for subentity in entity.properties:\n",
    "                    extend_coordinates(subentity)\n",
    "                extend_coordinates(entity)\n",
    "            else:\n",
    "                extend_coordinates(entity)\n",
    "\n",
    "    return json_data\n",
    "\n",
    "\n",
    "# calling functions\n",
    "file_name_list, file_path_dict = file_names(input_path)\n",
    "\n",
    "for n in range(len(file_name_list)):\n",
    "    file_path = (\n",
    "        \"gs://\" + input_path.split(\"/\")[2] + \"/\" + file_path_dict[file_name_list[n]]\n",
    "    )\n",
    "    print(file_name_list[n])\n",
    "    json_data = documentai_json_proto_downloader(\n",
    "        file_path.split(\"/\")[2], (\"/\").join(file_path.split(\"/\")[3:])\n",
    "    )\n",
    "    json_updated = update_extended_bbox(json_data)\n",
    "    store_document_as_json(\n",
    "        documentai.Document.to_json(json_updated),\n",
    "        output_path.split(\"/\")[2],\n",
    "        (\"/\").join(output_path.split(\"/\")[3:]) + \"/\" + file_name_list[n],\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62aa873d-f9be-46dc-b188-23e0d4d95dd0",
   "metadata": {},
   "source": [
    "### 4.Output\n",
    "\n",
    "The bounding boxes of the entities will be enlarged to the nearest text or line.\n",
    "\n",
    "If we Visualize both the bounding box ..looks like below image\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./Images/output.png\" width=800 height=400></img>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51c49c85-f598-4974-9ec7-90d058e76ce6",
   "metadata": {},
   "source": [
    "### 5.Limitations\n",
    "* This script works better if we have defined boxes or lines in the form  like shown in the above image\n",
    "* If there multiple entities very close to each other , the script might not work as expected like below \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed228c63-2d08-48a8-9cc1-ac3a05de0795",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
