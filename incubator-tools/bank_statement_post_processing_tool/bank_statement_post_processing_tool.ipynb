{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c335127a-4cf6-4c9a-ae49-b5daa2e8ea0b",
   "metadata": {},
   "source": [
    "# Bank Statement Post-Processing Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed350af2-ecbd-4a50-9073-44b27f493c59",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a8065c-5783-46b7-a41a-6f30e7625f53",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the DocAI Incubator Team. No guarantees of performance are implied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fca907-6254-4962-aac6-01de009c906c",
   "metadata": {},
   "source": [
    "## Purpose and Description\n",
    "\n",
    "<p><span>This tool is designed to take bank statements from Google Cloud Storage (GCS) and parse them via a DocAI bank statement processor and post process the response from the parser (post processing as per gatless requirement in </span><span><a href=\"https://www.google.com/url?q=http://go/gateless-bank-statement-parser-project-specs&amp;sa=D&amp;source=editors&amp;ust=1704297767371296&amp;usg=AOvVaw3e5p8Z7cQPeaJM3pJ6G1EU\">project specs</a></span><span>) then provide the output in json format.</span></p>\n",
    "      <p><span></span></p>\n",
    "      <p><span>Below are the steps the tool will follow</span></p>\n",
    "      <p><span></span></p>\n",
    "      <ol  start=\"1\">\n",
    "         <li><span>Bank statements are parsed through the bank statement processor.</span></li>\n",
    "         <li ><span>Post-processing the response from the bank statement processor and saving the result in json format in the bucket.</span></li>\n",
    "         <li ><span><a>This scipt</a></span><span>&nbsp;includes the option to parse checks for the top three banks, where it is required to train a CDE model. </span></li>\n",
    "         <li ><span>Top Three Banks: WellsFargo, Bank Of America, Chase</span></li>\n",
    "      </ol>\n",
    "      <p><span></span></p>\n",
    "      <img alt=\"\" src=\"images/image2.png\" style=\"width: 1020.00px; height: 92.67px; \" title=\"\"></span></h1>\n",
    "      <p><span style=\"padding-left:25%\"><b>Bank Statement Parser working chart</b></span><span>&nbsp;</span></p>\n",
    "<div style=\"padding-bottom:30px\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f334e64-d249-4bd2-8699-4eec1be943a0",
   "metadata": {},
   "source": [
    "# Installation Guide\n",
    "This Bank Statement Parser is in a Python notebook script format which can be used in <b>Vertex AI JupyterLab Environment </b>. First, put the Bank Statement script in JupyterLab and then, put all the reference documents in a specific folder . Also, create or use an existing folder as an output folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7821eb9d-cd7b-4db4-828c-f1d16fd32191",
   "metadata": {},
   "source": [
    "# step 1 : Installing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c238e8e-e9bd-4bb4-8eec-1fffd3203ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install deepparse\n",
    "%pip install google.cloud\n",
    "%pip install dataclasses\n",
    "%pip install difflib\n",
    "%pip install pandas\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ea65cf-cca5-4a62-85cf-4c324b52f53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41097325-6a38-4be2-925b-b09531de6706",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step by Step procedure\n",
    "<div style=\"background-color:#f5f569\"><b>NOTE:</b> Stepwise status can be seen in “logging.txt”</div><br>\n",
    "\n",
    " <img alt=\"\" src=\"images/image5.png\" style=\"width: 817.50px; height: 553.35px; \" title=\"\">\n",
    "      <p><span style=\"padding-left:25%\"><b>Bank Statement Parser logging.txt</b></span></p>\n",
    "   <h3> <b>Step 1:</b> Create a <span>Bank Statement Parser from Processor Gallery (Workbench).</span></h3><br>\n",
    "   <h4> Step 1.2:<h4> Make pretrained-bankstatement-v3.0-2022-05-16<span>&nbsp;as a default Processor Version.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f237c75-b139-4d4d-a47a-0017e9fed4b3",
   "metadata": {},
   "source": [
    "<h3><b>Step 3 : Input Parameters </b></h3><span> &nbsp; Fill the details in for </span><span>Project</span><span>&nbsp;and GCS folder </span><span>e</span><span>nter the path of the </span><span>input files</span><span>&nbsp;in the Code.</span></h3>\n",
    "     <img alt=\"\" src=\"./images/image4.png\" style=\"width: 720.00px; height: 138.67px; margin-left: 0.00px\" title=\"\">\n",
    "      <ul >\n",
    "         <li ><span style=\"font-weight:800\">Project name:</span><span>&nbsp;Enter the google cloud project name</span></li>\n",
    "         <li ><span style=\"font-weight:800\">Project_Id</span><span>: Enter the google cloud project id</span></li>\n",
    "         <li ><span style=\"font-weight:800\">Processor_Id:</span><span>&nbsp;Enter the bank statement processor id</span></li>\n",
    "         <li ><span style=\"font-weight:800\">gcs_input_dir: </span><span>Enter the path of files which have to be parsed</span></li>\n",
    "         <li ><span style=\"font-weight:800\">gcs_output_dir: </span><span>Enter the path of files where you want to save the output jsons after processing the files to bank statement parser</span></li>\n",
    "         <li ><span style=\"font-weight:800\">gcs_new_output_json_path: </span><span>Enter the path where the post processed json files have to be saved</span></li>\n",
    "      </ul>\n",
    "      <p><span></span></p>\n",
    "      <p><span>If Checks has to be parsed then fill the below details of CDE Trained processor</span></p>\n",
    "      <p><span></span></p>\n",
    "      <ul >\n",
    "         <li ><span style=\"font-weight:800\">checksFlag</span><span>: </span><span>True</span></li>\n",
    "      </ul>\n",
    "      <p><span>Update the checksFlag as True if you need checks to be parsed thru cde trained parser else it can be marked as </span><span>False</span></p>\n",
    "      <p><span>Fill the below details only &nbsp;the checksFlag is TRUE else not needed</span></p>\n",
    "      <p><span></span></p>\n",
    "      <ul >\n",
    "         <li ><span style=\"font-weight:800\">Processor_id_checks: </span><span>Enter the CDE trained processor id</span></li>\n",
    "         <li ><span style=\"font-weight:800\">Processor_version_checks: </span><span>Enter the CDE trained processor version</span></li>\n",
    "      </ul>\n",
    "<div style=\"padding-bottom:30px\"></div>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1b8a545-e631-45af-a8db-738fe2d9fc6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# input details\n",
    "project_name = \"xxxxxxx\"  # project name\n",
    "project_id = \"xxxxxxxx\"  # project number\n",
    "processor_id = \"xxxxxxxxxx\"  # processor id\n",
    "gcs_input_dir = \"gs://xxxxxxx/xxxxxxx/xxxxxxxx/input_pdfs\"  # input documents path\n",
    "gcs_output_dir = \"gs://xxxxxxx/xxxxxxx/xxxxxxxxxx/processor_output\"  # output documents for async parsing, suggested to use a diff bucket than ‘gcs_input_dir’\n",
    "gcs_new_output_json_path = \"gs://xxxxxx/xxxxxx/xxxxxx/pp_output/\"  # post process json path, , suggested to use a diff bucket than ‘gcs_input_dir’\n",
    "### To Parse Checks Table Items, Please Train a CDE Model , and provide CDE processorID and processorVersionID below, and set checksFlag=True\n",
    "checksFlag = (\n",
    "    True  # Checks Flag, if True, It will use CDE Model to parse the checks table\n",
    ")\n",
    "processor_id_checks = \"xxxxxxx\"  # CDE processor_id\n",
    "processor_version_checks = \"xxxxxx\"  # CDE processor_version_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4b6fea-1d55-450e-ac3d-714dba7285d9",
   "metadata": {},
   "source": [
    "\n",
    "<h3 ><b>Step 4 :</b></h3><p><span> Processing and Post processing the documents[run these cells without editing anything]</span></p>\n",
    "<img alt=\"\" src=\"images/image1.png\" style=\"width: 861.50px; height: 580.27px;\" title=\"\"><br>\n",
    "      <p><span>If consolidated CSV is needed please uncomment the below area for CSV generation.</span></p>\n",
    "      <img alt=\"\" src=\"images/image10.png\" style=\"width: 1020.00px; height: 202.00px;\" title=\"\">\n",
    "<div style=\"padding-bottom:30px\"></div>\n",
    "    \n",
    "    \n",
    " <p><span>Cheques Entity Detection:(currently considered only for top 3 banks)</span></p>\n",
    "      <p><span></span></p>\n",
    "      <ol start=\"1\">\n",
    "         <li ><span>Trained a CDE model for cheque entity detection , which further needs to be post processed and combined with the bank statement parser post processed json file.</span></li>\n",
    "      </ol>\n",
    "      <p><span></span></p>\n",
    "      <ol start=\"2\">\n",
    "         <li ><span>We has to train a CDE model as below: [</span><span>Refer</span><span><a href=\"https://www.google.com/url?q=https://cloud.google.com/document-ai/docs/workbench/build-custom-processor&amp;sa=D&amp;source=editors&amp;ust=1704297767379409&amp;usg=AOvVaw02t9j3qCz7KaDInejsTV3s\">&nbsp;DocAI Workbench CDE Guide</a></span><span>&nbsp;to setup a CDE processor</span><span>]</span></li>\n",
    "      </ol>\n",
    "      <ul >\n",
    "         <li ><span>Create a CDE parser with the below schema</span></li>\n",
    "         <li ><span>Select and Label documents which have Check details. Use the below convention and train the processor using those documents. Example Training instructions (illustration) given below.</span></li>\n",
    "      </ul>\n",
    "      <p><span style=\"background-color:#f5f569\">Incubator Implementation Notes:</span></p>\n",
    "      <ul style=\"padding-left:50px\">\n",
    "         <li ><span>For Check Description (check_desc), Incubator Team didn&rsquo;t have any sample, so disabled that entity while training the CDE model.</span></li>\n",
    "         <li ><span>For the POC, 45 Training and 15 Test Documents were used.</span></li>\n",
    "         <li ><span>Getting ~90 percent accuracy for the top 3 banks.</span></li>\n",
    "      </ul>\n",
    "      <p><span>Schema:</span></p>\n",
    "      <table style=\"border: 1px solid black;padding:0px; margin:0px\">\n",
    "         <tr style=\"border: 1px solid black; font-weight:700;\">\n",
    "            <td  style=\"border: 1px solid black;\" colspan=\"1\" rowspan=\"1\">\n",
    "               <p><span>#</span></p>\n",
    "            </td>\n",
    "            <td  style=\"border: 1px solid black;\" colspan=\"1\" rowspan=\"1\">\n",
    "               <p><span>Description of the item</span></p>\n",
    "            </td>\n",
    "            <td  style=\"border: 1px solid black;\" colspan=\"1\" rowspan=\"1\">\n",
    "               <p><span>Entity name</span></p>\n",
    "            </td>\n",
    "            <td  style=\"border: 1px solid black;\" colspan=\"1\" rowspan=\"1\">\n",
    "               <p><span>Occurrence Type</span></p>\n",
    "            </td>\n",
    "         </tr>\n",
    "         <tr style=\"border: 1px solid black;\">\n",
    "            <td  style=\"border: 1px solid black;\" colspan=\"1\" rowspan=\"1\">\n",
    "               <p><span>1. </span></p>\n",
    "            </td>\n",
    "            <td  style=\"border: 1px solid black;\" colspan=\"1\" rowspan=\"1\">\n",
    "               <p><span>Total line item (includes Check Number, check date and check amount, check description) &nbsp;</span><span>[</span><span>Parent</span><span>]</span></p>\n",
    "            </td>\n",
    "            <td  style=\"border: 1px solid black;\" colspan=\"1\" rowspan=\"1\">\n",
    "               <p><span>check_item</span></p>\n",
    "            </td>\n",
    "            <td  style=\"border: 1px solid black;\" colspan=\"1\" rowspan=\"1\">\n",
    "               <p><span>Optional multiple</span></p>\n",
    "            </td>\n",
    "         </tr>\n",
    "         <tr style=\"border: 1px solid black;\">\n",
    "            <td  style=\"border: 1px solid black;\" colspan=\"1\" rowspan=\"1\">\n",
    "               <p><span>2.</span></p>\n",
    "            </td>\n",
    "            <td  style=\"border: 1px solid black;\" colspan=\"1\" rowspan=\"1\">\n",
    "               <p><span>Check number [child]</span></p>\n",
    "            </td>\n",
    "            <td  style=\"border: 1px solid black;\" colspan=\"1\" rowspan=\"1\">\n",
    "               <p><span>check_number</span></p>\n",
    "            </td>\n",
    "            <td  style=\"border: 1px solid black;\" colspan=\"1\" rowspan=\"1\">\n",
    "               <p><span>Optional once</span></p>\n",
    "            </td>\n",
    "         </tr>\n",
    "         <tr style=\"border: 1px solid black;\">\n",
    "            <td  style=\"border: 1px solid black;\" colspan=\"1\" rowspan=\"1\">\n",
    "               <p><span>3.</span></p>\n",
    "            </td>\n",
    "            <td  style=\"border: 1px solid black;\" colspan=\"1\" rowspan=\"1\">\n",
    "               <p><span>Check date [child]</span></p>\n",
    "            </td>\n",
    "            <td  style=\"border: 1px solid black;\" colspan=\"1\" rowspan=\"1\">\n",
    "               <p><span>check_date</span></p>\n",
    "            </td>\n",
    "            <td  style=\"border: 1px solid black;\" colspan=\"1\" rowspan=\"1\">\n",
    "               <p><span>Optional once</span></p>\n",
    "            </td>\n",
    "         </tr>\n",
    "         <tr style=\"border: 1px solid black;\">\n",
    "            <td  style=\"border: 1px solid black;\" colspan=\"1\" rowspan=\"1\">\n",
    "               <p><span>4</span></p>\n",
    "            </td>\n",
    "            <td  style=\"border: 1px solid black;\" colspan=\"1\" rowspan=\"1\">\n",
    "               <p><span>Check amount [child]</span></p>\n",
    "            </td>\n",
    "            <td  style=\"border: 1px solid black;\" colspan=\"1\" rowspan=\"1\">\n",
    "               <p><span>check_amount</span></p>\n",
    "            </td>\n",
    "            <td  style=\"border: 1px solid black;\" colspan=\"1\" rowspan=\"1\">\n",
    "               <p><span>Optional once</span></p>\n",
    "            </td>\n",
    "         </tr>\n",
    "         <tr style=\"border: 1px solid black;\">\n",
    "            <td  style=\"border: 1px solid black;\" colspan=\"1\" rowspan=\"1\">\n",
    "               <p><span>5.</span></p>\n",
    "            </td>\n",
    "            <td  style=\"border: 1px solid black;\" colspan=\"1\" rowspan=\"1\">\n",
    "               <p><span>Check Description [child]</span></p>\n",
    "            </td>\n",
    "            <td  style=\"border: 1px solid black;\" colspan=\"1\" rowspan=\"1\">\n",
    "               <p><span>check_desc</span></p>\n",
    "            </td>\n",
    "            <td  style=\"border: 1px solid black;\" colspan=\"1\" rowspan=\"1\">\n",
    "               <p><span>Optional once</span></p>\n",
    "            </td>\n",
    "         </tr>\n",
    "      </table>\n",
    "      <p><span style=\"overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 667.33px; height: 265.00px;\"><img alt=\"\" src=\"images/image6.png\" style=\"width: 667.33px; height: 332.54px; margin-left: 0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);\" title=\"\"></span></p>\n",
    "      <p><span style=\"overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 667.36px; height: 253.50px;\"><img alt=\"\" src=\"images/image7.png\" style=\"width: 754.86px; height: 477.23px; margin-left: -87.50px; margin-top: -88.34px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);\" title=\"\"></span></p>\n",
    "      <p><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* </span><span>Blue</span><span>&nbsp;boxes are the labeled bounding boxes</span></p>\n",
    "      <ul class=\"c22 lst-kix_yqix1oj3s0ix-0 start\">\n",
    "         <li ><span>The Trained processor will detect the check entities but with the characteristic of detecting the whole row as a parent item(check_item) , if there are multiple tables (horizontally stacked).</span></li>\n",
    "      </ul>\n",
    "      <p><span style=\"overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 733.55px; height: 189.71px;\"><img alt=\"\" src=\"images/image3.png\" style=\"width: 758.84px; height: 189.71px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);\" title=\"\"></span></p>\n",
    "      <p><span></span></p>\n",
    "      <p><span>The above issue is taken care of in the post processing code and below is the output after post processing code.</span></p>\n",
    "      <p><span></span></p>\n",
    "      <p><span style=\"overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 734.00px; height: 426.60px;\"><img alt=\"\" src=\"./images/image8.png\" style=\"width: 1030.38px; height: 519.40px; margin-left: 0.00px; margin-top: -68.40px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);\" title=\"\"></span></p>\n",
    "      <p><span></span></p>\n",
    "      <p><span></span></p>\n",
    "      <p><span></span></p>\n",
    "      <ol class=\"c22 lst-kix_uwprj8q9rdjm-0\" start=\"3\">\n",
    "         <li ><span>The Post processing code for modifying the CDE output and combining with the Bank statement parser post processing json is given below</span></li>\n",
    "      </ol>\n",
    "<div style=\"padding-bottom:30px\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db54aeac-5f62-4387-ac16-06d1f5a75295",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "from dataclasses import dataclass\n",
    "from difflib import SequenceMatcher\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "from deepparse.parser import AddressParser\n",
    "from datetime import datetime\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from typing import Any, Dict, List, Optional, Sequence, Tuple, Union\n",
    "from utilities import (\n",
    "    file_names,\n",
    "    documentai_json_proto_downloader,\n",
    "    copy_blob,\n",
    "    process_document_sample,\n",
    "    store_document_as_json,\n",
    "    batch_process_documents_sample,\n",
    "    blob_downloader,\n",
    "    create_pdf_bytes_from_json,\n",
    ")\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# finds maximum id\n",
    "def maxIdFinder(jsonData: documentai.Document) -> int:\n",
    "    \"\"\"\n",
    "    Function to get the maximum id from the entity id attribute..\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    jsonData : documentai.Document\n",
    "            The document proto having all the entities with the id attribute.\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        Returns the maximum id.\n",
    "    \"\"\"\n",
    "    global maxId\n",
    "\n",
    "    allEntities = jsonData.entities\n",
    "    noOfEntitiesInJsonFile = len(allEntities)\n",
    "    jsonDict = {\n",
    "        \"confidence\": [],\n",
    "        \"id\": [],\n",
    "        \"mention_text\": [],\n",
    "        \"normalized_value\": [],\n",
    "        \"page_anchor\": [],\n",
    "        \"text_anchor\": [],\n",
    "        \"type\": [],\n",
    "    }\n",
    "    entitiesArray = []\n",
    "\n",
    "    for i in range(0, noOfEntitiesInJsonFile):\n",
    "        try:\n",
    "            if allEntities[i].id:\n",
    "                entitiesArray.append(allEntities[i])\n",
    "        except:\n",
    "            for j in allEntities[i].properties:\n",
    "                entitiesArray.append(j)\n",
    "\n",
    "    entitiesArray = sorted(entitiesArray, key=lambda x: x.id)\n",
    "\n",
    "    for i in range(0, len(entitiesArray)):\n",
    "        try:\n",
    "            jsonDict[\"confidence\"].append(entitiesArray[i].confidence)\n",
    "        except:\n",
    "            jsonDict[\"confidence\"] = 0\n",
    "        try:\n",
    "            jsonDict[\"id\"].append(entitiesArray[i].id)\n",
    "        except:\n",
    "            jsonDict[\"id\"].append(\"\")\n",
    "        try:\n",
    "            jsonDict[\"mention_text\"].append(entitiesArray[i].mention_text)\n",
    "        except:\n",
    "            jsonDict[\"mention_text\"].append(\"\")\n",
    "        try:\n",
    "            jsonDict[\"normalized_value\"].append(entitiesArray[i].normalized_value)\n",
    "        except:\n",
    "            jsonDict[\"normalized_value\"].append(\"\")\n",
    "        try:\n",
    "            jsonDict[\"page_anchor\"].append(entitiesArray[i].page_anchor)\n",
    "        except:\n",
    "            jsonDict[\"page_anchor\"].append(\"\")\n",
    "        try:\n",
    "            jsonDict[\"text_anchor\"].append(entitiesArray[i].text_anchor)\n",
    "        except:\n",
    "            jsonDict[\"text_anchor\"].append(\"\")\n",
    "        try:\n",
    "            jsonDict[\"type\"].append(entitiesArray[i].type)\n",
    "        except:\n",
    "            jsonDict[\"type\"].append(\"\")\n",
    "\n",
    "    tempList = []\n",
    "    for i in jsonDict[\"id\"]:\n",
    "        tempList.append(int(i))\n",
    "    maxId = max(tempList)\n",
    "    return maxId\n",
    "\n",
    "\n",
    "def delete_folder(bucket_name: str, folder_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Function to delete the folder in  a given bucket.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bucket_name : str\n",
    "            The bucket name where all the folder are stored.\n",
    "    folder_name : str\n",
    "            The folder name which needs to be removed.\n",
    "    \"\"\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    \"\"\"Delete object under folder\"\"\"\n",
    "    blobs = list(bucket.list_blobs(prefix=folder_name))\n",
    "    bucket.delete_blobs(blobs)\n",
    "    print(f\"Folder {folder_name} deleted.\")\n",
    "\n",
    "\n",
    "def get_files_not_parsed(gcs_input_dir: str, gcs_output_dir: str) -> Tuple:\n",
    "    \"\"\"\n",
    "    Function to get the file which are not processed by the processor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gcs_input_dir : str\n",
    "            The gcs path where the original documents(PDFs) are stored.\n",
    "    gcs_output_dir : str\n",
    "            The gcp path to store the output from the processor.\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple\n",
    "        Returns the Tuple with values of temporary folder name, temporary bucket name and temporary initial path\n",
    "    \"\"\"\n",
    "    now = datetime.now()\n",
    "    OutputDirPrefix = now.strftime(\"%H%M%S%d%m%Y\")\n",
    "    pdfs_names_list, pdfs_names_dict_1 = file_names(gcs_input_dir)\n",
    "    Jsons_names_list, Jsons_names_dict_1 = file_names(gcs_output_dir)\n",
    "    file_name_dict = {a.split(\".\")[0]: a for a in pdfs_names_list}\n",
    "    json_name_dict = {a.split(\".\")[0]: a for a in Jsons_names_list}\n",
    "    files_list = list(file_name_dict.keys())\n",
    "    list_json_name_dict = list(json_name_dict.keys())\n",
    "    dict_json = {}\n",
    "    for i in range(len(list_json_name_dict)):\n",
    "        if list_json_name_dict[i].endswith(\"-0\"):\n",
    "            dict_json[(list_json_name_dict[i][:-2])] = list_json_name_dict[i]\n",
    "        else:\n",
    "            dict_json[(list_json_name_dict[i])] = list_json_name_dict[i]\n",
    "    temp_bucket = gcs_input_dir.split(\"/\")[2]\n",
    "    storage_client = storage.Client()\n",
    "    source_bucket = storage_client.get_bucket(temp_bucket)\n",
    "    list_new = []\n",
    "    for i in range(len(files_list)):\n",
    "        if files_list[i] in dict_json.keys():\n",
    "            print(\n",
    "                \" Processed json file already exists for:{} \".format(\n",
    "                    file_name_dict[files_list[i]]\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            list_new.append(files_list[i])\n",
    "            source_blob = source_bucket.blob(file_name_dict[files_list[i]])\n",
    "            # print(file_name_dict[files_list[i]])\n",
    "            temp = f\"{file_name_dict[files_list[i]]}\"\n",
    "            file_name_temp = pdfs_names_dict_1[temp]\n",
    "            prefix = (\n",
    "                gcs_input_dir.split(\"/\")[-1]\n",
    "                + \"/\"\n",
    "                + \"temp_\"\n",
    "                + f\"{OutputDirPrefix}\"\n",
    "                + \"/\"\n",
    "                + temp\n",
    "            )\n",
    "            # new_blob = source_bucket.copy_blob(source_blob, destination_bucket, filename[i])\n",
    "            copy_blob(temp_bucket, file_name_temp, temp_bucket, prefix)\n",
    "    temp_initial_path = (\n",
    "        \"gs://\"\n",
    "        + temp_bucket\n",
    "        + \"/\"\n",
    "        + gcs_input_dir.split(\"/\")[-1]\n",
    "        + \"/\"\n",
    "        + \"temp_\"\n",
    "        + f\"{OutputDirPrefix}\"\n",
    "    )\n",
    "    temp_folder = (\n",
    "        (\"/\").join(gcs_input_dir.split(\"/\")[3:])\n",
    "        + \"/\"\n",
    "        + \"temp_\"\n",
    "        + f\"{OutputDirPrefix}\"\n",
    "        + \"/\"\n",
    "    )\n",
    "    return temp_initial_path, temp_folder, temp_bucket\n",
    "\n",
    "\n",
    "def get_files_not_postparsed(\n",
    "    gcs_output_dir: str, gcs_new_output_json_path: str\n",
    ") -> Tuple:\n",
    "    \"\"\"\n",
    "    Function to get the file which are not processed by the script.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gcs_output_dir : str\n",
    "            The gcs path where the processed documents are available which are already been parsed by processor.\n",
    "    gcs_new_output_json_path : str\n",
    "            The gcp path to store the output from this script.\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple\n",
    "        Returns the Tuple with values of temporary folder name, temporary bucket name and temporary initial path\n",
    "    \"\"\"\n",
    "    now = datetime.now()\n",
    "    OutputDirPrefix = now.strftime(\"%H%M%S%d%m%Y\")\n",
    "    pdfs_names_list, pdfs_names_dict_1 = file_names(gcs_output_dir)\n",
    "    Jsons_names_list, Jsons_names_dict_1 = file_names(gcs_new_output_json_path)\n",
    "    file_name_dict = {a.split(\".\")[0]: a for a in pdfs_names_list}\n",
    "    json_name_dict = {a.split(\".\")[0]: a for a in Jsons_names_list}\n",
    "    files_list = list(file_name_dict.keys())\n",
    "    temp_bucket = gcs_output_dir.split(\"/\")[2]\n",
    "    storage_client = storage.Client()\n",
    "    source_bucket = storage_client.get_bucket(temp_bucket)\n",
    "    list_new = []\n",
    "    for i in range(len(files_list)):\n",
    "        if files_list[i] in json_name_dict.keys():\n",
    "            print(\n",
    "                \" Processed json file already exists for:{} \".format(\n",
    "                    file_name_dict[files_list[i]]\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            list_new.append(files_list[i])\n",
    "            source_blob = source_bucket.blob(file_name_dict[files_list[i]])\n",
    "            # print(file_name_dict[files_list[i]])\n",
    "            temp = f\"{file_name_dict[files_list[i]]}\"\n",
    "            file_name_temp = pdfs_names_dict_1[temp]\n",
    "            prefix = (\n",
    "                gcs_output_dir.split(\"/\")[-1]\n",
    "                + \"/\"\n",
    "                + \"temp_\"\n",
    "                + f\"{OutputDirPrefix}\"\n",
    "                + \"/\"\n",
    "                + temp\n",
    "            )\n",
    "            # new_blob = source_bucket.copy_blob(source_blob, destination_bucket, filename[i])\n",
    "            copy_blob(temp_bucket, file_name_temp, temp_bucket, prefix)\n",
    "    temp_initial_path = (\n",
    "        \"gs://\"\n",
    "        + temp_bucket\n",
    "        + \"/\"\n",
    "        + gcs_output_dir.split(\"/\")[-1]\n",
    "        + \"/\"\n",
    "        + \"temp_\"\n",
    "        + f\"{OutputDirPrefix}\"\n",
    "    )\n",
    "    temp_folder = (\n",
    "        (\"/\").join(gcs_output_dir.split(\"/\")[3:])\n",
    "        + \"/\"\n",
    "        + \"temp_\"\n",
    "        + f\"{OutputDirPrefix}\"\n",
    "        + \"/\"\n",
    "    )\n",
    "    return temp_initial_path, temp_folder, temp_bucket\n",
    "\n",
    "\n",
    "# dictionary for entity renaming\n",
    "dict_ent_rename = {\n",
    "    \"statement_start_date\": \"Statement_Start_Date\",\n",
    "    \"statement_end_date\": \"Statement_End_Date\",\n",
    "    \"bank_name\": \"Financial_Institution\",\n",
    "}\n",
    "\n",
    "\n",
    "def accounttype_change(json_data: documentai.Document) -> documentai.Document:\n",
    "    \"\"\"\n",
    "    Function is for  comparing sequences for the account entity (account_type, account number) and update the entity name.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    document :documentai.Document\n",
    "            The document proto having all the entities\n",
    "    Returns\n",
    "    -------\n",
    "    documentai.Document\n",
    "        Returns the updated document proto .\n",
    "    \"\"\"\n",
    "    import difflib\n",
    "    from difflib import SequenceMatcher\n",
    "\n",
    "    accountnodict = {}\n",
    "    accountnamedict = {}\n",
    "\n",
    "    def detials_account(account_type):\n",
    "        account_dict_lst = []\n",
    "        for i in range(len(json_data.entities)):\n",
    "            if not hasattr(json_data.entities[i], \"properites\"):\n",
    "                if (\n",
    "                    difflib.SequenceMatcher(\n",
    "                        None, json_data.entities[i].type, account_type\n",
    "                    ).ratio()\n",
    "                    >= 0.9\n",
    "                ):\n",
    "                    try:\n",
    "                        id1 = json_data.entities[i].id\n",
    "                    except:\n",
    "                        id1 = \"\"\n",
    "                    try:\n",
    "                        page1 = json_data.entities[i].page_anchor.page_refs[0].page\n",
    "                    except:\n",
    "                        page1 = 0\n",
    "                    try:\n",
    "                        textSegments1 = json_data.entities[i].text_anchor.text_segments[\n",
    "                            0\n",
    "                        ]\n",
    "                    except:\n",
    "                        textSegments1 = \"\"\n",
    "                    try:\n",
    "                        temp_y_list = []\n",
    "                        temp_x_list = []\n",
    "                        for j in (\n",
    "                            json_data.entities[i]\n",
    "                            .page_anchor.page_refs[0]\n",
    "                            .bounding_poly.normalized_vertices\n",
    "                        ):\n",
    "                            temp_y_list.append(float(j.y))\n",
    "                        for j in (\n",
    "                            json_data.entities[i]\n",
    "                            .page_anchor.page_refs[0]\n",
    "                            .bounding_poly.normalized_vertices\n",
    "                        ):\n",
    "                            temp_x_list.append(float(j.x))\n",
    "                        x_max1 = max(temp_x_list)\n",
    "                        y_max1 = max(temp_y_list)\n",
    "\n",
    "                    except:\n",
    "                        y_max1 = \"\"\n",
    "                        x_max1 = \"\"\n",
    "                    account_dict_lst.append(\n",
    "                        {\n",
    "                            json_data.entities[i].mention_text: {\n",
    "                                \"id\": id1,\n",
    "                                \"page\": page1,\n",
    "                                \"text_segments\": textSegments1,\n",
    "                                \"x_max\": x_max1,\n",
    "                                \"y_max\": y_max1,\n",
    "                            }\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        return account_dict_lst\n",
    "\n",
    "    accountnamedict = detials_account(\"account_type\")\n",
    "    accountnodict = detials_account(\"account_i_number\")\n",
    "    accountnamedict\n",
    "    temp_del = []\n",
    "    for i in range(len(accountnamedict)):\n",
    "        for k in accountnamedict[i]:\n",
    "            if re.search(\"\\sstatement\", k, re.IGNORECASE):\n",
    "                temp_del.append(k)\n",
    "    for i in range(len(accountnamedict)):\n",
    "        try:\n",
    "            for k in accountnamedict[i]:\n",
    "                for m in temp_del:\n",
    "                    if k == m:\n",
    "                        del accountnamedict[i]\n",
    "        except:\n",
    "            pass\n",
    "    account_comp = []\n",
    "    for i in range(len(accountnamedict)):\n",
    "        for j in range(len(accountnodict)):\n",
    "            for k in accountnamedict[i]:\n",
    "                for m in accountnodict[j]:\n",
    "                    y_diff = abs(\n",
    "                        accountnamedict[i][k][\"y_max\"] - accountnodict[j][m][\"y_max\"]\n",
    "                    )\n",
    "                    account_comp.append({k: {m: y_diff}})\n",
    "    final_account_match = {}\n",
    "    for i in range(len(account_comp)):\n",
    "        for j in account_comp[i]:\n",
    "            for k in account_comp[i][j]:\n",
    "                if j.lower() not in final_account_match.keys():\n",
    "                    final_account_match[j.lower()] = {k: account_comp[i][j][k]}\n",
    "                else:\n",
    "                    for m in final_account_match:\n",
    "                        for n in final_account_match[m]:\n",
    "                            if j.lower() == m.lower():\n",
    "                                if account_comp[i][j][k] < final_account_match[m][n]:\n",
    "                                    final_account_match[j.lower()] = {\n",
    "                                        k: account_comp[i][j][k]\n",
    "                                    }\n",
    "                                else:\n",
    "                                    final_account_match[j.lower()] = {\n",
    "                                        n: final_account_match[m][n]\n",
    "                                    }\n",
    "\n",
    "    for i in json_data.entities:\n",
    "        if not hasattr(i, \"properites\"):\n",
    "            for j in final_account_match:\n",
    "                for k in final_account_match[j]:\n",
    "                    if i.mention_text:\n",
    "                        if i.mention_text.lower() == j.lower():\n",
    "                            for m in json_data.entities:\n",
    "                                if m.mention_text:\n",
    "                                    if m.mention_text.lower() == k.lower():\n",
    "                                        i.type = (\"_\").join(\n",
    "                                            m.type.split(\"_\")[:2]\n",
    "                                        ) + \"_name\"\n",
    "\n",
    "    account_names = {}\n",
    "    for i in range(len(json_data.entities)):\n",
    "        if not hasattr(json_data.entities[i], \"properites\"):\n",
    "            if (\n",
    "                difflib.SequenceMatcher(\n",
    "                    None, json_data.entities[i].type, \"account_name\"\n",
    "                ).ratio()\n",
    "                >= 0.9\n",
    "            ):\n",
    "                account_names[json_data.entities[i].mention_text] = {\n",
    "                    \"id\": json_data.entities[i].id,\n",
    "                    \"type\": json_data.entities[i].type,\n",
    "                }\n",
    "\n",
    "    for i in range(len(json_data.entities)):\n",
    "        if json_data.entities[i].type == \"account_type\":\n",
    "            for j in list(account_names.keys()):\n",
    "                if (\n",
    "                    difflib.SequenceMatcher(\n",
    "                        None, (json_data.entities[i].mention_text).lower(), j.lower()\n",
    "                    ).ratio()\n",
    "                ) > 0.9:\n",
    "                    json_data.entities[i].type = account_names[j].type\n",
    "    for i in range(len(json_data.entities)):\n",
    "        try:\n",
    "            while json_data.entities[i].type == \"account_type\":\n",
    "                del json_data.entities[i]\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    return json_data\n",
    "\n",
    "\n",
    "# logging function\n",
    "\n",
    "\n",
    "def logger(filename: str, message: str) -> None:\n",
    "    \"\"\"\n",
    "    Function to write the message (error message, warning messgae, info message) to the logging text file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "            The text file name where the message needs to be written.\n",
    "    message : str\n",
    "            The string message from functions(error message, warning messgae, info message).\n",
    "\n",
    "    \"\"\"\n",
    "    f = open(filename, \"a\")\n",
    "    f.write(\"{0} -- {1}\\n\".format(datetime.now().strftime(\"%Y-%m-%d %H:%M\"), message))\n",
    "    f.close()\n",
    "\n",
    "\n",
    "# Borrower name split and page Anchors\n",
    "def borrowerNameFix(jsonData):\n",
    "    \"\"\"\n",
    "    Function to fix the borrower name present in the document by fixing the suffix, prefix of the name and also by divding the full name into\n",
    "    smaller chunks of first name, middle name, last name\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    document :documentai.Document\n",
    "            The document proto having all the entities with the full name of borrower.\n",
    "    Returns\n",
    "    -------\n",
    "    documentai.Document\n",
    "        Returns the updated document proto .\n",
    "    \"\"\"\n",
    "    global maxId\n",
    "    extraDict = documentai.Document()\n",
    "    for i in jsonData.entities:\n",
    "        if i.type == \"client_name\":\n",
    "            extraDict.entities.append(i)\n",
    "\n",
    "    for i in jsonData.entities:\n",
    "        if i.type == \"client_name\":\n",
    "            jsonData.entities.remove(i)\n",
    "\n",
    "    def ent_rename_borrower_name(document):\n",
    "        google_name = \"client_name\"\n",
    "        entity_values = []\n",
    "        entity_dict = {}\n",
    "        dict_ent1 = {}\n",
    "        for i in range(len(document.entities)):\n",
    "            if not hasattr(document.entities[i], \"properites\"):\n",
    "                if document.entities[i].type == google_name:\n",
    "                    entity_dict[document.entities[i].mention_text] = []\n",
    "        for i in range(len(document.entities)):\n",
    "            if not hasattr(document.entities[i], \"properites\"):\n",
    "                if document.entities[i].type == google_name:\n",
    "                    ent_val = entity_values.append(document.entities[i].mention_text)\n",
    "                    if document.entities[i].mention_text in entity_dict.keys():\n",
    "                        entity_dict[document.entities[i].mention_text].append(\n",
    "                            document.entities[i].id\n",
    "                        )\n",
    "        sorted_list = []\n",
    "        sorted_dict = {}\n",
    "        for i in entity_dict:\n",
    "            temp_list = []\n",
    "            for j in range(len(entity_dict[i])):\n",
    "                temp_list.append(int(entity_dict[i][j]))\n",
    "            sorted_list.append(min(temp_list))\n",
    "        sorted_list.sort()\n",
    "        for i in range(len(sorted_list)):\n",
    "            for j in entity_dict:\n",
    "                if str(sorted_list[i]) in entity_dict[j]:\n",
    "                    if j not in sorted_dict:\n",
    "                        sorted_dict[j] = i\n",
    "\n",
    "        # return entity_dict,sorted(dict_ent1.items())\n",
    "        for i in range(len(document.entities)):\n",
    "            if not hasattr(document.entities[i], \"properites\"):\n",
    "                if document.entities[i].type == google_name:\n",
    "                    for k in entity_dict[document.entities[i].mention_text]:\n",
    "                        try:\n",
    "                            if document.entities[i].id == k:\n",
    "                                document.entities[i].type = (\n",
    "                                    \"Borrower_\"\n",
    "                                    + str(\n",
    "                                        sorted_dict[document.entities[i].mention_text]\n",
    "                                        + 1\n",
    "                                    )\n",
    "                                    + \"_Full_Name\"\n",
    "                                )\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "        return document, entity_dict\n",
    "\n",
    "    def suffix_checker(json_data):\n",
    "        possible_suffixes = [\"JR\", \"Jr\", \"III\", \"II\", \"MD\", \"PhD\", \"DVM\", \"DDS\"]\n",
    "        suffix_tracker = {}\n",
    "        for i in range(len(json_data.entities)):\n",
    "            if \"name\" in json_data.entities[i].type:\n",
    "                if json_data.entities[i].mention_text.split()[-1] in possible_suffixes:\n",
    "                    suffix = json_data.entities[i].mention_text.split()[-1]\n",
    "                    borrower_number = json_data.entities[i].type[:10]\n",
    "                    suffix_tracker[borrower_number] = suffix\n",
    "                    json_data.entities[i].mention_text = \" \".join(\n",
    "                        map(str, i.mention_text.split()[:-1])\n",
    "                    )\n",
    "                    temp = copy.deepcopy(json_data.entities[i])\n",
    "                    temp.type = borrower_number + \"_Suffix\"\n",
    "                    temp.text_anchor.text_segments[0].start_index = str(\n",
    "                        int(temp.text_anchor.text_segments[0].end_index - len(suffix))\n",
    "                    )\n",
    "                    temp.mention_text = suffix\n",
    "                    temp.text_anchor[\"content\"] = suffix\n",
    "                    json_data.entities.append(temp)\n",
    "        return json_data\n",
    "\n",
    "    def split_rename(document, entity_dict):\n",
    "        import copy\n",
    "\n",
    "        type_three_names = [\"first_name\", \"middle_name\", \"last_name\"]\n",
    "        type_three_names_with_comma = [\"last_name\", \"middle_name\", \"first_name\"]\n",
    "        type_two_names = [\"first_name\", \"last_name\"]\n",
    "        type_two_names_with_comma = [\"last_name\", \"first_name\"]\n",
    "        prefix = [\"mr\", \"mrs\", \"miss\", \"ms\", \"mx\", \"sir\", \"dr\"]\n",
    "        try:\n",
    "            for_entity_count = len(document.entities)\n",
    "            deleted_entites = []\n",
    "            for i in range(for_entity_count):\n",
    "                for j in entity_dict.keys():\n",
    "                    for k in range(len(entity_dict[j])):\n",
    "                        if not document.entities[i].properties:\n",
    "                            if document.entities[i].id == entity_dict[j][k]:\n",
    "                                name = document.entities[i].mention_text.split(\" \")\n",
    "\n",
    "                                try:\n",
    "                                    if name[0].lower() in prefix:\n",
    "                                        k = name[0] + \" \" + name[1]\n",
    "                                        name.pop(0)\n",
    "                                        name.pop(0)\n",
    "                                        name.insert(0, k)\n",
    "                                except:\n",
    "                                    pass\n",
    "\n",
    "                                if len(name) == 2:\n",
    "                                    for m in range(len(name)):\n",
    "                                        temp = copy.deepcopy(document.entities[i])\n",
    "                                        # del temp.id\n",
    "                                        temp.mention_text = name[m]\n",
    "                                        index = temp.text_anchor.text_segments[0].copy()\n",
    "\n",
    "                                        if m == 0:\n",
    "                                            temp.text_anchor.text_segments[\n",
    "                                                0\n",
    "                                            ].end_index = str(\n",
    "                                                int(index.start_index) + len(name[0])\n",
    "                                            )\n",
    "                                            if name[0].endswith(\",\"):\n",
    "                                                temp.type = (\n",
    "                                                    (\n",
    "                                                        (\"_\").join(\n",
    "                                                            temp.type.split(\"_\")[:2]\n",
    "                                                        )\n",
    "                                                    )\n",
    "                                                    + \"_\"\n",
    "                                                    + type_two_names_with_comma[m]\n",
    "                                                )\n",
    "                                            else:\n",
    "                                                temp.type = (\n",
    "                                                    (\n",
    "                                                        (\"_\").join(\n",
    "                                                            temp.type.split(\"_\")[:2]\n",
    "                                                        )\n",
    "                                                    )\n",
    "                                                    + \"_\"\n",
    "                                                    + type_two_names[m]\n",
    "                                                )\n",
    "                                        else:\n",
    "                                            temp.text_anchor.text_segments[\n",
    "                                                0\n",
    "                                            ].start_index = str(\n",
    "                                                int(index.end_index) - len(name[1])\n",
    "                                            )\n",
    "                                            if name[0].endswith(\",\"):\n",
    "                                                temp.type = (\n",
    "                                                    (\n",
    "                                                        (\"_\").join(\n",
    "                                                            temp.type.split(\"_\")[:2]\n",
    "                                                        )\n",
    "                                                    )\n",
    "                                                    + \"_\"\n",
    "                                                    + type_two_names_with_comma[m]\n",
    "                                                )\n",
    "                                            else:\n",
    "                                                temp.type = (\n",
    "                                                    (\n",
    "                                                        (\"_\").join(\n",
    "                                                            temp.type.split(\"_\")[:2]\n",
    "                                                        )\n",
    "                                                    )\n",
    "                                                    + \"_\"\n",
    "                                                    + type_two_names[m]\n",
    "                                                )\n",
    "                                        document.entities.append(temp)\n",
    "                                elif len(name) == 3:\n",
    "                                    for m in range(len(name)):\n",
    "                                        temp = copy.deepcopy(document.entities[i])\n",
    "                                        temp.mention_text = name[m]\n",
    "                                        index = temp.text_anchor.text_segments[0].copy()\n",
    "                                        if m == 0:\n",
    "                                            temp.text_anchor.text_segments[\n",
    "                                                0\n",
    "                                            ].end_index = str(\n",
    "                                                int(index.start_index) + len(name[0])\n",
    "                                            )\n",
    "                                            if name[0].endswith(\",\"):\n",
    "                                                temp.type = (\n",
    "                                                    (\n",
    "                                                        (\"_\").join(\n",
    "                                                            temp.type.split(\"_\")[:2]\n",
    "                                                        )\n",
    "                                                    )\n",
    "                                                    + \"_\"\n",
    "                                                    + type_three_names_with_comma[m]\n",
    "                                                )\n",
    "                                            else:\n",
    "                                                temp.type = (\n",
    "                                                    (\n",
    "                                                        (\"_\").join(\n",
    "                                                            temp.type.split(\"_\")[:2]\n",
    "                                                        )\n",
    "                                                    )\n",
    "                                                    + \"_\"\n",
    "                                                    + type_three_names[m]\n",
    "                                                )\n",
    "                                        elif k == 1:\n",
    "                                            temp.text_anchor.text_segments[\n",
    "                                                0\n",
    "                                            ].start_index = str(\n",
    "                                                int(index.start_index)\n",
    "                                                + len(name[0])\n",
    "                                                + 1\n",
    "                                            )\n",
    "                                            temp.text_anchor.text_segments[\n",
    "                                                0\n",
    "                                            ].end_index = str(\n",
    "                                                int(index.end_index) - len(name[2])\n",
    "                                            )\n",
    "                                            if name[0].endswith(\",\"):\n",
    "                                                temp.type = (\n",
    "                                                    (\n",
    "                                                        (\"_\").join(\n",
    "                                                            temp.type.split(\"_\")[:2]\n",
    "                                                        )\n",
    "                                                    )\n",
    "                                                    + \"_\"\n",
    "                                                    + type_three_names_with_comma[m]\n",
    "                                                )\n",
    "                                            else:\n",
    "                                                temp.type = (\n",
    "                                                    (\n",
    "                                                        (\"_\").join(\n",
    "                                                            temp.type.split(\"_\")[:2]\n",
    "                                                        )\n",
    "                                                    )\n",
    "                                                    + \"_\"\n",
    "                                                    + type_three_names[m]\n",
    "                                                )\n",
    "                                        else:\n",
    "                                            temp.text_anchor.text_segments[\n",
    "                                                0\n",
    "                                            ].start_index = str(\n",
    "                                                int(index.end_index) - len(name[2]) + 1\n",
    "                                            )\n",
    "                                            if name[0].endswith(\",\"):\n",
    "                                                temp.type = (\n",
    "                                                    (\n",
    "                                                        (\"_\").join(\n",
    "                                                            temp.type.split(\"_\")[:2]\n",
    "                                                        )\n",
    "                                                    )\n",
    "                                                    + \"_\"\n",
    "                                                    + type_three_names_with_comma[m]\n",
    "                                                )\n",
    "                                            else:\n",
    "                                                temp.type = (\n",
    "                                                    (\n",
    "                                                        (\"_\").join(\n",
    "                                                            temp.type.split(\"_\")[:2]\n",
    "                                                        )\n",
    "                                                    )\n",
    "                                                    + \"_\"\n",
    "                                                    + type_three_names[m]\n",
    "                                                )\n",
    "                                        document.entities.append(temp)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e, \" :: \", i)\n",
    "        return document\n",
    "\n",
    "    def text_anchorFix(jsonData, tempVar):\n",
    "        def text_anchorFixKid(jsonData, entityDict):\n",
    "            entityDict.text_anchor.content = entityDict.mention_text\n",
    "            if entityDict.type[-9:] != \"Full_Name\":\n",
    "                start = int(entityDict.text_anchor.text_segments[0].start_index)\n",
    "                end = int(entityDict.text_anchor.text_segments[0].end_index)\n",
    "\n",
    "                while (\n",
    "                    entityDict.mention_text != jsonData.text[start:end] and end > start\n",
    "                ):\n",
    "                    end -= 1\n",
    "                entityDict.text_anchor.text_segments[0].start_index = str(start)\n",
    "                entityDict.text_anchor.text_segments[0].end_index = str(end)\n",
    "            return entityDict\n",
    "\n",
    "        tempVarEntities = []\n",
    "        for i in tempVar.entities:\n",
    "            fixedDict = text_anchorFixKid(jsonData, i)\n",
    "            tempVarEntities.append(i)\n",
    "\n",
    "        tempVar.entities = tempVarEntities\n",
    "        return tempVar\n",
    "\n",
    "    def page_anchorFix(jsonData, tempVar):\n",
    "        tokenRange = {}\n",
    "        for i in range(0, len(jsonData.pages)):\n",
    "            for j in range(0, len(jsonData.pages[i].tokens)):\n",
    "                pageNumber = i\n",
    "                tokenNumber = j\n",
    "                try:\n",
    "                    startIndex = int(\n",
    "                        jsonData.pages[i]\n",
    "                        .tokens[j]\n",
    "                        .layout.text_anchor.text_segments[0]\n",
    "                        .start_index\n",
    "                    )\n",
    "                except:\n",
    "                    startIndex = 0\n",
    "                endIndex = int(\n",
    "                    jsonData.pages[i]\n",
    "                    .tokens[j]\n",
    "                    .layout.text_anchor.text_segments[0]\n",
    "                    .end_index\n",
    "                )\n",
    "                tokenRange[range(startIndex, endIndex)] = {\n",
    "                    \"page_number\": pageNumber,\n",
    "                    \"token_number\": tokenNumber,\n",
    "                }\n",
    "\n",
    "        for i in tempVar.entities:\n",
    "            if i.type is not \"Borrower_Full_Address\":\n",
    "                start = int(i.text_anchor.text_segments[0].start_index)\n",
    "                end = int(i.text_anchor.text_segments[0].end_index) - 1\n",
    "\n",
    "                for j in tokenRange:\n",
    "                    if start in j:\n",
    "                        lowerToken = tokenRange[j]\n",
    "                for j in tokenRange:\n",
    "                    if end in j:\n",
    "                        upperToken = tokenRange[j]\n",
    "\n",
    "                lowerTokenData = (\n",
    "                    jsonData.pages[int(lowerToken[\"page_number\"])]\n",
    "                    .tokens[int(lowerToken[\"token_number\"])]\n",
    "                    .layout.bounding_poly.normalized_vertices\n",
    "                )\n",
    "                upperTokenData = (\n",
    "                    jsonData.pages[int(upperToken[\"page_number\"])]\n",
    "                    .tokens[int(upperToken[\"token_number\"])]\n",
    "                    .layout.bounding_poly.normalized_vertices\n",
    "                )\n",
    "                # for A\n",
    "\n",
    "                xA = float(lowerTokenData[0].x)\n",
    "                yA = float(lowerTokenData[0].y)\n",
    "                xA_ = float(upperTokenData[0].x)\n",
    "                yA_ = float(upperTokenData[0].y)\n",
    "                # for B\n",
    "                xB = float(lowerTokenData[1].x)\n",
    "                yB = float(lowerTokenData[1].y)\n",
    "                xB_ = float(upperTokenData[1].x)\n",
    "                yB_ = float(upperTokenData[1].y)\n",
    "                # for C\n",
    "                xC = float(lowerTokenData[2].x)\n",
    "                yC = float(lowerTokenData[2].y)\n",
    "                xC_ = float(upperTokenData[2].x)\n",
    "                yC_ = float(upperTokenData[2].y)\n",
    "                # for D\n",
    "                xD = float(lowerTokenData[3].x)\n",
    "                yD = float(lowerTokenData[3].y)\n",
    "                xD_ = float(upperTokenData[3].x)\n",
    "                yD_ = float(upperTokenData[3].y)\n",
    "\n",
    "                A = {\"x\": min(xA, xA_), \"y\": min(yA, yA_)}\n",
    "                B = {\"x\": max(xB, xB_), \"y\": min(yB, yB_)}\n",
    "                C = {\"x\": max(xC, xC_), \"y\": max(yC, yC_)}\n",
    "                D = {\"x\": min(xD, xD_), \"y\": max(yD, yD_)}\n",
    "                i.page_anchor.page_refs[0].bounding_poly.normalized_vertices = [\n",
    "                    A,\n",
    "                    B,\n",
    "                    C,\n",
    "                    D,\n",
    "                ]\n",
    "        return tempVar\n",
    "\n",
    "    x1, y1 = ent_rename_borrower_name(extraDict)\n",
    "    extraDict = suffix_checker(extraDict)\n",
    "    tempVar = split_rename(x1, y1)\n",
    "\n",
    "    tempVar_2 = text_anchorFix(jsonData, tempVar)\n",
    "\n",
    "    tempVar_3 = page_anchorFix(jsonData, tempVar_2)\n",
    "\n",
    "    for i in tempVar_3.entities:\n",
    "        if i.type[-9:] != \"Full_Name\":\n",
    "            maxId += 1\n",
    "            i.id = str(maxId)\n",
    "    for i in tempVar_3.entities:\n",
    "        jsonData.entities.append(i)\n",
    "    return jsonData\n",
    "\n",
    "\n",
    "# Entities statement_start_date,statement_end_date,starting_balance,ending_balance,bank_name\n",
    "\n",
    "\n",
    "def ent_rename(\n",
    "    document: documentai.Document, google_name: str, specific_name: str\n",
    ") -> documentai.Document:\n",
    "    \"\"\"\n",
    "    Function to rename the entities given by the user in variable dict_ent_rename.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    document :documentai.Document\n",
    "            The document proto having all the entities\n",
    "    google_name : str\n",
    "            The entity name present in dict_ent_rename variable as key which need to be replaced .\n",
    "    specific_name : str\n",
    "            The specific name which will replace the entity name and present in dict_ent_rename variable as value.\n",
    "    Returns\n",
    "    -------\n",
    "    documentai.Document\n",
    "        Returns the updated document proto .\n",
    "    \"\"\"\n",
    "    for i in range(len(document.entities)):\n",
    "        if not hasattr(document.entities[i], \"properites\"):\n",
    "            if document.entities[i].type == google_name:\n",
    "                document.entities[i].type = specific_name\n",
    "        elif document.entities[i].properties:\n",
    "            for k in range(len(document.entities[i].properties)):\n",
    "                if document.entities[i].properties[k].type == google_name:\n",
    "                    document.entities[i].properties[k].type = specific_name\n",
    "    return document\n",
    "\n",
    "\n",
    "# adding pages entity\n",
    "# total pages function\n",
    "\n",
    "\n",
    "def add_total_pages(jsonData):\n",
    "    \"\"\"\n",
    "    Function to add the total page number and update the bounding poly.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    document : :documentai.Document\n",
    "            The document proto having all the pages data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    documentai.Document\n",
    "        Returns the updated document proto.\n",
    "    \"\"\"\n",
    "    pages_dict = documentai.Document.Entity()\n",
    "    pages_dict.type = \"Total_Pages\"\n",
    "    total = str(len(jsonData.pages))\n",
    "    pages_dict.mention_text = total\n",
    "    # jsonData.entities.append(pages_dict)\n",
    "    pages_dict.confidence = 1\n",
    "    tokenRange = {}\n",
    "    start = jsonData.text.rfind(\"Page\" + \" \" + str(len(jsonData.pages)))\n",
    "    end = int(start) + 11\n",
    "\n",
    "    for j in range(0, len(jsonData.pages[(len(jsonData.pages)) - 1].tokens)):\n",
    "        pageNumber = (len(jsonData.pages)) - 1\n",
    "        tokenNumber = j\n",
    "        try:\n",
    "            startIndex = int(\n",
    "                jsonData.pages[(len(jsonData.pages)) - 1]\n",
    "                .tokens[j]\n",
    "                .layout.text_anchor.text_segments[0]\n",
    "                .start_index\n",
    "            )\n",
    "        except:\n",
    "            startIndex = 0\n",
    "        endIndex = int(\n",
    "            jsonData.pages[(len(jsonData.pages)) - 1]\n",
    "            .tokens[j]\n",
    "            .layout.text_anchor.text_segments[0]\n",
    "            .end_index\n",
    "        )\n",
    "        tokenRange[range(startIndex, endIndex)] = {\n",
    "            \"pageNumber\": pageNumber,\n",
    "            \"tokenNumber\": tokenNumber,\n",
    "        }\n",
    "\n",
    "    for j in tokenRange:\n",
    "        if start in j:\n",
    "            lowerToken = tokenRange[j]\n",
    "    for j in tokenRange:\n",
    "        if end in j:\n",
    "            upperToken = tokenRange[j]\n",
    "\n",
    "    lowerTokenData = (\n",
    "        jsonData.pages[int(lowerToken[\"pageNumber\"])]\n",
    "        .tokens[int(lowerToken[\"tokenNumber\"])]\n",
    "        .layout.bounding_poly.normalized_vertices\n",
    "    )\n",
    "    upperTokenData = (\n",
    "        jsonData.pages[int(upperToken[\"pageNumber\"])]\n",
    "        .tokens[int(upperToken[\"tokenNumber\"])]\n",
    "        .layout.bounding_poly.normalized_vertices\n",
    "    )\n",
    "\n",
    "    # for A\n",
    "    xA = float(lowerTokenData[0].x)\n",
    "    yA = float(lowerTokenData[0].y)\n",
    "    xA_ = float(upperTokenData[0].x)\n",
    "    yA_ = float(upperTokenData[0].y)\n",
    "    # for B\n",
    "    xB = float(lowerTokenData[1].x)\n",
    "    yB = float(lowerTokenData[1].y)\n",
    "    xB_ = float(upperTokenData[1].x)\n",
    "    yB_ = float(upperTokenData[1].y)\n",
    "    # for C\n",
    "    xC = float(lowerTokenData[2].x)\n",
    "    yC = float(lowerTokenData[2].y)\n",
    "    xC_ = float(upperTokenData[2].x)\n",
    "    yC_ = float(upperTokenData[2].y)\n",
    "    # for D\n",
    "    xD = float(lowerTokenData[3].x)\n",
    "    yD = float(lowerTokenData[3].y)\n",
    "    xD_ = float(upperTokenData[3].x)\n",
    "    yD_ = float(upperTokenData[3].y)\n",
    "\n",
    "    A = {\"x\": min(xA, xA_), \"y\": min(yA, yA_)}\n",
    "    B = {\"x\": max(xB, xB_), \"y\": min(yB, yB_)}\n",
    "    C = {\"x\": max(xC, xC_), \"y\": max(yC, yC_)}\n",
    "    D = {\"x\": min(xD, xD_), \"y\": max(yD, yD_)}\n",
    "    boundpoly = {}\n",
    "    boundpoly[\"normalized_vertices\"] = [A, B, C, D]\n",
    "    # pages_dict.page_anchor.page_refs[0].bounding_poly.normalized_vertices = [A, B, C, D]\n",
    "    pages_dict.page_anchor = {\n",
    "        \"page_refs\": [\n",
    "            {\n",
    "                \"bounding_poly\": {\"normalized_vertices\": [A, B, C, D]},\n",
    "                \"page\": str(int(total) - 1),\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    pages_dict.text_anchor = {\n",
    "        \"content\": pages_dict.mention_text,\n",
    "        \"text_segments\": [{\"end_index\": str(end), \"start_index\": str(start)}],\n",
    "    }\n",
    "    jsonData.entities.append(pages_dict)\n",
    "\n",
    "    return jsonData\n",
    "\n",
    "\n",
    "def delete_empty(document: documentai.Document) -> documentai.Document:\n",
    "    \"\"\"\n",
    "    Function remove the  enitity from the entities list if the entity  is empty\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    document : :documentai.Document\n",
    "            The document proto having all the entities\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    documentai.Document\n",
    "        Returns the updated document proto after removal of empty entities\n",
    "    \"\"\"\n",
    "    for i in range(len(document.entities)):\n",
    "        try:\n",
    "            if document.entities[i] == \"\":\n",
    "                del document.entities[i]\n",
    "        except:\n",
    "            pass\n",
    "    return document\n",
    "\n",
    "\n",
    "# splitting client_address and creating new entities\n",
    "\n",
    "\n",
    "def has_digit(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Function to check if string have digit.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    s :str\n",
    "            The string which can have digit.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Returns true or false depends on if digit is present or not.\n",
    "    \"\"\"\n",
    "    return any(char.isdigit() for char in s)\n",
    "\n",
    "\n",
    "def parse_last_line(last_line: str) -> Tuple:\n",
    "    \"\"\"\n",
    "    Function to parse the address .\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    last_line :str\n",
    "            The string in address format\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple\n",
    "        Returns the tuple with city,state, zip .\n",
    "    \"\"\"\n",
    "    match = re.search(r\"([A-Z]{2})((\\)|\\s|,|\\.)*)(\\d{5})\", last_line)\n",
    "    # match = re.search(r'([A-Za-z]*)|\\(|([A-Z]{2})\\)|((\\s|,|\\.)*)(\\d{5})', last_line)\n",
    "\n",
    "    if not match:\n",
    "        return None\n",
    "    elif match.start() > 0 and last_line[match.start() - 1].isalnum():\n",
    "        return None\n",
    "    matched_state_zip = last_line[match.start() : match.end()]\n",
    "    zip_start = re.search(r\"\\d{5}\", matched_state_zip).start()\n",
    "\n",
    "    state, zip = (\n",
    "        re.sub(r\"[^\\w\\s]\", \"\", matched_state_zip[0:zip_start].strip()),\n",
    "        matched_state_zip[zip_start:],\n",
    "    )\n",
    "    zip_to_end = last_line[match.start() + zip_start :]\n",
    "\n",
    "    unmatched_tokens = [\n",
    "        t for t in last_line[0 : match.start()].split() if t and has_digit(t)\n",
    "    ]\n",
    "    city_candiates = [\n",
    "        t for t in last_line[0 : match.start()].split() if t and not has_digit(t)\n",
    "    ]\n",
    "\n",
    "    city = None\n",
    "    if city_candiates:\n",
    "        if len(city_candiates) > 2:\n",
    "            unmatched_tokens.extend(city_candiates[0:-2])\n",
    "            city = \" \".join(city_candiates[-2:0])\n",
    "        else:\n",
    "            city = \" \".join(city_candiates)\n",
    "    unmatched = \" \".join(unmatched_tokens)\n",
    "\n",
    "    return (city, state, zip, unmatched, zip_to_end)\n",
    "\n",
    "\n",
    "def split_address_entities(entity_type: str, mention_text: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Function to split the address into multiple address line like zip,stae,city,street address.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    entity_type : str\n",
    "            The entity name from the document proto object\n",
    "    mention_text : str\n",
    "            The OCR text of the entity having the actual data of address\n",
    "    Returns\n",
    "    -------\n",
    "    Dict\n",
    "        Returns the dictonary object with the splitted address inthe form of entity as key and value as text.\n",
    "    \"\"\"\n",
    "    text_lines = [line.strip() for line in mention_text.split(\"\\n\") if line.strip()]\n",
    "    if len(text_lines) == 2 or len(text_lines) == 3:\n",
    "        parsing = parse_last_line(text_lines[-1])\n",
    "        if parsing is not None and parsing[0] is not None:\n",
    "            if (\n",
    "                len(text_lines) == 3\n",
    "                and not has_digit(text_lines[0])\n",
    "                and \"box\" not in text_lines[0].casefold()\n",
    "                and not text_lines[0].startswith(\"o \")\n",
    "            ):\n",
    "                del text_lines[0]\n",
    "            line2 = text_lines[1] if len(text_lines) == 3 else \"\"\n",
    "            return {\n",
    "                f\"{entity_type}_StreetAddressOrPostalBox\": text_lines[0],\n",
    "                f\"{entity_type}_AdditionalStreetAddressOrPostalBox\": line2,\n",
    "                f\"{entity_type}_City\": parsing[0],\n",
    "                f\"{entity_type}_State\": parsing[1],\n",
    "                f\"{entity_type}_Zip\": parsing[4],\n",
    "            }\n",
    "\n",
    "    if len(text_lines) == 1:\n",
    "        parsing = parse_last_line(text_lines[0])\n",
    "        if parsing is None:\n",
    "            raise ValueError(\"Likely invalid redaction.\")\n",
    "        else:\n",
    "            return {\n",
    "                f\"{entity_type}_StreetAddressOrPostalBox\": parsing[3],\n",
    "                f\"{entity_type}_City\": parsing[0] if parsing[0] else \"\",\n",
    "                f\"{entity_type}_State\": parsing[1],\n",
    "                f\"{entity_type}_Zip\": parsing[2],\n",
    "            }\n",
    "    else:\n",
    "        last_line_candidates = [\n",
    "            i for i in range(len(text_lines)) if parse_last_line(text_lines[i])\n",
    "        ]\n",
    "        if not last_line_candidates:\n",
    "            all_tokens = \" \".join(text_lines).split()\n",
    "            state_token_id, zip_token_id = None, None\n",
    "            for i, token in enumerate(all_tokens):\n",
    "                if re.fullmatch(r\"([A-Z]{2})((\\s|,|\\.)*)\", token):\n",
    "                    state_token_id = i\n",
    "                if re.fullmatch(r\"\\d{5}\", token):\n",
    "                    zip_token_id = i\n",
    "\n",
    "            if state_token_id is None or zip_token_id is None:\n",
    "                raise ValueError(\"Likely invalid redaction, no zip or state.\")\n",
    "            else:\n",
    "                search_start = max(min(state_token_id, zip_token_id) - 1, 0)\n",
    "                search_end = min(max(state_token_id, zip_token_id) + 2, len(all_tokens))\n",
    "                city_candidates = [\n",
    "                    i\n",
    "                    for i in range(search_start, search_end)\n",
    "                    if i not in [state_token_id, zip_token_id]\n",
    "                    and not has_digit(all_tokens[i])\n",
    "                ]\n",
    "\n",
    "                ids_to_remove = [state_token_id, zip_token_id]\n",
    "                city = None\n",
    "                if city_candidates:\n",
    "                    city = all_tokens[city_candidates[-1]]\n",
    "                    ids_to_remove.append(city_candidates[-1])\n",
    "                line_1 = \" \".join(\n",
    "                    [\n",
    "                        all_tokens[i]\n",
    "                        for i in range(len(all_tokens))\n",
    "                        if i not in ids_to_remove\n",
    "                    ]\n",
    "                )\n",
    "                return {\n",
    "                    f\"{entity_type}_StreetAddressOrPostalBox\": line_1,\n",
    "                    f\"{entity_type}_City\": city if city else \"\",\n",
    "                    f\"{entity_type}_State\": all_tokens[state_token_id],\n",
    "                    f\"{entity_type}_Zip\": all_tokens[zip_token_id],\n",
    "                }\n",
    "        else:\n",
    "            last_line_id = max(last_line_candidates)\n",
    "            parsing = parse_last_line(text_lines[last_line_id])\n",
    "            remaining_lines = text_lines[0:last_line_id]\n",
    "            if not remaining_lines:\n",
    "                return {\n",
    "                    f\"{entity_type}_StreetAddressOrPostalBox\": parsing[3],\n",
    "                    f\"{entity_type}_City\": parsing[0] if parsing[0] else \"\",\n",
    "                    f\"{entity_type}_State\": parsing[1],\n",
    "                    f\"{entity_type}_Zip\": parsing[2],\n",
    "                }\n",
    "            else:\n",
    "                city = parsing[0]\n",
    "                if city is None and not has_digit(remaining_lines[-1]):\n",
    "                    city = remaining_lines[-1]\n",
    "                    remaining_lines = remaining_lines[0:-1]\n",
    "                if parsing[3]:\n",
    "                    remaining_lines.append(parsing[3])\n",
    "                line_1, line_2 = None, None\n",
    "                if remaining_lines:\n",
    "                    line_1 = remaining_lines[0]\n",
    "                    if len(remaining_lines) > 1:\n",
    "                        line_2 = \" \".join(remaining_lines[1:])\n",
    "                return {\n",
    "                    f\"{entity_type}_StreetAddressOrPostalBox\": line_1 if line_1 else \"\",\n",
    "                    f\"{entity_type}_AdditionalStreetAddressOrPostalBox\": line_2\n",
    "                    if line_2\n",
    "                    else \"\",\n",
    "                    f\"{entity_type}_City\": city if city else \"\",\n",
    "                    f\"{entity_type}_State\": parsing[1],\n",
    "                    f\"{entity_type}_Zip\": parsing[2],\n",
    "                }\n",
    "\n",
    "\n",
    "# Replacing the Address , splitting and page anchors\n",
    "\n",
    "\n",
    "def address_function(data: documentai.Document) -> documentai.Document:\n",
    "    \"\"\"\n",
    "    Function to fix the address by the entity name (ex : Borrower_Street_Address,Borrower_City,Borrower_State,Borrower_Zip)\n",
    "    by fixing the text anchor, page anchor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : documentai.Document\n",
    "            The document proto data having the entities which needs to be change.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    documentai.Document\n",
    "        Returns the updated document proto object.\n",
    "    \"\"\"\n",
    "    global maxId\n",
    "    newData = documentai.Document()\n",
    "    for i in data.entities:\n",
    "        if i.type == \"client_address\":\n",
    "            newData.entities.append(i)\n",
    "            data.entities.remove(i)\n",
    "\n",
    "    def address_text_anchor(jsonData, tempVar):\n",
    "        # text_anchor fix\n",
    "        for i in tempVar.entities:\n",
    "            if i.type == \"Borrower_Full_Address\":\n",
    "                start = int(i.text_anchor.text_segments[0].start_index)\n",
    "                end = int(i.text_anchor.text_segments[0].end_index)\n",
    "            else:\n",
    "                i.mention_text = i.mention_text.replace(\"\\\\n\", \" \")\n",
    "                end = start + len(i.mention_text)\n",
    "                while i.mention_text.split() != jsonData.text[start:end].split() and (\n",
    "                    end < len(jsonData.text) - 1\n",
    "                ):\n",
    "                    start += 1\n",
    "                    end += 1\n",
    "                i.text_anchor.text_segments[0].start_index = str(start)\n",
    "                i.text_anchor.text_segments[0].end_index = str(end)\n",
    "                i.text_anchor.content = i.mention_text\n",
    "                start = end\n",
    "        return tempVar\n",
    "\n",
    "    def address_page_anchor(jsonData, tempVar):\n",
    "        tokenRange = {}\n",
    "        for i in range(0, len(jsonData.pages)):\n",
    "            for j in range(0, len(jsonData.pages[i].tokens)):\n",
    "                pageNumber = i\n",
    "                tokenNumber = j\n",
    "                try:\n",
    "                    startIndex = int(\n",
    "                        jsonData.pages[i]\n",
    "                        .tokens[j]\n",
    "                        .layout.text_anchor.text_segments[0]\n",
    "                        .start_index\n",
    "                    )\n",
    "                except:\n",
    "                    startIndex = 0\n",
    "                endIndex = int(\n",
    "                    jsonData.pages[i]\n",
    "                    .tokens[j]\n",
    "                    .layout.text_anchor.text_segments[0]\n",
    "                    .end_index\n",
    "                )\n",
    "                tokenRange[range(startIndex, endIndex)] = {\n",
    "                    \"pageNumber\": pageNumber,\n",
    "                    \"tokenNumber\": tokenNumber,\n",
    "                }\n",
    "\n",
    "        for i in tempVar.entities:\n",
    "            if i.type is not \"Borrower_Full_Address\":\n",
    "                start = int(i.text_anchor.text_segments[0].start_index)\n",
    "                end = int(i.text_anchor.text_segments[0].end_index) - 1\n",
    "\n",
    "                for j in tokenRange:\n",
    "                    if start in j:\n",
    "                        lowerToken = tokenRange[j]\n",
    "                for j in tokenRange:\n",
    "                    if end in j:\n",
    "                        upperToken = tokenRange[j]\n",
    "\n",
    "                lowerTokenData = (\n",
    "                    jsonData.pages[int(lowerToken[\"pageNumber\"])]\n",
    "                    .tokens[int(lowerToken[\"tokenNumber\"])]\n",
    "                    .layout.bounding_poly.normalized_vertices\n",
    "                )\n",
    "                upperTokenData = (\n",
    "                    jsonData.pages[int(upperToken[\"pageNumber\"])]\n",
    "                    .tokens[int(upperToken[\"tokenNumber\"])]\n",
    "                    .layout.bounding_poly.normalized_vertices\n",
    "                )\n",
    "                # for A\n",
    "                # for A\n",
    "                xA = float(lowerTokenData[0].x)\n",
    "                yA = float(lowerTokenData[0].y)\n",
    "                xA_ = float(upperTokenData[0].x)\n",
    "                yA_ = float(upperTokenData[0].y)\n",
    "                # for B\n",
    "                xB = float(lowerTokenData[1].x)\n",
    "                yB = float(lowerTokenData[1].y)\n",
    "                xB_ = float(upperTokenData[1].x)\n",
    "                yB_ = float(upperTokenData[1].y)\n",
    "                # for C\n",
    "                xC = float(lowerTokenData[2].x)\n",
    "                yC = float(lowerTokenData[2].y)\n",
    "                xC_ = float(upperTokenData[2].x)\n",
    "                yC_ = float(upperTokenData[2].y)\n",
    "                # for D\n",
    "                xD = float(lowerTokenData[3].x)\n",
    "                yD = float(lowerTokenData[3].y)\n",
    "                xD_ = float(upperTokenData[3].x)\n",
    "                yD_ = float(upperTokenData[3].y)\n",
    "\n",
    "                A = {\"x\": min(xA, xA_), \"y\": min(yA, yA_)}\n",
    "                B = {\"x\": max(xB, xB_), \"y\": min(yB, yB_)}\n",
    "                C = {\"x\": max(xC, xC_), \"y\": max(yC, yC_)}\n",
    "                D = {\"x\": min(xD, xD_), \"y\": max(yD, yD_)}\n",
    "                i.page_anchor.page_refs[0].bounding_poly.normalized_vertices = [\n",
    "                    A,\n",
    "                    B,\n",
    "                    C,\n",
    "                    D,\n",
    "                ]\n",
    "        return tempVar\n",
    "\n",
    "    def address_function_new(data):\n",
    "        deleted_entities = []\n",
    "        address_entity_names = [\n",
    "            \"Borrower_Street_Address\",\n",
    "            \"Borrower_City\",\n",
    "            \"Borrower_State\",\n",
    "            \"Borrower_Zip\",\n",
    "        ]\n",
    "        address_entity_name_and_value = {}\n",
    "        address_parser = AddressParser(device=0)  # On GPU device 0\n",
    "        for i in range(len(data.entities)):\n",
    "            try:\n",
    "                if data.entities[i].type == \"client_address\":\n",
    "                    deleted_entities.append(i)\n",
    "                    full_address = \" \".join(data.entities[i].mention_text.split())\n",
    "                    parse_address = address_parser(full_address)\n",
    "\n",
    "                    StreetNameMatch = re.search(\n",
    "                        (\n",
    "                            parse_address.StreetNumber\n",
    "                            + \" \"\n",
    "                            + parse_address.StreetName\n",
    "                            + \" \"\n",
    "                        ),\n",
    "                        full_address,\n",
    "                        flags=re.IGNORECASE,\n",
    "                    )\n",
    "                    address_entity_name_and_value[\n",
    "                        \"Borrower_Street_Address\"\n",
    "                    ] = full_address[StreetNameMatch.start() : StreetNameMatch.end()]\n",
    "\n",
    "                    CityNameMatch = re.search(\n",
    "                        parse_address.Municipality, full_address, flags=re.IGNORECASE\n",
    "                    )\n",
    "                    address_entity_name_and_value[\"Borrower_City\"] = full_address[\n",
    "                        CityNameMatch.start() : CityNameMatch.end() + 1\n",
    "                    ]\n",
    "\n",
    "                    StateNameMatch = re.search(\n",
    "                        parse_address.Province.upper(), full_address\n",
    "                    )\n",
    "                    address_entity_name_and_value[\"Borrower_State\"] = full_address[\n",
    "                        StateNameMatch.start() : StateNameMatch.end()\n",
    "                    ]\n",
    "\n",
    "                    PostalCodeMatch = re.search(parse_address.PostalCode, full_address)\n",
    "                    address_entity_name_and_value[\"Borrower_Zip\"] = full_address[\n",
    "                        PostalCodeMatch.start() : PostalCodeMatch.end() + 1\n",
    "                    ]\n",
    "\n",
    "                    for j in range(4):\n",
    "                        temp = copy.deepcopy(data.entities[i])\n",
    "                        temp.type = address_entity_names[j]\n",
    "                        temp.mention_text = address_entity_name_and_value[\n",
    "                            address_entity_names[j]\n",
    "                        ]\n",
    "                        data.entities.append(temp)\n",
    "\n",
    "            except:\n",
    "                print(\"Can't split full_address in sub-parts using deepParse\")\n",
    "                print(data.entities[i])\n",
    "                if data.entities[i].type == \"client_address\":\n",
    "                    deleted_entities.append(i)\n",
    "                    s = data.entities[i].mention_text\n",
    "                    split_result = split_address_entities(\"client_address\", s)\n",
    "                    for j in range(4):\n",
    "                        temp = copy.deepcopy(data.entities[i])\n",
    "                        temp.type = address_entity_names[j]\n",
    "                        if address_entity_names[j] == \"Borrower_Street_Address\":\n",
    "                            if (\n",
    "                                \"client_address_AdditionalStreetAddressOrPostalBox\"\n",
    "                                in split_result.keys()\n",
    "                            ):\n",
    "                                temp.mention_text = (\n",
    "                                    split_result[\n",
    "                                        \"client_address_StreetAddressOrPostalBox\"\n",
    "                                    ]\n",
    "                                    + split_result[\n",
    "                                        \"client_address_AdditionalStreetAddressOrPostalBox\"\n",
    "                                    ]\n",
    "                                )\n",
    "                            else:\n",
    "                                temp.mention_text = split_result[\n",
    "                                    \"client_address_StreetAddressOrPostalBox\"\n",
    "                                ]\n",
    "                        elif address_entity_names[j] == \"Borrower_City\":\n",
    "                            temp.mention_text = split_result[\"client_address_City\"]\n",
    "                        elif address_entity_names[j] == \"Borrower_State\":\n",
    "                            temp.mention_text = split_result[\"client_address_State\"]\n",
    "                        else:\n",
    "                            temp.mention_text = split_result[\"client_address_Zip\"]\n",
    "                        data.entities.append(temp)\n",
    "        for i in deleted_entities[::-1]:\n",
    "            data.entities[i].type = \"Borrower_Full_Address\"\n",
    "        return data\n",
    "\n",
    "    tempVar = address_function_new(newData)\n",
    "    tempVar_2 = address_text_anchor(data, tempVar)\n",
    "    tempVar_3 = address_page_anchor(data, tempVar)\n",
    "    for i in tempVar_3.entities:\n",
    "        if i.type != \"Borrower_Full_Address\":\n",
    "            maxId += 1\n",
    "            i.id = str(maxId)\n",
    "    for i in tempVar_3.entities:\n",
    "        data.entities.append(i)\n",
    "    return data\n",
    "\n",
    "\n",
    "def fixAccountBalance(document: documentai.Document) -> documentai.Document:\n",
    "    \"\"\"\n",
    "    Function to fix the account balance by updatding the ,ention text of the entities.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    jsonData : documentai.Document\n",
    "            The document proto data having the entities which needs to be change.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    documentai.Document\n",
    "        Returns the updated document proto object.\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "\n",
    "    def most_frequent(List):\n",
    "        occurence_count = Counter(List)\n",
    "        return occurence_count.most_common(1)[0][0]\n",
    "\n",
    "    tempDict = {}\n",
    "    beginning_balance_unique = []\n",
    "    ending_balance_unique = []\n",
    "    for i in document.entities:\n",
    "        if \"beginning_balance\" in i.type:\n",
    "            if i.type not in beginning_balance_unique:\n",
    "                beginning_balance_unique.append(i.type)\n",
    "        if \"ending_balance\" in i.type:\n",
    "            if i.type not in ending_balance_unique:\n",
    "                ending_balance_unique.append(i.type)\n",
    "    beg_end_dict = {}\n",
    "    for i in beginning_balance_unique:\n",
    "        temp = []\n",
    "\n",
    "        for j in range(0, len(document.entities)):\n",
    "            if i == document.entities[j].type:\n",
    "                temp.append(document.entities[j].mention_text.strip(\"$#\"))\n",
    "        beg_end_dict[i] = most_frequent(temp)\n",
    "    for i in ending_balance_unique:\n",
    "        temp = []\n",
    "        for j in range(0, len(document.entities)):\n",
    "            if i == document.entities[j].type:\n",
    "                temp.append(document.entities[j].mention_text.strip(\"$#\"))\n",
    "        beg_end_dict[i] = most_frequent(temp)\n",
    "    for i in document.entities:\n",
    "        if i.type in beg_end_dict.keys():\n",
    "            if i.mention_text.strip(\"$#\") != beg_end_dict[\n",
    "                i.type\n",
    "            ] and i.mention_text.strip(\"$#\") in list(beg_end_dict.values()):\n",
    "                i.type = list(beg_end_dict.keys())[\n",
    "                    list(beg_end_dict.values()).index(i.mention_text.strip(\"$#\"))\n",
    "                ]\n",
    "            elif i.mention_text.strip(\"$#\") != beg_end_dict[i.type]:\n",
    "                document.entities.remove(i)\n",
    "    return document\n",
    "\n",
    "\n",
    "def Boundary_markers(\n",
    "    jsonData: documentai.Document,\n",
    ") -> documentai.Document:  # TODO : check jsonDict for dict or entity obj\n",
    "    \"\"\"\n",
    "    Function to mark the boundary bonding boxes  for the required entities.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    jsonData : documentai.Document\n",
    "            The document proto data having the entities which needs to be change.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    documentai.Document\n",
    "        Returns the updated document proto object.\n",
    "    \"\"\"\n",
    "    allEntities = jsonData.entities\n",
    "    noOfEntitiesInJsonFile = len(allEntities)\n",
    "\n",
    "    # Find entityIdSchema of Json\n",
    "    entityIdSchema = {}\n",
    "    for i in range(0, noOfEntitiesInJsonFile):\n",
    "        try:\n",
    "            if allEntities[i].id:\n",
    "                entityIdSchema[i] = [int(allEntities[i].id)]\n",
    "        except:\n",
    "            temp_arr = []\n",
    "            for j in allEntities[i].properties:\n",
    "                temp_arr.append(int(j.id))\n",
    "            entityIdSchema[i] = temp_arr\n",
    "\n",
    "    # Single Level Entities file : jsonDict\n",
    "    jsonDict = {\n",
    "        \"confidence\": [],\n",
    "        \"id\": [],\n",
    "        \"mention_text\": [],\n",
    "        \"normalized_value\": [],\n",
    "        \"page_anchor\": [],\n",
    "        \"text_anchor\": [],\n",
    "        \"type\": [],\n",
    "    }\n",
    "    entitiesArray = []\n",
    "\n",
    "    for i in range(0, noOfEntitiesInJsonFile):\n",
    "        try:\n",
    "            if allEntities[i].id:\n",
    "                entitiesArray.append(allEntities[i])\n",
    "        except:\n",
    "            for j in allEntities[i].properties:\n",
    "                entitiesArray.append(j)\n",
    "    entitiesArray = sorted(entitiesArray, key=lambda x: x.id)\n",
    "    for i in range(0, len(entitiesArray)):\n",
    "        try:\n",
    "            jsonDict[\"confidence\"].append(entitiesArray[i].confidence)\n",
    "        except:\n",
    "            jsonDict[\"confidence\"].append(\"\")\n",
    "        try:\n",
    "            jsonDict[\"id\"].append(entitiesArray[i].id)\n",
    "        except:\n",
    "            jsonDict[\"id\"].append(\"\")\n",
    "        try:\n",
    "            jsonDict[\"mention_text\"].append(entitiesArray[i].mention_text)\n",
    "        except:\n",
    "            jsonDict[\"mention_text\"].append(\"\")\n",
    "        try:\n",
    "            jsonDict[\"normalized_value\"].append(entitiesArray[i].normalized_value)\n",
    "        except:\n",
    "            jsonDict[\"normalized_value\"].append(\"\")\n",
    "        try:\n",
    "            jsonDict[\"page_anchor\"].append(entitiesArray[i].page_anchor)\n",
    "        except:\n",
    "            jsonDict[\"page_anchor\"].append(\"\")\n",
    "        try:\n",
    "            jsonDict[\"text_anchor\"].append(entitiesArray[i].text_anchor)\n",
    "        except:\n",
    "            jsonDict[\"text_anchor\"].append(\"\")\n",
    "        try:\n",
    "            jsonDict[\"type\"].append(entitiesArray[i].type)\n",
    "        except:\n",
    "            jsonDict[\"type\"].append(\"\")\n",
    "\n",
    "    # No startIndex handeling\n",
    "    for i in range(0, len(jsonDict[\"type\"])):\n",
    "        try:\n",
    "            if jsonDict[\"text_anchor\"][i][\"text_segments\"][0][\"start_index\"]:\n",
    "                pass\n",
    "        except:\n",
    "            try:\n",
    "                jsonDict[\"text_anchor\"][i][\"text_segments\"][0][\"start_index\"] = \"0\"\n",
    "            except:\n",
    "                pass\n",
    "    accountNumbers = dict()\n",
    "    for i in range(0, len(jsonDict[\"id\"])):\n",
    "        if jsonDict[\"type\"][i] == \"account_number\":\n",
    "            if (\n",
    "                re.sub(\"\\D\", \"\", jsonDict[\"mention_text\"][i].strip(\".#:' \"))\n",
    "                not in accountNumbers\n",
    "                and len(re.sub(\"\\D\", \"\", jsonDict[\"mention_text\"][i].strip(\".#:' \")))\n",
    "                > 5\n",
    "            ):\n",
    "                accountNumbers[\n",
    "                    re.sub(\"\\D\", \"\", jsonDict[\"mention_text\"][i].strip(\".#:' \"))\n",
    "                ] = (\"account_\" + str(len(accountNumbers)) + \"_number\")\n",
    "    account_number_dict = {}\n",
    "    import sys\n",
    "\n",
    "    accountNumberDict = {}\n",
    "    accountNumberPageDict = {}\n",
    "    for i in accountNumbers.keys():\n",
    "        temp_list = []\n",
    "        temp_page_list = set()\n",
    "        for j in range(len(jsonDict[\"mention_text\"])):\n",
    "            if re.sub(\"\\D\", \"\", jsonDict[\"mention_text\"][j].strip(\".#:' \")) == i:\n",
    "                page = 0\n",
    "                if jsonDict[\"page_anchor\"][j][\"page_refs\"][0][\"page\"]:\n",
    "                    page = int(jsonDict[\"page_anchor\"][j][\"page_refs\"][0][\"page\"])\n",
    "                temp_list.append(\n",
    "                    (\n",
    "                        int(\n",
    "                            jsonDict[\"text_anchor\"][j][\"text_segments\"][0][\n",
    "                                \"start_index\"\n",
    "                            ]\n",
    "                        ),\n",
    "                        int(\n",
    "                            jsonDict[\"text_anchor\"][j][\"text_segments\"][0][\"end_index\"]\n",
    "                        ),\n",
    "                        page,\n",
    "                    )\n",
    "                )\n",
    "                temp_page_list.add(page)\n",
    "        accountNumberPageDict[accountNumbers[i]] = temp_page_list\n",
    "        accountNumberDict[accountNumbers[i]] = temp_list\n",
    "    n = set(range(len(jsonData.pages)))\n",
    "    for i in accountNumberPageDict:\n",
    "        n = n & accountNumberPageDict[i]\n",
    "\n",
    "    n = list(n)\n",
    "    for i in accountNumberDict:\n",
    "        accountNumberDict[i].sort(key=lambda x: x[2])\n",
    "    accountNumbersToDelete = []\n",
    "    for i in accountNumberDict:\n",
    "        if i != \"account_0_number\":\n",
    "            minStartIndex = sys.maxsize\n",
    "            minEndIndex = sys.maxsize\n",
    "            minPage = sys.maxsize\n",
    "            tuppleToRemove = []\n",
    "            if len(accountNumberDict[i]) > 1:\n",
    "                for j in accountNumberDict[i]:\n",
    "                    if (\n",
    "                        j[2] in n\n",
    "                        and j[2] < 3\n",
    "                        and len(tuppleToRemove) < len(accountNumberDict[i]) - 1\n",
    "                    ):\n",
    "                        tuppleToRemove.append(j)\n",
    "                    else:\n",
    "                        minStartIndex = min(minStartIndex, j[0])\n",
    "                        minEndIndex = min(minEndIndex, j[1])\n",
    "                        minPage = min(minPage, j[2])\n",
    "                        tuppleToRemove.append(j)\n",
    "                # accountNumberDict[i]=[(minStartIndex,minEndIndex,minPage)]\n",
    "                for k in tuppleToRemove:\n",
    "                    accountNumberDict[i].remove(k)\n",
    "                accountNumberDict[i] = [(minStartIndex, minEndIndex, minPage)]\n",
    "            else:\n",
    "                accountNumbersToDelete.append(i)\n",
    "        else:\n",
    "            minStartIndex = 0\n",
    "            minEndIndex = 0\n",
    "            minPage = sys.maxsize\n",
    "            tuppleToRemove = []\n",
    "            for j in accountNumberDict[i]:\n",
    "                if j[2] == n[0]:\n",
    "                    minStartIndex = max(minStartIndex, j[0])\n",
    "                    minEndIndex = max(minEndIndex, j[1])\n",
    "                    minPage = min(minPage, j[2])\n",
    "                    tuppleToRemove.append(j)\n",
    "            for k in tuppleToRemove:\n",
    "                accountNumberDict[i].remove(k)\n",
    "            accountNumberDict[\"account_0_number\"] = [\n",
    "                (minStartIndex, minEndIndex, minPage)\n",
    "            ]\n",
    "    for i in accountNumbersToDelete:\n",
    "        del accountNumberDict[i]\n",
    "\n",
    "    if len(accountNumbers) > 1:\n",
    "        borderIndex = []\n",
    "        for i in accountNumberDict:\n",
    "            borderIndex.append((accountNumberDict[i][0][0], accountNumberDict[i][0][1]))\n",
    "\n",
    "        regionSplitter = []\n",
    "        for i in range(0, len(borderIndex)):\n",
    "            regionSplitter.append(borderIndex[i][0])\n",
    "\n",
    "        # regionSplitterDict = {0 : 'account_summary'}\n",
    "        regionSplitterDict = {}\n",
    "        for i in range(0, len(regionSplitter)):\n",
    "            regionSplitterDict[int(regionSplitter[i])] = \"account_\" + str(i)\n",
    "        regionSplitterDict[len(jsonData[\"text\"])] = \"last_index\"\n",
    "\n",
    "    else:\n",
    "        tempVar = len(jsonData.text)\n",
    "        regionSplitterDict = {tempVar: \"account_0\"}\n",
    "        regionSplitterDict[len(jsonData.text) + 1] = \"last_index\"\n",
    "\n",
    "    for i in range(0, len(jsonDict[\"id\"])):\n",
    "        if (\n",
    "            jsonDict[\"type\"][i] == \"account_number\"\n",
    "            and len(re.sub(\"\\D\", \"\", jsonDict[\"mention_text\"][i].strip(\".#:' \"))) > 5\n",
    "        ):\n",
    "            jsonDict[\"type\"][i] = accountNumbers[\n",
    "                re.sub(\"\\D\", \"\", jsonDict[\"mention_text\"][i].strip(\".#:' \"))\n",
    "            ]\n",
    "\n",
    "    for i in range(0, len(jsonDict[\"id\"])):\n",
    "        try:\n",
    "            si = jsonDict[\"text_anchor\"][i][\"text_segments\"][0][\"start_index\"]\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        if jsonDict[\"type\"][i] == \"starting_balance\":\n",
    "            for j in range(1, len(regionSplitterDict)):\n",
    "                if int(si) < list(regionSplitterDict.keys())[j]:\n",
    "                    jsonDict[\"type\"][i] = (\n",
    "                        regionSplitterDict[list(regionSplitterDict.keys())[j - 1]]\n",
    "                        + \"_beginning_balance\"\n",
    "                    )\n",
    "                    break\n",
    "        if jsonDict[\"type\"][i] == \"ending_balance\":\n",
    "            for j in range(1, len(regionSplitterDict)):\n",
    "                if int(si) < list(regionSplitterDict.keys())[j]:\n",
    "                    jsonDict[\"type\"][i] = (\n",
    "                        regionSplitterDict[list(regionSplitterDict.keys())[j - 1]]\n",
    "                        + \"_ending_balance\"\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "        if jsonDict[\"type\"][i] == \"table_item/transaction_deposit_date\":\n",
    "            for j in range(1, len(regionSplitterDict)):\n",
    "                if int(si) < list(regionSplitterDict.keys())[j]:\n",
    "                    jsonDict[\"type\"][i] = (\n",
    "                        regionSplitterDict[list(regionSplitterDict.keys())[j - 1]]\n",
    "                        + \"_transaction\"\n",
    "                        + \"/\"\n",
    "                        + \"deposit_date\"\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "        if jsonDict[\"type\"][i] == \"table_item/transaction_deposit_description\":\n",
    "            for j in range(1, len(regionSplitterDict)):\n",
    "                if int(si) < list(regionSplitterDict.keys())[j]:\n",
    "                    jsonDict[\"type\"][i] = (\n",
    "                        regionSplitterDict[list(regionSplitterDict.keys())[j - 1]]\n",
    "                        + \"_transaction\"\n",
    "                        + \"/\"\n",
    "                        + \"deposit_desc\"\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "        if jsonDict[\"type\"][i] == \"table_item/transaction_deposit\":\n",
    "            for j in range(1, len(regionSplitterDict)):\n",
    "                if int(si) < list(regionSplitterDict.keys())[j]:\n",
    "                    jsonDict[\"type\"][i] = (\n",
    "                        regionSplitterDict[list(regionSplitterDict.keys())[j - 1]]\n",
    "                        + \"_transaction\"\n",
    "                        + \"/\"\n",
    "                        + \"deposit_amount\"\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "        if jsonDict[\"type\"][i] == \"table_item/transaction_withdrawal_date\":\n",
    "            for j in range(1, len(regionSplitterDict)):\n",
    "                if int(si) < list(regionSplitterDict.keys())[j]:\n",
    "                    jsonDict[\"type\"][i] = (\n",
    "                        regionSplitterDict[list(regionSplitterDict.keys())[j - 1]]\n",
    "                        + \"_transaction\"\n",
    "                        + \"/\"\n",
    "                        + \"withdraw_date\"\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "        if jsonDict[\"type\"][i] == \"table_item/transaction_withdrawal_description\":\n",
    "            for j in range(1, len(regionSplitterDict)):\n",
    "                if int(si) < list(regionSplitterDict.keys())[j]:\n",
    "                    jsonDict[\"type\"][i] = (\n",
    "                        regionSplitterDict[list(regionSplitterDict.keys())[j - 1]]\n",
    "                        + \"_transaction\"\n",
    "                        + \"/\"\n",
    "                        + \"withdraw_desc\"\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "        if jsonDict[\"type\"][i] == \"table_item/transaction_withdrawal\":\n",
    "            for j in range(1, len(regionSplitterDict)):\n",
    "                if int(si) < list(regionSplitterDict.keys())[j]:\n",
    "                    jsonDict[\"type\"][i] = (\n",
    "                        regionSplitterDict[list(regionSplitterDict.keys())[j - 1]]\n",
    "                        + \"_transaction\"\n",
    "                        + \"/\"\n",
    "                        + \"withdraw_amount\"\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "    newEntitiesArray = documentai.Document()\n",
    "    for i in range(0, len(entitiesArray)):\n",
    "        newEntitiesArray.entities.append(\n",
    "            {\n",
    "                \"confidence\": jsonDict[\"confidence\"][i],\n",
    "                \"id\": jsonDict[\"id\"][i],\n",
    "                \"mention_text\": jsonDict[\"mention_text\"][i],\n",
    "                \"normalized_value\": jsonDict[\"normalized_value\"][i],\n",
    "                \"page_anchor\": jsonDict[\"page_anchor\"][i],\n",
    "                \"text_anchor\": jsonDict[\"text_anchor\"][i],\n",
    "                \"type\": jsonDict[\"type\"][i],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    newEntitiesArrayToIdDict = {}\n",
    "    for i in newEntitiesArray.entities:\n",
    "        newEntitiesArrayToIdDict[int(i.id)] = i\n",
    "\n",
    "    allEntitiesNewArray = (\" \" * len(entityIdSchema)).split(\" \")\n",
    "    for i in entityIdSchema:\n",
    "        if len(entityIdSchema[i]) == 1:\n",
    "            allEntitiesNewArray[i] = newEntitiesArrayToIdDict[entityIdSchema[i][0]]\n",
    "        else:\n",
    "            tempA = []\n",
    "            for j in range(0, len(entityIdSchema[i])):\n",
    "                tempA.append(newEntitiesArrayToIdDict[entityIdSchema[i][j]])\n",
    "            allEntitiesNewArray[i] = allEntities[i]\n",
    "            allEntitiesNewArray[i].properties = tempA\n",
    "    allEntitiesNewArray = [x for x in allEntitiesNewArray if x]\n",
    "    for i in allEntitiesNewArray:\n",
    "        if i == \"\":\n",
    "            allEntitiesNewArray.remove(i)\n",
    "        if i.id:\n",
    "            if i.id == \"\":\n",
    "                del i.id\n",
    "        if i.normalized_value:\n",
    "            if i.normalized_value == \"\":\n",
    "                del i.normalized_value\n",
    "        if i.confidence:\n",
    "            if i.confidence == \"\":\n",
    "                del i.confidence\n",
    "        if i.page_anchor:\n",
    "            if i.page_anchor == \"\":\n",
    "                del i.page_anchor\n",
    "        if i.mention_text:\n",
    "            if i.mention_text == \"\":\n",
    "                del i.mention_text\n",
    "        if i.text_anchor:\n",
    "            if i.text_anchor == \"\":\n",
    "                del i.text_anchor\n",
    "        if i.properties:\n",
    "            for j in i.properties:\n",
    "                if j.normalized_value:\n",
    "                    if j.normalized_value == \"\":\n",
    "                        del j.normalized_value\n",
    "                if j.confidence:\n",
    "                    if j.confidence == \"\":\n",
    "                        del j.confidence\n",
    "                if j.page_anchor:\n",
    "                    if j.page_anchor == \"\":\n",
    "                        del j.page_anchor\n",
    "                if j.id:\n",
    "                    if j.id == \"\":\n",
    "                        del j.id\n",
    "                if j.mention_text:\n",
    "                    if j.mention_text == \"\":\n",
    "                        del j.mention_text\n",
    "                if j.text_anchor:\n",
    "                    if j.text_anchor == \"\":\n",
    "                        del j.text_anchor\n",
    "    for i in allEntitiesNewArray:\n",
    "        if i.type == \"table_item\":\n",
    "            account_prefix = i.properties[0].type.split(\"/\")[0]\n",
    "            i.type = account_prefix\n",
    "\n",
    "    newJsonData = jsonData\n",
    "    newJsonData.entities = allEntitiesNewArray\n",
    "\n",
    "    return newJsonData\n",
    "\n",
    "\n",
    "def groupChecks(jsonData: documentai.Document, bankName: str) -> documentai.Document:\n",
    "    \"\"\"\n",
    "    Function to check for the bank name if they falls in top 3 banks(Wells Fargo, Bank of America, Chase),\n",
    "    if it found in the list entities will get sort.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    jsonData : documentai.Document\n",
    "            The document proto data having the entities which needs to be change.\n",
    "    bankName :str\n",
    "            bank name which are present in document OCR and need to be check if it falls in top 3 banks.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    documentai.Document\n",
    "        Returns the updated document proto object.\n",
    "    \"\"\"\n",
    "    # dictionary storing format of the table for top 3 banks\n",
    "    bankFormat = {\n",
    "        \"wellsfargo\": [\"check_number\", \"check_date\", \"check_amount\"],\n",
    "        \"bankofamerica\": [\"check_date\", \"check_number\", \"check_amount\"],\n",
    "        \"chase\": [\"check_number\", \"check_date\", \"check_amount\"],\n",
    "    }\n",
    "    bankChecksColumn = bankFormat[bankName]\n",
    "    allEntities = jsonData.entities\n",
    "    noOfEntitiesInJsonFile = len(allEntities)\n",
    "\n",
    "    # Find entityIdSchema of Json\n",
    "    entityIdSchema = {}\n",
    "    for i in range(0, noOfEntitiesInJsonFile):\n",
    "        try:\n",
    "            if allEntities[i].id:\n",
    "                entityIdSchema[i] = [int(allEntities[i].id)]\n",
    "        except:\n",
    "            temp_arr = []\n",
    "            for j in allEntities[i].properties:\n",
    "                temp_arr.append(int(j.id))\n",
    "            entityIdSchema[i] = temp_arr\n",
    "\n",
    "    # Single Level Entities file : jsonDict\n",
    "    jsonDict = documentai.Document.Entity()\n",
    "\n",
    "    entitiesArray = []\n",
    "\n",
    "    for i in range(0, noOfEntitiesInJsonFile):\n",
    "        try:\n",
    "            if allEntities[i].id:\n",
    "                entitiesArray.append(allEntities[i])\n",
    "        except:\n",
    "            for j in allEntities[i].properties:\n",
    "                entitiesArray.append(j)\n",
    "    # Sorting the entities using y-coordinates to order them according to the rows\n",
    "    entitiesArray = sorted(\n",
    "        entitiesArray,\n",
    "        key=lambda x: x.page_anchor.page_refs[0].bounding_poly.normalized_vertices[0].y,\n",
    "    )\n",
    "    newEntitiesArray = []\n",
    "    for i in entitiesArray:\n",
    "        print(\"-------------------\")\n",
    "        print(\"Parent : \", i.type, \" : \", i.mention_text)\n",
    "        # Sorting the properties of a single line item using x-coordinates to order them according to the table\n",
    "        if len(i.properties) > 0:\n",
    "            x2 = sorted(\n",
    "                i.properties,\n",
    "                key=lambda x: x.page_anchor.page_refs[0]\n",
    "                .bounding_poly.normalized_vertices[0]\n",
    "                .x,\n",
    "            )\n",
    "            j = 0\n",
    "            while j < len(x2):\n",
    "                k = 0\n",
    "                newEntity = documentai.Document.Entity()  # Adding a new parentItem\n",
    "                newEntity.confidence = i.confidence\n",
    "                newEntity.mention_text = \"\"\n",
    "                xValues = []\n",
    "                yValues = []\n",
    "                properties = []\n",
    "                textSegments = []\n",
    "                while k < len(bankChecksColumn) and j < len(x2):\n",
    "                    if x2[j].type == bankChecksColumn[k]:\n",
    "                        newEntity.mention_text = (\n",
    "                            newEntity.mention_text + x2[j].mention_text\n",
    "                        )\n",
    "                        for m in (\n",
    "                            x2[j]\n",
    "                            .page_anchor.page_refs[0]\n",
    "                            .bounding_poly.normalized_vertices\n",
    "                        ):\n",
    "                            xValues.append(m.x)\n",
    "                            yValues.append(m.y)\n",
    "                        print(x2[j].type, \":\", x2[j].mention_text)\n",
    "                        properties.append(x2[j])\n",
    "                        textSegments.append(x2[j].text_anchor.text_segments[0])\n",
    "                    else:\n",
    "                        k += 1\n",
    "                        continue\n",
    "                    j += 1\n",
    "                    if j < len(x2) and x2[j].type == bankChecksColumn[k]:\n",
    "                        newEntity.mention_text = (\n",
    "                            newEntity.mention_text + x2[j].mention_text\n",
    "                        )\n",
    "                        for m in (\n",
    "                            x2[j]\n",
    "                            .page_anchor.page_refs[0]\n",
    "                            .bounding_poly.normalized_vertices\n",
    "                        ):\n",
    "                            xValues.append(m.x)\n",
    "                            yValues.append(m.y)\n",
    "                        print(x2[j].type, \":\", x2[j].mention_text)\n",
    "                        properties.append(x2[j])\n",
    "                        textSegments.append(x2[j].text_anchor.text_segments[0])\n",
    "                        j += 1\n",
    "\n",
    "                    k += 1\n",
    "                    # j+=1\n",
    "                # if len(xValues)>0:\n",
    "                xParentMax = max(xValues)\n",
    "                xParentMin = min(xValues)\n",
    "                yParentMax = max(yValues)\n",
    "                yParentMin = min(yValues)\n",
    "                if i.page_anchor.page_refs[0].page:\n",
    "                    newEntity.page_anchor = {\n",
    "                        \"page_refs\": [\n",
    "                            {\n",
    "                                \"bounding_poly\": {\n",
    "                                    \"normalized_vertices\": [\n",
    "                                        {\"x\": xParentMin, \"y\": yParentMin},\n",
    "                                        {\"x\": xParentMax, \"y\": yParentMin},\n",
    "                                        {\"x\": xParentMax, \"y\": yParentMax},\n",
    "                                        {\"x\": xParentMin, \"y\": yParentMax},\n",
    "                                    ]\n",
    "                                },\n",
    "                                \"page\": i.page_anchor.page_refs[0].page,\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                else:\n",
    "                    newEntity.page_anchor = {\n",
    "                        \"page_refs\": [\n",
    "                            {\n",
    "                                \"bounding_poly\": {\n",
    "                                    \"normalized_vertices\": [\n",
    "                                        {\"x\": xParentMin, \"y\": yParentMin},\n",
    "                                        {\"x\": xParentMax, \"y\": yParentMin},\n",
    "                                        {\"x\": xParentMax, \"y\": yParentMax},\n",
    "                                        {\"x\": xParentMin, \"y\": yParentMax},\n",
    "                                    ]\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                newEntity.properties = properties\n",
    "                newEntity.text_anchor = {\"text_segments\": textSegments}\n",
    "                newEntity.type = i.type\n",
    "                print(\"*****************\")\n",
    "                print(newEntity)\n",
    "                newEntitiesArray.append(newEntity)\n",
    "                print(\"*****************\")\n",
    "    entitiesArray = newEntitiesArray\n",
    "    for e in entitiesArray:\n",
    "        if e.properties:\n",
    "            if len(e.properties) > 0:\n",
    "                for j in e.properties:\n",
    "                    if j.id:\n",
    "                        del j.id\n",
    "    for e in entitiesArray:\n",
    "        if e.properties:\n",
    "            if len(e.properties) > 0:\n",
    "                for j in e.properties:\n",
    "                    if j.type:\n",
    "                        print(e.type)\n",
    "                        j.type = e.type + \"/\" + j.type\n",
    "    jsonData.entities = entitiesArray\n",
    "    return jsonData\n",
    "\n",
    "\n",
    "def jsonToResultDf(jsonData: documentai.Document, jsonFileName: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert the document proto into a csv report with file name with the entity,id, confidence and text\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    jsonData : documentai.Document\n",
    "            The document proto with the updated data.\n",
    "    jsonFileName :str\n",
    "            Document file name.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Returns the csv report with the required columns.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    allEntities = jsonData.entities\n",
    "    noOfEntitiesInJsonFile = len(allEntities)\n",
    "\n",
    "    def get_jsonDict_prop(allEntities, i):\n",
    "        jsonDict_prop = documentai.Document.Entity()\n",
    "        try:\n",
    "            jsonDict_prop.id = allEntities[i].id\n",
    "\n",
    "        except:\n",
    "            jsonDict_prop.id = \"\"\n",
    "        try:\n",
    "            jsonDict_prop.type = allEntities[i].type\n",
    "        except:\n",
    "            jsonDict_prop.type = \"\"\n",
    "        try:\n",
    "            jsonDict_prop.confidence = allEntities[i].confidence\n",
    "        except:\n",
    "            jsonDict_prop.confidence = \"\"\n",
    "        try:\n",
    "            jsonDict_prop.mention_text = allEntities[i].mention_text\n",
    "        except:\n",
    "            jsonDict_prop.mention_text = \"\"\n",
    "        return jsonDict_prop\n",
    "\n",
    "    # Single Level Entities file : jsonDict\n",
    "    jsonDict = {\n",
    "        \"File Name\": [],\n",
    "        \"ID\": [],\n",
    "        \"Entity Type\": [],\n",
    "        \"Confidence\": [],\n",
    "        \"Text\": [],\n",
    "    }\n",
    "\n",
    "    entitiesArray = []\n",
    "\n",
    "    for i in range(0, noOfEntitiesInJsonFile):\n",
    "        try:\n",
    "            if allEntities[i].id:\n",
    "                entitiesArray.append(allEntities[i])\n",
    "        except:\n",
    "            try:\n",
    "                if allEntities[i].properties:\n",
    "                    json_dict_temp = get_jsonDict_prop(allEntities, i)\n",
    "\n",
    "                    entitiesArray.append(json_dict_temp)\n",
    "                for j in allEntities[i].properties:\n",
    "                    entitiesArray.append(j)\n",
    "            except:\n",
    "                entitiesArray.append(\n",
    "                    {\n",
    "                        \"type\": allEntities[i].type,\n",
    "                        \"mention_text\": allEntities[i].mention_text,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    for i in range(0, len(entitiesArray)):\n",
    "        jsonDict[\"File Name\"].append(jsonFileName)\n",
    "        try:\n",
    "            jsonDict[\"ID\"].append(entitiesArray[i].id)\n",
    "        except:\n",
    "            jsonDict[\"ID\"].append(\"\")\n",
    "        try:\n",
    "            jsonDict[\"Entity Type\"].append(entitiesArray[i].type)\n",
    "        except:\n",
    "            jsonDict[\"Entity Type\"].append(\"\")\n",
    "        try:\n",
    "            jsonDict[\"Confidence\"].append(entitiesArray[i].confidence)\n",
    "        except:\n",
    "            jsonDict[\"Confidence\"].append(None)\n",
    "        try:\n",
    "            jsonDict[\"Text\"].append(entitiesArray[i].mention_text)\n",
    "        except:\n",
    "            jsonDict[\"Text\"].append(\"\")\n",
    "\n",
    "    df = pd.DataFrame(jsonDict)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Asynchronous processing of files using Bank statement parser provided\n",
    "logger(\n",
    "    \"logging.txt\",\n",
    "    \"----------------------------------------LOGGING STARTED----------------------------------------\",\n",
    ")\n",
    "try:\n",
    "    temp_intital_pdfpath, temp_pdffolder, temp_pdfbucket = get_files_not_parsed(\n",
    "        gcs_input_dir, gcs_output_dir\n",
    "    )\n",
    "    logger(\"logging.txt\", \"Batch processing the documents.......\")\n",
    "    res = batch_process_documents_sample(\n",
    "        project_id, \"us\", processor_id, temp_intital_pdfpath, gcs_output_dir\n",
    "    )\n",
    "    logger(\"logging.txt\", \"Batch processing of documents done\")\n",
    "    delete_folder(temp_pdfbucket, temp_pdffolder)\n",
    "except Exception as e:\n",
    "    logger(\n",
    "        \"logging.txt\",\n",
    "        \"Files are not processed because of error message--> {}\".format(e),\n",
    "    )\n",
    "    delete_folder(temp_pdfbucket, temp_pdffolder)\n",
    "    pass\n",
    "# Getting bucket names and prefixes for further use\n",
    "Input_bucket_name = gcs_input_dir.split(\"/\")[2]\n",
    "prefix_input_files = \"/\".join(gcs_input_dir.split(\"/\")[3:])\n",
    "Output_bucket_name = gcs_output_dir.split(\"/\")[2]\n",
    "prefix_output_files = \"/\".join(gcs_output_dir.split(\"/\")[3:])\n",
    "New_output_json_bucket = gcs_new_output_json_path.split(\"/\")[2]\n",
    "New_prefix_output_jsons = \"/\".join(gcs_new_output_json_path.split(\"/\")[3:])\n",
    "\n",
    "\n",
    "df3 = pd.DataFrame()\n",
    "try:\n",
    "    temp_json_path, temp_json_folder, temp_json_bucket = get_files_not_postparsed(\n",
    "        gcs_output_dir, gcs_new_output_json_path\n",
    "    )\n",
    "    json_f, file_dict = file_names(temp_json_path)\n",
    "    json_files = list(file_dict.values())\n",
    "    logger(\"logging.txt\", \"list of json files prepared in the output folder\")\n",
    "    # delete_folder(temp_json_bucket,temp_json_folder)\n",
    "    try:\n",
    "        for i in range(len(json_f)):\n",
    "            try:\n",
    "                temp_json_gcs_path = temp_json_path + \"/\" + json_f[i]\n",
    "                temp_bucket_name = temp_json_gcs_path.split(\"/\")[2]\n",
    "                prefix_temp_file_name = \"/\".join(temp_json_gcs_path.split(\"/\")[3:])\n",
    "                document = documentai_json_proto_downloader(\n",
    "                    temp_bucket_name, prefix_temp_file_name\n",
    "                )\n",
    "                logger(\n",
    "                    \"logging.txt\",\n",
    "                    \"loaded json file--||{}||-- from the output GCS folder\".format(\n",
    "                        json_f[i]\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "                try:\n",
    "                    maxId = maxIdFinder(document)\n",
    "                    logger(\"logging.txt\", \"Getting the Max id in the json data\")\n",
    "                except Exception as e:\n",
    "                    logger(\n",
    "                        \"logging.txt\",\n",
    "                        \"Couldnt get the Max id because of error message--> {}\".format(\n",
    "                            e\n",
    "                        ),\n",
    "                    )\n",
    "                    continue\n",
    "                try:\n",
    "                    document = Boundary_markers(document)\n",
    "                    logger(\"logging.txt\", \"account number, transactions are renamed\")\n",
    "                except Exception as e:\n",
    "                    logger(\n",
    "                        \"logging.txt\",\n",
    "                        \"Couldnt rename account related entities because of error message--> {}\".format(\n",
    "                            e\n",
    "                        ),\n",
    "                    )\n",
    "                    continue\n",
    "                try:\n",
    "                    document = fixAccountBalance(document)\n",
    "                    logger(\n",
    "                        \"logging.txt\",\n",
    "                        \"Account balance (starting and ending balance ) is fixed\",\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    logger(\n",
    "                        \"logging.txt\",\n",
    "                        \"Couldnt fix starting and ending balance because of error message--> {}\".format(\n",
    "                            e\n",
    "                        ),\n",
    "                    )\n",
    "                    continue\n",
    "                try:\n",
    "                    document = accounttype_change(document)\n",
    "                    logger(\"logging.txt\", \"account_type is changed to account_name\")\n",
    "                except Exception as e:\n",
    "                    logger(\n",
    "                        \"logging.txt\",\n",
    "                        \"Couldnt change account_type is changed to account_name because of error message--> {}\".format(\n",
    "                            e\n",
    "                        ),\n",
    "                    )\n",
    "                    continue\n",
    "                try:\n",
    "                    document = borrowerNameFix(document)\n",
    "                    logger(\n",
    "                        \"logging.txt\",\n",
    "                        \"entities- client_name is split into first_name, last_name and middle_name if available\",\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    logger(\n",
    "                        \"logging.txt\",\n",
    "                        \"Couldnt split the client_name because of error message--> {}\".format(\n",
    "                            e\n",
    "                        ),\n",
    "                    )\n",
    "                    continue\n",
    "                try:\n",
    "                    for key in dict_ent_rename.keys():\n",
    "                        document = ent_rename(document, key, dict_ent_rename[key])\n",
    "                    logger(\n",
    "                        \"logging.txt\",\n",
    "                        \"Some entities are renamed as per gatless names given\",\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    logger(\n",
    "                        \"logging.txt\",\n",
    "                        \"Couldnt rename some entities because of error message--> {}\".format(\n",
    "                            e\n",
    "                        ),\n",
    "                    )\n",
    "                    continue\n",
    "                try:\n",
    "                    document = address_function(document)\n",
    "                    logger(\n",
    "                        \"logging.txt\",\n",
    "                        \"Client address is split into street name,zip code and city\",\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    logger(\n",
    "                        \"logging.txt\",\n",
    "                        \"Couldnt split the Client address because of error message--> {}\".format(\n",
    "                            e\n",
    "                        ),\n",
    "                    )\n",
    "                    continue\n",
    "                try:\n",
    "                    document = add_total_pages(document)  # TODO : here will start\n",
    "                    logger(\"logging.txt\", \"Adding total pages entity into json\")\n",
    "                except Exception as e:\n",
    "                    logger(\n",
    "                        \"logging.txt\",\n",
    "                        \"Couldnt radd total pages entity into pages because of error message--> {}\".format(\n",
    "                            e\n",
    "                        ),\n",
    "                    )\n",
    "                    pass\n",
    "                try:\n",
    "                    if checksFlag == True:\n",
    "                        Financial_Institution = \"\"\n",
    "                        for e in document.entities:\n",
    "                            if e.type == \"Financial_Institution\":\n",
    "                                Financial_Institution = e.mention_text\n",
    "                                break\n",
    "                        file_path = gcs_input_dir + \"/\" + json_f[i][:-7] + \".pdf\"\n",
    "                        file_bucket_name = file_path.split(\"/\")[2]\n",
    "                        prefex_file_path = \"/\".join(file_path.split(\"/\")[3:])\n",
    "                        Financial_Institution = \"\".join(\n",
    "                            Financial_Institution.strip().split()\n",
    "                        ).lower()\n",
    "                        if Financial_Institution in [\n",
    "                            \"wellsfargo\",\n",
    "                            \"chase\",\n",
    "                            \"bankofamerica\",\n",
    "                        ]:\n",
    "                            storage_client = storage.Client()\n",
    "                            bucket = storage_client.bucket(file_bucket_name)\n",
    "                            blob = bucket.blob(prefex_file_path)\n",
    "                            pdf_bytes = blob.download_as_string()\n",
    "                            check_json = process_document_sample(\n",
    "                                project_id,\n",
    "                                \"us\",\n",
    "                                processor_id_checks,\n",
    "                                file_path,\n",
    "                                pdf_bytes,\n",
    "                                processor_version_checks,\n",
    "                            )\n",
    "                            check_json_output = groupChecks(\n",
    "                                check_json.document, Financial_Institution\n",
    "                            )\n",
    "                            combined_entities = (\n",
    "                                document.entities + check_json_output.entities\n",
    "                            )\n",
    "                            document.entities = combined_entities\n",
    "                            # fs.pipe(gcs_new_output_checks_json_path+json_f[i].split('/')[-1],bytes(json.dumps(check_json_output,ensure_ascii=False),'utf-8'),content_type='application/json')\n",
    "                            logger(\"logging.txt\", \"Checks Details are added \")\n",
    "                        else:\n",
    "                            logger(\n",
    "                                \"logging.txt\",\n",
    "                                \"Financial Institution is out of Scope--> {}\".format(\n",
    "                                    Financial_Institution\n",
    "                                ),\n",
    "                            )\n",
    "                            pass\n",
    "                    else:\n",
    "                        pass\n",
    "                except:\n",
    "                    logger(\n",
    "                        \"logging.txt\",\n",
    "                        \"Couldnt find Checks Detail due to error message--> {}\".format(\n",
    "                            e\n",
    "                        ),\n",
    "                    )\n",
    "                    continue\n",
    "                try:\n",
    "                    final_files_list, final_files_dict = file_names(\n",
    "                        gcs_new_output_json_path\n",
    "                    )\n",
    "                    store_document_as_json(\n",
    "                        documentai.Document.to_json(document),\n",
    "                        New_output_json_bucket,\n",
    "                        New_prefix_output_jsons + final_files_list[i],\n",
    "                    )\n",
    "                    logger(\n",
    "                        \"logging.txt\",\n",
    "                        \"post processed json files are moved to gcs postprocessed folder provided\",\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    logger(\n",
    "                        \"logging.txt\",\n",
    "                        \"Couldnt upload the post processed json file because of error message--> {}\".format(\n",
    "                            e\n",
    "                        ),\n",
    "                    )\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "    except Exception as e:\n",
    "        logger(\n",
    "            \"logging.txt\",\n",
    "            \"Couldnt load json file because of error message--> {}\".format(e),\n",
    "        )\n",
    "        pass\n",
    "except Exception as e:\n",
    "    logger(\n",
    "        \"logging.txt\",\n",
    "        \"Couldnt get list of json files because of error message--> {}\".format(e),\n",
    "    )\n",
    "    delete_folder(temp_json_bucket, temp_json_folder)\n",
    "    pass\n",
    "\n",
    "# changing meta data of post processed files\n",
    "\n",
    "try:\n",
    "    !gsutil -m setmeta -h \"content-Type:application/json\" {gcs_new_output_json_path}*\n",
    "    logger(\n",
    "        \"logging.txt\",\n",
    "        \"meta data for post processed json files changed to application/json \",\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger(\n",
    "        \"logging.txt\",\n",
    "        \"Couldnt update meta data for post processed json files changed to application/json  because of error message--> {}\".format(\n",
    "            e\n",
    "        ),\n",
    "    )\n",
    "\n",
    "delete_folder(temp_json_bucket, temp_json_folder)\n",
    "# creating data frame and saving data into csv\n",
    "try:\n",
    "    logger(\"logging.txt\", \"creating dataframe to create consolidated csv\")\n",
    "    final_files_list, final_files_dict = file_names(gcs_new_output_json_path)\n",
    "    for i in range(len(final_files_list)):\n",
    "        json_2 = documentai_json_proto_downloader(\n",
    "            New_output_json_bucket, New_prefix_output_jsons + final_files_list[i]\n",
    "        )\n",
    "        df1 = jsonToResultDf(json_2, final_files_list[i])\n",
    "        df2 = df1\n",
    "        df3 = pd.concat([df3, df2], ignore_index=True)\n",
    "except Exception as e:\n",
    "    logger(\n",
    "        \"logging.txt\",\n",
    "        \"failed to create dataframe to create consolidated csv because of error--> {}\".format(\n",
    "            e\n",
    "        ),\n",
    "    )\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    df3.to_csv(\"Consolidated.csv\")\n",
    "    logger(\"logging.txt\", \"Consolidated CSV file created\")\n",
    "except Exception as e:\n",
    "    logger(\n",
    "        \"logging.txt\",\n",
    "        \"Couldnt create consolidated CSV file  because of error message--> {}\".format(\n",
    "            e\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "logger(\n",
    "    \"logging.txt\",\n",
    "    \"----------------------------------------END OF POST PROCESSING----------------------------------------\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0b1c78-9991-4b4c-b241-e18bc39e9d34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
