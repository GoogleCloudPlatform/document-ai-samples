{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9ad8788-654c-4e38-9d6d-42158d95c6ee",
   "metadata": {},
   "source": [
    "# Line Item Improver Using Column Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08400e10-6639-43a6-8e2a-0b4f28ba08e2",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc2d117-9e2a-4ebe-8aef-94ad4f2b46b1",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the **DocAI Incubator Team**. No guarantees of performance are implied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f1d1c0-823d-46d4-9a4a-e5ffa56c6b55",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "This tool is intended as a guide to help for tagging the column data in the line item to improve the Model F1-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe00a75-567e-4f33-b4b4-aa72f9f4fe50",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* Python : Jupyter Notebook (Vertex).\n",
    "* Storage Bucket for storing exported json files and output JSON files.\n",
    "* Permission For Google DocAI Processors, Storage and Vertex AI Notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b33b7f0-baf8-421c-b476-2d25b1ee9b02",
   "metadata": {},
   "source": [
    "## Step by Step Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadfe192-5691-4053-8670-32ef3e5bea8c",
   "metadata": {},
   "source": [
    "### 1. Import Modules/Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea066b24-5e3b-4c28-a56f-263fc6e9aa67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this cell to download utilities module\n",
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8e2a54-e250-4924-8f58-9a4642e6ddf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install google-cloud-documentai google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6495b161-ac1b-40b2-a668-fe835da2950a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from google.api_core.client_options import ClientOptions\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from google.cloud import storage\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "from utilities import file_names, blob_downloader, bbox_maker, store_document_as_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1709ea5e-bf14-44ab-9963-cda14b092dc2",
   "metadata": {},
   "source": [
    "### 2. Input Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce030a1-663b-445e-a38e-24dc415942c2",
   "metadata": {},
   "source": [
    "* **gcs_input_path**: Provide the gcs path of the parent folder where the sub-folders contain input files. Please follow the folder structure described earlier.\n",
    "* **output_jsons**: Provide gcs path where the output json files have to be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd49bdb7-e934-4b9e-afa0-1c6a80ca2fc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gcs_input_path = \"gs://<<bucket_name>>/<<sub_folder>>/\"\n",
    "output_jsons = \"gs://<<bucket_name>>/<<output_sub_folder>>/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a4b1ed-2e86-4a8e-b00a-ebf6e41921ef",
   "metadata": {},
   "source": [
    "### 3.Run the required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2afdc0-3873-4db9-a7b7-e64cf0dce00b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def column_item_child_entities_coordinates(entity: Dict[str, Any]) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Extracts bounding box coordinates for all child entities within a parent entity.\n",
    "\n",
    "    Args:\n",
    "    - entity (Dict[str, Any]): A dictionary representing the parent entity, containing its properties.\n",
    "\n",
    "    Returns:\n",
    "    - List[List[float]]: A list of bounding box coordinates for each child entity,\n",
    "      where each bounding box is represented as [min_x, min_y, max_x, max_y].\n",
    "    \"\"\"\n",
    "    all_entity_coordinates = []\n",
    "    for i in entity[\"properties\"]:\n",
    "        bounding_poly = i[\"pageAnchor\"][\"pageRefs\"][0][\"boundingPoly\"][\n",
    "            \"normalizedVertices\"\n",
    "        ]\n",
    "        entity_coordinates = bbox_maker(bounding_poly)\n",
    "        all_entity_coordinates.append(entity_coordinates)\n",
    "    return all_entity_coordinates\n",
    "\n",
    "\n",
    "def get_token_xy(token: Any) -> Tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Extracts the normalized bounding box coordinates (min_x, min_y, max_x, max_y) of a token.\n",
    "\n",
    "    Args:\n",
    "    - token (Any): A token object with layout information.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[float, float, float, float]: The normalized bounding box coordinates.\n",
    "    \"\"\"\n",
    "    vertices = token.layout.bounding_poly.normalized_vertices\n",
    "    minx_token, miny_token = min(point.x for point in vertices), min(\n",
    "        point.y for point in vertices\n",
    "    )\n",
    "    maxx_token, maxy_token = max(point.x for point in vertices), max(\n",
    "        point.y for point in vertices\n",
    "    )\n",
    "    return minx_token, miny_token, maxx_token, maxy_token\n",
    "\n",
    "\n",
    "def get_token_data(\n",
    "    json_dict: Any,\n",
    "    min_x: float,\n",
    "    max_x: float,\n",
    "    min_y: float,\n",
    "    max_y: float,\n",
    "    page_num: int,\n",
    ") -> Tuple[str, List[Dict[str, int]], List[Dict[str, float]]]:\n",
    "    \"\"\"\n",
    "    Extracts token data from the JSON dictionary based on provided bounding box coordinates and page number.\n",
    "\n",
    "    Args:\n",
    "    - json_dict (Any): The JSON dictionary containing token data.\n",
    "    - min_x (float): Minimum x-coordinate of the bounding box.\n",
    "    - max_x (float): Maximum x-coordinate of the bounding box.\n",
    "    - min_y (float): Minimum y-coordinate of the bounding box.\n",
    "    - max_y (float): Maximum y-coordinate of the bounding box.\n",
    "    - page_num (int): Page number.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[str, List[Dict[str, int]], List[Dict[str, float]]]: A tuple containing:\n",
    "        1. The extracted text from the tokens.\n",
    "        2. A list of dictionaries containing text anchor data for each token.\n",
    "        3. A list of dictionaries containing bounding box data.\n",
    "    \"\"\"\n",
    "    text_anc_temp = []\n",
    "    text_anc = []\n",
    "    text_anchor = []\n",
    "    page_anc_temp = {\"x\": [], \"y\": []}\n",
    "    y_allowance = 0.001\n",
    "    x_allowance = 0.01\n",
    "    for page in json_dict.pages:\n",
    "        if page_num == page.page_number - 1:\n",
    "            for token in page.tokens:\n",
    "                minx_token, miny_token, maxx_token, maxy_token = get_token_xy(token)\n",
    "                if (\n",
    "                    min_y <= miny_token + y_allowance\n",
    "                    and max_y >= maxy_token - y_allowance\n",
    "                    and min_x <= minx_token + x_allowance\n",
    "                    and max_x >= maxx_token - x_allowance\n",
    "                ):\n",
    "                    temp_anc = token.layout.text_anchor.text_segments[0]\n",
    "                    text_anc.append(temp_anc)\n",
    "                    page_anc_temp[\"x\"].extend([minx_token, maxx_token])\n",
    "                    page_anc_temp[\"y\"].extend([miny_token, maxy_token])\n",
    "                    for seg in token.layout.text_anchor.text_segments:\n",
    "                        text_anc_temp.append([seg.start_index, seg.end_index])\n",
    "                        text_anchor.append(\n",
    "                            {\"endIndex\": seg.end_index, \"startIndex\": seg.start_index}\n",
    "                        )\n",
    "    if page_anc_temp != {\"x\": [], \"y\": []}:\n",
    "        page_anc = [\n",
    "            {\"x\": min(page_anc_temp[\"x\"]), \"y\": min(page_anc_temp[\"y\"])},\n",
    "            {\"x\": max(page_anc_temp[\"x\"]), \"y\": min(page_anc_temp[\"y\"])},\n",
    "            {\"x\": min(page_anc_temp[\"x\"]), \"y\": max(page_anc_temp[\"y\"])},\n",
    "            {\"x\": max(page_anc_temp[\"x\"]), \"y\": max(page_anc_temp[\"y\"])},\n",
    "        ]\n",
    "    if text_anc_temp:\n",
    "        sorted_data = sorted(text_anc_temp, key=lambda x: x[0])\n",
    "        mention_text = \"\"\n",
    "        for start_index, end_index in sorted_data:\n",
    "            mention_text += json_dict.text[start_index:end_index]\n",
    "        return mention_text, text_anchor, page_anc\n",
    "\n",
    "\n",
    "def update_parent_entity(\n",
    "    entity: Dict[str, Any],\n",
    "    token: Tuple[str, List[Dict[str, int]], List[Dict[str, float]]],\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Updates a parent entity with new token data.\n",
    "\n",
    "    Args:\n",
    "    - entity (Dict[str, Any]): The parent entity to be updated.\n",
    "    - token (Tuple[str, List[Dict[str, int]], List[Dict[str, float]]]): Token data to be added.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, Any]: The updated parent entity.\n",
    "    \"\"\"\n",
    "    mention_text, text_anc, page_anc = token\n",
    "    page_num = \"0\"\n",
    "    if \"page\" in entity[\"properties\"][-1][\"pageAnchor\"][\"pageRefs\"][0]:\n",
    "        page_num = entity[\"properties\"][-1][\"pageAnchor\"][\"page\"]\n",
    "    new_entity = {\n",
    "        \"confidence\": 1,\n",
    "        \"mentionText\": mention_text,\n",
    "        \"pageAnchor\": {\n",
    "            \"pageRefs\": [\n",
    "                {\"boundingPoly\": {\"normalizedVertices\": page_anc}, \"page\": page_num}\n",
    "            ]\n",
    "        },\n",
    "        \"textAnchor\": {\"content\": mention_text, \"textSegments\": text_anc},\n",
    "        \"type\": entity[\"properties\"][-1][\"type\"],\n",
    "    }\n",
    "    entity[\"properties\"].append(new_entity)\n",
    "    entity[\"mentionText\"] += \" \" + mention_text\n",
    "    entity[\"textAnchor\"][\"content\"] += mention_text\n",
    "    entity[\"textAnchor\"][\"textSegments\"].extend(text_anc)\n",
    "    parent_bbox = bbox_maker(\n",
    "        entity[\"pageAnchor\"][\"pageRefs\"][0][\"boundingPoly\"][\"normalizedVertices\"]\n",
    "    )\n",
    "    child_bbox = bbox_maker(page_anc)\n",
    "    max_x = max(parent_bbox[2], child_bbox[2])\n",
    "    max_y = max(parent_bbox[3], child_bbox[3])\n",
    "    min_x = min(parent_bbox[0], child_bbox[0])\n",
    "    min_y = min(parent_bbox[1], child_bbox[1])\n",
    "    entity[\"pageAnchor\"][\"pageRefs\"][0][\"boundingPoly\"][\"normalizedVertices\"] = [\n",
    "        {\"x\": max_x, \"y\": max_y},\n",
    "        {\"x\": max_x, \"y\": min_y},\n",
    "        {\"x\": min_x, \"y\": min_y},\n",
    "        {\"x\": min_x, \"y\": max_y},\n",
    "    ]\n",
    "    return entity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1d85cf-7529-469b-be7e-3e00a778af87",
   "metadata": {},
   "source": [
    "### 4.Run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dab42f-6c98-4aac-b804-cd3ec81127f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    gs_file_name = list(file_names(gcs_input_path)[1].values())\n",
    "    for i in gs_file_name:\n",
    "        bucket_name = gcs_input_path.split(\"/\")[2]\n",
    "        file_name = i.split(\"/\")[-1]\n",
    "        json_data = blob_downloader(bucket_name, i)\n",
    "        json_dict = documentai.Document.from_json(json.dumps(json_data))\n",
    "        entities = []\n",
    "        for j in range(len(json_data[\"entities\"])):\n",
    "            # print(\"Before\",len(json_data[\"entities\"][j][\"properties\"]))\n",
    "            page_number = 0\n",
    "            all_entity_coordinates = column_item_child_entities_coordinates(\n",
    "                json_data[\"entities\"][j]\n",
    "            )  # [min(x_list), min(y_list), max(x_list), max(y_list)]\n",
    "            sorted_entity_coordinates = sorted(\n",
    "                all_entity_coordinates, key=lambda x: x[3]\n",
    "            )\n",
    "            if \"page\" in json_data[\"entities\"][j][\"pageAnchor\"][\"pageRefs\"][0].keys():\n",
    "                page_number = int(\n",
    "                    json_data[\"entities\"][j][\"pageAnchor\"][\"pageRefs\"][0][\"page\"]\n",
    "                )\n",
    "            for k in range(len(sorted_entity_coordinates) - 1):\n",
    "                entity_bbox = bbox_maker(\n",
    "                    json_data[\"entities\"][j][\"pageAnchor\"][\"pageRefs\"][0][\n",
    "                        \"boundingPoly\"\n",
    "                    ][\"normalizedVertices\"]\n",
    "                )\n",
    "                min_x = min(entity_bbox[0], entity_bbox[2])\n",
    "                max_x = max(entity_bbox[0], entity_bbox[2])\n",
    "                token_data = get_token_data(\n",
    "                    json_dict,\n",
    "                    min_x,\n",
    "                    max_x,\n",
    "                    sorted_entity_coordinates[k][3],\n",
    "                    sorted_entity_coordinates[k + 1][1],\n",
    "                    page_number,\n",
    "                )\n",
    "                if token_data != None:\n",
    "                    # print(token_data)\n",
    "                    json_data[\"entities\"][j] = update_parent_entity(\n",
    "                        json_data[\"entities\"][j], token_data\n",
    "                    )\n",
    "            # print(\"After\",len(json_data[\"entities\"][j][\"properties\"]))\n",
    "            entities.append(json_data[\"entities\"][j])\n",
    "        json_data[\"entities\"] = entities\n",
    "        store_document_as_json(\n",
    "            json.dumps(json_data),\n",
    "            output_jsons.split(\"/\")[2],\n",
    "            (\"/\").join(output_jsons.split(\"/\")[3:]) + \"new_\" + file_name,\n",
    "        )\n",
    "        print(\"updated Json\")\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e3f538-c1ab-4ff1-a720-80d07ea9245b",
   "metadata": {},
   "source": [
    "### Output Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4badf9b-1b40-4b6a-9ac9-b7a0f233018f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Before Tagging\n",
    "<img src='./images/before.png' width=600 height=600 alt=\"Sample Output\"></img>\n",
    "### After Tagging\n",
    "<img src='./images/after.png' width=600 height=600 alt=\"Sample Output\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c13d25-50de-4c31-ad8b-13eb022b0303",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
