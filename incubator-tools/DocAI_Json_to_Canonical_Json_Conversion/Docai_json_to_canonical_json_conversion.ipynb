{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62ac8784-c132-4866-acf0-0bfd0e569002",
   "metadata": {},
   "source": [
    "# DocAI JSON to Canonical Doc JSON conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b01c70-c6cf-4111-89fd-e830e3016db4",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133f6ba4-6180-4ac7-9084-68e29a4c40df",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the DocAI Incubator Team. No guarantees of performance are implied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189ef8bd-7434-4fd1-ba36-f9c31ce15d12",
   "metadata": {},
   "source": [
    "## Purpose and Description\n",
    "<div><span style=\"background-color:#f5f569;font-weight:800\" ><i><b>Note:</b> This feature is in Preview with allowlist. To turn on this feature , contact your Google account team.</i></span><div>\n",
    "\n",
    "\n",
    "A parsed, unstructured document(Canonical Doc JSONs) is represented by JSON that describes the unstructured document using a sequence of text, table, and list blocks. You import canonical JSON files with your parsed unstructured document data in the same way that you import other types of unstructured documents, such as PDFs. When this feature is turned on, whenever a JSON file is uploaded and identified by either an application/json MIME type or a .JSON extension, it is treated as a parsed document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a048aa80-bd14-488c-8f84-75c93a3cb00a",
   "metadata": {},
   "source": [
    "**Canonical Json :** Canonical Doc JSONs are a JSON representation of parsed unstructured documents. They use a sequence of text, table, and list blocks to describe the document's structure.\n",
    "\n",
    "Refer below cell, which gives details about **Canonical Json Schema**  \n",
    "Change the metadata_schema according to your need. Ex : if you want to add file info, add a key&value pair to structData key of metadata_schema object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20af14d1-8a02-4f13-8c63-9a13f16da33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre defined json structure\n",
    "\n",
    "conanical_schema = {\n",
    "    \"title\": \"Some Title\",\n",
    "    \"blocks\": [\n",
    "        {\n",
    "            \"textBlock\": {\"text\": \"Some PARAGRAPH 1\", \"type\": \"PARAGRAPH\"},\n",
    "            \"pageNumber\": 1,\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "metadata_schema = {\n",
    "    \"id\": \"your_random_id\",\n",
    "    \"structData\": {\n",
    "        \"Title\": \"File Title\",\n",
    "        \"Description\": \"Your file description\",\n",
    "        \"Source_url\": \"https://storage.mtls.cloud.google.com/\",\n",
    "    },\n",
    "    \"content\": {\"mimeType\": \"application/json\", \"uri\": \"gs://\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c358842d-da07-4d4e-8521-d55eb5197f5b",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Vertex AI Notebook\n",
    "2. Parser Json files in GCS Folders.\n",
    "3. PDFs files in GCS Folders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d2adea-a44f-4c06-863c-a77454080ae8",
   "metadata": {},
   "source": [
    "## Step by Step procedure "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8765cc08-24aa-4540-b0f7-32d3c32d457c",
   "metadata": {},
   "source": [
    "### 1. Import Modules/Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532dea4c-5f0b-40ff-ac3f-3e011c709437",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dbcf5eb5-1dc4-40be-bba1-f9ad160e40fa",
   "metadata": {},
   "source": [
    "import json\n",
    "import uuid\n",
    "from copy import deepcopy\n",
    "from google.cloud import documentai\n",
    "from typing import Any, Dict, List, Optional, Sequence, Tuple, Union\n",
    "from utilities import documentai_json_proto_downloader,store_document_as_json, file_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae09265-eaed-4fe5-abf6-c57aa66d8564",
   "metadata": {},
   "source": [
    "### 2. Input Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eb5b04-e17f-4402-95e5-98c40b905916",
   "metadata": {},
   "source": [
    "* <b>GCS_INPUT_PATH :</b> GCS path for input files. It should contain DocAI processed output json files and also the pdfs which got parsed by the processor with the same name as json files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b80a90a-9c93-4710-9f37-fb0faf63cd05",
   "metadata": {},
   "source": [
    "This is how the input bucket structure should look like : \n",
    "<img src=\"./Images/input_sample_image.png\" width=1200 height=400 alt=\"input bucket sample image\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e782c446-c601-4a07-b511-ae2c2b8821eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please follow the folders path to sucessfully create metadata\n",
    "\n",
    "# Given  folders in GCS_INPUT_PATH with the following structure :\n",
    "#\n",
    "# gs://path/to/input/folder\n",
    "#   ├──/processor_output/    Folder having parsed json from the processor.\n",
    "#   └──/pdfs/                Folder having all the pdfs which got parsed with same name as jsons.\n",
    "\n",
    "GCS_INPUT_PATH = \"gs://{bucket_name}/{folder_path}\"\n",
    "\n",
    "input_bucket_name = GCS_INPUT_PATH.split(\"/\")[2]\n",
    "input_prefix_path = \"/\".join(GCS_INPUT_PATH.split(\"/\")[3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3508a7de-06d7-48cc-95bf-be335562d344",
   "metadata": {},
   "source": [
    "#### Change the metadata_schema according to your need. Ex : if you want to add file info, add a key&value pair to structData key of metadata_schema object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062d74d4-7a32-4c16-b4a2-e314452f917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre defined json structure\n",
    "\n",
    "conanical_schema = {\n",
    "    \"title\": \"Some Title\",\n",
    "    \"blocks\": [\n",
    "        {\n",
    "            \"textBlock\": {\"text\": \"Some PARAGRAPH 1\", \"type\": \"PARAGRAPH\"},\n",
    "            \"pageNumber\": 1,\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "metadata_schema = {\n",
    "    \"id\": \"your_random_id\",\n",
    "    \"structData\": {\n",
    "        \"Title\": \"File Title\",\n",
    "        \"Description\": \"Your file description\",\n",
    "        \"Source_url\": \"https://storage.mtls.cloud.google.com/\",\n",
    "    },\n",
    "    \"content\": {\"mimeType\": \"application/json\", \"uri\": \"gs://\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6aaa615-f4dd-4520-a0bc-ad3a12cac009",
   "metadata": {},
   "source": [
    "### 3. Run the scipt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1618f90-feda-4802-9122-7463c8e75d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_doc_object_to_conanical_object(\n",
    "    doc_object: documentai.Document, file_path: str\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    To convert the document AI object structure to conanical object which is compactible with vertex AI search.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc_object : documentai.Document\n",
    "        The documnet AI  object from the input file provided by the user.\n",
    "\n",
    "    file_path : str\n",
    "        The GCS file path of the json file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict\n",
    "        Returns the converted conanical object.\n",
    "    \"\"\"\n",
    "\n",
    "    conanical_obj = {}\n",
    "    conanical_obj[\"blocks\"] = list()\n",
    "    conanical_schema_c = deepcopy(conanical_schema)\n",
    "    file_name = file_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    OCR_text = doc_object.text\n",
    "\n",
    "    conanical_schema_c[\"title\"] = file_name\n",
    "    conanical_obj[\"title\"] = conanical_schema_c[\"title\"]\n",
    "\n",
    "    # looping through all pages\n",
    "    for page in doc_object.pages:\n",
    "        page_number = page.page_number\n",
    "        conanical_schema_c[\"blocks\"][0][\"pageNumber\"] = page_number\n",
    "\n",
    "        # looping through all paragraph and getting OCR text by index\n",
    "        paragraph = page.paragraphs\n",
    "        if paragraph:\n",
    "            first_paragraph = True\n",
    "            for paragraph in page.paragraphs:\n",
    "                text_segments = paragraph.layout.text_anchor.text_segments[0]\n",
    "                if first_paragraph:\n",
    "                    page_starting_index = text_segments.start_index\n",
    "                first_paragraph = False\n",
    "                last_paragraph_index = text_segments.end_index\n",
    "            paragraph_text = OCR_text[page_starting_index:last_paragraph_index]\n",
    "            conanical_schema_c[\"blocks\"][0][\"textBlock\"][\"text\"] = paragraph_text\n",
    "            conanical_obj[\"blocks\"].append(deepcopy(conanical_schema_c[\"blocks\"][0]))\n",
    "    return conanical_obj\n",
    "\n",
    "\n",
    "def create_metadata(metadata: str, output_file: str) -> str:\n",
    "    \"\"\"\n",
    "    To create the metadata file by adding pdfs file location, content type, converted json location,other user defined schema.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    metadata : str\n",
    "        The metadata string with the older configuration which will get update with new configuration.\n",
    "\n",
    "    output_file : str\n",
    "        The GCS path of the Canonical json path.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Returns the updated metadata string having the latest file info attached with the older metadata string.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata_json_copy = deepcopy(metadata_schema)\n",
    "    file_path = output_file.replace(\"gs://\", \"\")\n",
    "    pdf_file_path = (\n",
    "        GCS_INPUT_PATH + \"/pdfs/\" + file_path.split(\"/\")[-1].split(\".\")[0] + \".pdf\"\n",
    "    )\n",
    "    pdf_file_path = pdf_file_path.replace(\"gs://\", \"\")\n",
    "    metadata_json_copy[\"id\"] = str(uuid.uuid4())\n",
    "    metadata_json_copy[\"content\"][\"uri\"] += file_path\n",
    "    metadata_json_copy[\"structData\"][\"Source_url\"] += pdf_file_path\n",
    "    metadata_json_copy[\"structData\"][\"Title\"] = file_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    metadata += json.dumps(metadata_json_copy) + \"\\n\"\n",
    "    return metadata\n",
    "\n",
    "\n",
    "output_files_for_metadata = []\n",
    "file_name_list = [\n",
    "    i\n",
    "    for i in list(file_names(f\"{GCS_INPUT_PATH}/processor_output\")[1].values())\n",
    "    if i.endswith(\".json\")\n",
    "]\n",
    "\n",
    "print(\"Converting files ...\")\n",
    "for file_name in file_name_list:\n",
    "    try:\n",
    "        document = documentai_json_proto_downloader(input_bucket_name, file_name)\n",
    "        conanical_object = convert_doc_object_to_conanical_object(document, file_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[x] {input_bucket_name}/{file_name} || Error : {str(e)}\")\n",
    "        continue\n",
    "    output_file_name = f\"{input_prefix_path}/output/{file_name.split('/')[-1]}\"\n",
    "    output_files_for_metadata.append(f\"gs://{input_bucket_name}/{output_file_name}\")\n",
    "    store_document_as_json(\n",
    "        json.dumps(conanical_object), input_bucket_name, output_file_name\n",
    "    )\n",
    "    print(f\"[✓] {input_bucket_name}/{output_file_name}\")\n",
    "\n",
    "print(\"\\n\\nCreating metadata file ...\")\n",
    "metadata_str = \"\"\n",
    "for gcs_output_file in output_files_for_metadata:\n",
    "    metadata_str = create_metadata(metadata_str, gcs_output_file)\n",
    "\n",
    "store_document_as_json(\n",
    "    metadata_str, input_bucket_name, input_prefix_path + \"/metadata/\" + \"metadata.jsonl\"\n",
    ")\n",
    "print(\n",
    "    \"Metadata created & stored in\",\n",
    "    f\"gs://{input_bucket_name}/{input_prefix_path}/metadata/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a390272-f2a8-435e-b626-6bcc9c91dd43",
   "metadata": {},
   "source": [
    "### 4. Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa841590-c1ac-4dd5-9301-d97b149236a4",
   "metadata": {},
   "source": [
    "Document AI json after conversion to Canonical json and store the files to output folder inside the GCS_INPUT_PATH folder. Each page text will get store inside each text block with their respective page number.<br>\n",
    "<img src=\"./Images/conanical_json_output_1.png\" width=1200 height=400 alt=\"conanical json output image\">\n",
    "\n",
    "Finally script will create metadata having GCS location of converted  Canonical json with the authentication link of pdfs.\n",
    "<img src=\"./Images/conanical_json_output_2.png\" width=800 height=400 alt=\"conanical json output image\"><br><hr>\n",
    "\n",
    "Import the metadata file in vertex AI search and conversation datastore, verify the files got imported.<br>\n",
    "<img src=\"./Images/conanical_json_output_3.png\" width=800 height=400 alt=\"conanical json output image\"><br>\n",
    "<img src=\"./Images/conanical_json_output_4.png\" width=800 height=400 alt=\"conanical json output image\">"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
