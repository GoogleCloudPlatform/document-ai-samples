{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9e4378f-3a34-4386-9bc8-1a7f6d2f3b81",
   "metadata": {},
   "source": [
    "# Tagging Customer Account Number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388f7f51-872d-4214-b2eb-7b8630bebc09",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e1116b-b004-4e2b-a0c6-a4592891dc34",
   "metadata": {},
   "source": [
    "# Disclaimer\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the DocAI Incubator Team. No guarantees of performance are implied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d75f65-58df-4ed7-9849-e9089bb668db",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Objective\n",
    "The purpose of this post-processing script is to identify and tag the \"customer_account_number\" from OCR text, especially when the default invoice or pre-trained processor falls short. The script is tailored to capture account numbers beginning with \"5\" or \"05\", comprising either 9 or 10 digits. It allows for modification to suit different pattern requirements or to identify new entities by adjusting the pattern and tagging them accordingly.\n",
    "\n",
    "\n",
    "**NOTE:** If any other number is of the same pattern as of “customer_account_number”, then that also be tagged.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6a9dcb-f593-40a0-a9a2-7f7f370ed651",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "* Vertex AI Notebook Or Colab (If using Colab, use authentication)\n",
    "* Storage Bucket for storing input and output json files\n",
    "* Permission For Google Storage and Vertex AI Notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943eedfc-55a4-4960-82e1-9cad0778bbae",
   "metadata": {},
   "source": [
    "# Step by Step Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3fb4b7-eb46-4498-b7c2-69960ceb9687",
   "metadata": {},
   "source": [
    "# 1. Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e000842-4d4d-41ec-908a-f745f7371999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download incubator-tools utilities module to present-working-directory\n",
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283d07a3-3da4-4fa0-a78f-30368bdb20d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-cloud-documentai google-cloud-storage tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f6d514b-57eb-420d-8af3-712d2fa57ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from google.cloud import storage\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import utilities\n",
    "from functools import reduce\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a322fe7-65b3-4fb2-8ecf-a60d4a0aea7f",
   "metadata": {},
   "source": [
    "# Input Variables Details\n",
    "**input_path**: GCS Input Path. It should contain DocAI processed output json files.  \n",
    "**output_path**: GCS Output Path. The post-processed json files stored in this path. \n",
    "\n",
    "**NOTE**: GCS path must ends-with trailing-slash (`/`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b905ba6-a518-4006-b802-c9193836cf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_bucket = (\n",
    "    \"gs://xxxxxxxxxxxxxxxx/xxxxxxxxxxxxxxxxx/xxxxxxx/\"  # Input Bucket GCS Path\n",
    ")\n",
    "output_bucket = \"gs://xxxxxxxxxxxxx/xxxxxxxxxxxxxx/\"  # Output Bucket GCS Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ac971c-e7ba-4ffe-83d6-9ea111b6d944",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_storage_bucket_name = input_bucket.split(\"/\")[2]\n",
    "input_bucket_path_prefix = \"/\".join(input_bucket.split(\"/\")[3:])\n",
    "output_storage_bucket_name = output_bucket.split(\"/\")[2]\n",
    "output_bucket_path_prefix = \"/\".join(output_bucket.split(\"/\")[3:])\n",
    "pattern = r\"\\b05[0-9]{8}|5[0-9]{8}\\b\"  # Change according to your CAN pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c532bc40-9bc9-4e94-8c93-0b3d831418e7",
   "metadata": {},
   "source": [
    "# Initialize Google Cloud Storage client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "305629df-4385-4d02-bae3-208b9ee70e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client()\n",
    "source_bucket = storage_client.bucket(input_storage_bucket_name)\n",
    "source_blob = source_bucket.list_blobs(prefix=input_bucket_path_prefix)\n",
    "destination_bucket = storage_client.bucket(output_storage_bucket_name)\n",
    "\n",
    "# Get a list of files and a dictionary of file names\n",
    "list_of_files, file_name_dict = utilities.file_names(input_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cda8f34-50ff-4c62-a687-6f3c33730bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_from_text_anchors(doc, text_anchors_check):\n",
    "    \"\"\"\n",
    "    Extracts X and Y coordinates from text anchors in a DocumentAI document.\n",
    "\n",
    "    Args:\n",
    "        doc (documentai.Document): DocumentAI document containing text anchors.\n",
    "        text_anchors_check (list): List of text anchors to check for coordinates.\n",
    "\n",
    "    Returns:\n",
    "        list: List of coordinates (A, B, C, D) and the page number.\n",
    "    \"\"\"\n",
    "    temp_xy = {\"x\": [], \"y\": []}\n",
    "    min_x = \"\"\n",
    "    page_number = 0\n",
    "\n",
    "    for page in range(len(doc.pages)):\n",
    "        for token in doc.pages[page].tokens:\n",
    "            text_anc = token.layout.text_anchor.text_segments\n",
    "            for anc in text_anc:\n",
    "                try:\n",
    "                    start_temp = anc.start_index\n",
    "                except:\n",
    "                    start_temp = 0\n",
    "                end_temp = anc.end_index\n",
    "\n",
    "            for anc3 in text_anchors_check:\n",
    "                start_check = anc3.start_index\n",
    "                end_check = anc3.end_index + 1\n",
    "\n",
    "            if (\n",
    "                (start_temp >= start_check)\n",
    "                and (end_temp <= end_check)\n",
    "                and ((end_temp - start_temp) > 3)\n",
    "            ):\n",
    "                normalized_vertices_temp = (\n",
    "                    token.layout.bounding_poly.normalized_vertices\n",
    "                )\n",
    "                for ver_xy in normalized_vertices_temp:\n",
    "                    temp_xy[\"x\"].append(ver_xy.x)\n",
    "                    temp_xy[\"y\"].append(ver_xy.y)\n",
    "                    page_number = page\n",
    "\n",
    "    min_x = min(temp_xy[\"x\"])\n",
    "    min_y = min(temp_xy[\"y\"])\n",
    "    max_x = max(temp_xy[\"x\"])\n",
    "    max_y = max(temp_xy[\"y\"])\n",
    "\n",
    "    A = {\"x\": min_x, \"y\": min_y}\n",
    "    B = {\"x\": max_x, \"y\": min_y}\n",
    "    C = {\"x\": max_x, \"y\": max_y}\n",
    "    D = {\"x\": min_x, \"y\": max_y}\n",
    "\n",
    "    return [A, B, C, D], page_number\n",
    "\n",
    "\n",
    "def create_entity(mention_text, type_, m) -> documentai.Document.Entity:\n",
    "    \"\"\"\n",
    "    Creates a DocumentAI entity based on mention text and type.\n",
    "\n",
    "    Args:\n",
    "        mention_text (str): Mentioned text.\n",
    "        type_ (str): Type of the entity.\n",
    "        m: Mention object.\n",
    "\n",
    "    Returns:\n",
    "        documentai.Document.Entity: Created entity.\n",
    "    \"\"\"\n",
    "    entity = documentai.Document.Entity()\n",
    "    entity.mention_text = mention_text\n",
    "    entity.type_ = type_\n",
    "    page_ref = documentai.Document.PageAnchor.PageRef()\n",
    "    entity.page_anchor.page_refs.extend([page_ref])\n",
    "    text_segment = documentai.Document.TextAnchor().TextSegment()\n",
    "    text_segment.start_index = m.start()\n",
    "    text_segment.end_index = m.end()\n",
    "    entity.text_anchor.text_segments.extend([text_segment])\n",
    "    return entity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c623b1d9-5d00-4123-bcd6-9e4347aca00c",
   "metadata": {},
   "source": [
    "# Loop through the list of files and process them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1e3dffe-5974-427b-8b33-42ff5f0d8606",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list_of_files:\n",
    "    print(i)\n",
    "    doc = utilities.documentai_json_proto_downloader(\n",
    "        input_storage_bucket_name, file_name_dict[i]\n",
    "    )\n",
    "    list_of_existing_custom_acc_num = []\n",
    "\n",
    "    try:\n",
    "        for entity in doc.entities:\n",
    "            if entity.type_ and entity.type_ == \"customer_account_number\":\n",
    "                list_of_existing_custom_acc_num.append(entity.mention_text)\n",
    "\n",
    "        occurrences = re.finditer(pattern, doc.text)\n",
    "\n",
    "        for m in occurrences:\n",
    "            if doc.text[m.start() : m.end()] not in list_of_existing_custom_acc_num:\n",
    "                entity = create_entity(\n",
    "                    doc.text[m.start() : m.end()], \"customer_account_number\", m\n",
    "                )\n",
    "                try:\n",
    "                    (\n",
    "                        entity.page_anchor.page_refs[\n",
    "                            0\n",
    "                        ].bounding_poly.normalized_vertices,\n",
    "                        page_number,\n",
    "                    ) = get_token_from_text_anchors(\n",
    "                        doc, entity.text_anchor.text_segments\n",
    "                    )\n",
    "                    entity.page_anchor.page_refs[0].page = page_number\n",
    "                except:\n",
    "                    print(\n",
    "                        \"Not able to find \"\n",
    "                        + doc.text[m.start() : m.end()]\n",
    "                        + \" in the OCR as a single token, so rejected.\"\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                doc.entities.append(entity)\n",
    "                # print(entity)\n",
    "\n",
    "        utilities.store_document_as_json(\n",
    "            documentai.Document.to_json(doc),\n",
    "            output_storage_bucket_name,\n",
    "            output_bucket_path_prefix + \"/\" + i,\n",
    "        )\n",
    "\n",
    "    except:\n",
    "        print(\"Not Able to Parse \" + file_name_dict[i])\n",
    "\n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44174e3c-ab64-4656-a19b-b2def7cfd2ae",
   "metadata": {},
   "source": [
    "# 3. Output\n",
    "\n",
    "The post processed json field can be found in the storage path provided by the user during the script execution that is **output_bucket**.\n",
    "\n",
    "### Input Sample\n",
    "<img src=\"./images/CAN_input.png\" width=800 height=200></img>\n",
    "\n",
    "### Output Sample\n",
    "<img src=\"./images/CAN_output.png\" width=800 height=200></img>"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m104",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m104"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
