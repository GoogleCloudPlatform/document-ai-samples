{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23e373ff-7c67-477a-afeb-e9572da05ab0",
   "metadata": {},
   "source": [
    "# Old OCR Json to New OCR Json Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381afb10-9894-4ca5-98f1-fe603e2c703a",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912cf0a0-b93f-4a8d-b30c-91228cd1d837",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the **DocAI Incubator Team**. No guarantees of performance are implied. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79d6500-91e8-4615-8701-37f30f208279",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "* The main purpose of this tool is to reprocess a provided set of old OCR-labeled Json with the new OCR engine, ensuring the entities stay consistent.\n",
    "* The output of the tool is a new OCR JSON file that replicates the entities present in the original OCR data.\n",
    "* The tool ensures that the entities identified by the new OCR Engine are mapped appropriately to their corresponding text and page layout information in the new OCR data.\n",
    "* The term \"Old OCR\" specifically includes all documents labeled before March 2023. If your dataset falls into this category, it is essential to utilize this tool to reprocess and relabel your documents with the new OCR engine. This step ensures that your data benefits from the latest OCR Engine, enhancing accuracy.\n",
    "\n",
    "**NOTE**: \n",
    "* The tool assumes that the bounding-box was drawn accurately in the last labeling, \n",
    "* Sometimes if the New OCR picked some noise (symbols) then those noise might come in the mentionText.\n",
    "* A Human Review is required to validate the changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350b95e3-112c-4484-8107-3d0579d4aeec",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "* Vertex AI Notebook.\n",
    "* Storage Bucket for storing input PDF files and output JSON files.\n",
    "* Permission For Google DocAI Processors, Storage and Vertex AI Notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87a922e-e61f-4d48-aabb-474ec2c41df9",
   "metadata": {},
   "source": [
    "## Step by Step procedure "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f953529e-f937-4224-aeb8-593737134b0c",
   "metadata": {},
   "source": [
    "### 1.Importing Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd44410-df79-4b63-807e-2cf20dc7ec82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install PyPDF2\n",
    "!pip install google-cloud-documentai==2.16.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a5a1a7-ecaa-4228-a7f6-7ded5eee0359",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f42ed7b-60ec-4e52-b603-ed7c00f97e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    Container,\n",
    "    Iterable,\n",
    "    Iterator,\n",
    "    List,\n",
    "    Mapping,\n",
    "    Optional,\n",
    "    Sequence,\n",
    "    Tuple,\n",
    "    Union,\n",
    "    Dict,\n",
    "    Any,\n",
    ")\n",
    "from io import BytesIO\n",
    "from pprint import pprint\n",
    "from google.cloud import storage\n",
    "from PIL import Image\n",
    "from PyPDF2 import PdfFileReader\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from tqdm.notebook import tqdm\n",
    "import io\n",
    "import json, copy\n",
    "import utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8790ea4b-33af-4eab-bcf3-0d304792ad9a",
   "metadata": {},
   "source": [
    "### 2.Setup the Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773c4aa6-d6c2-406f-8216-899c3bf85769",
   "metadata": {},
   "source": [
    "* `gcs_input_path`: GCS Storage name. It should contain DocAI processed output json files. This bucket is used for processing input files and saving output files in the folders.\n",
    "* `gcs_output_path`: GCS URI of the folder, where the dataset is exported from the processor.\n",
    "* `offset`: To expand the existing bounding box to include all the tokens corresponding to the entities, it can be adjusted to an optimal value. By Default it is 0.005.\n",
    "* `project_number`:  Project Number\n",
    "* `processor_id`: Processor ID To Call the new Processor with new OCR\n",
    "* `processor_version` :Processor version of the processor `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e15916c-70e8-4101-b914-30cd62466fa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# INPUT : storage bucket name\n",
    "gcs_input_path = \"gs://xxx-xxx-xxx/xxxx-xxx-xxxx\"\n",
    "# OUTPUT : storage bucket's path\n",
    "gcs_output_path = \"gs://xxx-xxx-xxx/xxxx-xxx-xxxx\"\n",
    "offset = 0.005  # To Expand the Existing bounding box in order to get all the tokens corrosponding to the entities. Can adjust with optimal value.\n",
    "project_number = \"xxx-xxx-xxx\"  # Project Number\n",
    "processor_id = (\n",
    "    \"xxx-xxx-xxx\"  # Processor ID -> To Call the new Invoice Processor with new OCR\n",
    ")\n",
    "processor_version = \"xxxxxxxxxxxxx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe922d73-96d1-4df1-83b3-7eaec0e37ac0",
   "metadata": {},
   "source": [
    "### 3.Run the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cce650-21ca-4afb-9153-d74697498423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_textSegment_list(\n",
    "    x_min: float, y_min: float, x_max: float, y_max: float, js: object, page: int\n",
    ") -> List:\n",
    "    \"\"\"\n",
    "    Finds the text segments within the specified coordinates on the given page of the document.\n",
    "\n",
    "    Args:\n",
    "    - x_min (float): Minimum X coordinate.\n",
    "    - y_min (float): Minimum Y coordinate.\n",
    "    - x_max (float): Maximum X coordinate.\n",
    "    - y_max (float): Maximum Y coordinate.\n",
    "    - js (Document): Document in protobuf format.\n",
    "    - page (int): Page number.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of text segments within the specified coordinates.\n",
    "    \"\"\"\n",
    "\n",
    "    textSegments_list = []\n",
    "    for token in js.pages[page].tokens:\n",
    "        vertices = token.layout.bounding_poly.normalized_vertices\n",
    "        token_xMin, token_yMin = min(point.x for point in vertices), min(\n",
    "            point.y for point in vertices\n",
    "        )\n",
    "        token_xMax, token_yMax = max(point.x for point in vertices), max(\n",
    "            point.y for point in vertices\n",
    "        )\n",
    "        if (\n",
    "            token_xMin >= x_min\n",
    "            and token_xMax <= x_max\n",
    "            and token_yMin >= y_min\n",
    "            and token_yMax < y_max\n",
    "        ):\n",
    "            textSegments_list.extend(token.layout.text_anchor.text_segments)\n",
    "    return textSegments_list\n",
    "\n",
    "\n",
    "def update_text_anchors_mention_text(\n",
    "    entity: object, js: object, new_js: object\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Updates text anchors and mention text for the given entity in the document.\n",
    "\n",
    "    Args:\n",
    "    - entity (Entity): Input entity in protobuf format.\n",
    "    - js (Document): Original document in protobuf format.\n",
    "    - new_js (Document): New document in protobuf format.\n",
    "    - offset (float): Offset value.\n",
    "\n",
    "    Returns:\n",
    "    - Dict: Updated entity with text anchors, mention text, page anchors, and entity type.\n",
    "    \"\"\"\n",
    "\n",
    "    new_entity = {}\n",
    "    text_anchor = {}\n",
    "    textAnchorList = []\n",
    "    x_min = min(\n",
    "        ver.x\n",
    "        for ver in entity.page_anchor.page_refs[0].bounding_poly.normalized_vertices\n",
    "    )\n",
    "    y_min = min(\n",
    "        ver.y\n",
    "        for ver in entity.page_anchor.page_refs[0].bounding_poly.normalized_vertices\n",
    "    )\n",
    "    x_max = max(\n",
    "        ver.x\n",
    "        for ver in entity.page_anchor.page_refs[0].bounding_poly.normalized_vertices\n",
    "    )\n",
    "    y_max = max(\n",
    "        ver.y\n",
    "        for ver in entity.page_anchor.page_refs[0].bounding_poly.normalized_vertices\n",
    "    )\n",
    "    page = 0\n",
    "    try:\n",
    "        page = int(entity.page_anchor.page_refs[0].page)\n",
    "    except:\n",
    "        page = 0\n",
    "    textSegmentList = find_textSegment_list(\n",
    "        x_min - offset, y_min - offset, x_max + offset, y_max + offset, new_js, page\n",
    "    )\n",
    "    for j in textSegmentList:\n",
    "        if not j.start_index:\n",
    "            j.start_index = \"0\"\n",
    "    text_anchor[\"text_segments\"] = []\n",
    "    for seg in textSegmentList:\n",
    "        text_anchor[\"text_segments\"].append(\n",
    "            {\"start_index\": seg.start_index, \"end_index\": seg.end_index}\n",
    "        )\n",
    "    textSegmentList = sorted(textSegmentList, key=lambda x: int(x.start_index))\n",
    "    mentionText = \"\"\n",
    "    listOfIndex = []\n",
    "    for j in textSegmentList:\n",
    "        mentionText += new_js.text[int(j.start_index) : int(j.end_index)]\n",
    "    text_anchor[\"content\"] = mentionText\n",
    "    new_entity[\"text_anchor\"] = text_anchor\n",
    "    new_entity[\"mention_text\"] = mentionText\n",
    "    temp_page_anchor = {}\n",
    "    list_of_page_refs = []\n",
    "    for i in entity.page_anchor.page_refs:\n",
    "        temp = {}\n",
    "        temp2 = {}\n",
    "        temp3 = []\n",
    "        for j in i.bounding_poly.normalized_vertices:\n",
    "            temp3.append({\"x\": j.x, \"y\": j.y})\n",
    "        temp2[\"normalized_vertices\"] = temp3\n",
    "        temp[\"bounding_poly\"] = temp2\n",
    "        temp[\"layout_type\"] = i.layout_type\n",
    "        temp[\"page\"] = str(page)\n",
    "        list_of_page_refs.append(temp)\n",
    "    temp_page_anchor[\"page_refs\"] = list_of_page_refs\n",
    "    new_entity[\"page_anchor\"] = temp_page_anchor\n",
    "    new_entity[\"type_\"] = entity.type_\n",
    "\n",
    "    return new_entity\n",
    "\n",
    "\n",
    "def make_parent_from_child_entities(temp_child: List, new_js: object) -> Dict:\n",
    "    \"\"\"\n",
    "    Combines child entities into a parent entity based on text anchors and mention text.\n",
    "\n",
    "    Args:\n",
    "    - temp_child (List[Entity]): List of child entities in protobuf format.\n",
    "    - new_js (Document): New document in protobuf format.\n",
    "\n",
    "    Returns:\n",
    "    - Dict : Parent entity with text anchors, mention text, and page anchors.\n",
    "    \"\"\"\n",
    "\n",
    "    def combine_two_entities(entity1, entity2, js):\n",
    "        new_entity = {}\n",
    "        new_entity[\"type_\"] = entity1[\"type_\"]\n",
    "        text_anchor = {}\n",
    "        textAnchorList = []\n",
    "        entity1[\"text_anchor\"][\"text_segments\"] = sorted(\n",
    "            entity1[\"text_anchor\"][\"text_segments\"], key=lambda x: int(x[\"start_index\"])\n",
    "        )\n",
    "        entity2[\"text_anchor\"][\"text_segments\"] = sorted(\n",
    "            entity2[\"text_anchor\"][\"text_segments\"], key=lambda x: int(x[\"start_index\"])\n",
    "        )\n",
    "        for j in entity1[\"text_anchor\"][\"text_segments\"]:\n",
    "            textAnchorList.append(j)\n",
    "        for j in entity2[\"text_anchor\"][\"text_segments\"]:\n",
    "            textAnchorList.append(j)\n",
    "        textAnchorList = sorted(textAnchorList, key=lambda x: int(x[\"start_index\"]))\n",
    "        mentionText = \"\"\n",
    "        for j in textAnchorList:\n",
    "            mentionText += js.text[int(j[\"start_index\"]) : int(j[\"end_index\"])]\n",
    "        new_entity[\"mention_text\"] = mentionText\n",
    "        text_anchor[\"content\"] = mentionText\n",
    "        temp_text_anchor_list = []\n",
    "        for i in range(len(entity1[\"text_anchor\"][\"text_segments\"])):\n",
    "            temp_text_anchor_list.append(\n",
    "                {\n",
    "                    \"start_index\": entity1[\"text_anchor\"][\"text_segments\"][i][\n",
    "                        \"start_index\"\n",
    "                    ],\n",
    "                    \"end_index\": entity1[\"text_anchor\"][\"text_segments\"][i][\n",
    "                        \"end_index\"\n",
    "                    ],\n",
    "                }\n",
    "            )\n",
    "        for i in range(len(entity2[\"text_anchor\"][\"text_segments\"])):\n",
    "            temp_text_anchor_list.append(\n",
    "                {\n",
    "                    \"start_index\": entity2[\"text_anchor\"][\"text_segments\"][i][\n",
    "                        \"start_index\"\n",
    "                    ],\n",
    "                    \"end_index\": entity2[\"text_anchor\"][\"text_segments\"][i][\n",
    "                        \"end_index\"\n",
    "                    ],\n",
    "                }\n",
    "            )\n",
    "        text_anchor[\"text_segments\"] = temp_text_anchor_list\n",
    "        new_entity[\"text_anchor\"] = text_anchor\n",
    "        norm_ent_1 = entity1[\"page_anchor\"][\"page_refs\"][0][\"bounding_poly\"][\n",
    "            \"normalized_vertices\"\n",
    "        ]\n",
    "        norm_ent_2 = entity2[\"page_anchor\"][\"page_refs\"][0][\"bounding_poly\"][\n",
    "            \"normalized_vertices\"\n",
    "        ]\n",
    "        min_x, max_x = min(v[\"x\"] for v in [*norm_ent_1, *norm_ent_2]), max(\n",
    "            v[\"x\"] for v in [*norm_ent_1, *norm_ent_2]\n",
    "        )\n",
    "        min_y, max_y = min(v[\"y\"] for v in [*norm_ent_1, *norm_ent_2]), max(\n",
    "            v[\"y\"] for v in [*norm_ent_1, *norm_ent_2]\n",
    "        )\n",
    "\n",
    "        A = {\"x\": min_x, \"y\": min_y}\n",
    "        B = {\"x\": max_x, \"y\": min_y}\n",
    "        C = {\"x\": max_x, \"y\": max_y}\n",
    "        D = {\"x\": min_x, \"y\": max_y}\n",
    "        new_entity[\"page_anchor\"] = entity1[\"page_anchor\"]\n",
    "        new_entity[\"page_anchor\"][\"page_refs\"][0][\"bounding_poly\"][\n",
    "            \"normalized_vertices\"\n",
    "        ] = [A, B, C, D]\n",
    "        return new_entity\n",
    "\n",
    "    if len(temp_child) == 1:\n",
    "        return temp_child[0]\n",
    "    if len(temp_child) == 2:\n",
    "        parent_entity = combine_two_entities(temp_child[0], temp_child[1], new_js)\n",
    "        return parent_entity\n",
    "    parent_entity = combine_two_entities(temp_child[0], temp_child[1], new_js)\n",
    "    for i in range(2, len(temp_child)):\n",
    "        parent_entity = combine_two_entities(parent_entity, temp_child[i], new_js)\n",
    "\n",
    "    return parent_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5c6dd5-8caa-4c30-b850-6f5b07028c7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_names_list, file_dict = utilities.file_names(gcs_input_path)\n",
    "for filename, filepath in tqdm(file_dict.items(), desc=\"Progress\"):\n",
    "    print(\">>>>>>>>>>>>>>> Processing File : \", filename)\n",
    "    input_bucket_name = gcs_input_path.split(\"/\")[2]\n",
    "    if \".json\" in filepath:\n",
    "        js = utilities.documentai_json_proto_downloader(input_bucket_name, filepath)\n",
    "        merged_pdf, images = utilities.create_pdf_bytes_from_json(\n",
    "            documentai.Document.to_dict(js)\n",
    "        )\n",
    "        res = utilities.process_document_sample(\n",
    "            project_number,\n",
    "            location_processor,\n",
    "            processor_id,\n",
    "            merged_pdf,\n",
    "            processor_version,\n",
    "        )\n",
    "        del res.document.entities\n",
    "        new_js = res.document\n",
    "        updated_entities = []\n",
    "        for entity in js.entities:\n",
    "            temp_child = []\n",
    "            ent = {}\n",
    "            if entity.properties:\n",
    "                for child_item in entity.properties:\n",
    "                    ent_ch = update_text_anchors_mention_text(child_item, js, new_js)\n",
    "                    temp_child.append(ent_ch)\n",
    "                ent = make_parent_from_child_entities(copy.deepcopy(temp_child), new_js)\n",
    "                ent[\"type_\"] = entity.type_\n",
    "                ent[\"properties\"] = temp_child\n",
    "            else:\n",
    "                ent = update_text_anchors_mention_text(entity, js, new_js)\n",
    "            updated_entities.append(ent)\n",
    "        new_js.entities = updated_entities\n",
    "        output_bucket_name = gcs_output_path.split(\"/\")[2]\n",
    "        output_path_within_bucket = \"/\".join(gcs_output_path.split(\"/\")[3:]) + filename\n",
    "        utilities.store_document_as_json(\n",
    "            documentai.Document.to_json(new_js),\n",
    "            output_bucket_name,\n",
    "            output_path_within_bucket,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f5c058-0736-4977-bc15-a34db276532f",
   "metadata": {},
   "source": [
    "### 4.Output\n",
    "The converted JSON file are stored in the output directory.\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
