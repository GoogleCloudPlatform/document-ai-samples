{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0396799c-a7b9-4572-813e-f1940e332b80",
   "metadata": {},
   "source": [
    "# Normalize Date Value 19xx to 20xx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05e4b88-d62c-41bb-9094-d448099bb2de",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d0d35f-1ca8-4b53-b508-2900e9e6fe58",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the **DocAI Incubator Team**. No guarantees of performance are implied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf8bebe-720c-4fb9-8449-ecdcf2b1c3de",
   "metadata": {},
   "source": [
    "# Objective\n",
    "This is a post processing tool to normalize year in date related entities from 19xx to 20xx. Document AI processors will give a normalized_value attribute for date entities in Document Object and sometimes this normalized value for year will be inferred as 19xx instead of 20xx."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8782c76f-ca1f-4341-83e3-8d9fba967461",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "* Vertex AI Notebook\n",
    "* GCS Folder Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e554aae7-245f-4342-83fc-a7d7ead70049",
   "metadata": {},
   "source": [
    "# Step-by-Step Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c857c3-230b-4c8d-80ee-46cc44f8493f",
   "metadata": {},
   "source": [
    "## 1. Import Modules/Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca65467-4ed8-4521-945d-57ed5411c487",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this cell to download utilities module\n",
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64c5868-9809-4518-8d36-1b56eae9897c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from utilities import (\n",
    "    file_names,\n",
    "    store_document_as_json,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c967e2-1b39-4335-906a-5135a613b503",
   "metadata": {},
   "source": [
    "## 2. Input Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096ad870-e76b-4190-8934-9b95f88c9865",
   "metadata": {},
   "source": [
    "* **INPUT_GCS_PATH** : It is input GCS folder path which contains DocumentAI processor JSON results\n",
    "* **OUTPUT_GCS_PATH** : It is a GCS folder path to store post-processing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e16b97-fce1-4593-8683-d23f3cd0092c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parser results as JSON, Data entities should contain Normalized Value data in it\n",
    "GCS_INPUT_DIR = \"gs://BUCKET_NAME/incubator/\"\n",
    "GCS_OUTPUT_DIR = \"gs://BUCKET_NAME/incubator/output/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b448dc-2572-417e-b685-afe8db2bcfe0",
   "metadata": {},
   "source": [
    "## 3. Run Below Code-Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92155d09-86dc-4a08-bc19-791dd91a32c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_date_entity(entity: documentai.Document.Entity):\n",
    "    \"\"\"\n",
    "    Normalize a date entity by adding 100 years to the year value.\n",
    "\n",
    "    This function takes a date entity extracted using Google Cloud Document AI\n",
    "    and normalizes it by adding 100 years to the year value.\n",
    "\n",
    "    Args:\n",
    "        entity (documentai.Document.Entity): The date entity to be normalized.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Example:\n",
    "        # Example usage:\n",
    "        entity = ...  # Assume entity is extracted from a document\n",
    "        normalize_date_entity(entity)\n",
    "        # The date entity will be normalized with the year increased by 100.\n",
    "    \"\"\"\n",
    "    print(\"\\t\\t\",entity.type_, entity.normalized_value.text, end=\" -> \")\n",
    "    accumulate = 100\n",
    "    date = entity.normalized_value.date_value\n",
    "    curr_year, curr_month, curr_day = date.year, date.month, date.day\n",
    "    updated_year = curr_year + accumulate\n",
    "    entity.normalized_value.date_value.year = updated_year\n",
    "    text = f\"{updated_year}-{curr_month:0>2}-{curr_day:0>2}\"\n",
    "    entity.normalized_value.text = text\n",
    "    print(entity.normalized_value.text)\n",
    "\n",
    "json_splits = GCS_INPUT_DIR.strip(\"/\").split(\"/\")\n",
    "input_bucket = json_splits[2]\n",
    "INPUT_FILES_DIR = \"/\".join(json_splits[3:])\n",
    "GCS_OUTPUT_DIR = GCS_OUTPUT_DIR.strip(\"/\")\n",
    "output_splits = GCS_OUTPUT_DIR.split(\"/\")\n",
    "output_bucket = output_splits[2]\n",
    "OUTPUT_FILES_DIR = \"/\".join(output_splits[3:])\n",
    "\n",
    "\n",
    "_, files_dict = file_names(GCS_INPUT_DIR)\n",
    "ip_storage_client = storage.Client()\n",
    "ip_storage_bucket = ip_storage_client.bucket(input_bucket)\n",
    "print(\"Process started for converting normalized dat value from 19xx to 20xx...\")\n",
    "for fn, fp in files_dict.items():\n",
    "    print(f\"\\tFile: {fn}\")\n",
    "    json_str = ip_storage_bucket.blob(fp).download_as_string()\n",
    "    doc = documentai.Document.from_json(json_str)\n",
    "    for ent in doc.entities:\n",
    "        if 100 < ent.normalized_value.date_value.year < 2000:\n",
    "            normalize_date_entity(ent)\n",
    "\n",
    "    json_str = documentai.Document.to_json(doc)\n",
    "    file_name = f\"{OUTPUT_FILES_DIR}/{fn}\"\n",
    "    print(f\"\\t  Output gcs uri - {file_name}\", output_bucket)\n",
    "    store_document_as_json(json_str, output_bucket, file_name)\n",
    "\n",
    "print(\"Process Completed!!!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8df1da-2ef2-424c-902e-e4cebc0515c6",
   "metadata": {},
   "source": [
    "# 4. Output Details\n",
    "\n",
    "Refer below images for preprocessed and postprocessed results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83949d45-cac6-48c2-a067-0b67c0d36910",
   "metadata": {
    "tags": []
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <b>Pre-processed data</b>\n",
    "        </td>\n",
    "        <td>\n",
    "            <b>Post-processed data</b>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src='./images/post_processing_image.png' width=400 height=600></img>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src='./images/pre_processing_image.png' width=400 height=600></img>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bf332a-2406-473d-8b0e-304181331dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
