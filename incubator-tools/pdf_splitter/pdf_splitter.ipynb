{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Splitter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the **DocAI Incubator Team**. No guarantees of performance are implied."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "The objective of this notebook is to provide python script which helps to split large pdf-file into smaller-chunk-files based on chunk size provided by user(number of pages per each chunk-pdf)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisite\n",
    "* Vertex AI Notebook and GCS path of large pdf files."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step By Step procedure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Modules/Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google-cloud-storage\n",
    "%pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to download utilities module\n",
    "# !wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "from google.cloud import storage\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "\n",
    "from utilities import copy_blob, file_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Input Details\n",
    "\n",
    "* **PROJECT_ID**: Provide GCP Project Id\n",
    "* **BUCKET_NAME**: Provide GCS bucket name \n",
    "* **INPUT_FOLDER_PATH**: Provide GCS folderpath which contains input PDF files, _gcs uri without bucket name_\n",
    "* **OUTPUT_FOLDER_PATH**: Provide GCS folderpath to store chunked-PDF files, _gcs uri without bucket name_\n",
    "* **CHUNK_SIZE**: Provide number of pages you are required for each pdf-chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"xx-xx-xx\"\n",
    "BUCKET_NAME = \"bucket_name\"\n",
    "INPUT_FOLDER_PATH = \"path_to/input_pdfs\"  # without bucket name\n",
    "OUTPUT_FOLDER_PATH = \"path_to/output\"  # without bucket name\n",
    "CHUNK_SIZE = 15  # no of pages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run the Below Code-cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_blob(bytes_stream: bytes, file: str) -> None:\n",
    "    \"\"\"To store PDF files in GCS\n",
    "\n",
    "    Args:\n",
    "        bytes_stream (bytes): Binary Format of pdf data\n",
    "        file (str): filename to store in specified GCS bucket\n",
    "    \"\"\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    result_bucket = storage_client.get_bucket(BUCKET_NAME)\n",
    "    document_blob = storage.Blob(name=str(file), bucket=result_bucket)\n",
    "    document_blob.upload_from_string(bytes_stream, content_type=\"application/pdf\")\n",
    "\n",
    "\n",
    "def split_pdfs(filepath: str) -> None:\n",
    "    \"\"\"Splits the PDF into multiple chunks based on provided CHUNK_SIZE\n",
    "\n",
    "    Args:\n",
    "        filepath (str): filepath to read from specified GCS bucket\n",
    "    \"\"\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket_obj = storage_client.get_bucket(BUCKET_NAME)\n",
    "    blob = bucket_obj.get_blob(str(filepath))\n",
    "    pdf_data = BytesIO(blob.download_as_bytes())\n",
    "    pdf_reader = PdfReader(pdf_data)\n",
    "    num_pages = len(pdf_reader.pages)\n",
    "    filename = filepath.split(\"/\")[-1]\n",
    "    sub_dir = filename.split(\".\")[0]\n",
    "    output_folder_path = OUTPUT_FOLDER_PATH.strip(\"/\")\n",
    "    if num_pages <= CHUNK_SIZE:\n",
    "        # copy the PDF file to the destination directory without splitting\n",
    "        destination_filename = f\"{output_folder_path}/{sub_dir}/{filename}\"\n",
    "        print(f\"\\tcopying blob to {destination_filename}\")\n",
    "        copy_blob(BUCKET_NAME, filepath, BUCKET_NAME, destination_filename)\n",
    "        return\n",
    "    print(\"Chuncking process started \")\n",
    "    # Split the PDF into multiple PDFs of user_pages pages each\n",
    "    num_splits = num_pages // CHUNK_SIZE + 1\n",
    "    for i in range(num_splits):\n",
    "        start_page = i * CHUNK_SIZE\n",
    "        end_page = min((i + 1) * CHUNK_SIZE, num_pages)\n",
    "\n",
    "        pdf_writer = PdfWriter()\n",
    "        for page_num in range(start_page, end_page):\n",
    "            pdf_writer.add_page(pdf_reader.pages[page_num])\n",
    "\n",
    "        # Save the split PDF as a new file in the destination directory\n",
    "        destination_filename = sub_dir + \"_\" + str(i + 1).zfill(5) + \".pdf\"\n",
    "        destination_filename = f\"{output_folder_path}/{sub_dir}/{destination_filename}\"\n",
    "\n",
    "        response_bytes_stream = BytesIO()\n",
    "        pdf_writer.write(response_bytes_stream)\n",
    "        bytes_stream = response_bytes_stream.getvalue()\n",
    "        print(\"\\tStoring to \", destination_filename)\n",
    "        store_blob(bytes_stream, destination_filename)\n",
    "\n",
    "\n",
    "_, filenames_dict = file_names(f\"gs://{BUCKET_NAME}/{INPUT_FOLDER_PATH}\")\n",
    "filenames_dict = {fn: fp for fn, fp in filenames_dict.items() if fn.endswith(\".pdf\")}\n",
    "for filename, filepath in filenames_dict.items():\n",
    "    print(f\"filename: {filename}\")\n",
    "    try:\n",
    "        split_pdfs(filepath)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "    print(f\"Processed: {filename} from {filepath}\")\n",
    "print(\"Process completed for all files\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Output Details\n",
    "\n",
    "If you check OUTPUT_FOLDER_PATH, you can see all large pdf files chunked to corresponding folders."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/output_sample_folders.png\" width=800 height=600>\n",
    "<br>\n",
    "<img src=\"./images/output_sample_chunks.png\" width=800 height=600>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m104",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m104"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
