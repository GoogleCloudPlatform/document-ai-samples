{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0396799c-a7b9-4572-813e-f1940e332b80",
   "metadata": {},
   "source": [
    "# Parse Table Into Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05e4b88-d62c-41bb-9094-d448099bb2de",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d0d35f-1ca8-4b53-b508-2900e9e6fe58",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the **DocAI Incubator Team**. No guarantees of performance are implied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf8bebe-720c-4fb9-8449-ecdcf2b1c3de",
   "metadata": {},
   "source": [
    "# Objective\n",
    "This tool is helpful to split the table into chunks, by using Form Parser we can get tables and their geometries and then we use Form parser results of all Row objects to group them as chunks based on user provided value i.e, `cells_limit` which is max cells limit per each chunked image. \n",
    "\n",
    "NOTE: Here rows per each chunked image is dependent on Table Object data of Form Parser results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8782c76f-ca1f-4341-83e3-8d9fba967461",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "* Vertex AI Notebook\n",
    "* GCS Folder Path\n",
    "* Form Parser & CDE Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e554aae7-245f-4342-83fc-a7d7ead70049",
   "metadata": {},
   "source": [
    "# Step-by-Step Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c857c3-230b-4c8d-80ee-46cc44f8493f",
   "metadata": {},
   "source": [
    "## 1. Import Modules/Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c6d9e8-aa31-4613-9e3e-3ea52cbce4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to install required packages\n",
    "!pip install google-cloud-documentai\n",
    "!pip install google-cloud-storage\n",
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca65467-4ed8-4521-945d-57ed5411c487",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this cell to download utilities module\n",
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dbe527-2187-4a70-9810-23c063d2d3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from io import BytesIO\n",
    "from typing import List, Sequence, Tuple\n",
    "\n",
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from google.cloud import storage\n",
    "from PIL import Image\n",
    "\n",
    "from utilities import file_names, process_document_sample, store_document_as_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c967e2-1b39-4335-906a-5135a613b503",
   "metadata": {},
   "source": [
    "## 2. Input Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096ad870-e76b-4190-8934-9b95f88c9865",
   "metadata": {},
   "source": [
    "* **PROJECT_ID**: Provide GCP Project Id\n",
    "* **LOCATION**: Processor Location\n",
    "* **FP_PROCESSOR_ID**: DocumentAI Form Parser Processor Id\n",
    "* **FP_PROCESSOR_VERSION_ID**: Form Parser processor version Id\n",
    "* **CDE_PROCESSOR_ID**: DocumentAI Custom Document Extractor Processor(CDE) Id\n",
    "* **CDE_PROCESSOR_VERSION_ID**: CDE processor version Id\n",
    "* **GCS_INPUT_PATH**: GCS folder path containing input samples. These samples are input to form parser.  \n",
    "    * **NOTE**: As of now this tool accepts jpeg, png & pdf in gcs_input_path\n",
    "\n",
    "* **GCS_OUTPUT_PATH**: GCS folder path to store cde results of each chunked image\n",
    "    * **NOTE**: This tool will create two directories/folders under provided gcs_output_path\n",
    "    * *chunked_files_raw*: It holds all chunked images\n",
    "    * *chunked_files_duai_results* : It holds CDE processor results of each chunked image as JSONs\n",
    "\n",
    "* **CELLS_LIMIT**:  integer value, this value decides how many rows are needed per each chunk of an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e16b97-fce1-4593-8683-d23f3cd0092c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"xx-xx-xx\"\n",
    "LOCATION = \"us\"  # Format is \"us\" or \"eu\"\n",
    "FP_PROCESSOR_ID = \"xx-xx-xx\"\n",
    "FP_PROCESSOR_VERSION_ID = \"pretrained-form-parser-v2.0-2022-11-10\"\n",
    "CDE_PROCESSOR_ID = \"xx-xx-xx\"\n",
    "CDE_PROCESSOR_VERSION_ID = \"pretrained-foundation-model-v1.0-2023-08-22\"\n",
    "GCS_INPUT_PATH = \"gs://BUCKET/path_to_split_table_into_chunks/input/\"\n",
    "GCS_OUTPUT_PATH = \"gs://BUCKET/path_to_split_table_into_chunks/output/\"\n",
    "CELLS_LIMIT = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b448dc-2572-417e-b685-afe8db2bcfe0",
   "metadata": {},
   "source": [
    "## 3. Run Below Code-Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92155d09-86dc-4a08-bc19-791dd91a32c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_xy_coords_to_chunk(\n",
    "    page: documentai.Document.Page,\n",
    "    table_rows: Sequence[documentai.Document.Page.Table.TableRow],\n",
    ") -> Tuple[int, int, int, int]:\n",
    "    \"\"\"\n",
    "    Helper function to get xy-coordinates of a table\n",
    "\n",
    "    Args:\n",
    "        page (documentai.Document.Page): DocumentAI Page object containing Table data\n",
    "        table_rows (Sequence[documentai.Document.Page.Table.TableRow]):\n",
    "            List of TableRow objects\n",
    "\n",
    "    Returns:\n",
    "        Tuple[int, int, int, int]:\n",
    "            Returns xy-coordinates of table based on provided table_rows\n",
    "    \"\"\"\n",
    "\n",
    "    width, height = page.dimension.width, page.dimension.height\n",
    "    x, y = [], []\n",
    "    for table_row in table_rows:\n",
    "        for cell in table_row.cells:\n",
    "            _x, _y = get_x_y_list(cell.layout.bounding_poly)\n",
    "            x.extend(_x)\n",
    "            y.extend(_y)\n",
    "            x = [min(x), max(x)]\n",
    "            y = [min(y), max(y)]\n",
    "    xy = (min(x) * width, min(y) * height, max(x) * width, max(y) * (height))\n",
    "    return xy\n",
    "\n",
    "\n",
    "def build_chunked_image(header_img: Image.Image, body_img: Image.Image) -> Image.Image:\n",
    "    \"\"\"\n",
    "    It will merge provided cropped-images\n",
    "\n",
    "    Args:\n",
    "        header_img (Image.Image):\n",
    "            It contains header-part (cropped) of table\n",
    "        body_img (Image.Image):\n",
    "            It contains body part(cropped) of table\n",
    "    Returns:\n",
    "        Image.Image: Returns an image containing header&body attached\n",
    "    \"\"\"\n",
    "\n",
    "    image_chunks = [header_img, body_img]\n",
    "    width, _ = header_img.size\n",
    "    total_height = sum(img.size[1] for img in image_chunks)\n",
    "    merged_image = Image.new(\"RGB\", (width, total_height))\n",
    "    y_offset = 0\n",
    "    for img_chunk in image_chunks:\n",
    "        merged_image.paste(img_chunk, (0, y_offset))\n",
    "        y_offset += img_chunk.size[1]\n",
    "    return merged_image\n",
    "\n",
    "\n",
    "def get_x_y_list(\n",
    "    bounding_poly: documentai.BoundingPoly,\n",
    ") -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Helper function to get x & y coordinates of BBox\n",
    "\n",
    "    Args:\n",
    "        bounding_poly (documentai.BoundingPoly): Boundingpoly object containig normalized vertices\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[float], List[float]]: Returns x&y coords as list\n",
    "    \"\"\"\n",
    "    x, y = [], []\n",
    "    for nvs in bounding_poly.normalized_vertices:\n",
    "        x.append(nvs.x)\n",
    "        y.append(nvs.y)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def store_image_as_jpeg(document: bytes, bucket_name: str, file_name: str) -> None:\n",
    "    \"\"\"It will push images to GCS path\n",
    "\n",
    "    Args:\n",
    "        document (bytes): Image in bytes format\n",
    "        bucket_name (str): Name of GCS bucket\n",
    "        file_name (str): Blob name to store in GCS bucket\n",
    "    \"\"\"\n",
    "\n",
    "    sc = storage.Client()\n",
    "    process_result_bucket = sc.get_bucket(bucket_name)\n",
    "    document_blob = storage.Blob(name=file_name, bucket=process_result_bucket)\n",
    "    document_blob.upload_from_string(document, content_type=\"application/jpeg\")\n",
    "\n",
    "\n",
    "def parse_table_into_chunks(\n",
    "    doc: documentai.Document, file_name: str, gcs_output_path: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    It is helper function to split PDF image into chunks to have compability\n",
    "    limit for Custom DocumentAI Processor\n",
    "\n",
    "    Args:\n",
    "        doc (documentai.Document): DocumentAI ptocessor result\n",
    "        file_name (str): File names to prefix with chunked images\n",
    "        gcs_output_path (str): GCS output path to store chunked images\n",
    "    \"\"\"\n",
    "\n",
    "    _output_bucket, _output_path = re.match(\"gs://(.*?)/(.*)\", gcs_output_path).groups()\n",
    "    for page in doc.pages:\n",
    "        base_64_image = page.image.content\n",
    "        bytes_io_img = BytesIO(base_64_image)\n",
    "        img = Image.open(bytes_io_img)\n",
    "        for table_idx, table in enumerate(page.tables):\n",
    "            hrs = table.header_rows\n",
    "            header_img = img.crop(get_xy_coords_to_chunk(page, hrs))\n",
    "            brs = table.body_rows\n",
    "            print(f\"Total Rows in current Table {len(brs)}\")\n",
    "            cells_count_per_row = len(hrs[0].cells)\n",
    "            rows_per_chunk = CELLS_LIMIT // cells_count_per_row\n",
    "            num_of_chunks = math.ceil(len(brs) / rows_per_chunk)\n",
    "            for i in range(num_of_chunks):\n",
    "                start = i * rows_per_chunk\n",
    "                end = (i + 1) * rows_per_chunk\n",
    "                chunk = brs[start:end]\n",
    "                print(f\"\\tno.of rows in current chunk are {len(chunk)}\")\n",
    "                cropped_img = img.crop(get_xy_coords_to_chunk(page, chunk))\n",
    "                chunked_image = build_chunked_image(header_img, cropped_img)\n",
    "                # to store in gcs\n",
    "                gcs_dir = file_name.split(\".\")[0]\n",
    "                filename = f\"{gcs_dir}/page_{page.page_number}_table_{table_idx}_chunk_{i}.jpeg\"\n",
    "                blob_prefix = f\"{_output_path.strip('/')}/{filename}\"\n",
    "                buffer = BytesIO()\n",
    "                chunked_image.save(buffer, format=\"JPEG\")\n",
    "                print(\n",
    "                    f\"\\tStoring chunked image in gcs at gs://{_output_bucket}/{blob_prefix}\"\n",
    "                )\n",
    "                store_image_as_jpeg(buffer.getvalue(), _output_bucket, blob_prefix)\n",
    "\n",
    "\n",
    "def mime_type_lookup(file_ext: str) -> str:\n",
    "    \"\"\"\n",
    "    Helper function to get MIMETYPE based on file-extension of input sample\n",
    "\n",
    "    Args:\n",
    "        file_ext (str): Valid file extension\n",
    "\n",
    "    Returns:\n",
    "        str: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    # Update this lookup table based on your file-extensions\n",
    "    lookup = {\n",
    "        \"jpeg\": \"image/jpeg\",\n",
    "        \"jpg\": \"image/jpeg\",\n",
    "        \"png\": \"image/png\",\n",
    "        \"pdf\": \"application/pdf\",\n",
    "    }\n",
    "    return lookup[file_ext.lower()]\n",
    "\n",
    "\n",
    "file_list, file_dict = file_names(GCS_INPUT_PATH)\n",
    "input_bucket, input_path = re.match(\"gs://(.*?)/(.*)\", GCS_INPUT_PATH).groups()\n",
    "output_bucket, output_path = re.match(\"gs://(.*?)/(.*)\", GCS_OUTPUT_PATH).groups()\n",
    "chunks_folder_raw = f\"{GCS_OUTPUT_PATH.strip('/')}/chunked_files_raw/\"\n",
    "chunks_folder_duai_results = f\"{GCS_OUTPUT_PATH.strip('/')}/chunked_files_duai_results/\"\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(input_bucket)\n",
    "\n",
    "print(\"Form Parser Processing Starting to split tables into chunks...\")\n",
    "for fn, fp in file_dict.items():\n",
    "    print(f\"File: {fn}\")\n",
    "    file_extension = fn.split(\".\")[-1]\n",
    "    mime_type = mime_type_lookup(file_extension)\n",
    "    print(mime_type, fn)\n",
    "    pdf_bytes = bucket.blob(fp).download_as_string()\n",
    "    try:\n",
    "        # print(\"Calling Form Parser API\")\n",
    "        res = process_document_sample(\n",
    "            PROJECT_ID,\n",
    "            LOCATION,\n",
    "            FP_PROCESSOR_ID,\n",
    "            FP_PROCESSOR_VERSION_ID,\n",
    "            pdf_bytes,\n",
    "            mime_type,\n",
    "        ).document\n",
    "    except Exception as e:\n",
    "        print(f\"Unable to parse document due to {type(e)}, {str(e)}\")\n",
    "        continue\n",
    "    parse_table_into_chunks(res, fn, chunks_folder_raw)\n",
    "print(\n",
    "    f\"Completed parsing tables into chunks based on provided cells_limt -> {chunks_folder_raw}\"\n",
    ")\n",
    "\n",
    "# run below code only if you want to call cde on chunked files\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(output_bucket)\n",
    "print(\"CDE Processing Starting for all chunks...\")\n",
    "output_bucket, output_path = re.match(\n",
    "    \"gs://(.*?)/(.*)\", chunks_folder_duai_results\n",
    ").groups()\n",
    "for fn, fp in file_dict.items():\n",
    "    print(f\"File: {fn}\")\n",
    "    folder = fn.split(\".\")[0]\n",
    "    gcs_chunks_folder_prefix = f\"{chunks_folder_raw.strip('/')}/{folder}\"\n",
    "    files_list, files_dict = file_names(gcs_chunks_folder_prefix)\n",
    "    for chunks_fn, chunks_fp in files_dict.items():\n",
    "        print(f\"\\tprocessing {chunks_fn}\")\n",
    "        pdf_bytes = bucket.blob(chunks_fp).download_as_bytes()\n",
    "        try:\n",
    "            # print(\"Calling CDE API\")\n",
    "            res = process_document_sample(\n",
    "                PROJECT_ID,\n",
    "                LOCATION,\n",
    "                CDE_PROCESSOR_ID,\n",
    "                CDE_PROCESSOR_VERSION_ID,\n",
    "                pdf_bytes,\n",
    "                \"image/jpeg\",\n",
    "            ).document\n",
    "        except Exception as e:\n",
    "            print(f\"\\tUnable to parse document due to {type(e)}, {str(e)}\")\n",
    "            continue\n",
    "        _chunk_fn = chunks_fn.split(\".\")[0]\n",
    "        out_fp = f\"{output_path.strip('/')}/{folder}/{_chunk_fn}.json\"\n",
    "        json_str = documentai.Document.to_json(\n",
    "            res, including_default_value_fields=False\n",
    "        )\n",
    "        print(f\"\\tStoring DocAI response in gcs at gs://{output_bucket}/{out_fp}\")\n",
    "        store_document_as_json(json_str, output_bucket, out_fp)\n",
    "print(\n",
    "    f\"Completed parsing chunks and docai results are stored -> {chunks_folder_duai_results}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8df1da-2ef2-424c-902e-e4cebc0515c6",
   "metadata": {},
   "source": [
    "# 4. Output Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db365cd5-6aa3-4645-8c36-3e7f25a230e7",
   "metadata": {},
   "source": [
    "In below sample details 1 table splits into 4 chunks along with headers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83949d45-cac6-48c2-a067-0b67c0d36910",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessed Table\n",
    "<img src='./images/sample_input.png' width=1000 height=600 alt=\"Sample Inpput\"></img>\n",
    "\n",
    "## Postprocessed Table\n",
    "<img src='./images/sample_output.png' width=1000 height=600 alt=\"Sample Output\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf43286-a60d-4e5e-8e22-5f6f9cc65be9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
