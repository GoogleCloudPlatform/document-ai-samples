{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6589fc93-39d1-4d10-be1f-e7eb33fe4087",
   "metadata": {},
   "source": [
    "# Format Based Line Items Extractor (Post-Processing) User Guide\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf22bf7-4a47-4f3a-9eef-6f19348a5250",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361f188e-fe11-4a49-b7c8-080e0e69ce7a",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. \n",
    "It is provided and supported on a best-effort basis by the DocAI Incubator Team.\n",
    "No guarantees of performance are implied. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1036937a-0221-48eb-862e-3fa0b8e646a8",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "This document provides the functions which can be used to get the line items tagged \n",
    "from a specific format where the default processor is failing to extract the line item entities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aad68e-ed06-4bf8-96dd-8419938cc3cf",
   "metadata": {},
   "source": [
    "## Note\n",
    "\n",
    "* This tool tags as entities from OCR output , the text below the headers_entities keys will be tagged as an child entity as per value.\n",
    "* If the line item has multiple lines , it doesnt give desired result and output will be clumsy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115a4e82-5e83-468a-b0e5-097ca14f15d5",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "* Vertex AI Notebook Or Colab (If using Colab, use authentication)\n",
    "* Storage Bucket for storing input and output json files\n",
    "* Permission For Google Storage and Vertex AI Notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe81de40-5c62-4c0b-adea-937f957b1a6e",
   "metadata": {},
   "source": [
    "## Step by Step procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142123d3-37b1-4aa8-841c-40c3bd52d70c",
   "metadata": {},
   "source": [
    "### 1. Importing Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0643c5f9-29fe-4252-9e6c-e2afc8c2f2b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install google-cloud-storage google-cloud-documentai==2.16.0 tqdm json\n",
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7588c13e-0e09-4a76-8c21-85a68ee262c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from utilities import *\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from utilities import *\n",
    "import re\n",
    "from typing import Dict, List, Any, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7c8c4c-68b8-413c-b4bc-c66f044d3b7a",
   "metadata": {},
   "source": [
    "### 2. Input and Output Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab32e1c4-2bf1-49ab-89d6-2d22443c3f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = \"xxxx-xxxx-xxxx\"  # project id\n",
    "gcs_input_path = \"gs://xxxx/xxxx/xxx\"  # path where the parsed jsons are stored\n",
    "gcs_output_path = \"gs://xxxx/xxxx/xxx/\"  # path to save the updated jsons\n",
    "headers = \"QTY EQUIPMENT Min Day Week Month Amount\"  # sample headers\n",
    "# header entities with corresponding headers\n",
    "headers_entities = {\n",
    "    \"QTY\": \"line_item/quantity\",\n",
    "    \"EQUIPMENT\": \"line_item/description\",\n",
    "    \"Min\": \"line_item/unit_price\",\n",
    "    \"Day\": \"line_item/unit_price\",\n",
    "    \"Week\": \"line_item/unit_price\",\n",
    "    \"Month\": \"line_item/unit_price\",\n",
    "    \"4 Week\": \"line_item/unit_price\",\n",
    "    \"Amount\": \"line_item/amount\",\n",
    "}\n",
    "stop_word = \"SALES ITEMS\"  # stop word where the line items should be stopped\n",
    "consider_ent = \"Amount\"  # reference entity which has to be tagged as first or present in all the line items."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd2de60-7d7d-4051-8f09-3429ba116f92",
   "metadata": {},
   "source": [
    "### Format specific input \n",
    "\n",
    "* To get the line items of a special invoice format document , you need below details to be entered from the format.\n",
    "\n",
    "<img src=\"./Images/Input.png\" width=800 height=400></img>\n",
    "\n",
    "### Headers:\n",
    "* The headers of the invoice have to be given as input in the form of a string as shown below example shown.\n",
    "\n",
    "**Headers=’QTY EQUIPMENT Min Day Week Month Amount’**\n",
    "\n",
    "### Headers_entities:\n",
    "\n",
    "* The entities which correspond to the header have to be given in a dictionary format . This is used to map the items   under the respective header mapped into the respective value given in the dictionary.\n",
    "\n",
    "**headers_entities={'QTY':'line_item/quantity','EQUIPMENT':'line_item/description','Min':'line_item/unit_price','Day':'line_item/unit_price','Week':'line_item/unit_price','Month':'line_item/unit_price','4 Week':'line_item/unit_price','Amount':'line_item/amount'}**\n",
    "\n",
    "### Stop_word:\n",
    "* The stop word helps us to identify the line items where it is getting ended and if there is no stop word needed then   it can be left as empty, so the function checks the total page from the headers. \n",
    "\n",
    "**stop_word='SALES ITEMS'**\n",
    "\n",
    "### Reference entity:\n",
    " * The Entity which has to be tagged first or exists in all the line items have to be specified for better performance of the tool.\n",
    "\n",
    "**consider_ent='Amount'**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737d1c70-fef5-49e3-a266-695bf8076a54",
   "metadata": {},
   "source": [
    "### 3. Run the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c22bfdd-abdc-4d1c-8f7c-86164e7c4103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# functions\n",
    "def get_page_wise_entities(json_dict: documentai.Document):\n",
    "    \"\"\"\n",
    "    Extracts entities from each page in the given loaded JSON file.\n",
    "\n",
    "    Args:\n",
    "    - json_dict (Dict[str, Any]): Loaded JSON file containing entities.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, List[Any]]: A dictionary where keys are page identifiers\n",
    "      and values are lists of entities associated with each page.\n",
    "    \"\"\"\n",
    "\n",
    "    entities_page = {}\n",
    "    for entity in json_dict.entities:\n",
    "        page = entity.page_anchor.page_refs[0].page\n",
    "        if page in entities_page.keys():\n",
    "            entities_page[page].append(entity)\n",
    "        else:\n",
    "            entities_page[page] = [entity]\n",
    "\n",
    "    return entities_page\n",
    "\n",
    "\n",
    "def get_text_anc_headers(\n",
    "    json_dict: documentai.Document,\n",
    "    page: int,\n",
    "    headers: str,\n",
    "    headers_entities: dict = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    THIS FUNCTION WILL SEARCH FOR THE HEADERS STRING IN LOADED JSON\n",
    "    USING FIRST AND LAST AS KEY WORDS AND GETS MINIMUM,\n",
    "    MAXIMUM OF X &Y COORDINTES OF THE HEADERS IN DICTIONARY FORMAT\n",
    "\n",
    "    Args: Loaded JSON\n",
    "            Page Number\n",
    "            headers\n",
    "            headers_entities\n",
    "\n",
    "    Returns: {'header_keyword: {'min_x':0....,'min_y':0....,'max_x':0....,'max_y':0....}}\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    import re\n",
    "\n",
    "    pattern = r\"{}.*{}\".format(\n",
    "        re.escape(headers.split(\" \")[0]), re.escape(headers.split(\" \")[-1])\n",
    "    )\n",
    "    match = re.search(pattern, json_dict.text, flags=re.DOTALL)\n",
    "    start = match.start()\n",
    "    end_temp = json_dict.text[start : start + 50].find(headers.split(\" \")[-1])\n",
    "    end = start + end_temp + len(headers.split(\" \")[-1])\n",
    "    if end - start >= len(headers):\n",
    "        ent_index = {}\n",
    "        if headers_entities == []:\n",
    "            for col in headers.split(\" \"):\n",
    "                start_temp = (json_dict.text[start:end].lower()).find(col.lower())\n",
    "                end_temp = start_temp + start + len(col) + 1\n",
    "                ent_index[col] = {\n",
    "                    \"start_index\": str(start_temp + start),\n",
    "                    \"end_index\": str(end_temp),\n",
    "                }\n",
    "        else:\n",
    "            for col in headers_entities.keys():\n",
    "                if col.lower() in json_dict.text[start:end].lower():\n",
    "                    start_temp = (json_dict.text[start:end].lower()).find(col.lower())\n",
    "                    end_temp = start_temp + start + len(col) + 1\n",
    "                    ent_index[col] = {\n",
    "                        \"start_index\": str(start_temp + start),\n",
    "                        \"end_index\": str(end_temp),\n",
    "                    }\n",
    "        ent_min_dict = {}\n",
    "        for col, anc in ent_index.items():\n",
    "            try:\n",
    "                ent_min_dict[col] = get_token_from_text_anc(json_dict, page, anc)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return ent_min_dict\n",
    "\n",
    "\n",
    "def get_token_xy(token: Any) -> Tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Extracts the normalized bounding box coordinates (min_x, min_y, max_x, max_y) of a token.\n",
    "\n",
    "    Args:\n",
    "    - token (Any): A token object with layout information.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[float, float, float, float]: The normalized bounding box coordinates.\n",
    "\n",
    "    \"\"\"\n",
    "    vertices = token.layout.bounding_poly.normalized_vertices\n",
    "    minx_token, miny_token = min(point.x for point in vertices), min(\n",
    "        point.y for point in vertices\n",
    "    )\n",
    "    maxx_token, maxy_token = max(point.x for point in vertices), max(\n",
    "        point.y for point in vertices\n",
    "    )\n",
    "\n",
    "    return minx_token, miny_token, maxx_token, maxy_token\n",
    "\n",
    "\n",
    "def get_token_from_text_anc(\n",
    "    json_dict: documentai.Document, page_num: int, text_anchors_check: Dict[str, str]\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Extracts the x and y coordinates of a token based on the provided text anchors.\n",
    "\n",
    "    Args:\n",
    "    - json_dict (Dict[str, Any]): Loaded JSON.\n",
    "    - page_num (int): Page number.\n",
    "    - text_anchors_check (Dict[str, str]): Text anchors to check for.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, float]: Dictionary containing x and y coordinates {'min_x': float,\n",
    "    'min_y': float, 'max_x': float, 'max_y': float}.\n",
    "    \"\"\"\n",
    "\n",
    "    for page in json_dict.pages:\n",
    "        if int(page_num) == int(page.page_number - 1):\n",
    "            for token in page.tokens:\n",
    "                for seg in token.layout.text_anchor.text_segments:\n",
    "                    if (\n",
    "                        seg.start_index == text_anchors_check[\"start_index\"]\n",
    "                        and seg.end_index == text_anchors_check[\"end_index\"]\n",
    "                    ):\n",
    "                        minx_token, miny_token, maxx_token, maxy_token = get_token_xy(\n",
    "                            token\n",
    "                        )\n",
    "                    elif (\n",
    "                        abs(\n",
    "                            int(seg.start_index)\n",
    "                            - int(text_anchors_check[\"start_index\"])\n",
    "                        )\n",
    "                        <= 2\n",
    "                        and abs(\n",
    "                            int(seg.end_index) - int(text_anchors_check[\"end_index\"])\n",
    "                        )\n",
    "                        <= 2\n",
    "                    ):\n",
    "                        minx_token, miny_token, maxx_token, maxy_token = get_token_xy(\n",
    "                            token\n",
    "                        )\n",
    "\n",
    "    return {\n",
    "        \"min_x\": minx_token,\n",
    "        \"min_y\": miny_token,\n",
    "        \"max_x\": maxx_token,\n",
    "        \"max_y\": maxy_token,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_entity_new(\n",
    "    mt_new: str,\n",
    "    norm_ver: List[Dict[str, float]],\n",
    "    text_seg: List[Dict[str, Any]],\n",
    "    type_line: str,\n",
    "    line_item: bool,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generates a new entity based on the provided parameters.\n",
    "\n",
    "    Args:\n",
    "    - mt_new (str): Mention text.\n",
    "    - norm_ver (List[Dict[str, float]]): Normalized vertices.\n",
    "    - text_seg (List[Dict[str, Any]]): Text segments.\n",
    "    - type_line (str): Type of the entity.\n",
    "    - line_item (bool): True if it's a line item entity, False otherwise.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, Any]: The generated entity.\n",
    "    \"\"\"\n",
    "\n",
    "    if line_item == True:\n",
    "        line_item_ent = {\n",
    "            \"confidence\": 1,\n",
    "            \"mention_text\": mt_new,\n",
    "            \"page_anchor\": {\n",
    "                \"page_refs\": [{\"bounding_poly\": {\"normalized_vertices\": norm_ver}}]\n",
    "            },\n",
    "            \"properties\": [],\n",
    "            \"text_anchor\": {\"text_segments\": text_seg},\n",
    "            \"type_\": type_line,\n",
    "        }\n",
    "        return line_item_ent\n",
    "    else:\n",
    "        sub_ent = {\n",
    "            \"confidence\": 1,\n",
    "            \"mention_text\": mt_new,\n",
    "            \"page_anchor\": {\n",
    "                \"page_refs\": [{\"bounding_poly\": {\"normalized_vertices\": norm_ver}}]\n",
    "            },\n",
    "            \"text_anchor\": {\"text_segments\": text_seg},\n",
    "            \"type_\": type_line,\n",
    "        }\n",
    "        return sub_ent\n",
    "\n",
    "\n",
    "def tag_ref_child_item(\n",
    "    json_dict: documentai.Document,\n",
    "    page: int,\n",
    "    ent_min_dict: Dict[str, Dict[str, float]],\n",
    "    consider_ent: str,\n",
    "    max_stop_y: float,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    THIS FUNCTION USED THE LOADED JSON, PAGE NUMBER , DICTIONARY OF HEADER KEYWORD AND VALUES AS\n",
    "    X AND Y COORDINATES AND THE STOP WORD Y COORDINATE\n",
    "\n",
    "    ARGS:\n",
    "    - json_dict (Dict[str, Any]): Loaded JSON.\n",
    "    - page (int): Page number.\n",
    "    - ent_min_dict (Dict[str, Dict[str, float]]): Dictionary\n",
    "    of header keyword and values as X and Y coordinates.\n",
    "    - consider_ent (str): Entity to be tagged.\n",
    "    - max_stop_y (float): Stop word Y coordinate.\n",
    "\n",
    "    RETURNS:\n",
    "    - List[Dict[str, Any]]: List of line items tagging the first entity provided.\n",
    "    \"\"\"\n",
    "    # parameter entity needed# ***********need to add some condition\n",
    "    # to check whether amount int similar to other entities need to add\n",
    "    page_num = 0\n",
    "    # consider_ent='Amount'\n",
    "    consider_type = headers_entities[consider_ent]\n",
    "    line_items_temp = []\n",
    "    for page in json_dict.pages:\n",
    "        if int(page_num) == int(page.page_number - 1):\n",
    "            for token in page.tokens:\n",
    "                min_x, min_y, max_x, max_y = get_token_xy(token)\n",
    "                norm_ver11 = [\n",
    "                    {\"x\": min_x, \"y\": min_y},\n",
    "                    {\"x\": min_x, \"y\": max_y},\n",
    "                    {\"x\": max_x, \"y\": min_y},\n",
    "                    {\"x\": max_x, \"y\": max_y},\n",
    "                ]\n",
    "                if (\n",
    "                    min_y > ent_min_dict[consider_ent][\"min_y\"]\n",
    "                    and min_x >= ent_min_dict[consider_ent][\"min_x\"] - 0.002\n",
    "                    and max_x <= ent_min_dict[consider_ent][\"max_x\"] + 0.002\n",
    "                    and max_y < max_stop_y\n",
    "                ):\n",
    "                    for seg in token.layout.text_anchor.text_segments:\n",
    "                        end_index = seg.end_index\n",
    "                        start_index = seg.start_index\n",
    "                    line_item_ent = get_entity_new(\n",
    "                        json_dict.text[int(start_index) : int(end_index)],\n",
    "                        norm_ver11,\n",
    "                        [{\"start_index\": start_index, \"end_index\": end_index}],\n",
    "                        \"line_item\",\n",
    "                        True,\n",
    "                    )\n",
    "                    sub_ent = get_entity_new(\n",
    "                        json_dict.text[int(start_index) : int(end_index)],\n",
    "                        norm_ver11,\n",
    "                        [{\"start_index\": start_index, \"end_index\": end_index}],\n",
    "                        consider_type,\n",
    "                        False,\n",
    "                    )\n",
    "                    line_item_ent[\"properties\"].append(sub_ent)\n",
    "                    line_items_temp.append(line_item_ent)\n",
    "    same_y_ent = []\n",
    "    for dup in line_items_temp:\n",
    "        temp_same_y = {\"mention_text\": \"\", \"min_y\": \"\", \"max_y\": \"\", \"text_anc\": []}\n",
    "        temp_same_y[\"mention_text\"] = dup[\"mention_text\"]\n",
    "        temp_norm_same_y = dup[\"page_anchor\"][\"page_refs\"][0][\"bounding_poly\"]\n",
    "        temp_same_y[\"min_y\"] = min(\n",
    "            vertex[\"y\"] for vertex in temp_norm_same_y[\"normalized_vertices\"]\n",
    "        )\n",
    "        temp_same_y[\"max_y\"] = max(\n",
    "            vertex[\"y\"] for vertex in temp_norm_same_y[\"normalized_vertices\"]\n",
    "        )\n",
    "        temp_same_y[\"text_anc\"] = dup[\"text_anchor\"][\"text_segments\"]\n",
    "        same_y_ent.append(temp_same_y)\n",
    "    same_y_ent\n",
    "    sorted_same_y_ent = sorted(same_y_ent, key=lambda x: x[\"min_y\"])\n",
    "    groups_same_y = []\n",
    "    current_group = [sorted_same_y_ent[0]]\n",
    "\n",
    "    for i in range(1, len(sorted_same_y_ent)):\n",
    "        if sorted_same_y_ent[i][\"min_y\"] - current_group[-1][\"min_y\"] < 0.005:\n",
    "            current_group.append(sorted_same_y_ent[i])\n",
    "        else:\n",
    "            groups_same_y.append(current_group)\n",
    "            current_group = [sorted_same_y_ent[i]]\n",
    "\n",
    "    # Append the last group\n",
    "    groups_same_y.append(current_group)\n",
    "\n",
    "    if len(groups_same_y) != 0:\n",
    "        for group in groups_same_y:\n",
    "            merge_mention_text = \"\"\n",
    "            merge_text_anc = []\n",
    "            merge_page_anc_xy = {\"x\": [], \"y\": []}\n",
    "            merge_type = \"\"\n",
    "            for dup1 in group:\n",
    "                for dup2 in line_items_temp:\n",
    "                    if dup2[\"text_anchor\"][\"text_segments\"] == dup1[\"text_anc\"]:\n",
    "                        merge_mention_text = merge_mention_text + dup2[\"mention_text\"]\n",
    "                        for anch2 in dup2[\"text_anchor\"][\"text_segments\"]:\n",
    "                            merge_text_anc.append(anch2)\n",
    "                        norm_dup = dup2[\"page_anchor\"][\"page_refs\"][0][\"bounding_poly\"][\n",
    "                            \"normalized_vertices\"\n",
    "                        ]\n",
    "                        for norm_dup_xy in norm_dup:\n",
    "                            merge_page_anc_xy[\"x\"].append(norm_dup_xy[\"x\"])\n",
    "                            merge_page_anc_xy[\"y\"].append(norm_dup_xy[\"y\"])\n",
    "                        line_items_temp.remove(dup2)\n",
    "            dup_minx, dup_miny, dup_maxx, dup_maxy = (\n",
    "                min(merge_page_anc_xy[\"x\"]),\n",
    "                min(merge_page_anc_xy[\"y\"]),\n",
    "                max(merge_page_anc_xy[\"x\"]),\n",
    "                max(merge_page_anc_xy[\"y\"]),\n",
    "            )\n",
    "            dup_norm_ver = [\n",
    "                {\"x\": dup_minx, \"y\": dup_miny},\n",
    "                {\"x\": dup_minx, \"y\": dup_maxy},\n",
    "                {\"x\": dup_maxx, \"y\": dup_miny},\n",
    "                {\"x\": dup_maxx, \"y\": dup_maxy},\n",
    "            ]\n",
    "            line_item_ent3 = get_entity_new(\n",
    "                merge_mention_text, dup_norm_ver, merge_text_anc, \"line_item\", True\n",
    "            )\n",
    "            sub_ent3 = get_entity_new(\n",
    "                merge_mention_text, dup_norm_ver, merge_text_anc, consider_type, False\n",
    "            )\n",
    "            line_item_ent3[\"properties\"].append(sub_ent3)\n",
    "            line_items_temp.append(line_item_ent3)\n",
    "\n",
    "    return line_items_temp\n",
    "\n",
    "\n",
    "def tagging_rest_child(\n",
    "    json_dict: documentai.Document,\n",
    "    page_num: int,\n",
    "    line_items_temp: List[Dict[str, Any]],\n",
    "    headers_entities: Dict[str, Any],\n",
    "    ent_min_dict: Dict[str, Dict[str, float]],\n",
    "    consider_ent: str,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    THIS FUNCTION USES LOADED JSON, PAGE NUMBER AND REFERENCED LINE ITEM LIST\n",
    "    TAGGED AND HEADER ENTITIES DICTIONARY AND TAGS ALL THE REST OF THE CHILD ITEMS\n",
    "\n",
    "    ARGS:\n",
    "    - json_dict (Dict[str, Any]): Loaded JSON.\n",
    "    - page_num (int): Page number.\n",
    "    - line_items_temp (List[Dict[str, Any]]): Referenced line item list tagged.\n",
    "    - headers_entities (Dict[str, Any]): Header entities dictionary.\n",
    "    - ent_min_dict (Dict[str, Dict[str, float]]): Dictionary of header\n",
    "    keyword and values as X and Y coordinates.\n",
    "    - consider_ent (str): Entity to be tagged.\n",
    "\n",
    "    RETURNS:\n",
    "    - List[Dict[str, Any]]: Updated list of line items with tagged child items.\n",
    "    \"\"\"\n",
    "    desired_values = [\"line_item/description\", \"description\"]\n",
    "\n",
    "    # Get keys that have the desired value\n",
    "    matching_keys = [\n",
    "        key for key, value in headers_entities.items() if value in desired_values\n",
    "    ]\n",
    "\n",
    "    for line_item in line_items_temp:\n",
    "        sub_ent_temp = []\n",
    "        for sub in line_item[\"properties\"]:\n",
    "            normalized_vertices = sub[\"page_anchor\"][\"page_refs\"][0][\"bounding_poly\"]\n",
    "            min_x, min_y = min(\n",
    "                (vertex[\"x\"], vertex[\"y\"])\n",
    "                for vertex in normalized_vertices[\"normalized_vertices\"]\n",
    "            )\n",
    "            max_x, max_y = max(\n",
    "                (vertex[\"x\"], vertex[\"y\"])\n",
    "                for vertex in normalized_vertices[\"normalized_vertices\"]\n",
    "            )\n",
    "            for en1, min_xy in ent_min_dict.items():\n",
    "                temp_mention_text = \"\"\n",
    "                temp_page_anchor = {\"x\": [], \"y\": []}\n",
    "                temp_text_anchor = []\n",
    "                if en1 != consider_ent:\n",
    "                    for page in json_dict.pages:\n",
    "                        if int(page_num) == int(page.page_number - 1):\n",
    "                            for token in page.tokens:\n",
    "                                (\n",
    "                                    min_x_token,\n",
    "                                    min_y_token,\n",
    "                                    max_x_token,\n",
    "                                    max_y_token,\n",
    "                                ) = get_token_xy(token)\n",
    "                                if (\n",
    "                                    en1 != matching_keys[0]\n",
    "                                    and min_xy[\"min_x\"] >= min_x_token - 0.02\n",
    "                                    and min_xy[\"max_x\"] <= max_x_token + 0.005\n",
    "                                    and abs(min_y - min_y_token) <= 0.005\n",
    "                                ) or (\n",
    "                                    en1 == matching_keys[0]\n",
    "                                    and min_xy[\"min_x\"] <= min_x_token\n",
    "                                    and min_xy[\"max_x\"] >= max_x_token - 0.35\n",
    "                                    and abs(min_y - min_y_token) <= 0.005\n",
    "                                ):\n",
    "                                    for seg in token.layout.text_anchor.text_segments:\n",
    "                                        end_index = seg.end_index\n",
    "                                        start_index = seg.start_index\n",
    "                                    temp_text_anchor.append(\n",
    "                                        {\n",
    "                                            \"start_index\": start_index,\n",
    "                                            \"end_index\": end_index,\n",
    "                                        }\n",
    "                                    )\n",
    "                                    temp_page_anchor[\"x\"].extend(\n",
    "                                        [min_x_token, max_x_token]\n",
    "                                    )\n",
    "                                    temp_page_anchor[\"y\"].extend(\n",
    "                                        [min_y_token, max_y_token]\n",
    "                                    )\n",
    "                                    temp_mention_text = (\n",
    "                                        temp_mention_text\n",
    "                                        + json_dict.text[\n",
    "                                            int(start_index) : int(end_index)\n",
    "                                        ]\n",
    "                                    )\n",
    "                if temp_mention_text != \"\":\n",
    "                    norm_vertices = [\n",
    "                        {\n",
    "                            \"x\": min(temp_page_anchor[\"x\"]),\n",
    "                            \"y\": min(temp_page_anchor[\"y\"]),\n",
    "                        },\n",
    "                        {\n",
    "                            \"x\": min(temp_page_anchor[\"x\"]),\n",
    "                            \"y\": max(temp_page_anchor[\"y\"]),\n",
    "                        },\n",
    "                        {\n",
    "                            \"x\": max(temp_page_anchor[\"x\"]),\n",
    "                            \"y\": min(temp_page_anchor[\"y\"]),\n",
    "                        },\n",
    "                        {\n",
    "                            \"x\": max(temp_page_anchor[\"x\"]),\n",
    "                            \"y\": max(temp_page_anchor[\"y\"]),\n",
    "                        },\n",
    "                    ]\n",
    "                    sub_ent = get_entity_new(\n",
    "                        temp_mention_text,\n",
    "                        norm_vertices,\n",
    "                        temp_text_anchor,\n",
    "                        headers_entities[en1],\n",
    "                        False,\n",
    "                    )\n",
    "                    sub_ent_temp.append(sub_ent)\n",
    "        for item in sub_ent_temp:\n",
    "            line_item[\"properties\"].append(item)\n",
    "        line_item_mention_text = \"\"\n",
    "        line_item_page_anchor = {\"x\": [], \"y\": []}\n",
    "        line_item_text_anchor = []\n",
    "        for sub1 in line_item[\"properties\"]:\n",
    "            line_item_mention_text = line_item_mention_text + sub1[\"mention_text\"]\n",
    "            for anch1 in sub1[\"text_anchor\"][\"text_segments\"]:\n",
    "                line_item_text_anchor.append(anch1)\n",
    "            norm_temp = sub1[\"page_anchor\"][\"page_refs\"][0][\"bounding_poly\"][\n",
    "                \"normalized_vertices\"\n",
    "            ]\n",
    "            for i in norm_temp:\n",
    "                line_item_page_anchor[\"x\"].append(i[\"x\"])\n",
    "                line_item_page_anchor[\"y\"].append(i[\"y\"])\n",
    "            min_line_x, min_line_y, max_line_x, max_line_y = (\n",
    "                min(line_item_page_anchor[\"x\"]),\n",
    "                min(line_item_page_anchor[\"y\"]),\n",
    "                max(line_item_page_anchor[\"x\"]),\n",
    "                max(line_item_page_anchor[\"y\"]),\n",
    "            )\n",
    "        line_norm_ver = [\n",
    "            {\"x\": min_line_x, \"y\": min_line_y},\n",
    "            {\"x\": min_line_x, \"y\": max_line_y},\n",
    "            {\"x\": max_line_x, \"y\": min_line_y},\n",
    "            {\"x\": max_line_x, \"y\": max_line_y},\n",
    "        ]\n",
    "        line_item[\"page_anchor\"][\"page_refs\"][0][\"bounding_poly\"][\n",
    "            \"normalized_vertices\"\n",
    "        ] = line_norm_ver\n",
    "        line_item[\"text_anchor\"][\"text_segments\"] = line_item_text_anchor\n",
    "        line_item[\"mention_text\"] = line_item_mention_text\n",
    "\n",
    "    return line_items_temp\n",
    "\n",
    "\n",
    "def tag_description_bw_regions(\n",
    "    json_dict: documentai.Document,\n",
    "    page_num: int,\n",
    "    line_items_temp: List[Dict[str, Any]],\n",
    "    max_stop_y: float,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    THIS FUNCTION USED LOADED JSON, PAGE AND LINE ITEMS TAGGED AND MAX Y FROM STOP WORD\n",
    "    AND GIVES THE UPDATED LINE ITEMS TAGGING THE OCR OUTPUT IN\n",
    "    BETWEEN THE LINE ITEMS AS line_item/description\n",
    "\n",
    "    ARGS:\n",
    "    - json_dict (Dict[str, Any]): Loaded JSON.\n",
    "    - page_num (int): Page number.\n",
    "    - line_items_temp (List[Dict[str, Any]]): Line items tagged.\n",
    "    - max_stop_y (float): Max Y from stop word.\n",
    "\n",
    "    RETURNS:\n",
    "    - List[Dict[str, Any]]: Updated line items with tagged descriptions between them.\n",
    "    \"\"\"\n",
    "\n",
    "    region = []\n",
    "    region_line_item = []\n",
    "    for n1 in range(len(line_items_temp)):\n",
    "        norm_temp_1 = line_items_temp[n1][\"page_anchor\"][\"page_refs\"][0][\n",
    "            \"bounding_poly\"\n",
    "        ][\"normalized_vertices\"]\n",
    "        y_min_1 = min(vertex[\"y\"] for vertex in norm_temp_1)\n",
    "        y_max_1 = max(vertex[\"y\"] for vertex in norm_temp_1)\n",
    "        if n1 < len(line_items_temp) - 1:\n",
    "            norm_temp_2 = line_items_temp[n1 + 1][\"page_anchor\"][\"page_refs\"][0][\n",
    "                \"bounding_poly\"\n",
    "            ][\"normalized_vertices\"]\n",
    "            y_min_2 = min(vertex[\"y\"] for vertex in norm_temp_2)\n",
    "            y_max_2 = max(vertex[\"y\"] for vertex in norm_temp_2)\n",
    "            region.append({\"min_y\": y_max_1, \"max_y\": y_min_2})\n",
    "            region_line_item.append(({\"min_y_1\": y_min_1, \"min_y_2\": y_min_2}))\n",
    "        else:\n",
    "            if max_stop_y != 1:\n",
    "                region.append({\"min_y\": y_max_1, \"max_y\": max_stop_y - 0.01})\n",
    "                region_line_item.append(({\"min_y_1\": y_min_1, \"min_y_2\": max_stop_y}))\n",
    "    line_desc_bw_regions = []\n",
    "    for reg in region:\n",
    "        temp_text = \"\"\n",
    "        desc_text_anc = []\n",
    "        desc_page_anc_xy = {\"x\": [], \"y\": []}\n",
    "        for page in json_dict.pages:\n",
    "            if int(page_num) == int(page.page_number - 1):\n",
    "                for token1 in page.tokens:\n",
    "                    (\n",
    "                        min_x_token_1,\n",
    "                        min_y_token_1,\n",
    "                        max_x_token_1,\n",
    "                        max_y_token_1,\n",
    "                    ) = get_token_xy(token1)\n",
    "                    if (\n",
    "                        min_y_token_1 >= reg[\"min_y\"] - 0.005\n",
    "                        and max_y_token_1 <= reg[\"max_y\"] + 0.005\n",
    "                    ):\n",
    "                        for seg in token1.layout.text_anchor.text_segments:\n",
    "                            end_index = seg.end_index\n",
    "                            start_index = seg.start_index\n",
    "                        temp_text = (\n",
    "                            temp_text\n",
    "                            + json_dict.text[int(start_index) : int(end_index)]\n",
    "                        )\n",
    "                        desc_text_anc.append(\n",
    "                            {\"start_index\": start_index, \"end_index\": end_index}\n",
    "                        )\n",
    "                        desc_page_anc_xy[\"x\"].extend([min_x_token_1, max_x_token_1])\n",
    "                        desc_page_anc_xy[\"y\"].extend([min_y_token_1, max_y_token_1])\n",
    "        if temp_text != \"\":\n",
    "            norm_vertices_1 = [\n",
    "                {\"x\": min(desc_page_anc_xy[\"x\"]), \"y\": min(desc_page_anc_xy[\"y\"])},\n",
    "                {\"x\": min(desc_page_anc_xy[\"x\"]), \"y\": max(desc_page_anc_xy[\"y\"])},\n",
    "                {\"x\": max(desc_page_anc_xy[\"x\"]), \"y\": min(desc_page_anc_xy[\"y\"])},\n",
    "                {\"x\": max(desc_page_anc_xy[\"x\"]), \"y\": max(desc_page_anc_xy[\"y\"])},\n",
    "            ]\n",
    "            sub_ent_desc = get_entity_new(\n",
    "                temp_text,\n",
    "                norm_vertices_1,\n",
    "                desc_text_anc,\n",
    "                \"line_item/description\",\n",
    "                False,\n",
    "            )\n",
    "            line_desc_bw_regions.append(sub_ent_desc)\n",
    "\n",
    "    for reg3 in region_line_item:\n",
    "        for line_5 in line_items_temp:\n",
    "            norm_temp_line5 = line_5[\"page_anchor\"][\"page_refs\"][0][\"bounding_poly\"][\n",
    "                \"normalized_vertices\"\n",
    "            ]\n",
    "            y_min_line_5 = min(vertex[\"y\"] for vertex in norm_temp_line5)\n",
    "            y_max_line_5 = max(vertex[\"y\"] for vertex in norm_temp_line5)\n",
    "            for line_desc in line_desc_bw_regions:\n",
    "                norm_temp_desc_2 = line_desc[\"page_anchor\"][\"page_refs\"][0][\n",
    "                    \"bounding_poly\"\n",
    "                ][\"normalized_vertices\"]\n",
    "                y_min_desc = min(vertex[\"y\"] for vertex in norm_temp_desc_2)\n",
    "                y_max_desc = max(vertex[\"y\"] for vertex in norm_temp_desc_2)\n",
    "                if (\n",
    "                    y_min_desc >= reg3[\"min_y_1\"] - 0.01\n",
    "                    and y_max_desc <= reg3[\"min_y_2\"] + 0.01\n",
    "                    and y_min_line_5 >= reg3[\"min_y_1\"] - 0.01\n",
    "                    and y_max_line_5 <= reg3[\"min_y_2\"] + 0.01\n",
    "                ):\n",
    "                    # print(line_desc['mention_text'])\n",
    "                    line_5[\"properties\"].append(line_desc)\n",
    "\n",
    "                    line_desc_bw_regions.remove(line_desc)\n",
    "\n",
    "    for line_fin in line_items_temp:\n",
    "        temp_text_2 = \"\"\n",
    "        temp_text_anc_2 = []\n",
    "        temp_page_anc_xy_2 = {\"x\": [], \"y\": []}\n",
    "        for subline in line_fin[\"properties\"]:\n",
    "            for an5 in subline[\"text_anchor\"][\"text_segments\"]:\n",
    "                temp_text_anc_2.append(an5)\n",
    "            for xy2 in subline[\"page_anchor\"][\"page_refs\"][0][\"bounding_poly\"][\n",
    "                \"normalized_vertices\"\n",
    "            ]:\n",
    "                temp_page_anc_xy_2[\"x\"].append(xy2[\"x\"])\n",
    "                temp_page_anc_xy_2[\"y\"].append(xy2[\"y\"])\n",
    "        # print(temp_text_anc_2)\n",
    "        sorted_temp_text_anc_2 = sorted(\n",
    "            temp_text_anc_2, key=lambda x: int(x[\"end_index\"])\n",
    "        )\n",
    "        temp_done_anc = []\n",
    "        for index_4 in sorted_temp_text_anc_2:\n",
    "            if index_4 not in temp_done_anc:\n",
    "                temp_text_2 = (\n",
    "                    temp_text_2\n",
    "                    + json_dict.text[\n",
    "                        int(index_4[\"start_index\"]) : int(index_4[\"end_index\"])\n",
    "                    ]\n",
    "                )\n",
    "                temp_done_anc.append(index_4)\n",
    "        min_x_line_fin, min_y_line_fin, max_x_line_fin, max_y_line_fin = (\n",
    "            min(temp_page_anc_xy_2[\"x\"]),\n",
    "            min(temp_page_anc_xy_2[\"y\"]),\n",
    "            max(temp_page_anc_xy_2[\"x\"]),\n",
    "            max(temp_page_anc_xy_2[\"y\"]),\n",
    "        )\n",
    "        line_fin[\"page_anchor\"][\"page_refs\"][0][\"bounding_poly\"][\n",
    "            \"normalized_vertices\"\n",
    "        ] = [\n",
    "            {\"x\": min_x_line_fin, \"y\": min_y_line_fin},\n",
    "            {\"x\": max_x_line_fin, \"y\": min_y_line_fin},\n",
    "            {\"x\": min_x_line_fin, \"y\": max_y_line_fin},\n",
    "            {\"x\": max_x_line_fin, \"y\": max_y_line_fin},\n",
    "        ]\n",
    "        line_fin[\"text_anchor\"][\"text_segments\"] = sorted_temp_text_anc_2\n",
    "        line_fin[\"mention_text\"] = temp_text_2\n",
    "\n",
    "        # pprint(line_fin)\n",
    "\n",
    "    return line_items_temp\n",
    "\n",
    "\n",
    "file_names_list, file_dict = file_names(gcs_input_path)\n",
    "bucket_name = gcs_input_path.split(\"/\")[2]\n",
    "for filename, filepath in tqdm(file_dict.items(), desc=\"Progress\"):\n",
    "    input_bucket_name = gcs_input_path.split(\"/\")[2]\n",
    "    if \".json\" in filepath:\n",
    "        json_dict = documentai_json_proto_downloader(bucket_name, filepath)\n",
    "        page_wise_ent = get_page_wise_entities(json_dict)\n",
    "        line_item_entities = []\n",
    "        for page, ent in page_wise_ent.items():\n",
    "            ent_min_dict = get_text_anc_headers(\n",
    "                json_dict, page, headers, headers_entities=headers_entities\n",
    "            )\n",
    "            try:\n",
    "                y_max_stop = get_text_anc_headers(json_dict, page, stop_word)\n",
    "                for stop, ver in y_max_stop.items():\n",
    "                    max_stop_y = ver[\"max_y\"]\n",
    "            except:\n",
    "                max_stop_y = 1\n",
    "            line_items_temp = tag_ref_child_item(\n",
    "                json_dict, page, ent_min_dict, consider_ent, max_stop_y\n",
    "            )\n",
    "            line_items_temp_1 = tagging_rest_child(\n",
    "                json_dict,\n",
    "                page,\n",
    "                line_items_temp,\n",
    "                headers_entities,\n",
    "                ent_min_dict,\n",
    "                consider_ent,\n",
    "            )\n",
    "            line_items_temp_page = tag_description_bw_regions(\n",
    "                json_dict, page, line_items_temp_1, max_stop_y\n",
    "            )\n",
    "            for line_temp_ent5 in line_items_temp_page:\n",
    "                line_item_entities.append(line_temp_ent5)\n",
    "\n",
    "        if line_item_entities != []:\n",
    "            final_entities = []\n",
    "            for entity in json_dict.entities:\n",
    "                if entity.type != \"line_item\":\n",
    "                    final_entities.append(entity)\n",
    "            for line_ent in line_item_entities:\n",
    "                final_entities.append(line_ent)\n",
    "            json_dict.entities = final_entities\n",
    "        else:\n",
    "            print(\"No change in the file\")\n",
    "        store_document_as_json(\n",
    "            documentai.Document.to_json(json_dict),\n",
    "            gcs_output_path.split(\"/\")[2],\n",
    "            (\"/\").join(gcs_output_path.split(\"/\")[3:]) + \"/\" + filename,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835e86d6-a433-4803-b9a8-87aa9adada19",
   "metadata": {},
   "source": [
    "## OUTPUT\n",
    "\n",
    "* Before and after the postprocessing code\n",
    "\n",
    "* Before post processing code\n",
    "\n",
    "<img src=\"./Images/format_input.png\" width=800 height=400></img>\n",
    "    \n",
    "* After using Post processing code\n",
    "\n",
    "<img src=\"./Images/output.png\" width=800 height=400></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf09348d-0c84-4e40-9269-d69d74b8d0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
