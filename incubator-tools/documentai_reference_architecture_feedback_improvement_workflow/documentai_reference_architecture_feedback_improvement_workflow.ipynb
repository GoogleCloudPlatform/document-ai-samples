{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75d1c173-b870-4dc3-b1e3-c6dfa02b6b91",
   "metadata": {},
   "source": [
    "# DocAI Reference Architecture - Feedback Improvement Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c460c621-02f6-44db-b6c7-d7666f5ea022",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa78c5b-4879-4662-8ab2-75b9e754ccc1",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "This tool facilitates the following:\n",
    "\n",
    "* <b>Batch Processing and Categorization</b>: It calls a batch process (requiring a minimum of 20 files) and categorizes files into \"human review\" and \"bypass human review\" folders based on their entity confidence score. For audit purposes, X% of bypassed files are copied to the human review folder, and all files requiring human review are then imported into the processor.\n",
    "* <b>Metadata Storage and New File Identification</b>: All metadata regarding processed files is stored in a GCS path. This ensures that in subsequent runs, the script only selects new, unprocessed files and waits until the input path contains at least 20 new files.\n",
    "* <b>Human-in-the-Loop (HITL) Review</b>: Files marked for HITL review are processed by human reviewers who correct labels after importing the documents into a processor.\n",
    "* <b>Post-Review Analysis and Data Management</b>: After human review, workflow 2 conducts pre-HITL and post-HITL analysis, logging the analysis report to a BigQuery table and copying it to a feedback folder. It also creates a backup of the processor's existing dataset to a backup folder before importing all files from the feedback folder into the processor as a new dataset.\n",
    "* <b>Dataset Validation and Model Training</b>: The dataset is validated against training criteria (minimum document and label counts), which then triggers the training of a new processor version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8becf8c-4af4-4852-8052-e83565ee5fe0",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "- GCP Project ID with billing setup\n",
    "- DocumentAI Processor IDs\n",
    "- Cloud Storage(GCS)\n",
    "- BigQuery\n",
    "- Cloud Functions\n",
    "- Workflows\n",
    "- Service Account with following permissions:\n",
    "     * BigQuery Admin\n",
    "     * Cloud Run Invoker\n",
    "     * Storage Object User\n",
    "     * Workflows Invoker\n",
    "     * Document AI Editor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c860cef4-b528-4438-91f0-1a6d45f8b791",
   "metadata": {},
   "source": [
    "## Public Documentation Reference \n",
    "\n",
    "- [Create Service Account](https://cloud.google.com/iam/docs/service-accounts-create#creating)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78129ec7-4afa-4db7-a1ce-2772cb7ec5e5",
   "metadata": {},
   "source": [
    "## Overview of the workflows\n",
    "\n",
    "- There are two workflows: Workflow 1 and Workflow 2, the first one handles the operations required to identify documents requiring human review(HITL), and the latter one performs HITL analysis, triggers training a new processor version upon dataset validation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556fd78a-7911-4be0-b2f6-7d4fe606b426",
   "metadata": {},
   "source": [
    "## Workflow-1\n",
    "\n",
    "<b>1. Create BigQuery Dataset and Table</b>\n",
    "\n",
    "- The workflow first validates whether the specified dataset exists in BigQuery.\n",
    "  - If the dataset does not exist, the workflow creates a new dataset and a table with the defined schema.\n",
    "  - If the dataset exists, the workflow checks if the specified table is present within the dataset.\n",
    "    - If the table does not exist, the workflow creates the table with the provided schema.\n",
    "    - If the table exists, the schema is updated as required.\n",
    "- The workflow then lists the files in a Google Cloud Storage (GCS) bucket and stores their metadata in the BigQuery table.\n",
    "\n",
    "<b>2. Split Batches</b>\n",
    "\n",
    "- The workflow checks if any data is present in the temporary folder in GCS. If data is found, it deletes the existing files in the folder.\n",
    "- It then splits the input data into smaller batches based on the specified batch size and copies these batches to the temporary folder.\n",
    "\n",
    "<b>3. Concurrent Processing</b>\n",
    "\n",
    "- The workflow retrieves data from the GCS temporary folder and performs batch processing on these batches concurrently.\n",
    "- Once the batch processing is complete, the workflow updates the BigQuery table with metadata about the processed files.\n",
    "\n",
    "<b>4. HITL (Human-in-the-Loop) Confidence Criteria</b>\n",
    "\n",
    "- The workflow retrieves metadata from the concurrent processing step and validates the confidence levels of all entities in each file.\n",
    "- If the confidence levels do not meet the required threshold, the workflow copies those files to a designated \"HITL Review\" folder in GCS for manual review.\n",
    "- For files that meet the confidence criteria, a configurable percentage (x%) is randomly selected for quality assurance and copied to the \"HITL Review\" folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49557993-38c0-4a77-8afa-2ed72c008528",
   "metadata": {},
   "source": [
    "## The below flowchart highlights the steps involved in Workflow 1\n",
    "<img src=\"./Images/Workflow-1.png\" widht=600 height=800 alt=\"workflow-1\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc95437-1278-4b1d-ab13-81e79f172275",
   "metadata": {},
   "source": [
    "## Workflow-2\n",
    "\n",
    "<b>1. Export HITL Reviewed Dataset</b>\n",
    "\n",
    "- The workflow deletes any existing dataset in the <b>post_HITL_output_URI</b> folder.\n",
    "- After manual verification in the HITL Reviewed processor, the workflow exports the reviewed dataset to the specified location.\n",
    "\n",
    "<b>2. HITL Analysis</b>\n",
    "\n",
    "- Two temporary GCS buckets are created to store pre-HITL and post-HITL JSON files.\n",
    "- The workflow compares data from these buckets based on file names, evaluating entity names, mention text, and bounding boxes.\n",
    "- Based on this comparison, the workflow updates a DataFrame with the analyzed results.\n",
    "- The updated DataFrame is then loaded into a BigQuery table for further analysis and reporting.\n",
    "\n",
    "<b>3. Dataset Export and Import</b>\n",
    "\n",
    "- A temporary folder with a timestamped name is created in the gcs_backup_uri location. The dataset from the training processor is exported into this folder for backup purposes.\n",
    "- The verified and exported dataset from the post_HITL_output_URI is then imported into the training processor to prepare for model training.\n",
    "\n",
    "<b>4. Trigger Training</b>\n",
    "\n",
    "- The workflow retrieves labeling statistics from the training processor.\n",
    "- The labeling stats are validated to ensure they meet the required thresholds.\n",
    "- If the stats are valid, the processor proceeds to train using the train and test datasets.\n",
    "- If the stats are not valid, the workflow halts the training process and logs the reasons for failure, providing detailed information for troubleshooting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6604b1cf-d6be-4ebf-babf-d654734f8858",
   "metadata": {},
   "source": [
    "## The below flowchart highlights the steps involved in Workflow 2\n",
    "<img src=\"./Images/workflow-2.png\" width=600 height=800 alt=\"workflow-2\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95c89d9-8e7d-417a-a9fa-42059773f4f9",
   "metadata": {},
   "source": [
    "## The Below flowchart is the Custom Retry Predicate\n",
    "<img src=\"./Images/retry-mechanism.png\" width=600 height=800 alt=\"retry-mechanism\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f18cc88-9d36-4f85-ae66-e2befe701f9e",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "The structure of the package is as follows:\n",
    "\n",
    "<img src=\"./Images/zip-folder-details.png\" width=600 height=800 alt=\"folder-structure\"></img>\n",
    "\n",
    "- The folders contain python scripts for individual cloud functions.\n",
    "- input.sh contains the input parameters required for the workflow and the user needs to provide these parameters.\n",
    "- setup.sh creates and deploys the cloud functions and workflow and triggers the workflow.\n",
    "- workflow_<1/2>.yaml contains the workflow source code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1976e7-331b-4d86-b08d-04908094bbc7",
   "metadata": {},
   "source": [
    "## Input parameters\n",
    "\n",
    "For Workflow 1\n",
    "\n",
    "- `project_id`: The unique identifier of your Google Cloud project. It is used in various API calls to specify which project to interact with.\n",
    "- `dataset_id`: The ID of the BigQuery dataset in which tables will be created or queried. It is used for operations related to the dataset in BigQuery. Name should be alphanumeric (plus underscores allowed)\n",
    "- `table_id`: The ID of the BigQuery table where data will be inserted. Name should be alphanumeric (plus underscores allowed)\n",
    "- `gcs_input_path`: The Google Cloud Storage (GCS) path to the input files in gs:// format.\n",
    "- `gcs_temp_path`: The GCS path for storing temporary files. This is where the files are copied into smaller batches for further processing.\n",
    "- `batch_size`: The number of files to be processed in each batch. This defines how large each batch is during the file splitting process. Eg: batch_size = 30\n",
    "- `gcs_output_uri`: The GCS path where the processed output files will be stored after processing. This is the final destination for the results.\n",
    "- `location`: The regional location of the Google Cloud services. It defines where resources like BigQuery and Document AI processors are located (e.g., us-central1).\n",
    "- `processor_id`: The ID of the Document AI processor used for processing the documents in batches. It refers to the specific Document AI model used for document extraction.\n",
    "- `Gcs_HITL_folder_path`:The GCS path for storing files that require Human-In-The-Loop (HITL) feedback or validation. These are files that need further human interaction after initial processing. This will be pre_HITL_outut_URI for workflow 2.\n",
    "- `critical_entities`: A list or reference to entities that are critical in the documents. These entities will be considered for checking HITL criteria. If no entity is mentioned(empty list), all the entities will be checked.\n",
    "- `confidence_threshold`: The threshold for determining whether a document extraction's confidence level is sufficient. If the confidence is below this value, it will be marked for HITL(Mention the value between 0 to 1). I.e confidence_threshold = 0.5\n",
    "- `test_files_percentage`: The percentage of input files to be used as test files for HITL. It defines how many files are set aside from the HITL passed category.(Mention the value between 0 to 100. Example : 20). I.e test_files_percentage = 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63433453-b15a-4572-abda-599b60100788",
   "metadata": {},
   "source": [
    "For Workflow 2:\n",
    "\n",
    "- `project_id`: The Google Cloud project identifier that is used in all the API calls and interactions with Google Cloud services.\n",
    "- `location`: Location of processor.\n",
    "- `hitl_processor_id`: The ID of the Human-In-The-Loop (HITL) processor. This is the processor where review of documents for manual corrections and validation in Document AI is performed.\n",
    "- `pre_HITL_output_URI`: The GCS URI of the output files that were generated before Human-In-The-Loop (HITL) review. This is used for comparison in the analysis step.\n",
    "- `post_HITL_output_URI`: The GCS URI of the output files that were generated after Human-In-The-Loop (HITL) review. These documents contain corrections and human validation and are stored in the gcs_dest_uri_reviewed location.\n",
    "- `dataset_id`: The BigQuery dataset ID used for storing or retrieving data during the workflow. This is the dataset in which analysis results will be updated. Name should be alphanumeric (plus underscores allowed)\n",
    "- `table_id`: The BigQuery table ID inside the specified dataset where the data related to pre-HITL and post-HITL analysis will be inserted. Name should be alphanumeric (plus underscores allowed)\n",
    "- `gcs_backup_uri`: The GCS URI where the dataset is backed up before initiating the training process. It ensures that the dataset is safe in case something goes wrong during training or performance of the new version is not satisfactory and may want to rollback the dataset to the previous version.\n",
    "- `train_processor_id`: The ID of the processor that will be used to train on the reviewed and corrected datasets. This processor is responsible for improving accuracy based on feedback.\n",
    "- `new_version_name`: The name for the new version of the processor that will be created after training. It helps in versioning the Document AI processor for tracking improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78259f0-6169-4749-9865-b8f4928abfcd",
   "metadata": {},
   "source": [
    "Variables for deployment\n",
    "\n",
    "- `REGION`: Region for the deploying GCP resources. eg: us-central1\n",
    "- `WORKFLOW_NAME`: Workflow name to be used for deployment\n",
    "- `YAML_FILE`: Workflow configuration file name\n",
    "- `SERVICE_ACCOUNT`: Service account to be used for creating and deploying GCP resources. Ensure that the service account has the necessary permissions. \n",
    "\n",
    "**Note: All GCS paths are in gs:// format**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327192dd-f379-49f2-aeee-fa1a487bdebd",
   "metadata": {},
   "source": [
    "## Steps for deployment\n",
    "1. There should be 2 folders each for Workflow 1 and 2.\n",
    "2. Navigate to the Workflow_1 folder. Make sure the scripts input.sh and setup.sh have executable permission (chmod 777 input.sh).\n",
    "3. Update the <b>input.sh</b> with required parameter details.\n",
    "4. Run <b>“./setup.sh”</b> in a terminal.\n",
    "5. The required cloud resources are created, cloud functions and workflow will be deployed. Workflow will be triggered.\n",
    "6. Once workflow is run successfully, files for HITL review can be found at <b>$Gcs_HITL_folder_path.</b>\n",
    "7. Once manual review(HITL) is done, repeat steps 2 to 5 for Workflow 2.\n",
    "8. Once workflow is completed successfully, training of a new processor version would have been initiated with reviewed data. Please note that the workflow only triggers the training and does not monitor whether training is successful or not.\n",
    "9. Logs can be viewed in Workflow UI as well as the cloud function UI.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4140a34-84c5-4de1-8450-e4d36cee924e",
   "metadata": {},
   "source": [
    "## General Troubleshooting\n",
    "1. Issue: Sometimes, workflow may fail when processing huge dataset or large files within a dataset. Logs would not show specific errors. \n",
    "  - Solution: You can try increasing the memory and CPU allocated to the cloud function by editing the cloud function details in the UI. At the time of creation, the cloud functions would be allocated a memory of 256MB. Based on your load, you can either increase or decrease this value.\n",
    "\n",
    "\n",
    "2. Issue: Service account doesn’t have required permissions\n",
    "  - Solution: Ensure the service account you are using has the required permissions to run all components of the workflows successfully.\n",
    "\n",
    "\n",
    "3. Issue: ERROR: (gcloud.functions.deploy) ResponseError: status=[400], code=[Ok], message=[One or more users named in the policy do not belong to a permitted customer,  perhaps due to an organization policy.]\n",
    "  - Solution:   You can ignore the error message, just verify that the cloud function has been created and deployed in the Cloud run function UI.\n",
    "\n",
    "- Drive link for Demo : [Recording Demo for Cloud-workflow-improvement](https://drive.google.com/file/d/1x0ghr7vFV_WWU5jxeFagKkIn6NsAb_op/view?usp=drive_link)\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
