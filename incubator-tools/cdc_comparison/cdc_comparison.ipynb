{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72fd064f-24f5-4d61-b0ad-2b2f3fe9427d",
   "metadata": {},
   "source": [
    "# CDC Ground Truth/Parsed Output Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5756f1a-631f-4c8a-bba0-98c6821d31a9",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1d12ef-55dd-4fbd-8389-db14ed038eb1",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the DocAI Incubator Team. No guarantees of performance are implied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94527514-1ae2-470b-96e2-0f48e4aa5e81",
   "metadata": {},
   "source": [
    "## Purpose and Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf4462a-fc0d-477f-9c39-4fec383ba4ca",
   "metadata": {},
   "source": [
    "This tool uses ground truth and parsed CDC json files to develop a confusion matrix and list of files where the predicted and ground truth doesn't match. This helps to identify which classes are being confused by the model and this can be retrained with more samples and strengthen the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8783f52-627b-4efa-b5d9-664ae2ca2564",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Vertex AI Notebook\n",
    "2. Parsed json files in GCS Folder\n",
    "3. Ground truth json files in GCS folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cc5540-deb1-4449-8278-716488c54e5c",
   "metadata": {},
   "source": [
    "## Step by Step procedure "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49f5b2d-f7fd-4403-a175-b95cc804f6ba",
   "metadata": {},
   "source": [
    "### 1. Input Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3db3073-998d-4b2a-9bfc-d5f246e25f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input details\n",
    "project_id = \"xxxx-xxxx-xxxx\"  # Enter your project ID#\n",
    "GT_GCS_path = \"gs://xxxx/xxx/xx\"  # GCS folder where groundtruth is saved#\n",
    "parsed_GCS_path = \"gs://xxx/xxx/xxx/xxx\"  # GCS folder where parsed json is saved#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fa4328-3d99-4de4-a5eb-0f2033d78b79",
   "metadata": {},
   "source": [
    "### 2. Run the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678ebb32-9b15-4a98-a982-55eb7137a1f9",
   "metadata": {},
   "source": [
    "Copy the code provided in the sample code section and run the code  to get the updated json files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02a0f30-9f93-46bf-b871-f34b4df08894",
   "metadata": {},
   "source": [
    "### 3. Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eca050-4366-4e81-bb75-cd13e5cc1b97",
   "metadata": {},
   "source": [
    "The tool provides output in 4 formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4e582d-aa89-44b8-a07a-5475517d1495",
   "metadata": {},
   "source": [
    "1. **CDC_comparision.csv**:\n",
    "   This CSV file is the comparison between classifier ground truth and parsed json file.\n",
    "\n",
    "   This tool considers only the type of document which has maximum confidence in the predicted json files \n",
    "\n",
    "   Below screenshot shows the sample of csv file , the column names are self explanatory and match column indicates TP(true positive) if ground truth and prediction matches else it is FP(false positive)\n",
    "   \n",
    "<img src=\"./images/cdc_1.png\" width=800 height=400></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342e1ad1-f60a-4cf3-8174-80d1e64ef039",
   "metadata": {},
   "source": [
    "2. **Confusion_matrix**:\n",
    "    This gives the confusion matrix for predicted and Actual classes as sample shown below.\n",
    "<img src=\"./images/cdc_3.png\" width=800 height=400></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5230d5-81d7-4246-8b15-77be1f794d2f",
   "metadata": {},
   "source": [
    "3. **Prediction_errors.csv**:\n",
    " This CSV file is the same as **CDC_comparision.csv** but with an added filter of files where there is difference  in ground truth and prediction.\n",
    "\n",
    "<img src=\"./images/cdc_2.png\" width=800 height=400></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56d2981-a42c-4c51-a640-dd5ec584e3a4",
   "metadata": {},
   "source": [
    "4. **Error_predicted_files**:\n",
    "It is the list of files which are predicted wrong.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2c2216-4282-4433-9a74-cd503d874dad",
   "metadata": {},
   "source": [
    "### Sample Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cbd4401-e66f-400a-ba68-0836fe1d8627",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f942754-ca9a-4c85-8803-eb54596b108f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# functions needed\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from utilities import file_names, documentai_json_proto_downloader\n",
    "from google.cloud import documentai\n",
    "from typing import Any, Dict, List, Optional, Sequence, Tuple, Union\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "def max_confidence_type(\n",
    "    data: List[documentai.Document.Entity],\n",
    ") -> documentai.Document.Entity:\n",
    "    \"\"\"Get the type which has max confidence\n",
    "    Args:\n",
    "        data: List of entities.\n",
    "\n",
    "    Returns:\n",
    "        Returns the entity having max confidence.\n",
    "    \"\"\"\n",
    "\n",
    "    max_confidence = 0.0\n",
    "    max_type = \"\"\n",
    "    final_entity = \"\"\n",
    "    for item in data:\n",
    "        confidence = item.confidence\n",
    "        if confidence > max_confidence:\n",
    "            max_confidence = confidence\n",
    "            max_type = item.type\n",
    "\n",
    "    # print(\"Type with the highest confidence:\", max_type)\n",
    "    for i in data:\n",
    "        if i.type == max_type:\n",
    "            final_entity = i\n",
    "\n",
    "    return final_entity\n",
    "\n",
    "\n",
    "def get_best_match_file(parsed_file_names_list: List, file_name: str):\n",
    "    \"\"\"To get the best match file in case if there is no matching file(matching minimum 95% of file name)\n",
    "    Args:\n",
    "        parsed_file_names_list: List of Parsed json file names.\n",
    "        file_name: single ground truth file name.\n",
    "\n",
    "    Returns:\n",
    "        Returns the best match between ground truth and parsed file name.\n",
    "    \"\"\"\n",
    "    from fuzzywuzzy import fuzz\n",
    "\n",
    "    best_match = None\n",
    "    best_ratio = 90\n",
    "    for file_name_y in parsed_file_names_list:\n",
    "        ratio = fuzz.ratio(file_name, file_name_y)\n",
    "        if ratio > best_ratio:\n",
    "            best_ratio = ratio\n",
    "            best_match = file_name_y\n",
    "\n",
    "    return best_match\n",
    "\n",
    "\n",
    "def get_match_files(GT_GCS_path: str, parsed_GCS_path: str) -> dict:\n",
    "    \"\"\"Finds the matching file name between ground truth and parsed documents.\n",
    "\n",
    "    Args:\n",
    "        GT_GCS_path: Ground truth gcs path.\n",
    "        parsed_GCS_path: Parsed json gcs path.\n",
    "\n",
    "    Returns:\n",
    "        Provide the dictonary, ground truth as key and parsed files as values of the\n",
    "        ground truth files.\n",
    "    \"\"\"\n",
    "    GT_file_names_list, GT_file_dict = file_names(GT_GCS_path)\n",
    "    parsed_file_names_list, parsed_file_dict = file_names(parsed_GCS_path)\n",
    "    import difflib\n",
    "\n",
    "    GT_bucket_name = GT_GCS_path.split(\"/\")[2]\n",
    "    parsed_bucket_name = parsed_GCS_path.split(\"/\")[2]\n",
    "    matches = {}\n",
    "    for GT_file, GT_file_path in GT_file_dict.items():\n",
    "        GT_path = GT_file_path\n",
    "        parsed_file = None\n",
    "        if GT_file in parsed_file_names_list:\n",
    "            parsed_file = GT_file\n",
    "        else:\n",
    "            parsed_file = get_best_match_file(parsed_file_names_list, GT_file)\n",
    "        if parsed_file != None:\n",
    "            parsed_path = parsed_file_dict[parsed_file]\n",
    "\n",
    "            matches[GT_path] = parsed_path\n",
    "\n",
    "    return matches\n",
    "\n",
    "\n",
    "def Compare_GT_parsed(\n",
    "    GT_GCS_path: str, parsed_GCS_path: str\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, List[str]]:\n",
    "    \"\"\"Compare the parsed documents from ground truth of the docuemts.\n",
    "\n",
    "    Args:\n",
    "        GT_GCS_path: Ground truth gcs path.\n",
    "        parsed_GCS_path: Parsed json gcs path.\n",
    "\n",
    "    Returns:\n",
    "        Data frame of predicted files having error, confusion matrix\n",
    "    \"\"\"\n",
    "    matches = get_match_files(GT_GCS_path, parsed_GCS_path)\n",
    "    compare_dict = {}\n",
    "    GT_bucket_name = GT_GCS_path.split(\"/\")[2]\n",
    "    parsed_bucket_name = GT_GCS_path.split(\"/\")[2]\n",
    "    for GT_file_match, parse_file_match in matches.items():\n",
    "        GT_json = documentai_json_proto_downloader(GT_bucket_name, GT_file_match)\n",
    "        parse_json = documentai_json_proto_downloader(\n",
    "            parsed_bucket_name, parse_file_match\n",
    "        )\n",
    "        if len(GT_json.entities) > 1:\n",
    "            GT_ent = max_confidence_type(GT_json.entities)\n",
    "        else:\n",
    "            GT_ent = GT_json.entities[0]\n",
    "        if len(parse_json.entities) > 1:\n",
    "            parsed_ent = max_confidence_type(parse_json.entities)\n",
    "        else:\n",
    "            parsed_ent = parse_json.entities[0]\n",
    "\n",
    "        try:\n",
    "            GT_type = GT_ent.type\n",
    "        except:\n",
    "            GT_type = None\n",
    "        try:\n",
    "            GT_confidence = GT_ent.confidence\n",
    "        except:\n",
    "            GT_confidence = 1\n",
    "        try:\n",
    "            parsed_type = parsed_ent.type\n",
    "        except:\n",
    "            parsed_type = None\n",
    "        try:\n",
    "            parsed_confidence = parsed_ent.confidence\n",
    "        except:\n",
    "            parsed_confidence = 1\n",
    "        compare_dict[GT_file_match.split(\"/\")[-1]] = {\n",
    "            \"GT_type\": GT_type,\n",
    "            \"GT_confidence\": GT_confidence,\n",
    "            \"parsed_type\": parsed_type,\n",
    "            \"parsed_confidence\": parsed_confidence,\n",
    "        }\n",
    "\n",
    "    df = pd.DataFrame.from_dict(compare_dict, orient=\"index\")\n",
    "    df[\"match\"] = df.apply(\n",
    "        lambda row: \"TP\" if row[\"parsed_type\"] == row[\"GT_type\"] else \"FP\", axis=1\n",
    "    )\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename(columns={\"index\": \"File_name\"}, inplace=True)\n",
    "    confusion_matrix = pd.crosstab(\n",
    "        df[\"GT_type\"], df[\"parsed_type\"], rownames=[\"Actual\"], colnames=[\"Predicted\"]\n",
    "    )\n",
    "    Error_predicted_df = df[df[\"match\"] == \"FP\"]\n",
    "    Error_predicted_files = Error_predicted_df.iloc[:, 0].tolist()\n",
    "    warnings.filterwarnings(\"default\", category=UserWarning)\n",
    "    return df, confusion_matrix, Error_predicted_df, Error_predicted_files\n",
    "\n",
    "\n",
    "# calling the function\n",
    "(\n",
    "    df_allfiles,\n",
    "    confusion_matrix,\n",
    "    Error_predicted_df,\n",
    "    Error_predicted_files,\n",
    ") = Compare_GT_parsed(GT_GCS_path, parsed_GCS_path)\n",
    "\n",
    "# To Generate CSV Files\n",
    "df_allfiles.to_csv(\"CDC_comparision.csv\")\n",
    "Error_predicted_df.to_csv(\"Prediction_errors.csv\")\n",
    "print(\"Confusion Matrix : \", confusion_matrix)\n",
    "print(\"Error predicted files : \", Error_predicted_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c14e57-5fa2-4a58-a175-ef55667b7cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
