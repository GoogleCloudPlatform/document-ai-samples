{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9ad8788-654c-4e38-9d6d-42158d95c6ee",
   "metadata": {},
   "source": [
    "# CDC Document Type Entity Addition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08400e10-6639-43a6-8e2a-0b4f28ba08e2",
   "metadata": {},
   "source": [
    "* Author: docai-incubator@google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc2d117-9e2a-4ebe-8aef-94ad4f2b46b1",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This tool is not supported by the Google engineering team or product team. It is provided and supported on a best-effort basis by the **DocAI Incubator Team**. No guarantees of performance are implied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f1d1c0-823d-46d4-9a4a-e5ffa56c6b55",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "Users of Custom Document Classifier (CDC) often have document files organized by class in separate folders and just want to import those as annotated into the CDC processor dataset, rather than label them in the processor console. This document is a guide for the tool to do that. \n",
    "\n",
    "The tool will run an OCR processor over the files residing at a provided GCS input directory and save the json files to a specified output GCS directory.  Ultimately, the tool will add an entity [document] ‘type’ into each json file with the value designated by each folder, as the documents are already segregated by the folders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe00a75-567e-4f33-b4b4-aa72f9f4fe50",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "* Vertex AI Notebook\n",
    "* CDC Processor ID\n",
    "* GCS Folder Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb154c7-868b-416f-bf46-6eb23a6ed97b",
   "metadata": {},
   "source": [
    "### Organize the folders \n",
    "\n",
    "* Follow this convention of folder structure:  \n",
    "Parent Folder  <pre>\n",
    "    \t|--->  folder for doc_type1  \n",
    "    \t|---> folder for  doc_type2  \n",
    "    \t|---> folder for doc_type3  </pre>\n",
    "#### Example\n",
    "<img src=\"./images/sample.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b33b7f0-baf8-421c-b476-2d25b1ee9b02",
   "metadata": {},
   "source": [
    "# Step by Step Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadfe192-5691-4053-8670-32ef3e5bea8c",
   "metadata": {},
   "source": [
    "## 1. Import Modules/Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88d64d8-b845-4abd-a926-dc76d78b9d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-cloud-documentai -q\n",
    "!pip install google-cloud-storage -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea066b24-5e3b-4c28-a56f-263fc6e9aa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to download utilities module\n",
    "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/document-ai-samples/main/incubator-tools/best-practices/utilities/utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6495b161-ac1b-40b2-a668-fe835da2950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import documentai_v1beta3 as documentai\n",
    "from google.cloud import storage\n",
    "\n",
    "from utilities import file_names, process_document_sample, store_document_as_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1709ea5e-bf14-44ab-9963-cda14b092dc2",
   "metadata": {},
   "source": [
    "## 2. Input Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce030a1-663b-445e-a38e-24dc415942c2",
   "metadata": {},
   "source": [
    "* **PROJECT_ID**: GCP project-id\n",
    "* **LOCATION**: Provide the location of processor created (`us` or `eu`)\n",
    "* **PROCESSOR_ID**: Provide Custom Document Classifier(CDC) Processor Id\n",
    "* **GCS_INPUT_PATH**: Provide the gcs path of the parent folder where the sub-folders contain input files. Please follow the folder structure described earlier.\n",
    "* **GCS_OUTPUT_PATH**: Provide gcs path where the output json files have to be saved\n",
    "* **DOCUMENT_TYPE_DICT**:  provide the folder name and type of documents in the folder available in a dictionary format  as below ({Folder_name1:Doc_type1,Folder_name2:Doc_type2})  \n",
    "        * Example: `{Folder1:Invoice, Folder2: Bank_statements}`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eb0a45-7e29-41b4-a9e0-44dab8f55c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"xx-xx-xx\"\n",
    "LOCATION = \"us\"\n",
    "PROCESSOR_ID = \"xx-xx-xx-xx\"  # CDC Processor ID\n",
    "GCS_INPUT_PATH = \"gs://BUCKET_NAME/cdc_document_type_entity_addition/input\"\n",
    "GCS_OUTPUT_PATH = \"gs://BUCKET_NAME/cdc_document_type_entity_addition/output\"\n",
    "DOCUMENT_TYPE_DICT = {\"banks\": \"bank_statement\", \"invoice\": \"invoice\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a4b1ed-2e86-4a8e-b00a-ebf6e41921ef",
   "metadata": {},
   "source": [
    "## 3. Run Below Code-Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2afdc0-3873-4db9-a7b7-e64cf0dce00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_bucket = GCS_INPUT_PATH.split(\"/\")[2]\n",
    "splits = GCS_OUTPUT_PATH.split(\"/\")\n",
    "output_bucket = splits[2]\n",
    "OUTPUT_FOLDER_PATH = \"/\".join(splits[3:])\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(input_bucket)\n",
    "for folder_name, entity_type in DOCUMENT_TYPE_DICT.items():\n",
    "    print(f\"Process started for folder - {folder_name}\")\n",
    "    folder_path = f\"{GCS_INPUT_PATH.strip('/')}/{folder_name}\"\n",
    "    output_fp = f\"{OUTPUT_FOLDER_PATH}/{folder_name}\"\n",
    "    files_list, files_dict = file_names(folder_path)\n",
    "    entity = documentai.Document.Entity(type_=folder_name)\n",
    "    for fn, fp in files_dict.items():\n",
    "        uri = f\"gs://{input_bucket}/{fp}\"\n",
    "        print(f\"\\tProcessing {fn}\")\n",
    "        pdf_bytes = bucket.blob(fp).download_as_bytes()\n",
    "        try:\n",
    "            doc = process_document_sample(\n",
    "                PROJECT_ID, LOCATION, PROCESSOR_ID, pdf_bytes, \"\"\n",
    "            ).document\n",
    "        except Exception as e:\n",
    "            print(f\"Unable to process file {fn} - {uri} because of {e}\")\n",
    "            continue\n",
    "        doc.entities.append(entity)\n",
    "        json_data = documentai.Document.to_json(\n",
    "            doc, including_default_value_fields=False\n",
    "        )\n",
    "        fn = fn.split(\".\")[0]\n",
    "        file_name = f\"{output_fp}/{fn}.json\"\n",
    "        store_document_as_json(json_data, output_bucket, file_name)\n",
    "        print(f\"\\t\\tSaved JSON-data to gs://{output_bucket}/{file_name}\")\n",
    "    print(f\"Process Completed for all files in {folder_name} folder.\")\n",
    "print(\"Process Completed for All Folders!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759c49f9-e79e-4e81-9a46-117e6e88a639",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
