{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[colab-2] Vision API for Creative Parsing.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caseynjustus/document-ai-samples/blob/main/colabs/CM360/%5Bcolab_2%5D_Vision_API_for_Creative_Parsing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GR3Bb04j9VK"
      },
      "source": [
        "The Project focuses on analysing Ad Creatives and its related performance metrics and tries to find correlations between the elements present in the past creatives to suggest what can be good add-ons and what can help to improve performance of future creatives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwAQRI1JkAP7"
      },
      "source": [
        "Install Python Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zfY31Ie-OL0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c177b04f-d783-4ab0-cca8-2a6a339c71d4"
      },
      "source": [
        "!pip install ndjson google-cloud-vision==2.0.0 selenium google-cloud-videointelligence==2.0.0  protobuf==3.14.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ndjson\n",
            "  Downloading https://files.pythonhosted.org/packages/70/c9/04ba0056011ba96a58163ebfd666d8385300bd12da1afe661a5a147758d7/ndjson-0.3.1-py2.py3-none-any.whl\n",
            "Collecting google-cloud-vision==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/38/754771fa9add8acb4ea79367f55ace7ec5c6da4b13226fcdb736b9015a36/google_cloud_vision-2.0.0-py2.py3-none-any.whl (450kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 24.6MB/s \n",
            "\u001b[?25hCollecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 38.5MB/s \n",
            "\u001b[?25hCollecting google-cloud-videointelligence==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/75/0bd0209c9675efecb9825d66b0a0a3801f39a31921ecb43a7f3a68b1362d/google_cloud_videointelligence-2.0.0-py2.py3-none-any.whl (182kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 40.1MB/s \n",
            "\u001b[?25hCollecting protobuf==3.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/dd/5c5d156ee1c4dba470d76dac5ae57084829b4e17547f28e9f636ce3fa54b/protobuf-3.14.0-cp37-cp37m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 43.1MB/s \n",
            "\u001b[?25hCollecting proto-plus>=1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/8a/61c5a9b9b6288f9b060b6e3d88374fc083953a29aeac7206616c2d3c9c8e/proto_plus-1.18.1-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.3MB/s \n",
            "\u001b[?25hCollecting libcst>=0.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/7f/4aa1419b0ecb8a31a79fef7a79b49e6a07b977baa6c94612aeeda0228d17/libcst-0.3.18-py3-none-any.whl (512kB)\n",
            "\u001b[K     |████████████████████████████████| 522kB 42.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.22.2 in /usr/local/lib/python3.7/dist-packages (from google-cloud-vision==2.0.0) (1.26.3)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.24.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf==3.14.0) (1.15.0)\n",
            "Collecting typing-inspect>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/42/1c/66402db44184904a2f14722d317a4da0b5c8c78acfc3faf74362566635c5/typing_inspect-0.6.0-py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.2 in /usr/local/lib/python3.7/dist-packages (from libcst>=0.2.5->google-cloud-vision==2.0.0) (3.7.4.3)\n",
            "Collecting pyyaml>=5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 32.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision==2.0.0) (20.9)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision==2.0.0) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision==2.0.0) (1.28.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision==2.0.0) (2018.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision==2.0.0) (1.53.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision==2.0.0) (54.2.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.29.0; extra == \"grpc\" in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision==2.0.0) (1.32.0)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/eb/975c7c080f3223a5cdaff09612f3a5221e4ba534f7039db34c35d95fa6a5/mypy_extensions-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision==2.0.0) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision==2.0.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision==2.0.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision==2.0.0) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision==2.0.0) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision==2.0.0) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision==2.0.0) (0.4.8)\n",
            "Installing collected packages: ndjson, protobuf, proto-plus, mypy-extensions, typing-inspect, pyyaml, libcst, google-cloud-vision, selenium, google-cloud-videointelligence\n",
            "  Found existing installation: protobuf 3.12.4\n",
            "    Uninstalling protobuf-3.12.4:\n",
            "      Successfully uninstalled protobuf-3.12.4\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed google-cloud-videointelligence-2.0.0 google-cloud-vision-2.0.0 libcst-0.3.18 mypy-extensions-0.4.3 ndjson-0.3.1 proto-plus-1.18.1 protobuf-3.14.0 pyyaml-5.4.1 selenium-3.141.0 typing-inspect-0.6.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScjWChuOkRsN"
      },
      "source": [
        "Install Selenium Driver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JvtS4mqWdRs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "171be91d-412f-413f-aefc-7c1dd2e98a4d"
      },
      "source": [
        "!apt-get update && apt install chromium-chromedriver"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Connecting to security.u\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Connecting to security.u\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rGet:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r                                                                               \rGet:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [3 InRelease 14.2 kB/88.7 k\r                                                                               \rHit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [3 InRelease 14.2 kB/88.7 k\r                                                                               \rIgn:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,756 kB]\n",
            "Ign:17 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:17 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [740 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,410 kB]\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [899 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,116 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [24.5 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [395 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,182 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,546 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [31.4 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [425 kB]\n",
            "Get:27 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [39.5 kB]\n",
            "Fetched 12.9 MB in 3s (4,037 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 79 not upgraded.\n",
            "Need to get 86.6 MB of archives.\n",
            "After this operation, 300 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 90.0.4430.72-0ubuntu0.18.04.1 [1,128 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 90.0.4430.72-0ubuntu0.18.04.1 [76.9 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 90.0.4430.72-0ubuntu0.18.04.1 [3,858 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 90.0.4430.72-0ubuntu0.18.04.1 [4,743 kB]\n",
            "Fetched 86.6 MB in 4s (21.1 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 160983 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_90.0.4430.72-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (90.0.4430.72-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_90.0.4430.72-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (90.0.4430.72-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_90.0.4430.72-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (90.0.4430.72-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_90.0.4430.72-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (90.0.4430.72-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (90.0.4430.72-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (90.0.4430.72-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (90.0.4430.72-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (90.0.4430.72-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cy9P1l6cktOY"
      },
      "source": [
        "### Input1\n",
        "Paste the json key downloaded from GCP in cell below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hE-3IUY73AWX"
      },
      "source": [
        "%%writefile credentials.json\n",
        "{\n",
        "  \"type\": \"service_account\",\n",
        "  \"project_id\": \"[project_id]\",\n",
        "  \"private_key_id\": \"[private_key_id]\",\n",
        "  \"private_key\": \"[private_key]\",\n",
        "  \"client_email\": \"[client_email]\",\n",
        "  \"client_id\": \"[client_id]\",\n",
        "  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
        "  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
        "  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
        "  \"client_x509_cert_url\": \"[client_x509_cert_url]\"\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODrZlVIOlaLB"
      },
      "source": [
        "### Input2\n",
        "\n",
        "Update Values in params below to specify the input dataset for urls, output dataset and other key params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaBZcGQGUo_5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09e1c302-2a78-473f-c22b-035524531ba1"
      },
      "source": [
        "%%writefile utils.py\n",
        "\n",
        "from google.oauth2 import service_account\n",
        "from google.cloud import storage\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import vision\n",
        "from selenium import webdriver\n",
        "from google.cloud import videointelligence\n",
        "\n",
        "job_type = 'image' #@param [ \"image\", \"video\"]\n",
        "gcp_project_id =  \"oculi-v2-dev\" #@param {type:'string'}\n",
        "bq = \"bq\" #@param [ \"bq\", \"gcs\"]\n",
        "input_dataset_name=\"demoverse\" #@param {type:'string'}\n",
        "input_table_name=\"creative_urls\" #@param {type:'string'}\n",
        "gcs_bucket_name=\"demoverse\" #@param {type:'string'}\n",
        "output_dataset_name=\"demoverse\" #@param {type:'string'}\n",
        "# limit= '' #@param {type:'string'}\n",
        "\n",
        "#bq_input_schema: row_num, creative_id, file_name (should include extension), full_url, creative_pixel_size (recommended) \n",
        "def get_params():\n",
        "  params = {'job_type': job_type, #image, video\n",
        "            'gcp_project': gcp_project_id, \n",
        "            'asset_source': bq , #bq, gcs\n",
        "            'bq_input_dataset': input_dataset_name, \n",
        "            'bq_input_tablename': input_table_name, #static_urls, html_urls, animated_gif_urls, video_urls\n",
        "            'gcs_bucket_name': gcs_bucket_name, #gcs-start-images-oculi-final, bq-start-oculi-final\n",
        "            'bq_output_dataset': output_dataset_name,\n",
        "            'image_config': {\n",
        "              'image_size_included': True, # static_image_urls_superset, video_urls2 - False | else True \n",
        "              'html5': {\n",
        "                  'every_n_seconds': 1,\n",
        "                  'max_animation_time': 7, #in seconds\n",
        "                  'chromedriver_path': 'chromedriver'\n",
        "              },\n",
        "              'gif':{\n",
        "                  'every_n_frames': 'all' # / 'all' / 'default' -f & l / 5 (any number)\n",
        "              }\n",
        "             }\n",
        "            }\n",
        "  return params\n",
        "\n",
        "def validate_params():\n",
        "  global params\n",
        "  project_id = params['gcp_project']\n",
        "  input_dataset = params['bq_input_dataset']\n",
        "  input_table = params['bq_input_tablename'] \n",
        "  gcs_bucket_name = params['gcs_bucket_name']\n",
        "  output_dataset = params['bq_output_dataset']\n",
        "\n",
        "  def exists_bq_dataset(project_id, dataset_name):\n",
        "    pass\n",
        "  \n",
        "  def exists_bq_table(project_id, dataset_name, table_name):\n",
        "    pass\n",
        "\n",
        "  def exists_gcs_bucket(project_id, bucket_name):\n",
        "    pass\n",
        "\n",
        "  def create_bq_dataset(project_id, dataset_name):\n",
        "    pass\n",
        "\n",
        "  def create_gcs_bucket(project_id, bucket_name):\n",
        "    pass\n",
        "\n",
        "  if params['asset_source'] == \"bq\":\n",
        "    if not (exists_bq_dataset() and exists_bq_table()):\n",
        "      return \"Source BQ dataset and/or table not found.\"\n",
        "    if not exists_gcs_bucket():\n",
        "      pass\n",
        "      \n",
        "\n",
        "\n",
        "def get_credentials():\n",
        "  credentials = service_account.Credentials.from_service_account_file(\n",
        "    'credentials.json', scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\n",
        "  return credentials\n",
        "\n",
        "def init():\n",
        "  params = get_params()\n",
        "  credentials = get_credentials()\n",
        "  bq_client = bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
        "  storage_client = storage.Client(credentials=credentials,  project=credentials.project_id)\n",
        "  gcs_bucket = storage_client.get_bucket(params['gcs_bucket_name'])\n",
        "  return params, credentials, bq_client, storage_client, gcs_bucket\n",
        "\n",
        "def init_webdriver():\n",
        "  global params\n",
        "  chromedriver_path = params['image_config']['html5']['chromedriver_path']\n",
        "  if chromedriver_path:\n",
        "    chrome_options = webdriver.ChromeOptions()\n",
        "    chrome_options.add_argument('--headless')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "    driver = webdriver.Chrome(chromedriver_path, options=chrome_options)\n",
        "    return driver\n",
        "\n",
        "def get_image_client():\n",
        "  global credentials\n",
        "  vision_client = vision.ImageAnnotatorClient(credentials=credentials)\n",
        "  return vision_client\n",
        "\n",
        "def get_video_api_params():\n",
        "  global credentials\n",
        "  vid_client = videointelligence.VideoIntelligenceServiceClient(\n",
        "        credentials=credentials)\n",
        "  vid_config = videointelligence.SpeechTranscriptionConfig(\n",
        "        language_code=\"en-US\", enable_automatic_punctuation=True)\n",
        "  video_context = videointelligence.VideoContext(\n",
        "      speech_transcription_config=vid_config)\n",
        "  vid_features = ['LABEL_DETECTION', 'TEXT_DETECTION', 'SPEECH_TRANSCRIPTION', \n",
        "              'EXPLICIT_CONTENT_DETECTION', 'SHOT_CHANGE_DETECTION',\n",
        "              'OBJECT_TRACKING']\n",
        "  return vid_client, vid_config, video_context, vid_features\n",
        "\n",
        "params, credentials, bq_client, storage_client, gcs_bucket = init()\n",
        "vision_client = get_image_client()\n",
        "driver = init_webdriver()\n",
        "vid_client, vid_config, video_context, vid_features = get_video_api_params()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZia4xqsTlNN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91696c94-74bf-4d45-abae-494cc4982b24"
      },
      "source": [
        "%%writefile bq_helper.py\n",
        "\n",
        "from utils import params, bq_client\n",
        "from google.cloud import bigquery\n",
        "import ndjson\n",
        "\n",
        "#bq_input_schema: row_num, creative_id, file_name (should include extension), full_url, creative_pixel_size (recommended) \n",
        "def read_from_bq(sql):\n",
        "  global bq_client\n",
        "  global params\n",
        "  query_job = bq_client.query(sql)\n",
        "  results = query_job.result()\n",
        "  input_rows = []\n",
        "  job_type = params['job_type']\n",
        "  image_size_included = params['image_config']['image_size_included']\n",
        "  for r in results:\n",
        "    row = {'row_num': r.row_num,\n",
        "           'creative_id': r.creative_id,\n",
        "           'file_name': r.file_name, \n",
        "           'full_url': r.full_url}\n",
        "    if job_type == \"image\" and image_size_included:\n",
        "      row['creative_pixel_size'] = r.creative_pixel_size\n",
        "    input_rows = input_rows + [row]\n",
        "  return input_rows\n",
        "\n",
        "\n",
        "def write_to_bq(dataset_name, table_name, table_data):\n",
        "  try:\n",
        "    global params, bq_client\n",
        "    table_file_name = table_name +'.json'\n",
        "    with open(table_file_name, 'w') as f:\n",
        "      ndjson.dump(table_data, f)\n",
        "    dataset_ref = bq_client.dataset(dataset_name)\n",
        "    table_ref = dataset_ref.table(table_name)\n",
        "    job_config = bigquery.LoadJobConfig()\n",
        "    job_config.write_disposition = 'WRITE_TRUNCATE'\n",
        "    job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
        "    job_config.autodetect = True\n",
        "    with open(table_file_name, \"rb\") as source_file:\n",
        "      job = bq_client.load_table_from_file(source_file, table_ref,\n",
        "                                           job_config=job_config)\n",
        "      job.result()\n",
        "    print(f\"Loaded {table_name} table\")\n",
        "  except Exception as e:\n",
        "    print(f\"ERROR - {table_name}: {e}\")\n",
        "\n",
        "\n",
        "def filter_output(metadata, endpoint):\n",
        "  global params\n",
        "  job_type = params[\"job_type\"]\n",
        "  if job_type == \"image\":\n",
        "    frames = metadata['frames']\n",
        "    process_frame = lambda f: {'frame_id': f['frame_id'],\n",
        "                              'frame_url': f['frame_url'],\n",
        "                                endpoint: f[endpoint]}\n",
        "    frames = [process_frame(f) for f in frames]\n",
        "    filtered_output = {\n",
        "        \"row_num\": metadata['row_num'],\n",
        "        \"creative_id\": metadata[\"creative_id\"],\n",
        "        \"gcs_url\": metadata[\"gcs_url\"],\n",
        "        \"frames\": frames}\n",
        "  else:\n",
        "    filtered_output = {\n",
        "        \"creative_id\": metadata[\"creative_id\"],\n",
        "        \"creative_url\": metadata[\"creative_url\"],\n",
        "         endpoint: metadata[endpoint]}\n",
        "  return filtered_output\n",
        "\n",
        "\n",
        "def get_endpoints(job_type):\n",
        "  image_endpoints = ['image_properties_annotation', 'text_annotations',\n",
        "                     'safe_search_annotation', 'label_annotations',\n",
        "                     'logo_annotations', 'face_annotations',\n",
        "                     'localized_object_annotations']\n",
        "  video_endpoints = ['text_annotations', 'segment_label_annotations',\n",
        "                     'shot_label_annotations', 'speech_transcriptions',\n",
        "                     'explicit_annotation', 'object_annotations',\n",
        "                     'shot_annotations']\n",
        "  if job_type == \"image\":\n",
        "    return image_endpoints\n",
        "  else:\n",
        "    return video_endpoints"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting bq_helper.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbcHHUcTX5R_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48c8bfa4-ccf1-4e7a-ff16-40fb9db8c84c"
      },
      "source": [
        "%%writefile gcs_helper.py\n",
        "\n",
        "from utils import params, storage_client, gcs_bucket, driver\n",
        "import requests\n",
        "import re\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import time\n",
        "import base64\n",
        "import math\n",
        "import datetime\n",
        "from selenium import webdriver\n",
        "\n",
        "\n",
        "def upload_content(content, job_type, file_name, frame_name=None):\n",
        "  global params\n",
        "  global gcs_bucket\n",
        "  \"\"\"\n",
        "  with open(frame_name, \"wb\") as f:\n",
        "      f.write(content)\n",
        "  \"\"\"\n",
        "  frame_path = \"\"\n",
        "  if frame_name:\n",
        "    frame_path = f\"/frames/{frame_name}\"\n",
        "    file_name = file_name.replace(\".\",\"_\")\n",
        "  gcs_file_name = f\"{job_type}/{file_name}{frame_path}\"\n",
        "  blob = gcs_bucket.blob(gcs_file_name)\n",
        "  content_type = \"image/jpg\" if job_type == \"image\" else \"video/mp4\"\n",
        "  if not blob.exists():\n",
        "    blob.upload_from_string(data=content, content_type=content_type)\n",
        "  gcs_url = f\"gs://{params['gcs_bucket_name']}/{gcs_file_name}\"\n",
        "  return gcs_url\n",
        "\n",
        "\n",
        "def get_html_frames(gcs_obj, base_file_name, url, image_size=''):\n",
        "  global params\n",
        "  #global driver\n",
        "  row_num = gcs_obj['row_num']\n",
        "  frames = []\n",
        "  sleep_time = params['image_config']['html5']['every_n_seconds']\n",
        "  animation_time = params['image_config']['html5']['max_animation_time']\n",
        "  num_loops = math.ceil(animation_time/sleep_time)\n",
        "  # -------- \n",
        "  chromedriver_path = params['image_config']['html5']['chromedriver_path']\n",
        "  chrome_options = webdriver.ChromeOptions()\n",
        "  chrome_options.add_argument('--headless')\n",
        "  chrome_options.add_argument('--no-sandbox')\n",
        "  chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "  driver = webdriver.Chrome(chromedriver_path, options=chrome_options)\n",
        "  # --- \n",
        "  if image_size:\n",
        "    width, height = image_size.split('x')\n",
        "    driver.set_window_size(width, height)\n",
        "  time.sleep(1)\n",
        "  driver.get(url)\n",
        "  time.sleep(1)\n",
        "  for i in range(num_loops):\n",
        "    raw_content = driver.get_screenshot_as_base64()\n",
        "    \n",
        "    byte_str = base64.b64decode(raw_content)\n",
        "    frame_id = f\"{row_num}_f{i}\"\n",
        "    frame_fname = f\"{frame_id}.png\"\n",
        "    frame_url = upload_content(byte_str, \"image\", base_file_name,\n",
        "                               frame_fname)\n",
        "    frame = {'frame_id':frame_id, 'frame_url': frame_url}\n",
        "    frames = frames + [frame]\n",
        "    screenshot_name = f\"{frame_id}_screenshot.png\"\n",
        "    #driver.save_screenshot(screenshot_name)\n",
        "    time.sleep(sleep_time)\n",
        "  return frames\n",
        "\n",
        "\n",
        "def get_gif_frames(gcs_obj, content, base_file_name):\n",
        "  global params\n",
        "  row_num, gcs_url = gcs_obj['row_num'], gcs_obj['gcs_url']\n",
        "  frames = []\n",
        "  image = Image.open(content)\n",
        "  step = params['image_config']['gif']['every_n_frames']\n",
        "  if step == \"all\":\n",
        "    step = 1\n",
        "  elif step == \"default\":\n",
        "    step = image.n_frames-1 if image.n_frames > 1 else 1\n",
        "  for i in range(0, image.n_frames, step):\n",
        "    image.seek(i)\n",
        "    imgByteArr = BytesIO()\n",
        "    image.save(imgByteArr,'GIF')\n",
        "    byte_str = imgByteArr.getvalue()\n",
        "    frame_id = f\"{row_num}_f{i}\"\n",
        "    frame_fname = f\"{frame_id}.gif\"\n",
        "    frame_url = upload_content(byte_str, \"image\", base_file_name,\n",
        "                             frame_fname)\n",
        "    frame = {'frame_id':frame_id, 'frame_url': frame_url}\n",
        "    frames = frames + [frame]\n",
        "  return frames\n",
        "    \n",
        "#bq_input_schema: row_num, creative_id, file_name (should include extension), full_url\n",
        "def read_from_gcs():\n",
        "  global params\n",
        "  global storage_client\n",
        "  global gcs_bucket\n",
        "  blobs = storage_client.list_blobs(gcs_bucket)\n",
        "  metadata_rows = []\n",
        "  for i, blob in enumerate(blobs):\n",
        "    row_num, creative_id = i+1, i+1\n",
        "    file_name = blob.name\n",
        "    full_url = f\"https://storage.cloud.google.com/{gcs_bucket.name}/{file_name}\"\n",
        "    metadata_row = {'row_num': row_num, \n",
        "                    'creative_id': creative_id, \n",
        "                    'file_name': file_name, \n",
        "                    'full_url': full_url}\n",
        "    metadata_rows = metadata_rows + [metadata_row]\n",
        "  return metadata_rows\n",
        "  \n",
        "def generate_signed_url(file_name):\n",
        "  global gcs_bucket\n",
        "  blob = gcs_bucket.blob(file_name)\n",
        "  current_time = datetime.datetime.now()\n",
        "  in_2_mins = current_time + datetime.timedelta(seconds=120)\n",
        "  url = blob.generate_signed_url(in_2_mins)\n",
        "  return url\n",
        "\n",
        "def get_content(full_url):\n",
        "  response = requests.get(full_url, verify=False)\n",
        "  content = bytes(response.content)\n",
        "  return content\n",
        "\n",
        "def write_to_gcs(asset):\n",
        "  try:\n",
        "    creative_id, row_num = asset['creative_id'], asset['row_num']\n",
        "    file_name, full_url, image_size = asset['file_name'], asset['full_url'], ''\n",
        "    global params\n",
        "    global storage_client\n",
        "    global gcs_bucket\n",
        "    job_type, gcs_bucket_name = params['job_type'], params['gcs_bucket_name']\n",
        "    gcp_project = params['gcp_project']\n",
        "    if job_type == \"image\" and params['image_config']['image_size_included']:\n",
        "      image_size = asset['creative_pixel_size']\n",
        "    is_html5 = lambda url: True if url[-4:] == \"html\" else False\n",
        "    is_gif = lambda url: True if url[-3:] == \"gif\" else False\n",
        "    is_static = lambda url: (job_type == \"image\" and not (is_html5(url) or is_gif(url)))\n",
        "    gcs_obj = {'row_num': row_num,\n",
        "               'creative_id': creative_id}\n",
        "    file_url = full_url\n",
        "    generated_gcs_url = False\n",
        "    #----\n",
        "    if params[\"asset_source\"] == \"gcs\":\n",
        "      gcs_url = f'gs://{full_url[33:]}'\n",
        "      generated_gcs_url = True\n",
        "      if job_type == \"video\":\n",
        "        gcs_obj['gcs_url'] = gcs_url\n",
        "        return gcs_obj\n",
        "      if is_static(full_url):\n",
        "        gcs_obj['gcs_url'] = full_url\n",
        "        gcs_obj['frames'] = [{'frame_id': f\"{row_num}_f0\", 'frame_url': gcs_url}]\n",
        "        return gcs_obj\n",
        "      full_url = generate_signed_url(file_name)\n",
        "    #-----\n",
        "    if is_html5(file_url):\n",
        "      gcs_prefix = f\"https://console.cloud.google.com/storage/browser\"\n",
        "      base_gcs_url = f\"{gcs_bucket_name}/{file_name}/frames\".replace(\".\",\"_\")\n",
        "      gcs_url = f\"{gcs_prefix}/{base_gcs_url}\"\n",
        "      gcs_obj['gcs_url'] = gcs_url\n",
        "      frames = get_html_frames(gcs_obj, file_name, full_url, image_size)\n",
        "      gcs_obj['frames'] = frames\n",
        "      return gcs_obj\n",
        "\n",
        "    if is_gif(file_url):\n",
        "      content = get_content(full_url)\n",
        "      if not generated_gcs_url:\n",
        "        gcs_url = upload_content(content, job_type, file_name)\n",
        "      gcs_obj['gcs_url'] = f\"https://storage.cloud.google.com/{gcs_url[4:]}\"\n",
        "      bytesio_content = BytesIO(content)\n",
        "      gcs_obj['frames'] = get_gif_frames(gcs_obj, bytesio_content, file_name)\n",
        "      return gcs_obj\n",
        "  \n",
        "    if is_static(file_url):\n",
        "      content = get_content(full_url)\n",
        "      if not generated_gcs_url:\n",
        "        gcs_url = upload_content(content, job_type, file_name)\n",
        "      gcs_obj['gcs_url'] = f\"https://storage.cloud.google.com/{gcs_url[4:]}\"\n",
        "      gcs_obj['frames'] = [{'frame_id': f\"{row_num}_f0\",\n",
        "                            'frame_url': gcs_url}]\n",
        "      return gcs_obj\n",
        "    \n",
        "    content = get_content(full_url)\n",
        "    gcs_obj['gcs_url'] = upload_content(content, job_type, file_name)\n",
        "    print(f\"GCS Uploader: {row_num}\")\n",
        "    return gcs_obj\n",
        "  except Exception as e:\n",
        "    creative_id, row_num = asset['creative_id'], asset['row_num']\n",
        "    print(f\"ERROR - {row_num} ID: {creative_id}: {e}, GCS_Uploader\")\n",
        "    gcs_obj = {'row_num': row_num,\n",
        "               'creative_id': creative_id,\n",
        "               'error': str(e),\n",
        "               'stage': 'GCS Uploader'}\n",
        "    return gcs_obj"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting gcs_helper.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0PbO4H3at7v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2f57233-7a02-4e11-d149-b80e3da8931a"
      },
      "source": [
        "%%writefile image_helper.py\n",
        "\n",
        "from utils import vision_client\n",
        "from google.cloud import vision\n",
        "from google.protobuf.json_format import MessageToDict\n",
        "import re\n",
        "import time\n",
        "\n",
        "def get_annotation_features():\n",
        "  features = [{\"type_\": vision.Feature.Type.TEXT_DETECTION},\n",
        "              {\"type_\": vision.Feature.Type.IMAGE_PROPERTIES},\n",
        "              {\"type_\": vision.Feature.Type.SAFE_SEARCH_DETECTION},\n",
        "              {\"type_\": vision.Feature.Type.LABEL_DETECTION},\n",
        "              {\"type_\": vision.Feature.Type.LOGO_DETECTION},\n",
        "              {\"type_\": vision.Feature.Type.FACE_DETECTION},\n",
        "              {\"type_\": vision.Feature.Type.OBJECT_LOCALIZATION}]\n",
        "  return features\n",
        "\n",
        "\n",
        "def extract_image_metadata(gcs_obj):\n",
        "  try:\n",
        "    global vision_client\n",
        "    time.sleep(10)\n",
        "    frame_id, frame_url = gcs_obj['frame_id'], gcs_obj['frame_url']\n",
        "    gcs_prefix = \"https://storage.cloud.google.com\"\n",
        "    image = {\"source\": {\"image_uri\": frame_url}}\n",
        "    features = get_annotation_features()\n",
        "    response = vision_client.annotate_image({\"image\": image, \"features\": features})\n",
        "    message = MessageToDict(response._pb)\n",
        "    endpoint_resp = lambda k: message[k] if k in message else None\n",
        "    raw_text = endpoint_resp(\"textAnnotations\")\n",
        "    face_annotations = endpoint_resp(\"faceAnnotations\")\n",
        "    if raw_text:\n",
        "      if \"locale\" in raw_text[0]:\n",
        "        raw_text = raw_text[1:]\n",
        "      else:\n",
        "        for i, text in raw_text.items():\n",
        "          if \"locale\" in text:\n",
        "            del raw_text[i][\"locale\"]\n",
        "\n",
        "    if face_annotations:\n",
        "      for idx, item in enumerate(face_annotations):\n",
        "        for i, value in face_annotations[idx].items():\n",
        "          if i == \"landmarks\":\n",
        "            for k, j in enumerate(value):\n",
        "              face_annotations[idx][i][k][\"type\"] = str(j[\"type\"])\n",
        "              \n",
        "    metadata = {\n",
        "      \"frame_id\": frame_id,\n",
        "      \"frame_url\": gcs_prefix + frame_url[4:],\n",
        "      \"text_annotations\": raw_text,\n",
        "      \"image_properties_annotation\": endpoint_resp(\"imagePropertiesAnnotation\"),\n",
        "      \"safe_search_annotation\": endpoint_resp(\"safeSearchAnnotation\"),\n",
        "      \"label_annotations\": endpoint_resp(\"labelAnnotations\"),\n",
        "      \"logo_annotations\": endpoint_resp(\"logoAnnotations\"),\n",
        "      \"face_annotations\": face_annotations,\n",
        "      \"localized_object_annotations\": endpoint_resp(\"localizedObjectAnnotations\")\n",
        "    }\n",
        "    #print(\"Image Metadata: \", frame_id)\n",
        "    return metadata\n",
        "  except Exception as e:\n",
        "    frame_id = gcs_obj['frame_id']\n",
        "    row_num = re.findall(r\".+_\",str(frame_id))[0][:-1]\n",
        "    print(f\"ERROR - {row_num} ID: {frame_id}: {e}, Image Annotation\")\n",
        "    metadata = {'row_num': row_num,\n",
        "                'frame_id': frame_id,\n",
        "                'error': str(e),\n",
        "                'stage': 'Image Metadata Extraction'}\n",
        "    return metadata"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting image_helper.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZO60T-YrVGd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaf741af-f8ff-482e-ced9-7b47991a287d"
      },
      "source": [
        "%%writefile video_helper.py\n",
        "\n",
        "from google.cloud import videointelligence\n",
        "from google.protobuf.json_format import MessageToDict\n",
        "from utils import params, credentials\n",
        "from utils import vid_client, vid_config, video_context, vid_features\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "def process_text(text_annotations):\n",
        "  filter_seg = lambda x: {\"confidence\": x[\"confidence\"], \"segment\": x[\"segment\"],\n",
        "                          \"frames\": x[\"frames\"]} \n",
        "  filter_text = lambda x: {\"text\": x[\"text\"],\n",
        "                           \"segments\": [filter_seg(s) for s in x[\"segments\"]]}\n",
        "  processed = [filter_text(text) for text in text_annotations]\n",
        "  return processed\n",
        "\n",
        "def process_labels(label_annotations):\n",
        "  filter_label = lambda x: {'entity':x['entity'],'segments':x['segments']}\n",
        "  processed = [filter_label(label) for label in label_annotations]\n",
        "  return processed\n",
        "\n",
        "def process_speech(speech):\n",
        "  filter_speech = lambda x: {\"alternatives\": x[\"alternatives\"]}\n",
        "  processed = [filter_speech(x) for x in speech if x[\"alternatives\"] != [{}] ]\n",
        "  return processed\n",
        "\n",
        "def split_metadata(all_metadata):\n",
        "  annotations, tran = all_metadata # tran for transcriptions\n",
        "  get_one_metadata = lambda x: annotations[x] if x in annotations else []\n",
        "  filtering_endpoints = [\"explicitAnnotation\", \"objectAnnotations\",\n",
        "                         \"textAnnotations\", \"segmentLabelAnnotations\",\n",
        "                         \"shotLabelAnnotations\", \"shotAnnotations\"] \n",
        "  values = [get_one_metadata(e) for e in filtering_endpoints]\n",
        "  speech = tran['speechTranscriptions'] if 'speechTranscriptions' in tran else []\n",
        "  values.append(speech)\n",
        "  return values\n",
        "\n",
        "def extract_video_metadata(gcs_obj):\n",
        "  try:\n",
        "    time.sleep(10)\n",
        "    gcs_prefix = \"https://storage.cloud.google.com\"\n",
        "    creative_id, creative_url = gcs_obj['creative_id'], gcs_obj['gcs_url']\n",
        "    row_num = gcs_obj['row_num']\n",
        "    global vid_client, vid_config, video_context, vid_features\n",
        "    job = vid_client.annotate_video(input_uri=creative_url,\n",
        "                                    features=vid_features,\n",
        "                                    video_context=video_context)\n",
        "    result = job.result()\n",
        "    message = MessageToDict(result)\n",
        "    all_metadata = message['annotationResults']\n",
        "    if len(all_metadata) != 2:\n",
        "      print(\"ERROR: \", row_num, \"Invalid format.\", \"Video Analysis\")\n",
        "      metadata = {'row_num': row_num,\n",
        "                  'creative_id': creative_id,\n",
        "                  'error': \"Invalid format.\",\n",
        "                  'stage': 'Video Metadata Extraction'}\n",
        "      return metadata\n",
        "    meta_values = split_metadata(all_metadata)\n",
        "    explicit, objects, text, segment_labels = meta_values[:4]\n",
        "    shot_labels, shots, speech = meta_values[4:]\n",
        "    row = {'row_num': row_num,\n",
        "           'creative_id': creative_id,\n",
        "           'creative_url': gcs_prefix + creative_url[4:],\n",
        "           'text_annotations': process_text(text),\n",
        "           'segment_label_annotations': process_labels(segment_labels),\n",
        "           'shot_label_annotations': process_labels(shot_labels),\n",
        "           'speech_transcriptions':process_speech(speech),\n",
        "           'explicit_annotation': explicit,\n",
        "           'object_annotations': objects,\n",
        "           'shot_annotations': shots}\n",
        "    \n",
        "    print(\"Finished Video Analysis: \", row_num)\n",
        "    return row\n",
        "  except Exception as e:\n",
        "    row_num, creative_id = gcs_obj['row_num'], gcs_obj['creative_id']\n",
        "    print(\"ERROR: \", row_num, e, \"Video Analysis\")\n",
        "    metadata = {'row_num': row_num,\n",
        "                'creative_id': creative_id, \n",
        "                'error': str(e),\n",
        "                'stage': 'Video Metadata Extraction'}\n",
        "    return metadata"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting video_helper.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBjwai2O1Idk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad5e17b6-3921-4007-de0e-a5b11a502c6c"
      },
      "source": [
        "%%writefile main.py\n",
        "import requests\n",
        "import warnings\n",
        "import time\n",
        "import os\n",
        "import ndjson\n",
        "from retrying import retry\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from google.cloud import storage\n",
        "from google.cloud import vision\n",
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account\n",
        "from google.protobuf.json_format import MessageToDict\n",
        "from utils import params\n",
        "import bq_helper\n",
        "import gcs_helper\n",
        "import image_helper\n",
        "import video_helper\n",
        "import re\n",
        "\n",
        "\n",
        "warnings.simplefilter(action=\"ignore\")\n",
        "\n",
        "def has_noretry_err(err_message):\n",
        "  error_keyphrases = ['404 ', 'Invalid format']\n",
        "  for e in error_keyphrases:\n",
        "    if re.search(e, err_message):\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "\n",
        "def upload_and_annotate(asset):\n",
        "  global params\n",
        "  job_type = params['job_type']\n",
        "  gcs_obj = gcs_helper.write_to_gcs(asset)\n",
        "  if 'error' in gcs_obj:\n",
        "    return gcs_obj\n",
        "  if job_type == \"image\":\n",
        "    frames = gcs_obj[\"frames\"]\n",
        "    frames_meta = [image_helper.extract_image_metadata(f) for f in frames]\n",
        "    for f in frames_meta:\n",
        "      if 'error' in f:\n",
        "        error = {'row_num': gcs_obj[\"row_num\"],\n",
        "                'creative_id': gcs_obj[\"creative_id\"],\n",
        "                'error': f['error'],\n",
        "                'stage': 'Image Metadata'}\n",
        "        return error\n",
        "    metadata = {'row_num': gcs_obj[\"row_num\"],\n",
        "                'creative_id': gcs_obj[\"creative_id\"],\n",
        "                'gcs_url': gcs_obj[\"gcs_url\"],\n",
        "                'frames': frames_meta}\n",
        "  else:\n",
        "    metadata = video_helper.extract_video_metadata(gcs_obj)\n",
        "  return metadata\n",
        "\n",
        "def get_read_query():\n",
        "  project = params['gcp_project']\n",
        "  input_dataset = params['bq_input_dataset'] \n",
        "  input_table = params['bq_input_tablename']\n",
        "  sql = f\"SELECT * FROM `{project}.{input_dataset}.{input_table}`\"\n",
        "  return sql\n",
        "\n",
        "def print_summary_stats(i, n_valid, n_errors, n_retry):\n",
        "  print(f\"After try {i},\")\n",
        "  print(f\"Valid results: {n_valid}\")\n",
        "  print(f\"No. of all errors: {n_errors}\")\n",
        "  print(f\"No. of retriable errors: {n_retry}\")\n",
        "\n",
        "def main():\n",
        "  global params\n",
        "  job_type, asset_source = params['job_type'], params['asset_source']\n",
        "  if asset_source == \"gcs\": \n",
        "    input_data = gcs_helper.read_from_gcs()\n",
        "    input_dataset = params['bq_input_dataset'] \n",
        "    input_table = params['bq_input_tablename']\n",
        "    bq_helper.write_to_bq(input_dataset, input_table, input_data)\n",
        "  base_sql = get_read_query()\n",
        "  sql = base_sql + f\" LIMIT {params['limit']}\" if 'limit' in params else base_sql\n",
        "  max_workers = 300 if job_type == \"image\" else 15\n",
        "  NUM_RETRIES = 3 if job_type == \"image\" else 10\n",
        "  executor =  ThreadPoolExecutor(max_workers=max_workers)\n",
        "  valid_res, noretry_error_ids = [], []\n",
        "  for i in range(1, NUM_RETRIES+1):\n",
        "    asset_urls = bq_helper.read_from_bq(sql)\n",
        "    results = list(executor.map(upload_and_annotate, asset_urls))\n",
        "    #results = list(map(upload_and_annotate, asset_urls))\n",
        "    valid_res = valid_res + [result for result in results if 'error' not in result]\n",
        "    err_rows = [result for result in results if 'error' in result]\n",
        "    all_nums = [r['row_num'] for r in err_rows]\n",
        "    noretry = [r['row_num'] for r in err_rows if has_noretry_err(r['error'])]\n",
        "    noretry_error_ids = noretry_error_ids + noretry\n",
        "    retry_nums = list(set(all_nums) - set(noretry))\n",
        "    n_valid, n_errors, n_retry = len(valid_res), len(err_rows), len(retry_nums)\n",
        "    print_summary_stats(i, n_valid, n_errors, n_retry)\n",
        "    if not retry_nums:\n",
        "      break\n",
        "    sql = base_sql+\" WHERE row_num IN \"+str(tuple(retry_nums))\n",
        "    sql = sql[:-2]+\")\" if sql[-2:] == \",)\" else sql\n",
        "    time.sleep(60)\n",
        "  print(f\"Total no. of non-retriable errors: {len(noretry_error_ids)}\")\n",
        "  endpoints = bq_helper.get_endpoints(job_type)\n",
        "  output_dataset = params['bq_output_dataset'] \n",
        "  for endpoint in endpoints:\n",
        "    filtered = [bq_helper.filter_output(v, endpoint) for v in valid_res]\n",
        "    executor.submit(bq_helper.write_to_bq, output_dataset, endpoint, filtered)\n",
        "  executor.shutdown()\n",
        "\n",
        "\n",
        "#time.sleep(60)\n",
        "start = time.time()\n",
        "main()\n",
        "end = time.time()\n",
        "print(f\"The pipeline took {end-start} seconds.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting main.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "turcF3MQm1l-"
      },
      "source": [
        "### Run below cell to start the job and get creative data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YQsIH_29_tL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a18fa7bd-8ca8-495b-f020-e8c42fd4f0d6"
      },
      "source": [
        "!python3 main.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After try 1,\n",
            "Valid results: 52\n",
            "No. of all errors: 0\n",
            "No. of retriable errors: 0\n",
            "Total no. of non-retriable errors: 0\n",
            "Loaded localized_object_annotations table\n",
            "Loaded face_annotations table\n",
            "Loaded label_annotations table\n",
            "Loaded safe_search_annotation table\n",
            "Loaded image_properties_annotation table\n",
            "Loaded text_annotations table\n",
            "Loaded logo_annotations table\n",
            "The pipeline took 1380.2823300361633 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}